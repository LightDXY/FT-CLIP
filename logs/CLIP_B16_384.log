/home/jianbao/.local/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
sh: 1: azcopy: not found
sh: 1: azcopy: not found
sh: 1: azcopy: not found
sh: 1: azcopy: not found
sh: 1: azcopy: not found
sh: 1: azcopy: not found
sh: 1: azcopy: not found
sh: 1: azcopy: not found
**********find WORLD_SIZE 8 in env**********
**********find WORLD_SIZE 8 in env**********
**********find WORLD_SIZE 8 in env**********
**********find WORLD_SIZE 8 in env**********
**********find WORLD_SIZE 8 in env**********
**********find WORLD_SIZE 8 in env**********
**********find WORLD_SIZE 8 in env**********
**********find WORLD_SIZE 8 in env**********
************World_size is 8, current rank 0 ***********, local rank 0
************World_size is 8, current rank 5 ***********, local rank 5
************World_size is 8, current rank 6 ***********, local rank 6
************World_size is 8, current rank 7 ***********, local rank 7
************World_size is 8, current rank 4 ***********, local rank 4
************World_size is 8, current rank 3 ***********, local rank 3
************World_size is 8, current rank 1 ***********, local rank 1
************World_size is 8, current rank 2 ***********, local rank 2
Namespace(aa='rand-m9-mstd0.5-inc1', abs_pos_emb=True, attn_drop_rate=0.0, auto_resume=True, backbone_decay=1.0, batch_size=64, bce_loss=False, clip_grad=None, clip_mean_and_std=True, color_jitter=0.4, crop_pct=None, crop_scale=0.08, cutmix=0.0, cutmix_minmax=None, data='imagenet', data_path='/tmp/DATASET/imagenet', data_set='IMNET', deepscale=False, deepscale_config=None, deepspeed=False, deepspeed_config=None, deepspeed_mpi=False, device='cuda:0', disable_eval_during_finetuning=False, disable_weight_decay_on_rel_pos_bias=False, dist_eval=True, dist_on_itp=False, dist_url='env://', distributed=True, drop=0.0, drop_path=0.0, enable_deepspeed=True, epochs=50, eval=False, eval_all=True, eval_data_path=None, finetune='OUTPUT/SLIP/declip_model/clip_vitb16.pth', freeze_layers=0, imagenet_default_mean_and_std=False, init_scale=0.001, input_size=384, layer_decay=0.6, layer_scale_init_value=0.0, local_rank=0, log_dir=None, lr=0.0007, min_lr=1e-06, mixup=0.0, mixup_mode='batch', mixup_prob=1.0, mixup_switch_prob=0.5, model='declip_B16_384', model_ema=True, model_ema_decay=0.9998, model_ema_force_cpu=False, model_key='state', model_prefix='visual.', momentum=0.9, nb_classes=1000, num_gpu=1, num_workers=8, opt='adamw', opt_betas=None, opt_eps=1e-08, output_dir='OUTPUT/SLIP_/maskclip_ft/CLIP_openai_P16_384/FT50_384_7E4_D06_EMA98_DPR00', pin_mem=True, rank=0, recount=1, rel_pos_bias=False, remode='pixel', reprob=0.25, resplit=False, resume='', resume_tag='best', save_ckpt=False, save_ckpt_freq=5, seed=0, smoothing=0.1, src=False, start_epoch=0, three_aug=False, train_interpolation='bicubic', train_set='zip', update_freq=4, use_mean_pooling=True, warmup_epochs=10, warmup_lr=1e-06, warmup_steps=-1, weight_decay=0.05, weight_decay_end=None, world_size=8)
use crop scale (0.08, 1)
Transform = 
RandomResizedCropAndInterpolation(size=(384, 384), scale=(0.08, 1), ratio=(0.75, 1.3333), interpolation=PIL.Image.BICUBIC)
RandomHorizontalFlip(p=0.5)
<timm.data.auto_augment.RandAugment object at 0x7f3315fdc460>
ToTensor()
Normalize(mean=tensor([0.4815, 0.4578, 0.4082]), std=tensor([0.2686, 0.2613, 0.2758]))
<timm.data.random_erasing.RandomErasing object at 0x7f3315fdc7c0>
---------------------------
Only ImageNet-1K, length 1281167
USE ZIP DATALOADER
Number of the class = 1000
Set crop pct to 1 for clip task
Transform = 
Resize(size=384, interpolation=bicubic, max_size=None, antialias=None)
CenterCrop(size=(384, 384))
ToTensor()
Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------------------------
Only ImageNet-1K, length 1281167
Only ImageNet-1K, length 1281167
Only ImageNet-1K, length 1281167
Only ImageNet-1K, length 1281167
Only ImageNet-1K, length 1281167
Only ImageNet-1K, length 1281167
Only ImageNet-1K, length 1281167
Only ImageNet-1K, length 50000
USE ZIP DATALOADER
Number of the class = 1000
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f3315fd0bb0>
No Layer Scale
{'num_classes': 1000, 'drop_rate': 0.0, 'drop_path_rate': 0.0, 'attn_drop_rate': 0.0, 'use_mean_pooling': True, 'init_scale': 0.001, 'use_rel_pos_bias': False, 'use_abs_pos_emb': True}
drop_path_rate: 0.0
layer_scale: False
freeze_layers: 0
Using DPR [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Only ImageNet-1K, length 50000
Only ImageNet-1K, length 50000
Only ImageNet-1K, length 50000
Only ImageNet-1K, length 50000
Only ImageNet-1K, length 50000
Only ImageNet-1K, length 50000
Only ImageNet-1K, length 50000
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Patch size = (16, 16)
Load ckpt from OUTPUT/SLIP/declip_model/clip_vitb16.pth
torch.Size([197, 768])
Position interpolate from 14x14 to 24x24
Weights of VisualTransformer not initialized from pretrained model: ['visual.patch_embed.proj.weight', 'visual.patch_embed.proj.bias', 'visual.fc_norm.weight', 'visual.fc_norm.bias', 'visual.head.weight', 'visual.head.bias']
Weights from pretrained model not used in VisualTransformer: ['visual.proj', 'visual.ln_post.weight', 'visual.ln_post.bias']
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Using EMA with decay = 0.99980000
Model = VisualTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
  )
  (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
  (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (transformer): Transformer(
    (dropout): Dropout(p=0, inplace=False)
    (resblocks): Sequential(
      (0): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (2): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (3): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (4): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (5): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (6): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (7): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (8): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (9): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (10): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (11): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
    )
  )
  (fc_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
number of params: 86860264
LR = 0.00070000
Batch size = 2048
Update frequent = 4
Number of training examples = 1281167
Number of training training per epoch = 625
Assigned values = [0.0013060694015999993, 0.002176782335999999, 0.0036279705599999985, 0.006046617599999997, 0.010077695999999997, 0.016796159999999994, 0.027993599999999993, 0.04665599999999999, 0.07775999999999998, 0.1296, 0.21599999999999997, 0.36, 0.6, 1.0]
Param groups = {
  "layer_0_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "class_embedding",
      "positional_embedding",
      "ln_pre.weight",
      "ln_pre.bias"
    ],
    "lr_scale": 0.0013060694015999993
  },
  "layer_0_decay": {
    "weight_decay": 0.05,
    "params": [
      "conv1.weight"
    ],
    "lr_scale": 0.0013060694015999993
  },
  "layer_1_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.0.attn.in_proj_weight",
      "transformer.resblocks.0.attn.out_proj.weight",
      "transformer.resblocks.0.mlp.c_fc.weight",
      "transformer.resblocks.0.mlp.c_proj.weight"
    ],
    "lr_scale": 0.002176782335999999
  },
  "layer_1_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.0.attn.in_proj_bias",
      "transformer.resblocks.0.attn.out_proj.bias",
      "transformer.resblocks.0.ln_1.weight",
      "transformer.resblocks.0.ln_1.bias",
      "transformer.resblocks.0.mlp.c_fc.bias",
      "transformer.resblocks.0.mlp.c_proj.bias",
      "transformer.resblocks.0.ln_2.weight",
      "transformer.resblocks.0.ln_2.bias"
    ],
    "lr_scale": 0.002176782335999999
  },
  "layer_2_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.1.attn.in_proj_weight",
      "transformer.resblocks.1.attn.out_proj.weight",
      "transformer.resblocks.1.mlp.c_fc.weight",
      "transformer.resblocks.1.mlp.c_proj.weight"
    ],
    "lr_scale": 0.0036279705599999985
  },
  "layer_2_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.1.attn.in_proj_bias",
      "transformer.resblocks.1.attn.out_proj.bias",
      "transformer.resblocks.1.ln_1.weight",
      "transformer.resblocks.1.ln_1.bias",
      "transformer.resblocks.1.mlp.c_fc.bias",
      "transformer.resblocks.1.mlp.c_proj.bias",
      "transformer.resblocks.1.ln_2.weight",
      "transformer.resblocks.1.ln_2.bias"
    ],
    "lr_scale": 0.0036279705599999985
  },
  "layer_3_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.2.attn.in_proj_weight",
      "transformer.resblocks.2.attn.out_proj.weight",
      "transformer.resblocks.2.mlp.c_fc.weight",
      "transformer.resblocks.2.mlp.c_proj.weight"
    ],
    "lr_scale": 0.006046617599999997
  },
  "layer_3_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.2.attn.in_proj_bias",
      "transformer.resblocks.2.attn.out_proj.bias",
      "transformer.resblocks.2.ln_1.weight",
      "transformer.resblocks.2.ln_1.bias",
      "transformer.resblocks.2.mlp.c_fc.bias",
      "transformer.resblocks.2.mlp.c_proj.bias",
      "transformer.resblocks.2.ln_2.weight",
      "transformer.resblocks.2.ln_2.bias"
    ],
    "lr_scale": 0.006046617599999997
  },
  "layer_4_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.3.attn.in_proj_weight",
      "transformer.resblocks.3.attn.out_proj.weight",
      "transformer.resblocks.3.mlp.c_fc.weight",
      "transformer.resblocks.3.mlp.c_proj.weight"
    ],
    "lr_scale": 0.010077695999999997
  },
  "layer_4_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.3.attn.in_proj_bias",
      "transformer.resblocks.3.attn.out_proj.bias",
      "transformer.resblocks.3.ln_1.weight",
      "transformer.resblocks.3.ln_1.bias",
      "transformer.resblocks.3.mlp.c_fc.bias",
      "transformer.resblocks.3.mlp.c_proj.bias",
      "transformer.resblocks.3.ln_2.weight",
      "transformer.resblocks.3.ln_2.bias"
    ],
    "lr_scale": 0.010077695999999997
  },
  "layer_5_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.4.attn.in_proj_weight",
      "transformer.resblocks.4.attn.out_proj.weight",
      "transformer.resblocks.4.mlp.c_fc.weight",
      "transformer.resblocks.4.mlp.c_proj.weight"
    ],
    "lr_scale": 0.016796159999999994
  },
  "layer_5_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.4.attn.in_proj_bias",
      "transformer.resblocks.4.attn.out_proj.bias",
      "transformer.resblocks.4.ln_1.weight",
      "transformer.resblocks.4.ln_1.bias",
      "transformer.resblocks.4.mlp.c_fc.bias",
      "transformer.resblocks.4.mlp.c_proj.bias",
      "transformer.resblocks.4.ln_2.weight",
      "transformer.resblocks.4.ln_2.bias"
    ],
    "lr_scale": 0.016796159999999994
  },
  "layer_6_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.5.attn.in_proj_weight",
      "transformer.resblocks.5.attn.out_proj.weight",
      "transformer.resblocks.5.mlp.c_fc.weight",
      "transformer.resblocks.5.mlp.c_proj.weight"
    ],
    "lr_scale": 0.027993599999999993
  },
  "layer_6_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.5.attn.in_proj_bias",
      "transformer.resblocks.5.attn.out_proj.bias",
      "transformer.resblocks.5.ln_1.weight",
      "transformer.resblocks.5.ln_1.bias",
      "transformer.resblocks.5.mlp.c_fc.bias",
      "transformer.resblocks.5.mlp.c_proj.bias",
      "transformer.resblocks.5.ln_2.weight",
      "transformer.resblocks.5.ln_2.bias"
    ],
    "lr_scale": 0.027993599999999993
  },
  "layer_7_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.6.attn.in_proj_weight",
      "transformer.resblocks.6.attn.out_proj.weight",
      "transformer.resblocks.6.mlp.c_fc.weight",
      "transformer.resblocks.6.mlp.c_proj.weight"
    ],
    "lr_scale": 0.04665599999999999
  },
  "layer_7_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.6.attn.in_proj_bias",
      "transformer.resblocks.6.attn.out_proj.bias",
      "transformer.resblocks.6.ln_1.weight",
      "transformer.resblocks.6.ln_1.bias",
      "transformer.resblocks.6.mlp.c_fc.bias",
      "transformer.resblocks.6.mlp.c_proj.bias",
      "transformer.resblocks.6.ln_2.weight",
      "transformer.resblocks.6.ln_2.bias"
    ],
    "lr_scale": 0.04665599999999999
  },
  "layer_8_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.7.attn.in_proj_weight",
      "transformer.resblocks.7.attn.out_proj.weight",
      "transformer.resblocks.7.mlp.c_fc.weight",
      "transformer.resblocks.7.mlp.c_proj.weight"
    ],
    "lr_scale": 0.07775999999999998
  },
  "layer_8_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.7.attn.in_proj_bias",
      "transformer.resblocks.7.attn.out_proj.bias",
      "transformer.resblocks.7.ln_1.weight",
      "transformer.resblocks.7.ln_1.bias",
      "transformer.resblocks.7.mlp.c_fc.bias",
      "transformer.resblocks.7.mlp.c_proj.bias",
      "transformer.resblocks.7.ln_2.weight",
      "transformer.resblocks.7.ln_2.bias"
    ],
    "lr_scale": 0.07775999999999998
  },
  "layer_9_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.8.attn.in_proj_weight",
      "transformer.resblocks.8.attn.out_proj.weight",
      "transformer.resblocks.8.mlp.c_fc.weight",
      "transformer.resblocks.8.mlp.c_proj.weight"
    ],
    "lr_scale": 0.1296
  },
  "layer_9_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.8.attn.in_proj_bias",
      "transformer.resblocks.8.attn.out_proj.bias",
      "transformer.resblocks.8.ln_1.weight",
      "transformer.resblocks.8.ln_1.bias",
      "transformer.resblocks.8.mlp.c_fc.bias",
      "transformer.resblocks.8.mlp.c_proj.bias",
      "transformer.resblocks.8.ln_2.weight",
      "transformer.resblocks.8.ln_2.bias"
    ],
    "lr_scale": 0.1296
  },
  "layer_10_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.9.attn.in_proj_weight",
      "transformer.resblocks.9.attn.out_proj.weight",
      "transformer.resblocks.9.mlp.c_fc.weight",
      "transformer.resblocks.9.mlp.c_proj.weight"
    ],
    "lr_scale": 0.21599999999999997
  },
  "layer_10_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.9.attn.in_proj_bias",
      "transformer.resblocks.9.attn.out_proj.bias",
      "transformer.resblocks.9.ln_1.weight",
      "transformer.resblocks.9.ln_1.bias",
      "transformer.resblocks.9.mlp.c_fc.bias",
      "transformer.resblocks.9.mlp.c_proj.bias",
      "transformer.resblocks.9.ln_2.weight",
      "transformer.resblocks.9.ln_2.bias"
    ],
    "lr_scale": 0.21599999999999997
  },
  "layer_11_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.10.attn.in_proj_weight",
      "transformer.resblocks.10.attn.out_proj.weight",
      "transformer.resblocks.10.mlp.c_fc.weight",
      "transformer.resblocks.10.mlp.c_proj.weight"
    ],
    "lr_scale": 0.36
  },
  "layer_11_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.10.attn.in_proj_bias",
      "transformer.resblocks.10.attn.out_proj.bias",
      "transformer.resblocks.10.ln_1.weight",
      "transformer.resblocks.10.ln_1.bias",
      "transformer.resblocks.10.mlp.c_fc.bias",
      "transformer.resblocks.10.mlp.c_proj.bias",
      "transformer.resblocks.10.ln_2.weight",
      "transformer.resblocks.10.ln_2.bias"
    ],
    "lr_scale": 0.36
  },
  "layer_12_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.11.attn.in_proj_weight",
      "transformer.resblocks.11.attn.out_proj.weight",
      "transformer.resblocks.11.mlp.c_fc.weight",
      "transformer.resblocks.11.mlp.c_proj.weight"
    ],
    "lr_scale": 0.6
  },
  "layer_12_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.11.attn.in_proj_bias",
      "transformer.resblocks.11.attn.out_proj.bias",
      "transformer.resblocks.11.ln_1.weight",
      "transformer.resblocks.11.ln_1.bias",
      "transformer.resblocks.11.mlp.c_fc.bias",
      "transformer.resblocks.11.mlp.c_proj.bias",
      "transformer.resblocks.11.ln_2.weight",
      "transformer.resblocks.11.ln_2.bias"
    ],
    "lr_scale": 0.6
  },
  "layer_13_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "fc_norm.weight",
      "fc_norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  },
  "layer_13_decay": {
    "weight_decay": 0.05,
    "params": [
      "head.weight"
    ],
    "lr_scale": 1.0
  }
}
Installed CUDA version 11.0 does not match the version torch was compiled with 11.3 but since the APIs are compatible, accepting this combination
Using /home/jianbao/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.906200647354126 seconds
Using /home/jianbao/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.8051795959472656 seconds
model.gradient_accumulation_steps() = 4
Use step level LR scheduler!
Set warmup steps = 6250
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
criterion = LabelSmoothingCrossEntropy()
latest_ckpt: -1
Start training for 50 epochs
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Epoch: [0]  [   0/2502]  eta: 1 day, 5:47:43  lr: 0.000000  min_lr: 0.000000  loss: 6.9062 (6.9062)  class_acc: 0.0000 (0.0000)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 42.8710  data: 40.1319  max mem: 27059
Epoch: [0]  [ 100/2502]  eta: 0:48:23  lr: 0.000003  min_lr: 0.000000  loss: 6.9062 (6.9062)  class_acc: 0.0156 (0.0051)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7821  data: 0.0003  max mem: 27222
Epoch: [0]  [ 200/2502]  eta: 0:38:13  lr: 0.000006  min_lr: 0.000000  loss: 6.9023 (6.9056)  class_acc: 0.0312 (0.0204)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7798  data: 0.0002  max mem: 27222
Epoch: [0]  [ 300/2502]  eta: 0:33:58  lr: 0.000008  min_lr: 0.000000  loss: 6.8945 (6.9037)  class_acc: 0.0625 (0.0277)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7840  data: 0.0002  max mem: 27222
Epoch: [0]  [ 400/2502]  eta: 0:31:11  lr: 0.000011  min_lr: 0.000000  loss: 6.8594 (6.8973)  class_acc: 0.1094 (0.0450)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7832  data: 0.0003  max mem: 27222
Epoch: [0]  [ 500/2502]  eta: 0:29:00  lr: 0.000014  min_lr: 0.000000  loss: 6.7148 (6.8737)  class_acc: 0.0625 (0.0514)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7905  data: 0.0002  max mem: 27222
Epoch: [0]  [ 600/2502]  eta: 0:27:10  lr: 0.000017  min_lr: 0.000000  loss: 6.5391 (6.8300)  class_acc: 0.0312 (0.0502)  loss_scale: 256.0000 (146.3161)  weight_decay: 0.0500 (0.0500)  time: 0.7945  data: 0.0002  max mem: 27222
Epoch: [0]  [ 700/2502]  eta: 0:25:28  lr: 0.000020  min_lr: 0.000000  loss: 6.3828 (6.7747)  class_acc: 0.0312 (0.0486)  loss_scale: 256.0000 (161.9629)  weight_decay: 0.0500 (0.0500)  time: 0.7899  data: 0.0003  max mem: 27222
Epoch: [0]  [ 800/2502]  eta: 0:23:52  lr: 0.000022  min_lr: 0.000000  loss: 6.2148 (6.7131)  class_acc: 0.0312 (0.0468)  loss_scale: 256.0000 (173.7029)  weight_decay: 0.0500 (0.0500)  time: 0.7889  data: 0.0002  max mem: 27222
Epoch: [0]  [ 900/2502]  eta: 0:22:20  lr: 0.000025  min_lr: 0.000000  loss: 6.0820 (6.6498)  class_acc: 0.0312 (0.0454)  loss_scale: 256.0000 (182.8368)  weight_decay: 0.0500 (0.0500)  time: 0.8118  data: 0.0002  max mem: 27222
Epoch: [0]  [1000/2502]  eta: 0:20:50  lr: 0.000028  min_lr: 0.000000  loss: 5.9453 (6.5840)  class_acc: 0.0469 (0.0451)  loss_scale: 256.0000 (190.1459)  weight_decay: 0.0500 (0.0500)  time: 0.8118  data: 0.0003  max mem: 27222
Epoch: [0]  [1100/2502]  eta: 0:19:22  lr: 0.000031  min_lr: 0.000000  loss: 5.7695 (6.5158)  class_acc: 0.0469 (0.0453)  loss_scale: 512.0000 (213.3333)  weight_decay: 0.0500 (0.0500)  time: 0.7880  data: 0.0002  max mem: 27222
Epoch: [0]  [1200/2502]  eta: 0:17:55  lr: 0.000034  min_lr: 0.000000  loss: 5.6016 (6.4464)  class_acc: 0.0625 (0.0459)  loss_scale: 512.0000 (238.2015)  weight_decay: 0.0500 (0.0500)  time: 0.7914  data: 0.0002  max mem: 27222
Epoch: [0]  [1300/2502]  eta: 0:16:30  lr: 0.000036  min_lr: 0.000000  loss: 5.4336 (6.3752)  class_acc: 0.0625 (0.0474)  loss_scale: 512.0000 (259.2467)  weight_decay: 0.0500 (0.0500)  time: 0.7938  data: 0.0002  max mem: 27222
Epoch: [0]  [1400/2502]  eta: 0:15:06  lr: 0.000039  min_lr: 0.000000  loss: 5.3086 (6.3056)  class_acc: 0.0781 (0.0494)  loss_scale: 512.0000 (277.2877)  weight_decay: 0.0500 (0.0500)  time: 0.8078  data: 0.0003  max mem: 27222
Epoch: [0]  [1500/2502]  eta: 0:13:42  lr: 0.000042  min_lr: 0.000000  loss: 5.2188 (6.2347)  class_acc: 0.1094 (0.0524)  loss_scale: 512.0000 (292.9247)  weight_decay: 0.0500 (0.0500)  time: 0.7940  data: 0.0003  max mem: 27222
Epoch: [0]  [1600/2502]  eta: 0:12:19  lr: 0.000045  min_lr: 0.000000  loss: 5.0078 (6.1637)  class_acc: 0.1250 (0.0566)  loss_scale: 1024.0000 (326.4360)  weight_decay: 0.0500 (0.0500)  time: 0.7984  data: 0.0003  max mem: 27222
Epoch: [0]  [1700/2502]  eta: 0:10:56  lr: 0.000048  min_lr: 0.000000  loss: 4.8789 (6.0929)  class_acc: 0.1406 (0.0611)  loss_scale: 1024.0000 (367.4450)  weight_decay: 0.0500 (0.0500)  time: 0.8138  data: 0.0002  max mem: 27222
Epoch: [0]  [1800/2502]  eta: 0:09:34  lr: 0.000050  min_lr: 0.000000  loss: 4.7852 (6.0224)  class_acc: 0.1406 (0.0660)  loss_scale: 1024.0000 (403.9001)  weight_decay: 0.0500 (0.0500)  time: 0.8162  data: 0.0010  max mem: 27222
Epoch: [0]  [1900/2502]  eta: 0:08:11  lr: 0.000053  min_lr: 0.000000  loss: 4.6094 (5.9507)  class_acc: 0.2031 (0.0730)  loss_scale: 1024.0000 (436.5197)  weight_decay: 0.0500 (0.0500)  time: 0.7868  data: 0.0002  max mem: 27222
Epoch: [0]  [2000/2502]  eta: 0:06:49  lr: 0.000056  min_lr: 0.000000  loss: 4.3867 (5.8791)  class_acc: 0.2344 (0.0803)  loss_scale: 1024.0000 (465.8791)  weight_decay: 0.0500 (0.0500)  time: 0.7904  data: 0.0002  max mem: 27222
Epoch: [0]  [2100/2502]  eta: 0:05:28  lr: 0.000059  min_lr: 0.000000  loss: 4.2734 (5.8080)  class_acc: 0.2344 (0.0880)  loss_scale: 2048.0000 (516.8129)  weight_decay: 0.0500 (0.0500)  time: 0.8300  data: 0.0005  max mem: 27222
Epoch: [0]  [2200/2502]  eta: 0:04:06  lr: 0.000062  min_lr: 0.000000  loss: 4.2734 (5.7367)  class_acc: 0.2656 (0.0960)  loss_scale: 2048.0000 (586.3807)  weight_decay: 0.0500 (0.0500)  time: 0.8116  data: 0.0004  max mem: 27222
Epoch: [0]  [2300/2502]  eta: 0:02:44  lr: 0.000064  min_lr: 0.000000  loss: 4.0469 (5.6652)  class_acc: 0.3125 (0.1055)  loss_scale: 2048.0000 (649.9018)  weight_decay: 0.0500 (0.0500)  time: 0.7904  data: 0.0002  max mem: 27222
Epoch: [0]  [2400/2502]  eta: 0:01:23  lr: 0.000067  min_lr: 0.000000  loss: 3.8418 (5.5939)  class_acc: 0.3125 (0.1145)  loss_scale: 2048.0000 (708.1316)  weight_decay: 0.0500 (0.0500)  time: 0.7942  data: 0.0003  max mem: 27222
Epoch: [0]  [2500/2502]  eta: 0:00:01  lr: 0.000070  min_lr: 0.000000  loss: 3.8164 (5.5242)  class_acc: 0.3438 (0.1241)  loss_scale: 2048.0000 (761.1904)  weight_decay: 0.0500 (0.0500)  time: 0.7565  data: 0.0012  max mem: 27222
Epoch: [0]  [2501/2502]  eta: 0:00:00  lr: 0.000070  min_lr: 0.000000  loss: 3.8164 (5.5242)  class_acc: 0.3438 (0.1241)  loss_scale: 2048.0000 (761.1904)  weight_decay: 0.0500 (0.0500)  time: 0.7173  data: 0.0012  max mem: 27222
Epoch: [0] Total time: 0:33:56 (0.8139 s / it)
Averaged stats: lr: 0.000070  min_lr: 0.000000  loss: 3.8164 (5.5236)  class_acc: 0.3438 (0.1239)  loss_scale: 2048.0000 (761.1904)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Test:  [ 0/98]  eta: 0:10:42  loss: 2.3436 (2.3436)  acc1: 50.0000 (50.0000)  acc5: 90.6250 (90.6250)  time: 6.5573  data: 6.1803  max mem: 27222
Test:  [97/98]  eta: 0:00:00  loss: 2.6000 (2.6795)  acc1: 51.5625 (49.6320)  acc5: 81.2500 (82.2560)  time: 0.4510  data: 0.0035  max mem: 27222
Test: Total time: 0:00:43 (0.4454 s / it)
* Acc@1 48.826 Acc@5 82.544 loss 2.686
Accuracy of the network on the 50000 test images: 48.8%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Test:  [ 0/98]  eta: 0:10:26  loss: 6.8515 (6.8515)  acc1: 29.6875 (29.6875)  acc5: 62.5000 (62.5000)  time: 6.3881  data: 6.0514  max mem: 27222
Test:  [97/98]  eta: 0:00:00  loss: 6.8538 (6.8546)  acc1: 23.4375 (19.1360)  acc5: 45.3125 (43.8240)  time: 0.3337  data: 0.0014  max mem: 27222
Test: Total time: 0:00:40 (0.4151 s / it)
* Acc@1 18.798 Acc@5 44.226 loss 6.854
EMA Accuracy of the network on the 50000 test images: 18.8%
Max accuracy: 18.80%
{"train_lr": 3.494959193470958e-05, "train_min_lr": 4.564659262433039e-08, "train_loss": 5.523623046875, "train_class_acc": 0.12385546875, "train_loss_scale": 761.1904, "train_weight_decay": 0.04999999999999801, "test_loss": 6.854437325681959, "test_acc1": 18.79799999994278, "test_acc5": 44.226000001068115, "epoch": 0, "n_parameters": 86860264}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Epoch: [1]  [   0/2502]  eta: 1 day, 4:59:42  lr: 0.000070  min_lr: 0.000000  loss: 3.4766 (3.4766)  class_acc: 0.3750 (0.3750)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  time: 41.7195  data: 40.9448  max mem: 27222
Epoch: [1]  [ 100/2502]  eta: 0:47:37  lr: 0.000073  min_lr: 0.000000  loss: 3.6191 (3.7026)  class_acc: 0.3906 (0.3806)  loss_scale: 4096.0000 (2818.5347)  weight_decay: 0.0500 (0.0500)  time: 0.7843  data: 0.0002  max mem: 27222
Epoch: [1]  [ 200/2502]  eta: 0:38:01  lr: 0.000076  min_lr: 0.000000  loss: 3.5410 (3.6465)  class_acc: 0.4219 (0.3929)  loss_scale: 4096.0000 (3454.0896)  weight_decay: 0.0500 (0.0500)  time: 0.7824  data: 0.0003  max mem: 27222
Epoch: [1]  [ 300/2502]  eta: 0:34:10  lr: 0.000078  min_lr: 0.000000  loss: 3.3555 (3.5879)  class_acc: 0.4531 (0.4067)  loss_scale: 4096.0000 (3667.3488)  weight_decay: 0.0500 (0.0500)  time: 0.8231  data: 0.0003  max mem: 27222
Epoch: [1]  [ 400/2502]  eta: 0:31:29  lr: 0.000081  min_lr: 0.000000  loss: 3.3457 (3.5334)  class_acc: 0.4688 (0.4158)  loss_scale: 4096.0000 (3774.2444)  weight_decay: 0.0500 (0.0500)  time: 0.8123  data: 0.0002  max mem: 27222
Epoch: [1]  [ 500/2502]  eta: 0:29:18  lr: 0.000084  min_lr: 0.000000  loss: 3.2617 (3.4887)  class_acc: 0.4688 (0.4246)  loss_scale: 4096.0000 (3838.4671)  weight_decay: 0.0500 (0.0500)  time: 0.8031  data: 0.0002  max mem: 27222
Epoch: [1]  [ 600/2502]  eta: 0:27:26  lr: 0.000087  min_lr: 0.000000  loss: 3.1523 (3.4415)  class_acc: 0.4688 (0.4331)  loss_scale: 8192.0000 (4058.5158)  weight_decay: 0.0500 (0.0500)  time: 0.7971  data: 0.0002  max mem: 27222
Epoch: [1]  [ 700/2502]  eta: 0:25:45  lr: 0.000090  min_lr: 0.000000  loss: 3.1270 (3.3961)  class_acc: 0.5000 (0.4414)  loss_scale: 4096.0000 (4098.9215)  weight_decay: 0.0500 (0.0500)  time: 0.8333  data: 0.0002  max mem: 27222
Epoch: [1]  [ 800/2502]  eta: 0:24:06  lr: 0.000092  min_lr: 0.000000  loss: 3.0156 (3.3600)  class_acc: 0.5312 (0.4488)  loss_scale: 4096.0000 (4098.5568)  weight_decay: 0.0500 (0.0500)  time: 0.8009  data: 0.0004  max mem: 27222
Epoch: [1]  [ 900/2502]  eta: 0:22:33  lr: 0.000095  min_lr: 0.000000  loss: 2.8945 (3.3177)  class_acc: 0.5312 (0.4574)  loss_scale: 4096.0000 (4098.2730)  weight_decay: 0.0500 (0.0500)  time: 0.8101  data: 0.0002  max mem: 27222
Epoch: [1]  [1000/2502]  eta: 0:21:01  lr: 0.000098  min_lr: 0.000000  loss: 2.8496 (3.2817)  class_acc: 0.5469 (0.4640)  loss_scale: 4096.0000 (4098.0460)  weight_decay: 0.0500 (0.0500)  time: 0.7946  data: 0.0002  max mem: 27222
Epoch: [1]  [1100/2502]  eta: 0:19:33  lr: 0.000101  min_lr: 0.000000  loss: 2.8926 (3.2506)  class_acc: 0.5469 (0.4699)  loss_scale: 4096.0000 (4097.8601)  weight_decay: 0.0500 (0.0500)  time: 0.8001  data: 0.0003  max mem: 27222
Epoch: [1]  [1200/2502]  eta: 0:18:06  lr: 0.000104  min_lr: 0.000000  loss: 2.7090 (3.2192)  class_acc: 0.5781 (0.4761)  loss_scale: 8192.0000 (4363.7236)  weight_decay: 0.0500 (0.0500)  time: 0.7977  data: 0.0002  max mem: 27222
Epoch: [1]  [1300/2502]  eta: 0:16:40  lr: 0.000106  min_lr: 0.000000  loss: 2.8320 (3.1891)  class_acc: 0.5469 (0.4817)  loss_scale: 8192.0000 (4657.9800)  weight_decay: 0.0500 (0.0500)  time: 0.8139  data: 0.0003  max mem: 27222
Epoch: [1]  [1400/2502]  eta: 0:15:15  lr: 0.000109  min_lr: 0.000000  loss: 2.6641 (3.1619)  class_acc: 0.5625 (0.4874)  loss_scale: 8192.0000 (4910.2298)  weight_decay: 0.0500 (0.0500)  time: 0.7970  data: 0.0002  max mem: 27222
Epoch: [1]  [1500/2502]  eta: 0:13:50  lr: 0.000112  min_lr: 0.000000  loss: 2.7383 (3.1331)  class_acc: 0.5625 (0.4930)  loss_scale: 8192.0000 (5128.8688)  weight_decay: 0.0500 (0.0500)  time: 0.8095  data: 0.0002  max mem: 27222
Epoch: [1]  [1600/2502]  eta: 0:12:26  lr: 0.000115  min_lr: 0.000000  loss: 2.6621 (3.1077)  class_acc: 0.5781 (0.4983)  loss_scale: 8192.0000 (5320.1949)  weight_decay: 0.0500 (0.0500)  time: 0.8404  data: 0.0002  max mem: 27222
Epoch: [1]  [1700/2502]  eta: 0:11:02  lr: 0.000118  min_lr: 0.000000  loss: 2.7051 (3.0834)  class_acc: 0.5938 (0.5033)  loss_scale: 16384.0000 (5806.8807)  weight_decay: 0.0500 (0.0500)  time: 0.7991  data: 0.0003  max mem: 27222
Epoch: [1]  [1800/2502]  eta: 0:09:39  lr: 0.000120  min_lr: 0.000000  loss: 2.7324 (3.0632)  class_acc: 0.5938 (0.5070)  loss_scale: 8192.0000 (6021.1882)  weight_decay: 0.0500 (0.0500)  time: 0.8068  data: 0.0002  max mem: 27222
Epoch: [1]  [1900/2502]  eta: 0:08:16  lr: 0.000123  min_lr: 0.000000  loss: 2.7617 (3.0419)  class_acc: 0.5469 (0.5110)  loss_scale: 8192.0000 (6135.3814)  weight_decay: 0.0500 (0.0500)  time: 0.8410  data: 0.0002  max mem: 27222
Epoch: [1]  [2000/2502]  eta: 0:06:53  lr: 0.000126  min_lr: 0.000000  loss: 2.5664 (3.0199)  class_acc: 0.5781 (0.5153)  loss_scale: 8192.0000 (6238.1609)  weight_decay: 0.0500 (0.0500)  time: 0.8302  data: 0.0002  max mem: 27222
Epoch: [1]  [2100/2502]  eta: 0:05:30  lr: 0.000129  min_lr: 0.000000  loss: 2.5820 (3.0010)  class_acc: 0.6094 (0.5194)  loss_scale: 8192.0000 (6331.1566)  weight_decay: 0.0500 (0.0500)  time: 0.8004  data: 0.0002  max mem: 27222
Epoch: [1]  [2200/2502]  eta: 0:04:08  lr: 0.000132  min_lr: 0.000000  loss: 2.5176 (2.9824)  class_acc: 0.5781 (0.5233)  loss_scale: 8192.0000 (6415.7020)  weight_decay: 0.0500 (0.0500)  time: 0.7916  data: 0.0010  max mem: 27222
Epoch: [1]  [2300/2502]  eta: 0:02:45  lr: 0.000134  min_lr: 0.000000  loss: 2.6016 (2.9662)  class_acc: 0.6250 (0.5265)  loss_scale: 16384.0000 (6727.8714)  weight_decay: 0.0500 (0.0500)  time: 0.8158  data: 0.0007  max mem: 27222
Epoch: [1]  [2400/2502]  eta: 0:01:23  lr: 0.000137  min_lr: 0.000000  loss: 2.5293 (2.9492)  class_acc: 0.6094 (0.5297)  loss_scale: 16384.0000 (7130.0425)  weight_decay: 0.0500 (0.0500)  time: 0.8113  data: 0.0002  max mem: 27222
Epoch: [1]  [2500/2502]  eta: 0:00:01  lr: 0.000140  min_lr: 0.000000  loss: 2.4766 (2.9346)  class_acc: 0.6250 (0.5325)  loss_scale: 16384.0000 (7496.4992)  weight_decay: 0.0500 (0.0500)  time: 0.7547  data: 0.0009  max mem: 27222
Epoch: [1]  [2501/2502]  eta: 0:00:00  lr: 0.000140  min_lr: 0.000000  loss: 2.4766 (2.9346)  class_acc: 0.6250 (0.5325)  loss_scale: 16384.0000 (7496.4992)  weight_decay: 0.0500 (0.0500)  time: 0.7161  data: 0.0009  max mem: 27222
Epoch: [1] Total time: 0:34:11 (0.8199 s / it)
Averaged stats: lr: 0.000140  min_lr: 0.000000  loss: 2.4766 (2.9327)  class_acc: 0.6250 (0.5329)  loss_scale: 16384.0000 (7496.4992)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Test:  [ 0/98]  eta: 0:10:40  loss: 0.7464 (0.7464)  acc1: 82.8125 (82.8125)  acc5: 98.4375 (98.4375)  time: 6.5322  data: 6.1927  max mem: 27222
Test:  [97/98]  eta: 0:00:00  loss: 0.9655 (0.9843)  acc1: 76.5625 (77.0080)  acc5: 95.3125 (95.1040)  time: 0.3338  data: 0.0020  max mem: 27222
Test: Total time: 0:00:40 (0.4163 s / it)
* Acc@1 76.900 Acc@5 95.084 loss 0.984
Accuracy of the network on the 50000 test images: 76.9%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Test:  [ 0/98]  eta: 0:09:55  loss: 6.5739 (6.5739)  acc1: 43.7500 (43.7500)  acc5: 79.6875 (79.6875)  time: 6.0743  data: 5.7288  max mem: 27222
Test:  [97/98]  eta: 0:00:00  loss: 6.5899 (6.6007)  acc1: 39.0625 (35.6480)  acc5: 65.6250 (66.0320)  time: 0.3345  data: 0.0020  max mem: 27222
Test: Total time: 0:00:43 (0.4482 s / it)
* Acc@1 35.058 Acc@5 65.850 loss 6.600
EMA Accuracy of the network on the 50000 test images: 35.1%
Max accuracy: 35.06%
{"train_lr": 0.00010496079372699659, "train_min_lr": 1.3708608105447927e-07, "train_loss": 2.932668701171875, "train_class_acc": 0.53291171875, "train_loss_scale": 7496.4992, "train_weight_decay": 0.04999999999999801, "test_loss": 6.599888903754098, "test_acc1": 35.05800000038147, "test_acc5": 65.85000000457764, "epoch": 1, "n_parameters": 86860264}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Epoch: [2]  [   0/2502]  eta: 1 day, 5:22:23  lr: 0.000140  min_lr: 0.000000  loss: 2.2676 (2.2676)  class_acc: 0.7031 (0.7031)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 42.2634  data: 34.5012  max mem: 27222
Epoch: [2]  [ 100/2502]  eta: 0:47:50  lr: 0.000143  min_lr: 0.000000  loss: 2.4727 (2.4914)  class_acc: 0.6094 (0.6214)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7827  data: 0.0002  max mem: 27222
Epoch: [2]  [ 200/2502]  eta: 0:38:28  lr: 0.000146  min_lr: 0.000000  loss: 2.4375 (2.4787)  class_acc: 0.6250 (0.6233)  loss_scale: 8192.0000 (14509.2139)  weight_decay: 0.0500 (0.0500)  time: 0.8313  data: 0.0002  max mem: 27222
Epoch: [2]  [ 300/2502]  eta: 0:34:22  lr: 0.000148  min_lr: 0.000000  loss: 2.4219 (2.4719)  class_acc: 0.6406 (0.6251)  loss_scale: 8192.0000 (12410.4718)  weight_decay: 0.0500 (0.0500)  time: 0.7943  data: 0.0002  max mem: 27222
Epoch: [2]  [ 400/2502]  eta: 0:31:41  lr: 0.000151  min_lr: 0.000000  loss: 2.5801 (2.4844)  class_acc: 0.5938 (0.6219)  loss_scale: 8192.0000 (11358.4838)  weight_decay: 0.0500 (0.0500)  time: 0.8176  data: 0.0002  max mem: 27222
Epoch: [2]  [ 500/2502]  eta: 0:29:29  lr: 0.000154  min_lr: 0.000000  loss: 2.4355 (2.4799)  class_acc: 0.6406 (0.6225)  loss_scale: 8192.0000 (10726.4511)  weight_decay: 0.0500 (0.0500)  time: 0.7961  data: 0.0002  max mem: 27222
Epoch: [2]  [ 600/2502]  eta: 0:27:35  lr: 0.000157  min_lr: 0.000000  loss: 2.4414 (2.4765)  class_acc: 0.6406 (0.6238)  loss_scale: 8192.0000 (10304.7454)  weight_decay: 0.0500 (0.0500)  time: 0.7936  data: 0.0002  max mem: 27222
Epoch: [2]  [ 700/2502]  eta: 0:25:49  lr: 0.000160  min_lr: 0.000000  loss: 2.4395 (2.4728)  class_acc: 0.5938 (0.6243)  loss_scale: 16384.0000 (10353.9401)  weight_decay: 0.0500 (0.0500)  time: 0.7912  data: 0.0003  max mem: 27222
Epoch: [2]  [ 800/2502]  eta: 0:24:12  lr: 0.000162  min_lr: 0.000000  loss: 2.4434 (2.4671)  class_acc: 0.6406 (0.6256)  loss_scale: 16384.0000 (11106.7566)  weight_decay: 0.0500 (0.0500)  time: 0.7991  data: 0.0002  max mem: 27222
Epoch: [2]  [ 900/2502]  eta: 0:22:41  lr: 0.000165  min_lr: 0.000000  loss: 2.4590 (2.4667)  class_acc: 0.6094 (0.6259)  loss_scale: 16384.0000 (11692.4661)  weight_decay: 0.0500 (0.0500)  time: 0.7910  data: 0.0002  max mem: 27222
Epoch: [2]  [1000/2502]  eta: 0:21:08  lr: 0.000168  min_lr: 0.000000  loss: 2.4629 (2.4642)  class_acc: 0.5938 (0.6264)  loss_scale: 16384.0000 (12161.1508)  weight_decay: 0.0500 (0.0500)  time: 0.8172  data: 0.0002  max mem: 27222
Epoch: [2]  [1100/2502]  eta: 0:19:38  lr: 0.000171  min_lr: 0.000000  loss: 2.4043 (2.4620)  class_acc: 0.6250 (0.6273)  loss_scale: 16384.0000 (12544.6975)  weight_decay: 0.0500 (0.0500)  time: 0.8061  data: 0.0002  max mem: 27222
Epoch: [2]  [1200/2502]  eta: 0:18:12  lr: 0.000174  min_lr: 0.000000  loss: 2.3828 (2.4574)  class_acc: 0.6406 (0.6288)  loss_scale: 32768.0000 (13109.9284)  weight_decay: 0.0500 (0.0500)  time: 0.8370  data: 0.0002  max mem: 27222
Epoch: [2]  [1300/2502]  eta: 0:16:46  lr: 0.000176  min_lr: 0.000000  loss: 2.4160 (2.4573)  class_acc: 0.6250 (0.6284)  loss_scale: 32768.0000 (14620.9254)  weight_decay: 0.0500 (0.0500)  time: 0.8197  data: 0.0002  max mem: 27222
Epoch: [2]  [1400/2502]  eta: 0:15:20  lr: 0.000179  min_lr: 0.000000  loss: 2.3848 (2.4534)  class_acc: 0.6094 (0.6293)  loss_scale: 16384.0000 (14863.7145)  weight_decay: 0.0500 (0.0500)  time: 0.8055  data: 0.0002  max mem: 27222
Epoch: [2]  [1500/2502]  eta: 0:13:54  lr: 0.000182  min_lr: 0.000000  loss: 2.3926 (2.4486)  class_acc: 0.6250 (0.6303)  loss_scale: 16384.0000 (14964.9993)  weight_decay: 0.0500 (0.0500)  time: 0.7938  data: 0.0002  max mem: 27222
Epoch: [2]  [1600/2502]  eta: 0:12:30  lr: 0.000185  min_lr: 0.000000  loss: 2.3633 (2.4442)  class_acc: 0.6562 (0.6315)  loss_scale: 16384.0000 (15053.6315)  weight_decay: 0.0500 (0.0500)  time: 0.7968  data: 0.0003  max mem: 27222
Epoch: [2]  [1700/2502]  eta: 0:11:05  lr: 0.000188  min_lr: 0.000000  loss: 2.2559 (2.4402)  class_acc: 0.6719 (0.6327)  loss_scale: 16384.0000 (15131.8424)  weight_decay: 0.0500 (0.0500)  time: 0.7927  data: 0.0004  max mem: 27222
Epoch: [2]  [1800/2502]  eta: 0:09:41  lr: 0.000190  min_lr: 0.000000  loss: 2.3262 (2.4355)  class_acc: 0.6250 (0.6338)  loss_scale: 16384.0000 (15201.3681)  weight_decay: 0.0500 (0.0500)  time: 0.8061  data: 0.0002  max mem: 27222
Epoch: [2]  [1900/2502]  eta: 0:08:18  lr: 0.000193  min_lr: 0.000000  loss: 2.3379 (2.4313)  class_acc: 0.6250 (0.6351)  loss_scale: 32768.0000 (15901.3572)  weight_decay: 0.0500 (0.0500)  time: 0.8026  data: 0.0002  max mem: 27222
Epoch: [2]  [2000/2502]  eta: 0:06:55  lr: 0.000196  min_lr: 0.000000  loss: 2.2715 (2.4260)  class_acc: 0.6406 (0.6361)  loss_scale: 32768.0000 (16744.2679)  weight_decay: 0.0500 (0.0500)  time: 0.7931  data: 0.0002  max mem: 27222
Epoch: [2]  [2100/2502]  eta: 0:05:32  lr: 0.000199  min_lr: 0.000000  loss: 2.3594 (2.4221)  class_acc: 0.6406 (0.6370)  loss_scale: 32768.0000 (17506.9396)  weight_decay: 0.0500 (0.0500)  time: 0.8142  data: 0.0003  max mem: 27222
Epoch: [2]  [2200/2502]  eta: 0:04:09  lr: 0.000202  min_lr: 0.000000  loss: 2.3672 (2.4196)  class_acc: 0.6406 (0.6375)  loss_scale: 32768.0000 (18200.3090)  weight_decay: 0.0500 (0.0500)  time: 0.8355  data: 0.0002  max mem: 27222
Epoch: [2]  [2300/2502]  eta: 0:02:46  lr: 0.000204  min_lr: 0.000000  loss: 2.2695 (2.4167)  class_acc: 0.6562 (0.6384)  loss_scale: 32768.0000 (18833.4116)  weight_decay: 0.0500 (0.0500)  time: 0.7915  data: 0.0003  max mem: 27222
Epoch: [2]  [2400/2502]  eta: 0:01:24  lr: 0.000207  min_lr: 0.000000  loss: 2.3320 (2.4132)  class_acc: 0.6562 (0.6393)  loss_scale: 32768.0000 (19468.3682)  weight_decay: 0.0500 (0.0500)  time: 0.7928  data: 0.0003  max mem: 27222
Epoch: [2]  [2500/2502]  eta: 0:00:01  lr: 0.000210  min_lr: 0.000000  loss: 2.3301 (2.4096)  class_acc: 0.6562 (0.6398)  loss_scale: 16384.0000 (19726.3360)  weight_decay: 0.0500 (0.0500)  time: 0.7525  data: 0.0010  max mem: 27222
Epoch: [2]  [2501/2502]  eta: 0:00:00  lr: 0.000210  min_lr: 0.000000  loss: 2.3301 (2.4096)  class_acc: 0.6562 (0.6398)  loss_scale: 16384.0000 (19726.3360)  weight_decay: 0.0500 (0.0500)  time: 0.7148  data: 0.0009  max mem: 27222
Epoch: [2] Total time: 0:34:20 (0.8234 s / it)
Averaged stats: lr: 0.000210  min_lr: 0.000000  loss: 2.3301 (2.4151)  class_acc: 0.6562 (0.6384)  loss_scale: 16384.0000 (19726.3360)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Test:  [ 0/98]  eta: 0:09:51  loss: 0.4678 (0.4678)  acc1: 90.6250 (90.6250)  acc5: 100.0000 (100.0000)  time: 6.0338  data: 5.6912  max mem: 27222
Test:  [97/98]  eta: 0:00:00  loss: 0.8252 (0.8361)  acc1: 80.9524 (80.0640)  acc5: 95.3125 (95.9040)  time: 0.3331  data: 0.0012  max mem: 27222
Test: Total time: 0:00:41 (0.4208 s / it)
* Acc@1 79.932 Acc@5 96.006 loss 0.832
Accuracy of the network on the 50000 test images: 79.9%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Test:  [ 0/98]  eta: 0:08:01  loss: 6.0613 (6.0613)  acc1: 67.1875 (67.1875)  acc5: 93.7500 (93.7500)  time: 4.9136  data: 4.5667  max mem: 27222
Test:  [97/98]  eta: 0:00:00  loss: 6.1333 (6.1497)  acc1: 54.6875 (51.8240)  acc5: 81.2500 (80.6720)  time: 0.3546  data: 0.0155  max mem: 27222
Test: Total time: 0:00:41 (0.4193 s / it)
* Acc@1 51.664 Acc@5 81.026 loss 6.148
EMA Accuracy of the network on the 50000 test images: 51.7%
Max accuracy: 51.66%
{"train_lr": 0.00017497199551928306, "train_min_lr": 2.2852556948462851e-07, "train_loss": 2.415134521484375, "train_class_acc": 0.638353125, "train_loss_scale": 19726.336, "train_weight_decay": 0.04999999999999801, "test_loss": 6.148115363656258, "test_acc1": 51.66400000808716, "test_acc5": 81.02599999938965, "epoch": 2, "n_parameters": 86860264}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Epoch: [3]  [   0/2502]  eta: 1 day, 5:08:13  lr: 0.000210  min_lr: 0.000000  loss: 2.2129 (2.2129)  class_acc: 0.6719 (0.6719)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 41.9238  data: 41.1978  max mem: 27222
Epoch: [3]  [ 100/2502]  eta: 0:47:45  lr: 0.000213  min_lr: 0.000000  loss: 2.1562 (2.2884)  class_acc: 0.6719 (0.6700)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7854  data: 0.0002  max mem: 27222
Epoch: [3]  [ 200/2502]  eta: 0:38:13  lr: 0.000216  min_lr: 0.000000  loss: 2.1191 (2.2840)  class_acc: 0.6875 (0.6699)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7886  data: 0.0002  max mem: 27222
Epoch: [3]  [ 300/2502]  eta: 0:34:11  lr: 0.000218  min_lr: 0.000000  loss: 2.2988 (2.2901)  class_acc: 0.6406 (0.6680)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7923  data: 0.0002  max mem: 27222
Epoch: [3]  [ 400/2502]  eta: 0:31:31  lr: 0.000221  min_lr: 0.000000  loss: 2.3223 (2.2922)  class_acc: 0.6562 (0.6666)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7943  data: 0.0002  max mem: 27222
Epoch: [3]  [ 500/2502]  eta: 0:29:25  lr: 0.000224  min_lr: 0.000000  loss: 2.2422 (2.2916)  class_acc: 0.6875 (0.6675)  loss_scale: 32768.0000 (17234.2675)  weight_decay: 0.0500 (0.0500)  time: 0.8027  data: 0.0013  max mem: 27222
Epoch: [3]  [ 600/2502]  eta: 0:27:32  lr: 0.000227  min_lr: 0.000000  loss: 2.3184 (2.2923)  class_acc: 0.6562 (0.6671)  loss_scale: 32768.0000 (19818.9151)  weight_decay: 0.0500 (0.0500)  time: 0.8032  data: 0.0007  max mem: 27222
Epoch: [3]  [ 700/2502]  eta: 0:25:50  lr: 0.000230  min_lr: 0.000000  loss: 2.2559 (2.2935)  class_acc: 0.6562 (0.6669)  loss_scale: 32768.0000 (21666.1455)  weight_decay: 0.0500 (0.0500)  time: 0.8156  data: 0.0002  max mem: 27222
Epoch: [3]  [ 800/2502]  eta: 0:24:11  lr: 0.000232  min_lr: 0.000000  loss: 2.0605 (2.2932)  class_acc: 0.6875 (0.6667)  loss_scale: 32768.0000 (23052.1448)  weight_decay: 0.0500 (0.0500)  time: 0.7950  data: 0.0003  max mem: 27222
Epoch: [3]  [ 900/2502]  eta: 0:22:40  lr: 0.000235  min_lr: 0.000000  loss: 2.2793 (2.2902)  class_acc: 0.6562 (0.6669)  loss_scale: 32768.0000 (24130.4861)  weight_decay: 0.0500 (0.0500)  time: 0.7886  data: 0.0002  max mem: 27222
Epoch: [3]  [1000/2502]  eta: 0:21:08  lr: 0.000238  min_lr: 0.000000  loss: 2.2363 (2.2917)  class_acc: 0.6719 (0.6669)  loss_scale: 32768.0000 (25124.3157)  weight_decay: 0.0500 (0.0500)  time: 0.7935  data: 0.0002  max mem: 27222
Epoch: [3]  [1100/2502]  eta: 0:19:39  lr: 0.000241  min_lr: 0.000000  loss: 2.2305 (2.2921)  class_acc: 0.6719 (0.6669)  loss_scale: 32768.0000 (25818.5649)  weight_decay: 0.0500 (0.0500)  time: 0.7956  data: 0.0005  max mem: 27222
Epoch: [3]  [1200/2502]  eta: 0:18:11  lr: 0.000244  min_lr: 0.000000  loss: 2.2266 (2.2909)  class_acc: 0.6875 (0.6668)  loss_scale: 32768.0000 (26397.2023)  weight_decay: 0.0500 (0.0500)  time: 0.7938  data: 0.0002  max mem: 27222
Epoch: [3]  [1300/2502]  eta: 0:16:45  lr: 0.000246  min_lr: 0.000000  loss: 2.2715 (2.2885)  class_acc: 0.6719 (0.6674)  loss_scale: 32768.0000 (26886.8870)  weight_decay: 0.0500 (0.0500)  time: 0.7983  data: 0.0002  max mem: 27222
Epoch: [3]  [1400/2502]  eta: 0:15:19  lr: 0.000249  min_lr: 0.000000  loss: 2.2754 (2.2883)  class_acc: 0.6719 (0.6674)  loss_scale: 32768.0000 (27306.6667)  weight_decay: 0.0500 (0.0500)  time: 0.8329  data: 0.0003  max mem: 27222
Epoch: [3]  [1500/2502]  eta: 0:13:53  lr: 0.000252  min_lr: 0.000000  loss: 2.2871 (2.2894)  class_acc: 0.6875 (0.6671)  loss_scale: 32768.0000 (27670.5130)  weight_decay: 0.0500 (0.0500)  time: 0.7903  data: 0.0002  max mem: 27222
Epoch: [3]  [1600/2502]  eta: 0:12:29  lr: 0.000255  min_lr: 0.000000  loss: 2.2324 (2.2862)  class_acc: 0.6875 (0.6681)  loss_scale: 32768.0000 (28070.7758)  weight_decay: 0.0500 (0.0500)  time: 0.7961  data: 0.0002  max mem: 27222
Epoch: [3]  [1700/2502]  eta: 0:11:05  lr: 0.000258  min_lr: 0.000000  loss: 2.2305 (2.2843)  class_acc: 0.7031 (0.6687)  loss_scale: 32768.0000 (28346.9206)  weight_decay: 0.0500 (0.0500)  time: 0.7987  data: 0.0002  max mem: 27222
Epoch: [3]  [1800/2502]  eta: 0:09:41  lr: 0.000260  min_lr: 0.000000  loss: 2.1504 (2.2829)  class_acc: 0.6875 (0.6691)  loss_scale: 32768.0000 (28592.3998)  weight_decay: 0.0500 (0.0500)  time: 0.8075  data: 0.0002  max mem: 27222
Epoch: [3]  [1900/2502]  eta: 0:08:17  lr: 0.000263  min_lr: 0.000000  loss: 2.1758 (2.2801)  class_acc: 0.6875 (0.6697)  loss_scale: 32768.0000 (28812.0526)  weight_decay: 0.0500 (0.0500)  time: 0.7944  data: 0.0002  max mem: 27222
Epoch: [3]  [2000/2502]  eta: 0:06:54  lr: 0.000266  min_lr: 0.000000  loss: 2.2207 (2.2788)  class_acc: 0.6875 (0.6699)  loss_scale: 32768.0000 (29009.7511)  weight_decay: 0.0500 (0.0500)  time: 0.8017  data: 0.0002  max mem: 27222
Epoch: [3]  [2100/2502]  eta: 0:05:31  lr: 0.000269  min_lr: 0.000000  loss: 2.2305 (2.2767)  class_acc: 0.6875 (0.6703)  loss_scale: 32768.0000 (29438.1723)  weight_decay: 0.0500 (0.0500)  time: 0.8399  data: 0.0002  max mem: 27222
Epoch: [3]  [2200/2502]  eta: 0:04:08  lr: 0.000272  min_lr: 0.000000  loss: 2.2852 (2.2742)  class_acc: 0.6562 (0.6706)  loss_scale: 32768.0000 (29589.4593)  weight_decay: 0.0500 (0.0500)  time: 0.8039  data: 0.0002  max mem: 27222
Epoch: [3]  [2300/2502]  eta: 0:02:46  lr: 0.000274  min_lr: 0.000000  loss: 2.1113 (2.2740)  class_acc: 0.7031 (0.6706)  loss_scale: 32768.0000 (29727.5967)  weight_decay: 0.0500 (0.0500)  time: 0.8427  data: 0.0002  max mem: 27222
Epoch: [3]  [2400/2502]  eta: 0:01:23  lr: 0.000277  min_lr: 0.000000  loss: 2.2148 (2.2719)  class_acc: 0.6719 (0.6710)  loss_scale: 32768.0000 (29854.2274)  weight_decay: 0.0500 (0.0500)  time: 0.8285  data: 0.0002  max mem: 27222
Epoch: [3]  [2500/2502]  eta: 0:00:01  lr: 0.000280  min_lr: 0.000000  loss: 2.2637 (2.2715)  class_acc: 0.6562 (0.6711)  loss_scale: 32768.0000 (29969.6128)  weight_decay: 0.0500 (0.0500)  time: 0.7484  data: 0.0011  max mem: 27222
Epoch: [3]  [2501/2502]  eta: 0:00:00  lr: 0.000280  min_lr: 0.000000  loss: 2.2637 (2.2715)  class_acc: 0.6562 (0.6711)  loss_scale: 32768.0000 (29969.6128)  weight_decay: 0.0500 (0.0500)  time: 0.7090  data: 0.0011  max mem: 27222
Epoch: [3] Total time: 0:34:17 (0.8223 s / it)
Averaged stats: lr: 0.000280  min_lr: 0.000000  loss: 2.2637 (2.2733)  class_acc: 0.6562 (0.6712)  loss_scale: 32768.0000 (29969.6128)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Test:  [ 0/98]  eta: 0:09:13  loss: 0.4591 (0.4591)  acc1: 90.6250 (90.6250)  acc5: 98.4375 (98.4375)  time: 5.6431  data: 5.2730  max mem: 27222
Test:  [97/98]  eta: 0:00:00  loss: 0.7851 (0.7716)  acc1: 81.2500 (81.7600)  acc5: 96.8750 (96.4160)  time: 0.3326  data: 0.0002  max mem: 27222
Test: Total time: 0:00:40 (0.4156 s / it)
* Acc@1 81.472 Acc@5 96.514 loss 0.773
Accuracy of the network on the 50000 test images: 81.5%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Test:  [ 0/98]  eta: 0:10:59  loss: 5.3372 (5.3372)  acc1: 81.2500 (81.2500)  acc5: 98.4375 (98.4375)  time: 6.7300  data: 6.3935  max mem: 27222
Test:  [97/98]  eta: 0:00:00  loss: 5.5107 (5.5128)  acc1: 64.2857 (63.1840)  acc5: 89.0625 (88.9920)  time: 0.3347  data: 0.0002  max mem: 27222
Test: Total time: 0:00:41 (0.4216 s / it)
* Acc@1 63.432 Acc@5 89.142 loss 5.511
EMA Accuracy of the network on the 50000 test images: 63.4%
Max accuracy: 63.43%
{"train_lr": 0.00024498319731157, "train_min_lr": 3.1996505791477694e-07, "train_loss": 2.2732890625, "train_class_acc": 0.67120625, "train_loss_scale": 29969.6128, "train_weight_decay": 0.04999999999999801, "test_loss": 5.510608106243367, "test_acc1": 63.43199999786377, "test_acc5": 89.14200002166749, "epoch": 3, "n_parameters": 86860264}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Epoch: [4]  [   0/2502]  eta: 1 day, 4:25:42  lr: 0.000280  min_lr: 0.000000  loss: 2.3125 (2.3125)  class_acc: 0.6094 (0.6094)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 40.9042  data: 36.1067  max mem: 27222
Epoch: [4]  [ 100/2502]  eta: 0:47:20  lr: 0.000283  min_lr: 0.000000  loss: 2.2520 (2.2173)  class_acc: 0.6719 (0.6818)  loss_scale: 32768.0000 (36661.2277)  weight_decay: 0.0500 (0.0500)  time: 0.7877  data: 0.0002  max mem: 27222
Epoch: [4]  [ 200/2502]  eta: 0:37:57  lr: 0.000286  min_lr: 0.000000  loss: 2.2500 (2.2242)  class_acc: 0.6719 (0.6811)  loss_scale: 32768.0000 (34724.2985)  weight_decay: 0.0500 (0.0500)  time: 0.7871  data: 0.0002  max mem: 27222
Epoch: [4]  [ 300/2502]  eta: 0:34:06  lr: 0.000288  min_lr: 0.000000  loss: 2.1777 (2.2269)  class_acc: 0.6875 (0.6819)  loss_scale: 32768.0000 (34074.3654)  weight_decay: 0.0500 (0.0500)  time: 0.8199  data: 0.0002  max mem: 27222
Epoch: [4]  [ 400/2502]  eta: 0:31:30  lr: 0.000291  min_lr: 0.000000  loss: 2.2227 (2.2288)  class_acc: 0.6719 (0.6813)  loss_scale: 32768.0000 (33748.5885)  weight_decay: 0.0500 (0.0500)  time: 0.8362  data: 0.0012  max mem: 27222
Epoch: [4]  [ 500/2502]  eta: 0:29:27  lr: 0.000294  min_lr: 0.000000  loss: 2.1875 (2.2255)  class_acc: 0.7031 (0.6831)  loss_scale: 32768.0000 (33552.8623)  weight_decay: 0.0500 (0.0500)  time: 0.8363  data: 0.0002  max mem: 27222
Epoch: [4]  [ 600/2502]  eta: 0:27:35  lr: 0.000297  min_lr: 0.000000  loss: 2.1543 (2.2241)  class_acc: 0.7031 (0.6833)  loss_scale: 32768.0000 (33858.4493)  weight_decay: 0.0500 (0.0500)  time: 0.8338  data: 0.0002  max mem: 27222
Epoch: [4]  [ 700/2502]  eta: 0:25:56  lr: 0.000300  min_lr: 0.000000  loss: 2.1816 (2.2187)  class_acc: 0.6875 (0.6847)  loss_scale: 32768.0000 (33702.8930)  weight_decay: 0.0500 (0.0500)  time: 0.7997  data: 0.0002  max mem: 27222
Epoch: [4]  [ 800/2502]  eta: 0:24:19  lr: 0.000302  min_lr: 0.000000  loss: 2.1621 (2.2161)  class_acc: 0.7031 (0.6846)  loss_scale: 32768.0000 (33586.1773)  weight_decay: 0.0500 (0.0500)  time: 0.8000  data: 0.0002  max mem: 27222
Epoch: [4]  [ 900/2502]  eta: 0:22:44  lr: 0.000305  min_lr: 0.000000  loss: 2.2520 (2.2180)  class_acc: 0.6719 (0.6839)  loss_scale: 32768.0000 (33495.3696)  weight_decay: 0.0500 (0.0500)  time: 0.7965  data: 0.0006  max mem: 27222
Epoch: [4]  [1000/2502]  eta: 0:21:12  lr: 0.000308  min_lr: 0.000000  loss: 2.2051 (2.2183)  class_acc: 0.6562 (0.6837)  loss_scale: 32768.0000 (33422.7053)  weight_decay: 0.0500 (0.0500)  time: 0.8001  data: 0.0002  max mem: 27222
Epoch: [4]  [1100/2502]  eta: 0:19:42  lr: 0.000311  min_lr: 0.000000  loss: 2.1602 (2.2164)  class_acc: 0.6875 (0.6842)  loss_scale: 32768.0000 (33363.2407)  weight_decay: 0.0500 (0.0500)  time: 0.7981  data: 0.0003  max mem: 27222
Epoch: [4]  [1200/2502]  eta: 0:18:16  lr: 0.000314  min_lr: 0.000000  loss: 2.1172 (2.2140)  class_acc: 0.7031 (0.6842)  loss_scale: 32768.0000 (33422.8143)  weight_decay: 0.0500 (0.0500)  time: 0.8044  data: 0.0002  max mem: 27222
Epoch: [4]  [1300/2502]  eta: 0:16:49  lr: 0.000316  min_lr: 0.000000  loss: 2.1172 (2.2122)  class_acc: 0.7031 (0.6844)  loss_scale: 32768.0000 (33372.4827)  weight_decay: 0.0500 (0.0500)  time: 0.8000  data: 0.0002  max mem: 27222
Epoch: [4]  [1400/2502]  eta: 0:15:22  lr: 0.000319  min_lr: 0.000000  loss: 2.1504 (2.2103)  class_acc: 0.7188 (0.6854)  loss_scale: 32768.0000 (33329.3362)  weight_decay: 0.0500 (0.0500)  time: 0.8039  data: 0.0002  max mem: 27222
Epoch: [4]  [1500/2502]  eta: 0:13:56  lr: 0.000322  min_lr: 0.000000  loss: 2.1211 (2.2081)  class_acc: 0.7031 (0.6861)  loss_scale: 32768.0000 (33291.9387)  weight_decay: 0.0500 (0.0500)  time: 0.8063  data: 0.0002  max mem: 27222
Epoch: [4]  [1600/2502]  eta: 0:12:31  lr: 0.000325  min_lr: 0.000000  loss: 2.1152 (2.2081)  class_acc: 0.7031 (0.6865)  loss_scale: 32768.0000 (33259.2130)  weight_decay: 0.0500 (0.0500)  time: 0.7982  data: 0.0002  max mem: 27222
Epoch: [4]  [1700/2502]  eta: 0:11:07  lr: 0.000328  min_lr: 0.000000  loss: 2.1484 (2.2060)  class_acc: 0.6875 (0.6871)  loss_scale: 32768.0000 (33307.3909)  weight_decay: 0.0500 (0.0500)  time: 0.8010  data: 0.0002  max mem: 27222
Epoch: [4]  [1800/2502]  eta: 0:09:42  lr: 0.000330  min_lr: 0.000000  loss: 2.2188 (2.2061)  class_acc: 0.6875 (0.6869)  loss_scale: 32768.0000 (33277.4414)  weight_decay: 0.0500 (0.0500)  time: 0.7936  data: 0.0002  max mem: 27222
Epoch: [4]  [1900/2502]  eta: 0:08:18  lr: 0.000333  min_lr: 0.000000  loss: 2.1660 (2.2047)  class_acc: 0.7031 (0.6873)  loss_scale: 32768.0000 (33250.6428)  weight_decay: 0.0500 (0.0500)  time: 0.7965  data: 0.0003  max mem: 27222
Epoch: [4]  [2000/2502]  eta: 0:06:55  lr: 0.000336  min_lr: 0.000000  loss: 2.1152 (2.2040)  class_acc: 0.7031 (0.6876)  loss_scale: 32768.0000 (33226.5227)  weight_decay: 0.0500 (0.0500)  time: 0.8203  data: 0.0002  max mem: 27222
Epoch: [4]  [2100/2502]  eta: 0:05:32  lr: 0.000339  min_lr: 0.000000  loss: 2.1855 (2.2030)  class_acc: 0.6875 (0.6877)  loss_scale: 32768.0000 (33204.6987)  weight_decay: 0.0500 (0.0500)  time: 0.8348  data: 0.0002  max mem: 27222
Epoch: [4]  [2200/2502]  eta: 0:04:09  lr: 0.000342  min_lr: 0.000000  loss: 2.0605 (2.2024)  class_acc: 0.6875 (0.6879)  loss_scale: 32768.0000 (33303.9600)  weight_decay: 0.0500 (0.0500)  time: 0.7937  data: 0.0002  max mem: 27222
Epoch: [4]  [2300/2502]  eta: 0:02:46  lr: 0.000344  min_lr: 0.000000  loss: 2.2031 (2.2023)  class_acc: 0.6875 (0.6881)  loss_scale: 32768.0000 (33280.6675)  weight_decay: 0.0500 (0.0500)  time: 0.8013  data: 0.0013  max mem: 27222
Epoch: [4]  [2400/2502]  eta: 0:01:24  lr: 0.000347  min_lr: 0.000000  loss: 2.1094 (2.2008)  class_acc: 0.7188 (0.6887)  loss_scale: 32768.0000 (33259.3153)  weight_decay: 0.0500 (0.0500)  time: 0.8260  data: 0.0002  max mem: 27222
Epoch: [4]  [2500/2502]  eta: 0:00:01  lr: 0.000350  min_lr: 0.000000  loss: 2.1328 (2.1996)  class_acc: 0.7031 (0.6890)  loss_scale: 32768.0000 (33239.8592)  weight_decay: 0.0500 (0.0500)  time: 0.7500  data: 0.0011  max mem: 27222
Epoch: [4]  [2501/2502]  eta: 0:00:00  lr: 0.000350  min_lr: 0.000000  loss: 2.1328 (2.1996)  class_acc: 0.7031 (0.6890)  loss_scale: 32768.0000 (33239.8592)  weight_decay: 0.0500 (0.0500)  time: 0.7097  data: 0.0011  max mem: 27222
Epoch: [4] Total time: 0:34:22 (0.8245 s / it)
Averaged stats: lr: 0.000350  min_lr: 0.000000  loss: 2.1328 (2.1970)  class_acc: 0.7031 (0.6898)  loss_scale: 32768.0000 (33239.8592)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Test:  [ 0/98]  eta: 0:09:01  loss: 0.3768 (0.3768)  acc1: 92.1875 (92.1875)  acc5: 100.0000 (100.0000)  time: 5.5207  data: 5.1584  max mem: 27222
Test:  [97/98]  eta: 0:00:00  loss: 0.7697 (0.7515)  acc1: 81.2500 (82.2240)  acc5: 96.8750 (96.6240)  time: 0.3367  data: 0.0007  max mem: 27222
Test: Total time: 0:00:39 (0.4076 s / it)
* Acc@1 82.076 Acc@5 96.672 loss 0.748
Accuracy of the network on the 50000 test images: 82.1%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Test:  [ 0/98]  eta: 0:09:58  loss: 4.4941 (4.4941)  acc1: 84.3750 (84.3750)  acc5: 100.0000 (100.0000)  time: 6.1036  data: 5.7498  max mem: 27222
Test:  [97/98]  eta: 0:00:00  loss: 4.7555 (4.7478)  acc1: 70.3125 (70.5120)  acc5: 92.1875 (93.1520)  time: 0.3354  data: 0.0002  max mem: 27222
Test: Total time: 0:00:40 (0.4133 s / it)
* Acc@1 70.690 Acc@5 92.858 loss 4.745
EMA Accuracy of the network on the 50000 test images: 70.7%
Max accuracy: 70.69%
{"train_lr": 0.0003149943991038565, "train_min_lr": 4.114045463449248e-07, "train_loss": 2.19697041015625, "train_class_acc": 0.6897578125, "train_loss_scale": 33239.8592, "train_weight_decay": 0.04999999999999801, "test_loss": 4.745437317356771, "test_acc1": 70.69000000411987, "test_acc5": 92.85800000946045, "epoch": 4, "n_parameters": 86860264}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Epoch: [5]  [   0/2502]  eta: 1 day, 5:39:46  lr: 0.000350  min_lr: 0.000000  loss: 2.5078 (2.5078)  class_acc: 0.5938 (0.5938)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 42.6804  data: 39.5947  max mem: 27222
Epoch: [5]  [ 100/2502]  eta: 0:48:03  lr: 0.000353  min_lr: 0.000000  loss: 2.1465 (2.1615)  class_acc: 0.7031 (0.6969)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7849  data: 0.0003  max mem: 27222
Epoch: [5]  [ 200/2502]  eta: 0:38:35  lr: 0.000356  min_lr: 0.000000  loss: 2.0723 (2.1587)  class_acc: 0.7188 (0.7000)  loss_scale: 32768.0000 (34072.1990)  weight_decay: 0.0500 (0.0500)  time: 0.8357  data: 0.0003  max mem: 27222
Epoch: [5]  [ 300/2502]  eta: 0:34:28  lr: 0.000358  min_lr: 0.000000  loss: 2.0586 (2.1453)  class_acc: 0.7031 (0.7026)  loss_scale: 32768.0000 (33638.9103)  weight_decay: 0.0500 (0.0500)  time: 0.8119  data: 0.0002  max mem: 27222
Epoch: [5]  [ 400/2502]  eta: 0:31:48  lr: 0.000361  min_lr: 0.000000  loss: 2.1660 (2.1453)  class_acc: 0.6875 (0.7022)  loss_scale: 32768.0000 (33421.7257)  weight_decay: 0.0500 (0.0500)  time: 0.8051  data: 0.0002  max mem: 27222
Epoch: [5]  [ 500/2502]  eta: 0:29:37  lr: 0.000364  min_lr: 0.000000  loss: 2.1387 (2.1431)  class_acc: 0.7031 (0.7030)  loss_scale: 32768.0000 (33291.2415)  weight_decay: 0.0500 (0.0500)  time: 0.8177  data: 0.0003  max mem: 27222
Epoch: [5]  [ 600/2502]  eta: 0:27:42  lr: 0.000367  min_lr: 0.000000  loss: 2.1016 (2.1420)  class_acc: 0.6875 (0.7034)  loss_scale: 32768.0000 (33204.1797)  weight_decay: 0.0500 (0.0500)  time: 0.8349  data: 0.0002  max mem: 27222
Epoch: [5]  [ 700/2502]  eta: 0:25:58  lr: 0.000370  min_lr: 0.000000  loss: 2.0977 (2.1422)  class_acc: 0.7031 (0.7039)  loss_scale: 32768.0000 (33235.4465)  weight_decay: 0.0500 (0.0500)  time: 0.8001  data: 0.0002  max mem: 27222
Epoch: [5]  [ 800/2502]  eta: 0:24:17  lr: 0.000372  min_lr: 0.000000  loss: 2.1641 (2.1442)  class_acc: 0.7031 (0.7035)  loss_scale: 32768.0000 (33422.5418)  weight_decay: 0.0500 (0.0500)  time: 0.7957  data: 0.0003  max mem: 27222
Epoch: [5]  [ 900/2502]  eta: 0:22:41  lr: 0.000375  min_lr: 0.000000  loss: 2.1035 (2.1429)  class_acc: 0.7188 (0.7037)  loss_scale: 32768.0000 (33349.8957)  weight_decay: 0.0500 (0.0500)  time: 0.7939  data: 0.0002  max mem: 27222
Epoch: [5]  [1000/2502]  eta: 0:21:07  lr: 0.000378  min_lr: 0.000000  loss: 2.1035 (2.1422)  class_acc: 0.6875 (0.7038)  loss_scale: 32768.0000 (33291.7642)  weight_decay: 0.0500 (0.0500)  time: 0.7869  data: 0.0002  max mem: 27222
Epoch: [5]  [1100/2502]  eta: 0:19:38  lr: 0.000381  min_lr: 0.000000  loss: 2.0957 (2.1453)  class_acc: 0.7031 (0.7030)  loss_scale: 32768.0000 (33244.1926)  weight_decay: 0.0500 (0.0500)  time: 0.7935  data: 0.0002  max mem: 27222
Epoch: [5]  [1200/2502]  eta: 0:18:11  lr: 0.000384  min_lr: 0.000001  loss: 2.0352 (2.1450)  class_acc: 0.7031 (0.7029)  loss_scale: 32768.0000 (33204.5429)  weight_decay: 0.0500 (0.0500)  time: 0.8110  data: 0.0002  max mem: 27222
Epoch: [5]  [1300/2502]  eta: 0:16:44  lr: 0.000386  min_lr: 0.000001  loss: 2.1191 (2.1425)  class_acc: 0.6875 (0.7031)  loss_scale: 32768.0000 (33271.7356)  weight_decay: 0.0500 (0.0500)  time: 0.7917  data: 0.0002  max mem: 27222
Epoch: [5]  [1400/2502]  eta: 0:15:18  lr: 0.000389  min_lr: 0.000001  loss: 2.1113 (2.1428)  class_acc: 0.7031 (0.7035)  loss_scale: 32768.0000 (33235.7802)  weight_decay: 0.0500 (0.0500)  time: 0.7906  data: 0.0002  max mem: 27222
Epoch: [5]  [1500/2502]  eta: 0:13:53  lr: 0.000392  min_lr: 0.000001  loss: 2.1680 (2.1422)  class_acc: 0.6875 (0.7037)  loss_scale: 32768.0000 (33204.6156)  weight_decay: 0.0500 (0.0500)  time: 0.7937  data: 0.0002  max mem: 27222
Epoch: [5]  [1600/2502]  eta: 0:12:28  lr: 0.000395  min_lr: 0.000001  loss: 2.1094 (2.1421)  class_acc: 0.7031 (0.7035)  loss_scale: 32768.0000 (33177.3442)  weight_decay: 0.0500 (0.0500)  time: 0.8069  data: 0.0002  max mem: 27222
Epoch: [5]  [1700/2502]  eta: 0:11:04  lr: 0.000398  min_lr: 0.000001  loss: 2.1914 (2.1426)  class_acc: 0.6875 (0.7038)  loss_scale: 32768.0000 (33153.2792)  weight_decay: 0.0500 (0.0500)  time: 0.8352  data: 0.0002  max mem: 27222
Epoch: [5]  [1800/2502]  eta: 0:09:40  lr: 0.000400  min_lr: 0.000001  loss: 2.1250 (2.1427)  class_acc: 0.6875 (0.7038)  loss_scale: 32768.0000 (33204.6641)  weight_decay: 0.0500 (0.0500)  time: 0.8003  data: 0.0002  max mem: 27222
Epoch: [5]  [1900/2502]  eta: 0:08:17  lr: 0.000403  min_lr: 0.000001  loss: 2.1602 (2.1437)  class_acc: 0.6875 (0.7035)  loss_scale: 32768.0000 (33181.6938)  weight_decay: 0.0500 (0.0500)  time: 0.7967  data: 0.0002  max mem: 27222
Epoch: [5]  [2000/2502]  eta: 0:06:54  lr: 0.000406  min_lr: 0.000001  loss: 2.0703 (2.1437)  class_acc: 0.7031 (0.7034)  loss_scale: 32768.0000 (33161.0195)  weight_decay: 0.0500 (0.0500)  time: 0.8124  data: 0.0002  max mem: 27222
Epoch: [5]  [2100/2502]  eta: 0:05:31  lr: 0.000409  min_lr: 0.000001  loss: 2.1758 (2.1442)  class_acc: 0.6875 (0.7034)  loss_scale: 32768.0000 (33142.3132)  weight_decay: 0.0500 (0.0500)  time: 0.8304  data: 0.0002  max mem: 27222
Epoch: [5]  [2200/2502]  eta: 0:04:08  lr: 0.000412  min_lr: 0.000001  loss: 2.0215 (2.1429)  class_acc: 0.7344 (0.7038)  loss_scale: 32768.0000 (33125.3067)  weight_decay: 0.0500 (0.0500)  time: 0.8491  data: 0.0003  max mem: 27222
Epoch: [5]  [2300/2502]  eta: 0:02:46  lr: 0.000414  min_lr: 0.000001  loss: 2.1777 (2.1423)  class_acc: 0.6875 (0.7039)  loss_scale: 32768.0000 (33280.6675)  weight_decay: 0.0500 (0.0500)  time: 0.8360  data: 0.0002  max mem: 27222
Epoch: [5]  [2400/2502]  eta: 0:01:23  lr: 0.000417  min_lr: 0.000001  loss: 2.1543 (2.1408)  class_acc: 0.6719 (0.7045)  loss_scale: 32768.0000 (33259.3153)  weight_decay: 0.0500 (0.0500)  time: 0.8137  data: 0.0002  max mem: 27222
Epoch: [5]  [2500/2502]  eta: 0:00:01  lr: 0.000420  min_lr: 0.000001  loss: 2.2188 (2.1422)  class_acc: 0.6719 (0.7040)  loss_scale: 32768.0000 (33239.8592)  weight_decay: 0.0500 (0.0500)  time: 0.7822  data: 0.0010  max mem: 27222
Epoch: [5]  [2501/2502]  eta: 0:00:00  lr: 0.000420  min_lr: 0.000001  loss: 2.2188 (2.1422)  class_acc: 0.6719 (0.7040)  loss_scale: 32768.0000 (33239.8592)  weight_decay: 0.0500 (0.0500)  time: 0.7444  data: 0.0010  max mem: 27222
Epoch: [5] Total time: 0:34:16 (0.8221 s / it)
Averaged stats: lr: 0.000420  min_lr: 0.000001  loss: 2.2188 (2.1444)  class_acc: 0.6719 (0.7023)  loss_scale: 32768.0000 (33239.8592)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Test:  [ 0/98]  eta: 0:09:16  loss: 0.3281 (0.3281)  acc1: 92.1875 (92.1875)  acc5: 100.0000 (100.0000)  time: 5.6808  data: 5.3185  max mem: 27222
Test:  [97/98]  eta: 0:00:00  loss: 0.7537 (0.7431)  acc1: 79.6875 (82.3360)  acc5: 96.8750 (97.0240)  time: 0.3340  data: 0.0002  max mem: 27222
Test: Total time: 0:00:39 (0.4073 s / it)
* Acc@1 82.166 Acc@5 96.778 loss 0.744
Accuracy of the network on the 50000 test images: 82.2%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Test:  [ 0/98]  eta: 0:08:18  loss: 3.6300 (3.6300)  acc1: 84.3750 (84.3750)  acc5: 100.0000 (100.0000)  time: 5.0892  data: 4.6824  max mem: 27222
Test:  [97/98]  eta: 0:00:00  loss: 3.9645 (3.9386)  acc1: 75.0000 (75.5360)  acc5: 95.3125 (94.9760)  time: 0.3354  data: 0.0013  max mem: 27222
Test: Total time: 0:00:40 (0.4132 s / it)
* Acc@1 75.444 Acc@5 94.790 loss 3.937
EMA Accuracy of the network on the 50000 test images: 75.4%
Max accuracy: 75.44%
{"train_lr": 0.0003850056008961437, "train_min_lr": 5.028440347750743e-07, "train_loss": 2.144416064453125, "train_class_acc": 0.70225546875, "train_loss_scale": 33239.8592, "train_weight_decay": 0.04999999999999801, "test_loss": 3.9372067372409667, "test_acc1": 75.44400000061034, "test_acc5": 94.79000000579833, "epoch": 5, "n_parameters": 86860264}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Epoch: [6]  [   0/2502]  eta: 1 day, 6:28:36  lr: 0.000420  min_lr: 0.000001  loss: 2.3359 (2.3359)  class_acc: 0.6406 (0.6406)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 43.8515  data: 40.8487  max mem: 27222
Epoch: [6]  [ 100/2502]  eta: 0:48:31  lr: 0.000423  min_lr: 0.000001  loss: 2.0527 (2.0656)  class_acc: 0.7031 (0.7211)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7871  data: 0.0003  max mem: 27222
Epoch: [6]  [ 200/2502]  eta: 0:38:36  lr: 0.000426  min_lr: 0.000001  loss: 1.9502 (2.0647)  class_acc: 0.7344 (0.7220)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7888  data: 0.0003  max mem: 27222
Epoch: [6]  [ 300/2502]  eta: 0:34:21  lr: 0.000428  min_lr: 0.000001  loss: 2.1484 (2.0711)  class_acc: 0.6875 (0.7203)  loss_scale: 32768.0000 (33203.4551)  weight_decay: 0.0500 (0.0500)  time: 0.7909  data: 0.0002  max mem: 27222
Epoch: [6]  [ 400/2502]  eta: 0:31:37  lr: 0.000431  min_lr: 0.000001  loss: 2.0039 (2.0816)  class_acc: 0.7188 (0.7178)  loss_scale: 32768.0000 (33094.8628)  weight_decay: 0.0500 (0.0500)  time: 0.8211  data: 0.0002  max mem: 27222
Epoch: [6]  [ 500/2502]  eta: 0:29:27  lr: 0.000434  min_lr: 0.000001  loss: 2.0938 (2.0855)  class_acc: 0.7188 (0.7176)  loss_scale: 32768.0000 (33029.6208)  weight_decay: 0.0500 (0.0500)  time: 0.7966  data: 0.0002  max mem: 27222
Epoch: [6]  [ 600/2502]  eta: 0:27:34  lr: 0.000437  min_lr: 0.000001  loss: 2.0059 (2.0873)  class_acc: 0.7344 (0.7163)  loss_scale: 32768.0000 (32986.0899)  weight_decay: 0.0500 (0.0500)  time: 0.8209  data: 0.0002  max mem: 27222
Epoch: [6]  [ 700/2502]  eta: 0:25:50  lr: 0.000440  min_lr: 0.000001  loss: 2.1543 (2.0901)  class_acc: 0.7188 (0.7165)  loss_scale: 32768.0000 (32954.9786)  weight_decay: 0.0500 (0.0500)  time: 0.7887  data: 0.0002  max mem: 27222
Epoch: [6]  [ 800/2502]  eta: 0:24:13  lr: 0.000442  min_lr: 0.000001  loss: 2.0918 (2.0926)  class_acc: 0.7344 (0.7158)  loss_scale: 32768.0000 (32931.6355)  weight_decay: 0.0500 (0.0500)  time: 0.8321  data: 0.0002  max mem: 27222
Epoch: [6]  [ 900/2502]  eta: 0:22:39  lr: 0.000445  min_lr: 0.000001  loss: 2.1934 (2.0957)  class_acc: 0.7031 (0.7155)  loss_scale: 32768.0000 (33058.9478)  weight_decay: 0.0500 (0.0500)  time: 0.7952  data: 0.0002  max mem: 27222
Epoch: [6]  [1000/2502]  eta: 0:21:08  lr: 0.000448  min_lr: 0.000001  loss: 2.0684 (2.0969)  class_acc: 0.7031 (0.7138)  loss_scale: 32768.0000 (33029.8821)  weight_decay: 0.0500 (0.0500)  time: 0.7962  data: 0.0002  max mem: 27222
Epoch: [6]  [1100/2502]  eta: 0:19:39  lr: 0.000451  min_lr: 0.000001  loss: 2.1055 (2.0969)  class_acc: 0.7031 (0.7136)  loss_scale: 32768.0000 (33006.0963)  weight_decay: 0.0500 (0.0500)  time: 0.7993  data: 0.0002  max mem: 27222
Epoch: [6]  [1200/2502]  eta: 0:18:12  lr: 0.000454  min_lr: 0.000001  loss: 2.0781 (2.1013)  class_acc: 0.7031 (0.7127)  loss_scale: 32768.0000 (32986.2714)  weight_decay: 0.0500 (0.0500)  time: 0.8039  data: 0.0002  max mem: 27222
Epoch: [6]  [1300/2502]  eta: 0:16:45  lr: 0.000456  min_lr: 0.000001  loss: 2.0527 (2.1011)  class_acc: 0.7031 (0.7127)  loss_scale: 32768.0000 (32969.4942)  weight_decay: 0.0500 (0.0500)  time: 0.8226  data: 0.0002  max mem: 27222
Epoch: [6]  [1400/2502]  eta: 0:15:20  lr: 0.000459  min_lr: 0.000001  loss: 2.0430 (2.0988)  class_acc: 0.7188 (0.7133)  loss_scale: 32768.0000 (33048.6681)  weight_decay: 0.0500 (0.0500)  time: 0.8369  data: 0.0002  max mem: 27222
Epoch: [6]  [1500/2502]  eta: 0:13:54  lr: 0.000462  min_lr: 0.000001  loss: 2.0781 (2.1010)  class_acc: 0.7344 (0.7130)  loss_scale: 32768.0000 (33029.9694)  weight_decay: 0.0500 (0.0500)  time: 0.7934  data: 0.0002  max mem: 27222
Epoch: [6]  [1600/2502]  eta: 0:12:29  lr: 0.000465  min_lr: 0.000001  loss: 2.0352 (2.1017)  class_acc: 0.7188 (0.7129)  loss_scale: 32768.0000 (33013.6065)  weight_decay: 0.0500 (0.0500)  time: 0.8145  data: 0.0002  max mem: 27222
Epoch: [6]  [1700/2502]  eta: 0:11:05  lr: 0.000468  min_lr: 0.000001  loss: 2.1309 (2.0999)  class_acc: 0.7031 (0.7132)  loss_scale: 32768.0000 (32999.1675)  weight_decay: 0.0500 (0.0500)  time: 0.8422  data: 0.0002  max mem: 27222
Epoch: [6]  [1800/2502]  eta: 0:09:41  lr: 0.000470  min_lr: 0.000001  loss: 2.0410 (2.1009)  class_acc: 0.7344 (0.7131)  loss_scale: 32768.0000 (32986.3320)  weight_decay: 0.0500 (0.0500)  time: 0.7929  data: 0.0002  max mem: 27222
Epoch: [6]  [1900/2502]  eta: 0:08:18  lr: 0.000473  min_lr: 0.000001  loss: 2.0938 (2.1000)  class_acc: 0.7031 (0.7133)  loss_scale: 32768.0000 (33043.7959)  weight_decay: 0.0500 (0.0500)  time: 0.8370  data: 0.0003  max mem: 27222
Epoch: [6]  [2000/2502]  eta: 0:06:54  lr: 0.000476  min_lr: 0.000001  loss: 2.1074 (2.0992)  class_acc: 0.7344 (0.7133)  loss_scale: 32768.0000 (33030.0130)  weight_decay: 0.0500 (0.0500)  time: 0.8157  data: 0.0002  max mem: 27222
Epoch: [6]  [2100/2502]  eta: 0:05:31  lr: 0.000479  min_lr: 0.000001  loss: 2.0703 (2.0993)  class_acc: 0.7031 (0.7131)  loss_scale: 32768.0000 (33017.5421)  weight_decay: 0.0500 (0.0500)  time: 0.7964  data: 0.0002  max mem: 27222
Epoch: [6]  [2200/2502]  eta: 0:04:09  lr: 0.000482  min_lr: 0.000001  loss: 2.0254 (2.0987)  class_acc: 0.7188 (0.7132)  loss_scale: 32768.0000 (33006.2045)  weight_decay: 0.0500 (0.0500)  time: 0.8275  data: 0.0002  max mem: 27222
Epoch: [6]  [2300/2502]  eta: 0:02:46  lr: 0.000484  min_lr: 0.000001  loss: 2.2402 (2.0985)  class_acc: 0.7031 (0.7133)  loss_scale: 32768.0000 (32995.8522)  weight_decay: 0.0500 (0.0500)  time: 0.8052  data: 0.0002  max mem: 27222
Epoch: [6]  [2400/2502]  eta: 0:01:24  lr: 0.000487  min_lr: 0.000001  loss: 2.0918 (2.0978)  class_acc: 0.7188 (0.7135)  loss_scale: 32768.0000 (33040.9529)  weight_decay: 0.0500 (0.0500)  time: 0.7962  data: 0.0002  max mem: 27222
Epoch: [6]  [2500/2502]  eta: 0:00:01  lr: 0.000490  min_lr: 0.000001  loss: 2.0762 (2.0983)  class_acc: 0.6875 (0.7133)  loss_scale: 32768.0000 (33030.1440)  weight_decay: 0.0500 (0.0500)  time: 0.7500  data: 0.0012  max mem: 27222
Epoch: [6]  [2501/2502]  eta: 0:00:00  lr: 0.000490  min_lr: 0.000001  loss: 2.0762 (2.0983)  class_acc: 0.6875 (0.7133)  loss_scale: 32768.0000 (33030.1440)  weight_decay: 0.0500 (0.0500)  time: 0.7101  data: 0.0012  max mem: 27222
Epoch: [6] Total time: 0:34:18 (0.8226 s / it)
Averaged stats: lr: 0.000490  min_lr: 0.000001  loss: 2.0762 (2.1045)  class_acc: 0.6875 (0.7126)  loss_scale: 32768.0000 (33030.1440)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Test:  [ 0/98]  eta: 0:08:20  loss: 0.3439 (0.3439)  acc1: 92.1875 (92.1875)  acc5: 100.0000 (100.0000)  time: 5.1039  data: 4.7698  max mem: 27222
Test:  [97/98]  eta: 0:00:00  loss: 0.7326 (0.7280)  acc1: 81.2500 (82.5920)  acc5: 96.8750 (97.0080)  time: 0.3345  data: 0.0002  max mem: 27222
Test: Total time: 0:00:41 (0.4185 s / it)
* Acc@1 82.498 Acc@5 96.860 loss 0.728
Accuracy of the network on the 50000 test images: 82.5%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Test:  [ 0/98]  eta: 0:09:27  loss: 2.8182 (2.8182)  acc1: 87.5000 (87.5000)  acc5: 100.0000 (100.0000)  time: 5.7868  data: 5.3976  max mem: 27222
Test:  [97/98]  eta: 0:00:00  loss: 3.2190 (3.1580)  acc1: 78.1250 (78.6080)  acc5: 96.8750 (96.1600)  time: 0.3329  data: 0.0005  max mem: 27222
Test: Total time: 0:00:40 (0.4168 s / it)
* Acc@1 78.456 Acc@5 95.910 loss 3.157
EMA Accuracy of the network on the 50000 test images: 78.5%
Max accuracy: 78.46%
{"train_lr": 0.0004550168026884303, "train_min_lr": 5.942835232052238e-07, "train_loss": 2.104515478515625, "train_class_acc": 0.71255703125, "train_loss_scale": 33030.144, "train_weight_decay": 0.04999999999999801, "test_loss": 3.157120476875986, "test_acc1": 78.45600001037597, "test_acc5": 95.91000000335693, "epoch": 6, "n_parameters": 86860264}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Epoch: [7]  [   0/2502]  eta: 1 day, 4:58:45  lr: 0.000490  min_lr: 0.000001  loss: 2.1641 (2.1641)  class_acc: 0.7344 (0.7344)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 41.6967  data: 33.8818  max mem: 27222
Epoch: [7]  [ 100/2502]  eta: 0:47:40  lr: 0.000493  min_lr: 0.000001  loss: 2.0098 (2.0727)  class_acc: 0.7344 (0.7195)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7854  data: 0.0002  max mem: 27222
Epoch: [7]  [ 200/2502]  eta: 0:38:09  lr: 0.000496  min_lr: 0.000001  loss: 2.0410 (2.0605)  class_acc: 0.7344 (0.7226)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8055  data: 0.0002  max mem: 27222
Epoch: [7]  [ 300/2502]  eta: 0:34:09  lr: 0.000498  min_lr: 0.000001  loss: 2.0723 (2.0554)  class_acc: 0.7188 (0.7240)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8156  data: 0.0002  max mem: 27222
Epoch: [7]  [ 400/2502]  eta: 0:31:26  lr: 0.000501  min_lr: 0.000001  loss: 2.2070 (2.0668)  class_acc: 0.6875 (0.7216)  loss_scale: 32768.0000 (33421.7257)  weight_decay: 0.0500 (0.0500)  time: 0.7957  data: 0.0003  max mem: 27222
Epoch: [7]  [ 500/2502]  eta: 0:29:21  lr: 0.000504  min_lr: 0.000001  loss: 2.0645 (2.0683)  class_acc: 0.7344 (0.7210)  loss_scale: 32768.0000 (33291.2415)  weight_decay: 0.0500 (0.0500)  time: 0.8127  data: 0.0002  max mem: 27222
Epoch: [7]  [ 600/2502]  eta: 0:27:29  lr: 0.000507  min_lr: 0.000001  loss: 1.9844 (2.0681)  class_acc: 0.7344 (0.7203)  loss_scale: 32768.0000 (33204.1797)  weight_decay: 0.0500 (0.0500)  time: 0.7945  data: 0.0002  max mem: 27222
Epoch: [7]  [ 700/2502]  eta: 0:25:46  lr: 0.000510  min_lr: 0.000001  loss: 2.1289 (2.0721)  class_acc: 0.7188 (0.7202)  loss_scale: 32768.0000 (33141.9572)  weight_decay: 0.0500 (0.0500)  time: 0.7987  data: 0.0002  max mem: 27222
Epoch: [7]  [ 800/2502]  eta: 0:24:08  lr: 0.000512  min_lr: 0.000001  loss: 1.9980 (2.0713)  class_acc: 0.7031 (0.7210)  loss_scale: 32768.0000 (33095.2709)  weight_decay: 0.0500 (0.0500)  time: 0.7939  data: 0.0003  max mem: 27222
Epoch: [7]  [ 900/2502]  eta: 0:22:35  lr: 0.000515  min_lr: 0.000001  loss: 1.9971 (2.0732)  class_acc: 0.7188 (0.7206)  loss_scale: 32768.0000 (33058.9478)  weight_decay: 0.0500 (0.0500)  time: 0.7922  data: 0.0002  max mem: 27222
Epoch: [7]  [1000/2502]  eta: 0:21:04  lr: 0.000518  min_lr: 0.000001  loss: 1.9961 (2.0708)  class_acc: 0.7188 (0.7206)  loss_scale: 32768.0000 (33160.8232)  weight_decay: 0.0500 (0.0500)  time: 0.7961  data: 0.0002  max mem: 27222
Epoch: [7]  [1100/2502]  eta: 0:19:35  lr: 0.000521  min_lr: 0.000001  loss: 1.9834 (2.0695)  class_acc: 0.7188 (0.7209)  loss_scale: 32768.0000 (33125.1444)  weight_decay: 0.0500 (0.0500)  time: 0.8151  data: 0.0003  max mem: 27222
Epoch: [7]  [1200/2502]  eta: 0:18:07  lr: 0.000524  min_lr: 0.000001  loss: 2.0918 (2.0689)  class_acc: 0.7031 (0.7208)  loss_scale: 32768.0000 (33095.4072)  weight_decay: 0.0500 (0.0500)  time: 0.8110  data: 0.0002  max mem: 27222
Epoch: [7]  [1300/2502]  eta: 0:16:41  lr: 0.000526  min_lr: 0.000001  loss: 2.1113 (2.0688)  class_acc: 0.7031 (0.7211)  loss_scale: 32768.0000 (33070.2414)  weight_decay: 0.0500 (0.0500)  time: 0.7916  data: 0.0002  max mem: 27222
Epoch: [7]  [1400/2502]  eta: 0:15:15  lr: 0.000529  min_lr: 0.000001  loss: 1.9990 (2.0687)  class_acc: 0.7344 (0.7211)  loss_scale: 32768.0000 (33048.6681)  weight_decay: 0.0500 (0.0500)  time: 0.8232  data: 0.0002  max mem: 27222
Epoch: [7]  [1500/2502]  eta: 0:13:50  lr: 0.000532  min_lr: 0.000001  loss: 2.0566 (2.0691)  class_acc: 0.7344 (0.7209)  loss_scale: 32768.0000 (33117.2925)  weight_decay: 0.0500 (0.0500)  time: 0.8056  data: 0.0002  max mem: 27222
Epoch: [7]  [1600/2502]  eta: 0:12:26  lr: 0.000535  min_lr: 0.000001  loss: 1.9365 (2.0667)  class_acc: 0.7500 (0.7214)  loss_scale: 32768.0000 (33095.4753)  weight_decay: 0.0500 (0.0500)  time: 0.8023  data: 0.0002  max mem: 27222
Epoch: [7]  [1700/2502]  eta: 0:11:03  lr: 0.000538  min_lr: 0.000001  loss: 1.9531 (2.0674)  class_acc: 0.7344 (0.7213)  loss_scale: 32768.0000 (33076.2234)  weight_decay: 0.0500 (0.0500)  time: 0.7969  data: 0.0002  max mem: 27222
Epoch: [7]  [1800/2502]  eta: 0:09:39  lr: 0.000540  min_lr: 0.000001  loss: 1.9297 (2.0659)  class_acc: 0.7344 (0.7215)  loss_scale: 32768.0000 (33059.1094)  weight_decay: 0.0500 (0.0500)  time: 0.7949  data: 0.0010  max mem: 27222
Epoch: [7]  [1900/2502]  eta: 0:08:16  lr: 0.000543  min_lr: 0.000001  loss: 2.1035 (2.0686)  class_acc: 0.7031 (0.7205)  loss_scale: 32768.0000 (33043.7959)  weight_decay: 0.0500 (0.0500)  time: 0.7929  data: 0.0002  max mem: 27222
Epoch: [7]  [2000/2502]  eta: 0:06:53  lr: 0.000546  min_lr: 0.000001  loss: 2.0684 (2.0688)  class_acc: 0.7188 (0.7205)  loss_scale: 32768.0000 (33161.0195)  weight_decay: 0.0500 (0.0500)  time: 0.7933  data: 0.0002  max mem: 27222
Epoch: [7]  [2100/2502]  eta: 0:05:30  lr: 0.000549  min_lr: 0.000001  loss: 2.0449 (2.0704)  class_acc: 0.7031 (0.7201)  loss_scale: 32768.0000 (33142.3132)  weight_decay: 0.0500 (0.0500)  time: 0.7915  data: 0.0002  max mem: 27222
Epoch: [7]  [2200/2502]  eta: 0:04:07  lr: 0.000552  min_lr: 0.000001  loss: 2.0605 (2.0714)  class_acc: 0.7031 (0.7199)  loss_scale: 32768.0000 (33125.3067)  weight_decay: 0.0500 (0.0500)  time: 0.8445  data: 0.0002  max mem: 27222
Epoch: [7]  [2300/2502]  eta: 0:02:45  lr: 0.000554  min_lr: 0.000001  loss: 2.0859 (2.0719)  class_acc: 0.7188 (0.7198)  loss_scale: 32768.0000 (33109.7784)  weight_decay: 0.0500 (0.0500)  time: 0.7911  data: 0.0002  max mem: 27222
Epoch: [7]  [2400/2502]  eta: 0:01:23  lr: 0.000557  min_lr: 0.000001  loss: 2.1543 (2.0723)  class_acc: 0.6875 (0.7197)  loss_scale: 32768.0000 (33095.5435)  weight_decay: 0.0500 (0.0500)  time: 0.7961  data: 0.0002  max mem: 27222
Epoch: [7]  [2500/2502]  eta: 0:00:01  lr: 0.000560  min_lr: 0.000001  loss: 2.0566 (2.0719)  class_acc: 0.7188 (0.7197)  loss_scale: 32768.0000 (33135.0016)  weight_decay: 0.0500 (0.0500)  time: 0.7487  data: 0.0009  max mem: 27222
Epoch: [7]  [2501/2502]  eta: 0:00:00  lr: 0.000560  min_lr: 0.000001  loss: 2.0566 (2.0719)  class_acc: 0.7188 (0.7197)  loss_scale: 32768.0000 (33135.0016)  weight_decay: 0.0500 (0.0500)  time: 0.7093  data: 0.0009  max mem: 27222
Epoch: [7] Total time: 0:34:09 (0.8191 s / it)
Averaged stats: lr: 0.000560  min_lr: 0.000001  loss: 2.0566 (2.0748)  class_acc: 0.7188 (0.7193)  loss_scale: 32768.0000 (33135.0016)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Test:  [ 0/98]  eta: 0:09:51  loss: 0.3351 (0.3351)  acc1: 93.7500 (93.7500)  acc5: 100.0000 (100.0000)  time: 6.0340  data: 5.6992  max mem: 27222
Test:  [97/98]  eta: 0:00:00  loss: 0.7440 (0.7143)  acc1: 81.2500 (83.1360)  acc5: 96.8750 (96.8960)  time: 0.3343  data: 0.0002  max mem: 27222
Test: Total time: 0:00:40 (0.4163 s / it)
* Acc@1 82.518 Acc@5 96.804 loss 0.724
Accuracy of the network on the 50000 test images: 82.5%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Test:  [ 0/98]  eta: 0:10:54  loss: 2.1104 (2.1104)  acc1: 87.5000 (87.5000)  acc5: 100.0000 (100.0000)  time: 6.6808  data: 6.3422  max mem: 27222
Test:  [97/98]  eta: 0:00:00  loss: 2.4877 (2.4552)  acc1: 78.1250 (80.3680)  acc5: 96.8750 (96.7360)  time: 0.3360  data: 0.0003  max mem: 27222
Test: Total time: 0:00:42 (0.4300 s / it)
* Acc@1 80.456 Acc@5 96.530 loss 2.455
EMA Accuracy of the network on the 50000 test images: 80.5%
Max accuracy: 80.46%
{"train_lr": 0.0005250280044807166, "train_min_lr": 6.857230116353716e-07, "train_loss": 2.074794775390625, "train_class_acc": 0.71934765625, "train_loss_scale": 33135.0016, "train_weight_decay": 0.04999999999999801, "test_loss": 2.454882043965009, "test_acc1": 80.45600001220703, "test_acc5": 96.53000000335693, "epoch": 7, "n_parameters": 86860264}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Epoch: [8]  [   0/2502]  eta: 1 day, 4:25:24  lr: 0.000560  min_lr: 0.000001  loss: 2.1992 (2.1992)  class_acc: 0.6719 (0.6719)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 40.8971  data: 35.9150  max mem: 27222
Epoch: [8]  [ 100/2502]  eta: 0:47:17  lr: 0.000563  min_lr: 0.000001  loss: 2.0273 (2.0753)  class_acc: 0.7344 (0.7234)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7854  data: 0.0002  max mem: 27222
Epoch: [8]  [ 200/2502]  eta: 0:37:55  lr: 0.000566  min_lr: 0.000001  loss: 2.0430 (2.0570)  class_acc: 0.7031 (0.7270)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7875  data: 0.0002  max mem: 27222
Epoch: [8]  [ 300/2502]  eta: 0:33:59  lr: 0.000568  min_lr: 0.000001  loss: 2.0020 (2.0508)  class_acc: 0.7344 (0.7284)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7922  data: 0.0002  max mem: 27222
Epoch: [8]  [ 400/2502]  eta: 0:31:17  lr: 0.000571  min_lr: 0.000001  loss: 2.1484 (2.0553)  class_acc: 0.7188 (0.7266)  loss_scale: 16384.0000 (30234.8130)  weight_decay: 0.0500 (0.0500)  time: 0.7893  data: 0.0002  max mem: 27222
Epoch: [8]  [ 500/2502]  eta: 0:29:09  lr: 0.000574  min_lr: 0.000001  loss: 1.9951 (2.0529)  class_acc: 0.7344 (0.7262)  loss_scale: 16384.0000 (27470.1796)  weight_decay: 0.0500 (0.0500)  time: 0.7916  data: 0.0002  max mem: 27222
Epoch: [8]  [ 600/2502]  eta: 0:27:19  lr: 0.000577  min_lr: 0.000001  loss: 2.0859 (2.0502)  class_acc: 0.7188 (0.7264)  loss_scale: 16384.0000 (25625.5574)  weight_decay: 0.0500 (0.0500)  time: 0.8113  data: 0.0002  max mem: 27222
Epoch: [8]  [ 700/2502]  eta: 0:25:39  lr: 0.000580  min_lr: 0.000001  loss: 1.9805 (2.0451)  class_acc: 0.7500 (0.7276)  loss_scale: 16384.0000 (24307.2183)  weight_decay: 0.0500 (0.0500)  time: 0.7886  data: 0.0002  max mem: 27222
Epoch: [8]  [ 800/2502]  eta: 0:24:02  lr: 0.000582  min_lr: 0.000001  loss: 2.0312 (2.0418)  class_acc: 0.7031 (0.7280)  loss_scale: 16384.0000 (23318.0524)  weight_decay: 0.0500 (0.0500)  time: 0.7934  data: 0.0002  max mem: 27222
Epoch: [8]  [ 900/2502]  eta: 0:22:28  lr: 0.000585  min_lr: 0.000001  loss: 1.9580 (2.0436)  class_acc: 0.7500 (0.7276)  loss_scale: 32768.0000 (23384.9323)  weight_decay: 0.0500 (0.0500)  time: 0.7978  data: 0.0002  max mem: 27222
Epoch: [8]  [1000/2502]  eta: 0:20:58  lr: 0.000588  min_lr: 0.000001  loss: 2.0332 (2.0446)  class_acc: 0.7344 (0.7276)  loss_scale: 32768.0000 (24322.3017)  weight_decay: 0.0500 (0.0500)  time: 0.8308  data: 0.0003  max mem: 27222
Epoch: [8]  [1100/2502]  eta: 0:19:31  lr: 0.000591  min_lr: 0.000001  loss: 1.9854 (2.0449)  class_acc: 0.7344 (0.7274)  loss_scale: 32768.0000 (25089.3951)  weight_decay: 0.0500 (0.0500)  time: 0.8191  data: 0.0003  max mem: 27222
Epoch: [8]  [1200/2502]  eta: 0:18:04  lr: 0.000594  min_lr: 0.000001  loss: 2.0586 (2.0456)  class_acc: 0.7188 (0.7271)  loss_scale: 32768.0000 (25728.7460)  weight_decay: 0.0500 (0.0500)  time: 0.7923  data: 0.0002  max mem: 27222
Epoch: [8]  [1300/2502]  eta: 0:16:38  lr: 0.000596  min_lr: 0.000001  loss: 2.0840 (2.0464)  class_acc: 0.7031 (0.7267)  loss_scale: 32768.0000 (26269.8109)  weight_decay: 0.0500 (0.0500)  time: 0.8018  data: 0.0002  max mem: 27222
Epoch: [8]  [1400/2502]  eta: 0:15:13  lr: 0.000599  min_lr: 0.000001  loss: 1.9785 (2.0458)  class_acc: 0.7188 (0.7268)  loss_scale: 32768.0000 (26827.1920)  weight_decay: 0.0500 (0.0500)  time: 0.8194  data: 0.0002  max mem: 27222
Epoch: [8]  [1500/2502]  eta: 0:13:49  lr: 0.000602  min_lr: 0.000001  loss: 2.1309 (2.0475)  class_acc: 0.7344 (0.7266)  loss_scale: 32768.0000 (27222.9820)  weight_decay: 0.0500 (0.0500)  time: 0.8611  data: 0.0002  max mem: 27222
Epoch: [8]  [1600/2502]  eta: 0:12:25  lr: 0.000605  min_lr: 0.000001  loss: 2.0977 (2.0496)  class_acc: 0.7344 (0.7263)  loss_scale: 32768.0000 (27569.3292)  weight_decay: 0.0500 (0.0500)  time: 0.8106  data: 0.0002  max mem: 27222
Epoch: [8]  [1700/2502]  eta: 0:11:02  lr: 0.000608  min_lr: 0.000001  loss: 1.9922 (2.0496)  class_acc: 0.7188 (0.7260)  loss_scale: 32768.0000 (27874.9536)  weight_decay: 0.0500 (0.0500)  time: 0.8394  data: 0.0002  max mem: 27222
Epoch: [8]  [1800/2502]  eta: 0:09:39  lr: 0.000610  min_lr: 0.000001  loss: 1.9600 (2.0509)  class_acc: 0.7500 (0.7259)  loss_scale: 32768.0000 (28146.6385)  weight_decay: 0.0500 (0.0500)  time: 0.8056  data: 0.0002  max mem: 27222
Epoch: [8]  [1900/2502]  eta: 0:08:16  lr: 0.000613  min_lr: 0.000001  loss: 2.0664 (2.0504)  class_acc: 0.7188 (0.7257)  loss_scale: 32768.0000 (28527.6381)  weight_decay: 0.0500 (0.0500)  time: 0.8093  data: 0.0003  max mem: 27222
Epoch: [8]  [2000/2502]  eta: 0:06:53  lr: 0.000616  min_lr: 0.000001  loss: 2.0410 (2.0518)  class_acc: 0.7188 (0.7253)  loss_scale: 32768.0000 (28739.5502)  weight_decay: 0.0500 (0.0500)  time: 0.7979  data: 0.0002  max mem: 27222
Epoch: [8]  [2100/2502]  eta: 0:05:30  lr: 0.000619  min_lr: 0.000001  loss: 1.9668 (2.0514)  class_acc: 0.7500 (0.7254)  loss_scale: 32768.0000 (28931.2899)  weight_decay: 0.0500 (0.0500)  time: 0.8141  data: 0.0002  max mem: 27222
Epoch: [8]  [2200/2502]  eta: 0:04:07  lr: 0.000622  min_lr: 0.000001  loss: 2.0645 (2.0502)  class_acc: 0.7188 (0.7256)  loss_scale: 32768.0000 (29105.6065)  weight_decay: 0.0500 (0.0500)  time: 0.8040  data: 0.0002  max mem: 27222
Epoch: [8]  [2300/2502]  eta: 0:02:45  lr: 0.000624  min_lr: 0.000001  loss: 2.0195 (2.0494)  class_acc: 0.7500 (0.7256)  loss_scale: 32768.0000 (29264.7718)  weight_decay: 0.0500 (0.0500)  time: 0.7955  data: 0.0002  max mem: 27222
Epoch: [8]  [2400/2502]  eta: 0:01:23  lr: 0.000627  min_lr: 0.000001  loss: 2.0469 (2.0497)  class_acc: 0.7188 (0.7255)  loss_scale: 32768.0000 (29410.6789)  weight_decay: 0.0500 (0.0500)  time: 0.8410  data: 0.0002  max mem: 27222
Epoch: [8]  [2500/2502]  eta: 0:00:01  lr: 0.000630  min_lr: 0.000001  loss: 1.9551 (2.0496)  class_acc: 0.7500 (0.7256)  loss_scale: 32768.0000 (29596.0576)  weight_decay: 0.0500 (0.0500)  time: 0.7452  data: 0.0009  max mem: 27222
Epoch: [8]  [2501/2502]  eta: 0:00:00  lr: 0.000630  min_lr: 0.000001  loss: 1.9551 (2.0496)  class_acc: 0.7500 (0.7256)  loss_scale: 32768.0000 (29596.0576)  weight_decay: 0.0500 (0.0500)  time: 0.7073  data: 0.0009  max mem: 27222
Epoch: [8] Total time: 0:34:09 (0.8190 s / it)
Averaged stats: lr: 0.000630  min_lr: 0.000001  loss: 1.9551 (2.0516)  class_acc: 0.7500 (0.7250)  loss_scale: 32768.0000 (29596.0576)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Test:  [ 0/98]  eta: 0:09:08  loss: 0.3276 (0.3276)  acc1: 93.7500 (93.7500)  acc5: 100.0000 (100.0000)  time: 5.5955  data: 5.2440  max mem: 27222
Test:  [97/98]  eta: 0:00:00  loss: 0.7190 (0.7313)  acc1: 82.8125 (82.6240)  acc5: 96.8750 (96.8640)  time: 0.3319  data: 0.0009  max mem: 27222
Test: Total time: 0:00:40 (0.4117 s / it)
* Acc@1 82.432 Acc@5 96.844 loss 0.731
Accuracy of the network on the 50000 test images: 82.4%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Test:  [ 0/98]  eta: 0:09:32  loss: 1.5330 (1.5330)  acc1: 89.0625 (89.0625)  acc5: 100.0000 (100.0000)  time: 5.8390  data: 5.4738  max mem: 27222
Test:  [97/98]  eta: 0:00:00  loss: 1.8875 (1.8751)  acc1: 78.1250 (81.6960)  acc5: 98.4375 (97.0720)  time: 0.3377  data: 0.0008  max mem: 27222
Test: Total time: 0:00:41 (0.4215 s / it)
* Acc@1 81.862 Acc@5 96.884 loss 1.874
EMA Accuracy of the network on the 50000 test images: 81.9%
Max accuracy: 81.86%
{"train_lr": 0.0005950392062730036, "train_min_lr": 7.77162500065521e-07, "train_loss": 2.051620068359375, "train_class_acc": 0.72500546875, "train_loss_scale": 29596.0576, "train_weight_decay": 0.04999999999999801, "test_loss": 1.8742670608418328, "test_acc1": 81.86200001251221, "test_acc5": 96.88400000579834, "epoch": 8, "n_parameters": 86860264}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Epoch: [9]  [   0/2502]  eta: 1 day, 5:08:41  lr: 0.000630  min_lr: 0.000001  loss: 2.5039 (2.5039)  class_acc: 0.6250 (0.6250)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 41.9351  data: 41.1026  max mem: 27222
Epoch: [9]  [ 100/2502]  eta: 0:47:46  lr: 0.000633  min_lr: 0.000001  loss: 1.8643 (2.0039)  class_acc: 0.7656 (0.7440)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7891  data: 0.0002  max mem: 27222
Epoch: [9]  [ 200/2502]  eta: 0:38:12  lr: 0.000636  min_lr: 0.000001  loss: 1.9502 (2.0146)  class_acc: 0.7344 (0.7380)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7958  data: 0.0002  max mem: 27222
Epoch: [9]  [ 300/2502]  eta: 0:34:09  lr: 0.000639  min_lr: 0.000001  loss: 2.0801 (2.0322)  class_acc: 0.7031 (0.7336)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8007  data: 0.0002  max mem: 27222
Epoch: [9]  [ 400/2502]  eta: 0:31:29  lr: 0.000641  min_lr: 0.000001  loss: 1.9922 (2.0304)  class_acc: 0.7188 (0.7339)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7953  data: 0.0002  max mem: 27222
Epoch: [9]  [ 500/2502]  eta: 0:29:25  lr: 0.000644  min_lr: 0.000001  loss: 1.9893 (2.0301)  class_acc: 0.7344 (0.7327)  loss_scale: 32768.0000 (33552.8623)  weight_decay: 0.0500 (0.0500)  time: 0.8418  data: 0.0002  max mem: 27222
Epoch: [9]  [ 600/2502]  eta: 0:27:32  lr: 0.000647  min_lr: 0.000001  loss: 1.8672 (2.0264)  class_acc: 0.7656 (0.7339)  loss_scale: 32768.0000 (33422.2696)  weight_decay: 0.0500 (0.0500)  time: 0.7948  data: 0.0004  max mem: 27222
Epoch: [9]  [ 700/2502]  eta: 0:25:48  lr: 0.000650  min_lr: 0.000001  loss: 2.0195 (2.0239)  class_acc: 0.7344 (0.7345)  loss_scale: 32768.0000 (33328.9358)  weight_decay: 0.0500 (0.0500)  time: 0.7928  data: 0.0002  max mem: 27222
Epoch: [9]  [ 800/2502]  eta: 0:24:12  lr: 0.000653  min_lr: 0.000001  loss: 2.1426 (2.0259)  class_acc: 0.7031 (0.7338)  loss_scale: 32768.0000 (33258.9064)  weight_decay: 0.0500 (0.0500)  time: 0.7891  data: 0.0005  max mem: 27222
Epoch: [9]  [ 900/2502]  eta: 0:22:42  lr: 0.000655  min_lr: 0.000001  loss: 1.9990 (2.0263)  class_acc: 0.7500 (0.7336)  loss_scale: 32768.0000 (33204.4218)  weight_decay: 0.0500 (0.0500)  time: 0.8290  data: 0.0002  max mem: 27222
Epoch: [9]  [1000/2502]  eta: 0:21:10  lr: 0.000658  min_lr: 0.000001  loss: 2.0059 (2.0270)  class_acc: 0.7031 (0.7326)  loss_scale: 32768.0000 (33422.7053)  weight_decay: 0.0500 (0.0500)  time: 0.8233  data: 0.0002  max mem: 27222
Epoch: [9]  [1100/2502]  eta: 0:19:44  lr: 0.000661  min_lr: 0.000001  loss: 1.9297 (2.0265)  class_acc: 0.7500 (0.7322)  loss_scale: 32768.0000 (33363.2407)  weight_decay: 0.0500 (0.0500)  time: 0.8303  data: 0.0002  max mem: 27222
Epoch: [9]  [1200/2502]  eta: 0:18:15  lr: 0.000664  min_lr: 0.000001  loss: 1.9307 (2.0248)  class_acc: 0.7344 (0.7325)  loss_scale: 32768.0000 (33313.6786)  weight_decay: 0.0500 (0.0500)  time: 0.8416  data: 0.0002  max mem: 27222
Epoch: [9]  [1300/2502]  eta: 0:16:47  lr: 0.000667  min_lr: 0.000001  loss: 2.0703 (2.0257)  class_acc: 0.7031 (0.7322)  loss_scale: 32768.0000 (33271.7356)  weight_decay: 0.0500 (0.0500)  time: 0.7948  data: 0.0002  max mem: 27222
Epoch: [9]  [1400/2502]  eta: 0:15:21  lr: 0.000669  min_lr: 0.000001  loss: 1.9414 (2.0264)  class_acc: 0.7500 (0.7323)  loss_scale: 32768.0000 (33235.7802)  weight_decay: 0.0500 (0.0500)  time: 0.8333  data: 0.0010  max mem: 27222
Epoch: [9]  [1500/2502]  eta: 0:13:56  lr: 0.000672  min_lr: 0.000001  loss: 2.1074 (2.0288)  class_acc: 0.7188 (0.7316)  loss_scale: 32768.0000 (33291.9387)  weight_decay: 0.0500 (0.0500)  time: 0.8459  data: 0.0002  max mem: 27222
Epoch: [9]  [1600/2502]  eta: 0:12:31  lr: 0.000675  min_lr: 0.000001  loss: 2.0059 (2.0298)  class_acc: 0.7188 (0.7313)  loss_scale: 32768.0000 (33259.2130)  weight_decay: 0.0500 (0.0500)  time: 0.8176  data: 0.0002  max mem: 27222
Epoch: [9]  [1700/2502]  eta: 0:11:06  lr: 0.000678  min_lr: 0.000001  loss: 2.0840 (2.0327)  class_acc: 0.7031 (0.7304)  loss_scale: 32768.0000 (33230.3351)  weight_decay: 0.0500 (0.0500)  time: 0.7927  data: 0.0002  max mem: 27222
Epoch: [9]  [1800/2502]  eta: 0:09:42  lr: 0.000681  min_lr: 0.000001  loss: 1.9512 (2.0313)  class_acc: 0.7500 (0.7306)  loss_scale: 16384.0000 (32858.9717)  weight_decay: 0.0500 (0.0500)  time: 0.7946  data: 0.0002  max mem: 27222
Epoch: [9]  [1900/2502]  eta: 0:08:19  lr: 0.000683  min_lr: 0.000001  loss: 1.9512 (2.0317)  class_acc: 0.7656 (0.7309)  loss_scale: 16384.0000 (31992.3240)  weight_decay: 0.0500 (0.0500)  time: 0.8271  data: 0.0002  max mem: 27222
Epoch: [9]  [2000/2502]  eta: 0:06:55  lr: 0.000686  min_lr: 0.000001  loss: 1.9873 (2.0321)  class_acc: 0.7344 (0.7308)  loss_scale: 16384.0000 (31212.2979)  weight_decay: 0.0500 (0.0500)  time: 0.7924  data: 0.0002  max mem: 27222
Epoch: [9]  [2100/2502]  eta: 0:05:32  lr: 0.000689  min_lr: 0.000001  loss: 1.9648 (2.0316)  class_acc: 0.7500 (0.7309)  loss_scale: 16384.0000 (30506.5245)  weight_decay: 0.0500 (0.0500)  time: 0.8355  data: 0.0002  max mem: 27222
Epoch: [9]  [2200/2502]  eta: 0:04:09  lr: 0.000692  min_lr: 0.000001  loss: 1.9727 (2.0312)  class_acc: 0.7344 (0.7310)  loss_scale: 16384.0000 (29864.8832)  weight_decay: 0.0500 (0.0500)  time: 0.7887  data: 0.0004  max mem: 27222
Epoch: [9]  [2300/2502]  eta: 0:02:46  lr: 0.000695  min_lr: 0.000001  loss: 1.9658 (2.0313)  class_acc: 0.7344 (0.7309)  loss_scale: 32768.0000 (29435.6610)  weight_decay: 0.0500 (0.0500)  time: 0.7965  data: 0.0002  max mem: 27222
Epoch: [9]  [2400/2502]  eta: 0:01:24  lr: 0.000697  min_lr: 0.000001  loss: 2.0156 (2.0341)  class_acc: 0.7188 (0.7303)  loss_scale: 32768.0000 (29574.4506)  weight_decay: 0.0500 (0.0500)  time: 0.8111  data: 0.0002  max mem: 27222
Epoch: [9]  [2500/2502]  eta: 0:00:01  lr: 0.000700  min_lr: 0.000001  loss: 2.0469 (2.0352)  class_acc: 0.7188 (0.7300)  loss_scale: 32768.0000 (29700.9152)  weight_decay: 0.0500 (0.0500)  time: 0.7473  data: 0.0009  max mem: 27222
Epoch: [9]  [2501/2502]  eta: 0:00:00  lr: 0.000700  min_lr: 0.000001  loss: 2.0469 (2.0352)  class_acc: 0.7188 (0.7300)  loss_scale: 32768.0000 (29700.9152)  weight_decay: 0.0500 (0.0500)  time: 0.7092  data: 0.0009  max mem: 27222
Epoch: [9] Total time: 0:34:20 (0.8237 s / it)
Averaged stats: lr: 0.000700  min_lr: 0.000001  loss: 2.0469 (2.0331)  class_acc: 0.7188 (0.7302)  loss_scale: 32768.0000 (29700.9152)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Test:  [ 0/98]  eta: 0:10:18  loss: 0.3671 (0.3671)  acc1: 93.7500 (93.7500)  acc5: 98.4375 (98.4375)  time: 6.3114  data: 5.9773  max mem: 27222
Test:  [97/98]  eta: 0:00:00  loss: 0.7118 (0.7230)  acc1: 80.9524 (82.4480)  acc5: 96.8750 (96.9920)  time: 0.3322  data: 0.0005  max mem: 27222
Test: Total time: 0:00:40 (0.4165 s / it)
* Acc@1 82.718 Acc@5 96.964 loss 0.723
Accuracy of the network on the 50000 test images: 82.7%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Test:  [ 0/98]  eta: 0:05:24  loss: 1.1085 (1.1085)  acc1: 90.6250 (90.6250)  acc5: 100.0000 (100.0000)  time: 3.3108  data: 2.9676  max mem: 27222
Test:  [97/98]  eta: 0:00:00  loss: 1.4437 (1.4376)  acc1: 79.6875 (82.6400)  acc5: 98.4375 (97.2320)  time: 0.3351  data: 0.0003  max mem: 27222
Test: Total time: 0:00:40 (0.4113 s / it)
* Acc@1 82.756 Acc@5 97.124 loss 1.436
EMA Accuracy of the network on the 50000 test images: 82.8%
Max accuracy: 82.76%
{"train_lr": 0.0006650504080652901, "train_min_lr": 8.686019884956696e-07, "train_loss": 2.03310595703125, "train_class_acc": 0.7301796875, "train_loss_scale": 29700.9152, "train_weight_decay": 0.04999999999999801, "test_loss": 1.4356433343218298, "test_acc1": 82.7560000100708, "test_acc5": 97.12400000457764, "epoch": 9, "n_parameters": 86860264}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Epoch: [10]  [   0/2502]  eta: 1 day, 4:35:53  lr: 0.000700  min_lr: 0.000001  loss: 1.8613 (1.8613)  class_acc: 0.7969 (0.7969)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 41.1486  data: 36.2425  max mem: 27222
Epoch: [10]  [ 100/2502]  eta: 0:47:28  lr: 0.000700  min_lr: 0.000001  loss: 2.0801 (2.0225)  class_acc: 0.7188 (0.7302)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7885  data: 0.0013  max mem: 27222
Epoch: [10]  [ 200/2502]  eta: 0:38:10  lr: 0.000700  min_lr: 0.000001  loss: 1.9473 (2.0108)  class_acc: 0.7344 (0.7359)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7957  data: 0.0002  max mem: 27222
Epoch: [10]  [ 300/2502]  eta: 0:34:07  lr: 0.000700  min_lr: 0.000001  loss: 1.9648 (2.0009)  class_acc: 0.7500 (0.7398)  loss_scale: 32768.0000 (33203.4551)  weight_decay: 0.0500 (0.0500)  time: 0.7941  data: 0.0003  max mem: 27222
Epoch: [10]  [ 400/2502]  eta: 0:31:22  lr: 0.000700  min_lr: 0.000001  loss: 1.9961 (2.0015)  class_acc: 0.7500 (0.7397)  loss_scale: 32768.0000 (33094.8628)  weight_decay: 0.0500 (0.0500)  time: 0.7943  data: 0.0002  max mem: 27222
Epoch: [10]  [ 500/2502]  eta: 0:29:18  lr: 0.000700  min_lr: 0.000001  loss: 1.9717 (2.0066)  class_acc: 0.7188 (0.7376)  loss_scale: 32768.0000 (33029.6208)  weight_decay: 0.0500 (0.0500)  time: 0.8355  data: 0.0002  max mem: 27222
Epoch: [10]  [ 600/2502]  eta: 0:27:28  lr: 0.000700  min_lr: 0.000001  loss: 1.9678 (2.0056)  class_acc: 0.7500 (0.7379)  loss_scale: 32768.0000 (32986.0899)  weight_decay: 0.0500 (0.0500)  time: 0.8260  data: 0.0002  max mem: 27222
Epoch: [10]  [ 700/2502]  eta: 0:25:47  lr: 0.000700  min_lr: 0.000001  loss: 1.9502 (2.0060)  class_acc: 0.7500 (0.7375)  loss_scale: 32768.0000 (32954.9786)  weight_decay: 0.0500 (0.0500)  time: 0.7914  data: 0.0002  max mem: 27222
Epoch: [10]  [ 800/2502]  eta: 0:24:10  lr: 0.000700  min_lr: 0.000001  loss: 1.9365 (2.0062)  class_acc: 0.7344 (0.7371)  loss_scale: 32768.0000 (32931.6355)  weight_decay: 0.0500 (0.0500)  time: 0.7857  data: 0.0002  max mem: 27222
Epoch: [10]  [ 900/2502]  eta: 0:22:36  lr: 0.000700  min_lr: 0.000001  loss: 1.9932 (2.0064)  class_acc: 0.7344 (0.7368)  loss_scale: 32768.0000 (33058.9478)  weight_decay: 0.0500 (0.0500)  time: 0.8079  data: 0.0002  max mem: 27222
Epoch: [10]  [1000/2502]  eta: 0:21:05  lr: 0.000700  min_lr: 0.000001  loss: 1.9619 (2.0058)  class_acc: 0.7500 (0.7374)  loss_scale: 32768.0000 (33029.8821)  weight_decay: 0.0500 (0.0500)  time: 0.7913  data: 0.0002  max mem: 27222
Epoch: [10]  [1100/2502]  eta: 0:19:37  lr: 0.000700  min_lr: 0.000001  loss: 1.9873 (2.0046)  class_acc: 0.7344 (0.7378)  loss_scale: 32768.0000 (33006.0963)  weight_decay: 0.0500 (0.0500)  time: 0.7934  data: 0.0002  max mem: 27222
Epoch: [10]  [1200/2502]  eta: 0:18:10  lr: 0.000700  min_lr: 0.000001  loss: 2.0449 (2.0060)  class_acc: 0.7344 (0.7378)  loss_scale: 32768.0000 (32986.2714)  weight_decay: 0.0500 (0.0500)  time: 0.7977  data: 0.0002  max mem: 27222
Epoch: [10]  [1300/2502]  eta: 0:16:44  lr: 0.000700  min_lr: 0.000001  loss: 2.1035 (2.0082)  class_acc: 0.7188 (0.7369)  loss_scale: 32768.0000 (32969.4942)  weight_decay: 0.0500 (0.0500)  time: 0.8207  data: 0.0002  max mem: 27222
Epoch: [10]  [1400/2502]  eta: 0:15:17  lr: 0.000700  min_lr: 0.000001  loss: 1.9258 (2.0098)  class_acc: 0.7188 (0.7364)  loss_scale: 32768.0000 (33048.6681)  weight_decay: 0.0500 (0.0500)  time: 0.8093  data: 0.0002  max mem: 27222
Epoch: [10]  [1500/2502]  eta: 0:13:52  lr: 0.000700  min_lr: 0.000001  loss: 2.0664 (2.0085)  class_acc: 0.7188 (0.7368)  loss_scale: 32768.0000 (33029.9694)  weight_decay: 0.0500 (0.0500)  time: 0.7914  data: 0.0002  max mem: 27222
Epoch: [10]  [1600/2502]  eta: 0:12:27  lr: 0.000700  min_lr: 0.000001  loss: 1.9932 (2.0087)  class_acc: 0.7344 (0.7370)  loss_scale: 32768.0000 (33013.6065)  weight_decay: 0.0500 (0.0500)  time: 0.8038  data: 0.0002  max mem: 27222
Epoch: [10]  [1700/2502]  eta: 0:11:03  lr: 0.000700  min_lr: 0.000001  loss: 2.0059 (2.0090)  class_acc: 0.7188 (0.7367)  loss_scale: 32768.0000 (32999.1675)  weight_decay: 0.0500 (0.0500)  time: 0.7943  data: 0.0002  max mem: 27222
Epoch: [10]  [1800/2502]  eta: 0:09:39  lr: 0.000699  min_lr: 0.000001  loss: 1.9580 (2.0085)  class_acc: 0.7500 (0.7367)  loss_scale: 32768.0000 (32986.3320)  weight_decay: 0.0500 (0.0500)  time: 0.8230  data: 0.0002  max mem: 27222
Epoch: [10]  [1900/2502]  eta: 0:08:16  lr: 0.000699  min_lr: 0.000001  loss: 1.9912 (2.0081)  class_acc: 0.7344 (0.7366)  loss_scale: 32768.0000 (33112.7449)  weight_decay: 0.0500 (0.0500)  time: 0.7976  data: 0.0002  max mem: 27222
Epoch: [10]  [2000/2502]  eta: 0:06:53  lr: 0.000699  min_lr: 0.000001  loss: 2.0059 (2.0085)  class_acc: 0.7188 (0.7366)  loss_scale: 32768.0000 (33095.5162)  weight_decay: 0.0500 (0.0500)  time: 0.8071  data: 0.0002  max mem: 27222
Epoch: [10]  [2100/2502]  eta: 0:05:30  lr: 0.000699  min_lr: 0.000001  loss: 2.0098 (2.0092)  class_acc: 0.7500 (0.7365)  loss_scale: 32768.0000 (33079.9277)  weight_decay: 0.0500 (0.0500)  time: 0.8331  data: 0.0003  max mem: 27222
Epoch: [10]  [2200/2502]  eta: 0:04:08  lr: 0.000699  min_lr: 0.000001  loss: 2.0117 (2.0096)  class_acc: 0.7188 (0.7364)  loss_scale: 32768.0000 (33065.7556)  weight_decay: 0.0500 (0.0500)  time: 0.8013  data: 0.0003  max mem: 27222
Epoch: [10]  [2300/2502]  eta: 0:02:45  lr: 0.000699  min_lr: 0.000001  loss: 1.9102 (2.0099)  class_acc: 0.7344 (0.7361)  loss_scale: 32768.0000 (33052.8153)  weight_decay: 0.0500 (0.0500)  time: 0.8077  data: 0.0002  max mem: 27222
Epoch: [10]  [2400/2502]  eta: 0:01:23  lr: 0.000699  min_lr: 0.000001  loss: 1.9199 (2.0101)  class_acc: 0.7500 (0.7362)  loss_scale: 32768.0000 (33259.3153)  weight_decay: 0.0500 (0.0500)  time: 0.8012  data: 0.0002  max mem: 27222
Epoch: [10]  [2500/2502]  eta: 0:00:01  lr: 0.000699  min_lr: 0.000001  loss: 1.9590 (2.0100)  class_acc: 0.7344 (0.7362)  loss_scale: 32768.0000 (33239.8592)  weight_decay: 0.0500 (0.0500)  time: 0.7645  data: 0.0009  max mem: 27222
Epoch: [10]  [2501/2502]  eta: 0:00:00  lr: 0.000699  min_lr: 0.000001  loss: 1.9590 (2.0100)  class_acc: 0.7344 (0.7362)  loss_scale: 32768.0000 (33239.8592)  weight_decay: 0.0500 (0.0500)  time: 0.7262  data: 0.0009  max mem: 27222
Epoch: [10] Total time: 0:34:09 (0.8193 s / it)
Averaged stats: lr: 0.000699  min_lr: 0.000001  loss: 1.9590 (2.0086)  class_acc: 0.7344 (0.7363)  loss_scale: 32768.0000 (33239.8592)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Test:  [ 0/98]  eta: 0:07:18  loss: 0.3760 (0.3760)  acc1: 90.6250 (90.6250)  acc5: 100.0000 (100.0000)  time: 4.4711  data: 4.1324  max mem: 27222
Test:  [97/98]  eta: 0:00:00  loss: 0.7879 (0.7235)  acc1: 80.9524 (82.7040)  acc5: 96.8750 (97.2000)  time: 0.3359  data: 0.0018  max mem: 27222
Test: Total time: 0:00:40 (0.4131 s / it)
* Acc@1 83.132 Acc@5 96.974 loss 0.720
Accuracy of the network on the 50000 test images: 83.1%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Test:  [ 0/98]  eta: 0:05:55  loss: 0.8154 (0.8154)  acc1: 90.6250 (90.6250)  acc5: 100.0000 (100.0000)  time: 3.6251  data: 3.2881  max mem: 27222
Test:  [97/98]  eta: 0:00:00  loss: 1.1502 (1.1335)  acc1: 81.2500 (83.3440)  acc5: 98.4375 (97.3280)  time: 0.3342  data: 0.0002  max mem: 27222
Test: Total time: 0:00:41 (0.4190 s / it)
* Acc@1 83.514 Acc@5 97.336 loss 1.130
EMA Accuracy of the network on the 50000 test images: 83.5%
Max accuracy: 83.51%
{"train_lr": 0.0006996416569743994, "train_min_lr": 9.13780560258988e-07, "train_loss": 2.00855849609375, "train_class_acc": 0.73627109375, "train_loss_scale": 33239.8592, "train_weight_decay": 0.04999999999999801, "test_loss": 1.1304921895569684, "test_acc1": 83.51400000274658, "test_acc5": 97.33600000457764, "epoch": 10, "n_parameters": 86860264}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Epoch: [11]  [   0/2502]  eta: 1 day, 4:47:14  lr: 0.000699  min_lr: 0.000001  loss: 1.8857 (1.8857)  class_acc: 0.7656 (0.7656)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 41.4209  data: 40.6951  max mem: 27222
Epoch: [11]  [ 100/2502]  eta: 0:47:36  lr: 0.000699  min_lr: 0.000001  loss: 1.9268 (1.9678)  class_acc: 0.7344 (0.7471)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7879  data: 0.0002  max mem: 27222
Epoch: [11]  [ 200/2502]  eta: 0:38:17  lr: 0.000699  min_lr: 0.000001  loss: 1.8975 (1.9616)  class_acc: 0.7500 (0.7474)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7949  data: 0.0002  max mem: 27222
Epoch: [11]  [ 300/2502]  eta: 0:34:21  lr: 0.000699  min_lr: 0.000001  loss: 1.9092 (1.9676)  class_acc: 0.7344 (0.7465)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8361  data: 0.0002  max mem: 27222
Epoch: [11]  [ 400/2502]  eta: 0:31:40  lr: 0.000699  min_lr: 0.000001  loss: 1.9502 (1.9662)  class_acc: 0.7500 (0.7474)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8368  data: 0.0002  max mem: 27222
Epoch: [11]  [ 500/2502]  eta: 0:29:32  lr: 0.000698  min_lr: 0.000001  loss: 1.8906 (1.9614)  class_acc: 0.7500 (0.7477)  loss_scale: 32768.0000 (33291.2415)  weight_decay: 0.0500 (0.0500)  time: 0.8227  data: 0.0002  max mem: 27222
Epoch: [11]  [ 600/2502]  eta: 0:27:37  lr: 0.000698  min_lr: 0.000001  loss: 1.9268 (1.9610)  class_acc: 0.7500 (0.7482)  loss_scale: 32768.0000 (33204.1797)  weight_decay: 0.0500 (0.0500)  time: 0.8161  data: 0.0002  max mem: 27222
Epoch: [11]  [ 700/2502]  eta: 0:25:54  lr: 0.000698  min_lr: 0.000001  loss: 1.9736 (1.9611)  class_acc: 0.7344 (0.7483)  loss_scale: 32768.0000 (33141.9572)  weight_decay: 0.0500 (0.0500)  time: 0.8022  data: 0.0003  max mem: 27222
Epoch: [11]  [ 800/2502]  eta: 0:24:13  lr: 0.000698  min_lr: 0.000001  loss: 2.0156 (1.9676)  class_acc: 0.7031 (0.7461)  loss_scale: 32768.0000 (33095.2709)  weight_decay: 0.0500 (0.0500)  time: 0.8018  data: 0.0010  max mem: 27222
Epoch: [11]  [ 900/2502]  eta: 0:22:41  lr: 0.000698  min_lr: 0.000001  loss: 1.9717 (1.9680)  class_acc: 0.7344 (0.7464)  loss_scale: 32768.0000 (33058.9478)  weight_decay: 0.0500 (0.0500)  time: 0.8211  data: 0.0002  max mem: 27222
Epoch: [11]  [1000/2502]  eta: 0:21:08  lr: 0.000698  min_lr: 0.000001  loss: 1.8408 (1.9676)  class_acc: 0.7812 (0.7462)  loss_scale: 32768.0000 (33422.7053)  weight_decay: 0.0500 (0.0500)  time: 0.7941  data: 0.0002  max mem: 27222
Epoch: [11]  [1100/2502]  eta: 0:19:38  lr: 0.000698  min_lr: 0.000001  loss: 1.9033 (1.9673)  class_acc: 0.7656 (0.7465)  loss_scale: 32768.0000 (33363.2407)  weight_decay: 0.0500 (0.0500)  time: 0.7982  data: 0.0002  max mem: 27222
Epoch: [11]  [1200/2502]  eta: 0:18:11  lr: 0.000698  min_lr: 0.000001  loss: 2.0059 (1.9693)  class_acc: 0.7500 (0.7463)  loss_scale: 32768.0000 (33313.6786)  weight_decay: 0.0500 (0.0500)  time: 0.8180  data: 0.0002  max mem: 27222
Epoch: [11]  [1300/2502]  eta: 0:16:45  lr: 0.000698  min_lr: 0.000001  loss: 2.0117 (1.9703)  class_acc: 0.7344 (0.7461)  loss_scale: 32768.0000 (33271.7356)  weight_decay: 0.0500 (0.0500)  time: 0.7970  data: 0.0002  max mem: 27222
Epoch: [11]  [1400/2502]  eta: 0:15:18  lr: 0.000697  min_lr: 0.000001  loss: 2.0039 (1.9715)  class_acc: 0.7188 (0.7458)  loss_scale: 32768.0000 (33235.7802)  weight_decay: 0.0500 (0.0500)  time: 0.7965  data: 0.0002  max mem: 27222
Epoch: [11]  [1500/2502]  eta: 0:13:54  lr: 0.000697  min_lr: 0.000001  loss: 1.8887 (1.9707)  class_acc: 0.7656 (0.7458)  loss_scale: 32768.0000 (33466.5849)  weight_decay: 0.0500 (0.0500)  time: 0.7973  data: 0.0003  max mem: 27222
Epoch: [11]  [1600/2502]  eta: 0:12:30  lr: 0.000697  min_lr: 0.000001  loss: 2.0078 (1.9717)  class_acc: 0.7344 (0.7461)  loss_scale: 32768.0000 (33422.9507)  weight_decay: 0.0500 (0.0500)  time: 0.8759  data: 0.0002  max mem: 27222
Epoch: [11]  [1700/2502]  eta: 0:11:07  lr: 0.000697  min_lr: 0.000001  loss: 1.9072 (1.9722)  class_acc: 0.7656 (0.7457)  loss_scale: 32768.0000 (33384.4468)  weight_decay: 0.0500 (0.0500)  time: 0.7978  data: 0.0002  max mem: 27222
Epoch: [11]  [1800/2502]  eta: 0:09:44  lr: 0.000697  min_lr: 0.000001  loss: 1.9365 (1.9724)  class_acc: 0.7344 (0.7458)  loss_scale: 32768.0000 (33350.2188)  weight_decay: 0.0500 (0.0500)  time: 0.8479  data: 0.0002  max mem: 27222
Epoch: [11]  [1900/2502]  eta: 0:08:20  lr: 0.000697  min_lr: 0.000001  loss: 1.9121 (1.9727)  class_acc: 0.7344 (0.7456)  loss_scale: 32768.0000 (33319.5918)  weight_decay: 0.0500 (0.0500)  time: 0.8324  data: 0.0010  max mem: 27222
Epoch: [11]  [2000/2502]  eta: 0:06:56  lr: 0.000697  min_lr: 0.000001  loss: 1.8691 (1.9725)  class_acc: 0.7656 (0.7457)  loss_scale: 32768.0000 (33357.5292)  weight_decay: 0.0500 (0.0500)  time: 0.8197  data: 0.0002  max mem: 27222
Epoch: [11]  [2100/2502]  eta: 0:05:33  lr: 0.000696  min_lr: 0.000001  loss: 1.9385 (1.9725)  class_acc: 0.7656 (0.7458)  loss_scale: 32768.0000 (33329.4698)  weight_decay: 0.0500 (0.0500)  time: 0.8158  data: 0.0002  max mem: 27222
Epoch: [11]  [2200/2502]  eta: 0:04:09  lr: 0.000696  min_lr: 0.000001  loss: 2.0176 (1.9738)  class_acc: 0.7344 (0.7455)  loss_scale: 16384.0000 (32931.7656)  weight_decay: 0.0500 (0.0500)  time: 0.8023  data: 0.0002  max mem: 27222
Epoch: [11]  [2300/2502]  eta: 0:02:46  lr: 0.000696  min_lr: 0.000001  loss: 1.9131 (1.9735)  class_acc: 0.7344 (0.7455)  loss_scale: 16384.0000 (32212.6102)  weight_decay: 0.0500 (0.0500)  time: 0.7926  data: 0.0003  max mem: 27222
Epoch: [11]  [2400/2502]  eta: 0:01:24  lr: 0.000696  min_lr: 0.000001  loss: 1.9824 (1.9732)  class_acc: 0.7344 (0.7455)  loss_scale: 16384.0000 (31553.3594)  weight_decay: 0.0500 (0.0500)  time: 0.8315  data: 0.0003  max mem: 27222
Epoch: [11]  [2500/2502]  eta: 0:00:01  lr: 0.000696  min_lr: 0.000001  loss: 1.9141 (1.9723)  class_acc: 0.7344 (0.7457)  loss_scale: 16384.0000 (30952.6528)  weight_decay: 0.0500 (0.0500)  time: 0.7529  data: 0.0010  max mem: 27222
Epoch: [11]  [2501/2502]  eta: 0:00:00  lr: 0.000696  min_lr: 0.000001  loss: 1.9141 (1.9723)  class_acc: 0.7344 (0.7457)  loss_scale: 16384.0000 (30952.6528)  weight_decay: 0.0500 (0.0500)  time: 0.7140  data: 0.0009  max mem: 27222
Epoch: [11] Total time: 0:34:22 (0.8245 s / it)
Averaged stats: lr: 0.000696  min_lr: 0.000001  loss: 1.9141 (1.9744)  class_acc: 0.7344 (0.7446)  loss_scale: 16384.0000 (30952.6528)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Test:  [ 0/98]  eta: 0:10:35  loss: 0.3584 (0.3584)  acc1: 90.6250 (90.6250)  acc5: 100.0000 (100.0000)  time: 6.4842  data: 6.1216  max mem: 27222
Test:  [97/98]  eta: 0:00:00  loss: 0.7548 (0.7124)  acc1: 80.9524 (83.0720)  acc5: 96.8750 (97.1040)  time: 0.3340  data: 0.0002  max mem: 27222
Test: Total time: 0:00:41 (0.4200 s / it)
* Acc@1 83.400 Acc@5 97.014 loss 0.714
Accuracy of the network on the 50000 test images: 83.4%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Test:  [ 0/98]  eta: 0:08:28  loss: 0.6243 (0.6243)  acc1: 92.1875 (92.1875)  acc5: 100.0000 (100.0000)  time: 5.1858  data: 4.8358  max mem: 27222
Test:  [97/98]  eta: 0:00:00  loss: 0.9672 (0.9340)  acc1: 81.2500 (83.8720)  acc5: 98.4375 (97.5840)  time: 0.3372  data: 0.0003  max mem: 27222
Test: Total time: 0:00:40 (0.4135 s / it)
* Acc@1 84.114 Acc@5 97.502 loss 0.931
EMA Accuracy of the network on the 50000 test images: 84.1%
Max accuracy: 84.11%
{"train_lr": 0.0006974908063847493, "train_min_lr": 9.109714001164311e-07, "train_loss": 1.9744380859375, "train_class_acc": 0.74463515625, "train_loss_scale": 30952.6528, "train_weight_decay": 0.04999999999999801, "test_loss": 0.9311555073577531, "test_acc1": 84.11400000396729, "test_acc5": 97.50200000976562, "epoch": 11, "n_parameters": 86860264}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Epoch: [12]  [   0/2502]  eta: 1 day, 5:16:59  lr: 0.000696  min_lr: 0.000001  loss: 1.9082 (1.9082)  class_acc: 0.8125 (0.8125)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 42.1343  data: 39.0358  max mem: 27222
Epoch: [12]  [ 100/2502]  eta: 0:47:48  lr: 0.000696  min_lr: 0.000001  loss: 1.8457 (1.9312)  class_acc: 0.7812 (0.7618)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7835  data: 0.0003  max mem: 27222
Epoch: [12]  [ 200/2502]  eta: 0:38:10  lr: 0.000695  min_lr: 0.000001  loss: 1.8945 (1.9224)  class_acc: 0.7656 (0.7624)  loss_scale: 32768.0000 (19155.4229)  weight_decay: 0.0500 (0.0500)  time: 0.8052  data: 0.0002  max mem: 27222
Epoch: [12]  [ 300/2502]  eta: 0:34:13  lr: 0.000695  min_lr: 0.000001  loss: 1.8867 (1.9272)  class_acc: 0.7656 (0.7591)  loss_scale: 32768.0000 (23677.8738)  weight_decay: 0.0500 (0.0500)  time: 0.8140  data: 0.0004  max mem: 27222
Epoch: [12]  [ 400/2502]  eta: 0:31:30  lr: 0.000695  min_lr: 0.000001  loss: 1.8799 (1.9289)  class_acc: 0.7656 (0.7567)  loss_scale: 32768.0000 (25944.7382)  weight_decay: 0.0500 (0.0500)  time: 0.7976  data: 0.0003  max mem: 27222
Epoch: [12]  [ 500/2502]  eta: 0:29:23  lr: 0.000695  min_lr: 0.000001  loss: 1.9375 (1.9285)  class_acc: 0.7500 (0.7572)  loss_scale: 32768.0000 (27306.6667)  weight_decay: 0.0500 (0.0500)  time: 0.8368  data: 0.0003  max mem: 27222
Epoch: [12]  [ 600/2502]  eta: 0:27:32  lr: 0.000695  min_lr: 0.000001  loss: 1.8730 (1.9298)  class_acc: 0.7812 (0.7573)  loss_scale: 32768.0000 (28215.3744)  weight_decay: 0.0500 (0.0500)  time: 0.7933  data: 0.0002  max mem: 27222
Epoch: [12]  [ 700/2502]  eta: 0:25:50  lr: 0.000694  min_lr: 0.000001  loss: 1.9131 (1.9352)  class_acc: 0.7344 (0.7553)  loss_scale: 32768.0000 (29051.8003)  weight_decay: 0.0500 (0.0500)  time: 0.7925  data: 0.0002  max mem: 27222
Epoch: [12]  [ 800/2502]  eta: 0:24:15  lr: 0.000694  min_lr: 0.000001  loss: 1.9600 (1.9338)  class_acc: 0.7344 (0.7552)  loss_scale: 32768.0000 (29515.7453)  weight_decay: 0.0500 (0.0500)  time: 0.8368  data: 0.0002  max mem: 27222
Epoch: [12]  [ 900/2502]  eta: 0:22:44  lr: 0.000694  min_lr: 0.000001  loss: 1.9395 (1.9359)  class_acc: 0.7500 (0.7547)  loss_scale: 32768.0000 (29876.7059)  weight_decay: 0.0500 (0.0500)  time: 0.8420  data: 0.0003  max mem: 27222
Epoch: [12]  [1000/2502]  eta: 0:21:12  lr: 0.000694  min_lr: 0.000001  loss: 1.9258 (1.9382)  class_acc: 0.7344 (0.7541)  loss_scale: 32768.0000 (30165.5465)  weight_decay: 0.0500 (0.0500)  time: 0.7911  data: 0.0003  max mem: 27222
Epoch: [12]  [1100/2502]  eta: 0:19:42  lr: 0.000694  min_lr: 0.000001  loss: 2.0078 (1.9387)  class_acc: 0.7344 (0.7536)  loss_scale: 32768.0000 (30401.9183)  weight_decay: 0.0500 (0.0500)  time: 0.8129  data: 0.0002  max mem: 27222
Epoch: [12]  [1200/2502]  eta: 0:18:15  lr: 0.000693  min_lr: 0.000001  loss: 1.9570 (1.9383)  class_acc: 0.7500 (0.7533)  loss_scale: 32768.0000 (30653.4954)  weight_decay: 0.0500 (0.0500)  time: 0.8288  data: 0.0003  max mem: 27222
Epoch: [12]  [1300/2502]  eta: 0:16:47  lr: 0.000693  min_lr: 0.000001  loss: 1.9629 (1.9393)  class_acc: 0.7188 (0.7531)  loss_scale: 32768.0000 (30866.3982)  weight_decay: 0.0500 (0.0500)  time: 0.7962  data: 0.0002  max mem: 27222
Epoch: [12]  [1400/2502]  eta: 0:15:20  lr: 0.000693  min_lr: 0.000001  loss: 1.9746 (1.9408)  class_acc: 0.7344 (0.7526)  loss_scale: 32768.0000 (31002.1299)  weight_decay: 0.0500 (0.0500)  time: 0.7901  data: 0.0003  max mem: 27222
Epoch: [12]  [1500/2502]  eta: 0:13:54  lr: 0.000693  min_lr: 0.000001  loss: 1.9873 (1.9448)  class_acc: 0.7188 (0.7521)  loss_scale: 32768.0000 (31119.7761)  weight_decay: 0.0500 (0.0500)  time: 0.7932  data: 0.0002  max mem: 27222
Epoch: [12]  [1600/2502]  eta: 0:12:29  lr: 0.000693  min_lr: 0.000001  loss: 1.8447 (1.9439)  class_acc: 0.7812 (0.7523)  loss_scale: 32768.0000 (31222.7258)  weight_decay: 0.0500 (0.0500)  time: 0.8092  data: 0.0010  max mem: 27222
Epoch: [12]  [1700/2502]  eta: 0:11:05  lr: 0.000692  min_lr: 0.000001  loss: 1.8398 (1.9433)  class_acc: 0.7656 (0.7524)  loss_scale: 32768.0000 (31313.5708)  weight_decay: 0.0500 (0.0500)  time: 0.8051  data: 0.0002  max mem: 27222
Epoch: [12]  [1800/2502]  eta: 0:09:41  lr: 0.000692  min_lr: 0.000001  loss: 1.9365 (1.9428)  class_acc: 0.7500 (0.7523)  loss_scale: 32768.0000 (31467.1049)  weight_decay: 0.0500 (0.0500)  time: 0.7934  data: 0.0015  max mem: 27222
Epoch: [12]  [1900/2502]  eta: 0:08:18  lr: 0.000692  min_lr: 0.000001  loss: 1.9307 (1.9436)  class_acc: 0.7344 (0.7522)  loss_scale: 32768.0000 (31535.5371)  weight_decay: 0.0500 (0.0500)  time: 0.7965  data: 0.0002  max mem: 27222
Epoch: [12]  [2000/2502]  eta: 0:06:54  lr: 0.000692  min_lr: 0.000001  loss: 1.8682 (1.9445)  class_acc: 0.7656 (0.7522)  loss_scale: 32768.0000 (31597.1294)  weight_decay: 0.0500 (0.0500)  time: 0.7957  data: 0.0003  max mem: 27222
Epoch: [12]  [2100/2502]  eta: 0:05:31  lr: 0.000691  min_lr: 0.000001  loss: 1.9434 (1.9450)  class_acc: 0.7344 (0.7520)  loss_scale: 32768.0000 (31652.8586)  weight_decay: 0.0500 (0.0500)  time: 0.8270  data: 0.0002  max mem: 27222
Epoch: [12]  [2200/2502]  eta: 0:04:08  lr: 0.000691  min_lr: 0.000001  loss: 1.8271 (1.9454)  class_acc: 0.7656 (0.7520)  loss_scale: 32768.0000 (31703.5239)  weight_decay: 0.0500 (0.0500)  time: 0.7955  data: 0.0003  max mem: 27222
Epoch: [12]  [2300/2502]  eta: 0:02:46  lr: 0.000691  min_lr: 0.000001  loss: 1.9209 (1.9452)  class_acc: 0.7656 (0.7523)  loss_scale: 32768.0000 (31806.7484)  weight_decay: 0.0500 (0.0500)  time: 0.7956  data: 0.0003  max mem: 27222
Epoch: [12]  [2400/2502]  eta: 0:01:23  lr: 0.000691  min_lr: 0.000001  loss: 1.9932 (1.9439)  class_acc: 0.7344 (0.7526)  loss_scale: 32768.0000 (31846.7838)  weight_decay: 0.0500 (0.0500)  time: 0.7924  data: 0.0002  max mem: 27222
Epoch: [12]  [2500/2502]  eta: 0:00:01  lr: 0.000690  min_lr: 0.000001  loss: 1.9580 (1.9441)  class_acc: 0.7188 (0.7523)  loss_scale: 32768.0000 (31883.2640)  weight_decay: 0.0500 (0.0500)  time: 0.7771  data: 0.0009  max mem: 27222
Epoch: [12]  [2501/2502]  eta: 0:00:00  lr: 0.000690  min_lr: 0.000001  loss: 1.9580 (1.9441)  class_acc: 0.7188 (0.7523)  loss_scale: 32768.0000 (31883.2640)  weight_decay: 0.0500 (0.0500)  time: 0.7372  data: 0.0009  max mem: 27222
Epoch: [12] Total time: 0:34:15 (0.8216 s / it)
Averaged stats: lr: 0.000690  min_lr: 0.000001  loss: 1.9580 (1.9469)  class_acc: 0.7188 (0.7514)  loss_scale: 32768.0000 (31883.2640)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Test:  [ 0/98]  eta: 0:09:27  loss: 0.3552 (0.3552)  acc1: 89.0625 (89.0625)  acc5: 100.0000 (100.0000)  time: 5.7947  data: 5.4142  max mem: 27222
Test:  [97/98]  eta: 0:00:00  loss: 0.7123 (0.6835)  acc1: 82.8125 (83.9200)  acc5: 96.8750 (97.1360)  time: 0.3359  data: 0.0005  max mem: 27222
Test: Total time: 0:00:40 (0.4123 s / it)
* Acc@1 83.594 Acc@5 97.068 loss 0.691
Accuracy of the network on the 50000 test images: 83.6%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Test:  [ 0/98]  eta: 0:08:17  loss: 0.5004 (0.5004)  acc1: 92.1875 (92.1875)  acc5: 100.0000 (100.0000)  time: 5.0753  data: 4.7398  max mem: 27222
Test:  [97/98]  eta: 0:00:00  loss: 0.8355 (0.8060)  acc1: 82.8125 (84.2240)  acc5: 98.4375 (97.6640)  time: 0.3474  data: 0.0089  max mem: 27222
Test: Total time: 0:00:40 (0.4162 s / it)
* Acc@1 84.496 Acc@5 97.602 loss 0.804
EMA Accuracy of the network on the 50000 test images: 84.5%
Max accuracy: 84.50%
{"train_lr": 0.000693200642087578, "train_min_lr": 9.053681478000593e-07, "train_loss": 1.94690732421875, "train_class_acc": 0.75141171875, "train_loss_scale": 31883.264, "train_weight_decay": 0.04999999999999801, "test_loss": 0.8038359138430381, "test_acc1": 84.49599999908448, "test_acc5": 97.60200000854492, "epoch": 12, "n_parameters": 86860264}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Epoch: [13]  [   0/2502]  eta: 1 day, 6:34:20  lr: 0.000690  min_lr: 0.000001  loss: 2.0117 (2.0117)  class_acc: 0.7656 (0.7656)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 43.9892  data: 43.2620  max mem: 27222
Epoch: [13]  [ 100/2502]  eta: 0:48:36  lr: 0.000690  min_lr: 0.000001  loss: 1.9102 (1.9233)  class_acc: 0.7500 (0.7607)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7899  data: 0.0003  max mem: 27222
Epoch: [13]  [ 200/2502]  eta: 0:39:00  lr: 0.000690  min_lr: 0.000001  loss: 1.8076 (1.9194)  class_acc: 0.7812 (0.7613)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8368  data: 0.0003  max mem: 27222
Epoch: [13]  [ 300/2502]  eta: 0:34:47  lr: 0.000690  min_lr: 0.000001  loss: 1.8047 (1.9054)  class_acc: 0.7656 (0.7626)  loss_scale: 32768.0000 (34074.3654)  weight_decay: 0.0500 (0.0500)  time: 0.8027  data: 0.0003  max mem: 27222
Epoch: [13]  [ 400/2502]  eta: 0:31:55  lr: 0.000689  min_lr: 0.000001  loss: 2.0020 (1.9094)  class_acc: 0.7188 (0.7612)  loss_scale: 32768.0000 (33748.5885)  weight_decay: 0.0500 (0.0500)  time: 0.7917  data: 0.0003  max mem: 27222
Epoch: [13]  [ 500/2502]  eta: 0:29:44  lr: 0.000689  min_lr: 0.000001  loss: 1.8906 (1.9096)  class_acc: 0.7656 (0.7620)  loss_scale: 32768.0000 (33552.8623)  weight_decay: 0.0500 (0.0500)  time: 0.7923  data: 0.0003  max mem: 27222
Epoch: [13]  [ 600/2502]  eta: 0:27:44  lr: 0.000689  min_lr: 0.000001  loss: 1.8184 (1.9108)  class_acc: 0.7656 (0.7606)  loss_scale: 32768.0000 (33422.2696)  weight_decay: 0.0500 (0.0500)  time: 0.7952  data: 0.0003  max mem: 27222
Epoch: [13]  [ 700/2502]  eta: 0:25:57  lr: 0.000688  min_lr: 0.000001  loss: 1.8555 (1.9103)  class_acc: 0.7500 (0.7605)  loss_scale: 32768.0000 (33328.9358)  weight_decay: 0.0500 (0.0500)  time: 0.7932  data: 0.0003  max mem: 27222
Epoch: [13]  [ 800/2502]  eta: 0:24:17  lr: 0.000688  min_lr: 0.000001  loss: 1.9277 (1.9103)  class_acc: 0.7344 (0.7606)  loss_scale: 65536.0000 (33749.8127)  weight_decay: 0.0500 (0.0500)  time: 0.7901  data: 0.0002  max mem: 27222
Epoch: [13]  [ 900/2502]  eta: 0:22:40  lr: 0.000688  min_lr: 0.000001  loss: 1.9316 (1.9080)  class_acc: 0.7344 (0.7607)  loss_scale: 32768.0000 (33640.8435)  weight_decay: 0.0500 (0.0500)  time: 0.7927  data: 0.0002  max mem: 27222
Epoch: [13]  [1000/2502]  eta: 0:21:09  lr: 0.000688  min_lr: 0.000001  loss: 2.0273 (1.9138)  class_acc: 0.7344 (0.7595)  loss_scale: 32768.0000 (33553.6464)  weight_decay: 0.0500 (0.0500)  time: 0.7924  data: 0.0002  max mem: 27222
Epoch: [13]  [1100/2502]  eta: 0:19:39  lr: 0.000687  min_lr: 0.000001  loss: 1.8213 (1.9111)  class_acc: 0.7812 (0.7601)  loss_scale: 32768.0000 (33482.2888)  weight_decay: 0.0500 (0.0500)  time: 0.7947  data: 0.0002  max mem: 27222
Epoch: [13]  [1200/2502]  eta: 0:18:10  lr: 0.000687  min_lr: 0.000001  loss: 1.9473 (1.9122)  class_acc: 0.7500 (0.7595)  loss_scale: 32768.0000 (33422.8143)  weight_decay: 0.0500 (0.0500)  time: 0.8157  data: 0.0002  max mem: 27222
Epoch: [13]  [1300/2502]  eta: 0:16:43  lr: 0.000687  min_lr: 0.000001  loss: 1.9141 (1.9135)  class_acc: 0.7500 (0.7589)  loss_scale: 32768.0000 (33372.4827)  weight_decay: 0.0500 (0.0500)  time: 0.7961  data: 0.0002  max mem: 27222
Epoch: [13]  [1400/2502]  eta: 0:15:18  lr: 0.000686  min_lr: 0.000001  loss: 1.8525 (1.9151)  class_acc: 0.7500 (0.7586)  loss_scale: 32768.0000 (33422.8922)  weight_decay: 0.0500 (0.0500)  time: 0.8332  data: 0.0002  max mem: 27222
Epoch: [13]  [1500/2502]  eta: 0:13:53  lr: 0.000686  min_lr: 0.000001  loss: 1.8984 (1.9165)  class_acc: 0.7656 (0.7582)  loss_scale: 32768.0000 (33379.2618)  weight_decay: 0.0500 (0.0500)  time: 0.8690  data: 0.0002  max mem: 27222
Epoch: [13]  [1600/2502]  eta: 0:12:29  lr: 0.000686  min_lr: 0.000001  loss: 1.9395 (1.9192)  class_acc: 0.7344 (0.7574)  loss_scale: 32768.0000 (33341.0818)  weight_decay: 0.0500 (0.0500)  time: 0.7998  data: 0.0003  max mem: 27222
Epoch: [13]  [1700/2502]  eta: 0:11:05  lr: 0.000686  min_lr: 0.000001  loss: 1.8438 (1.9186)  class_acc: 0.7812 (0.7575)  loss_scale: 32768.0000 (33307.3909)  weight_decay: 0.0500 (0.0500)  time: 0.7959  data: 0.0003  max mem: 27222
Epoch: [13]  [1800/2502]  eta: 0:09:41  lr: 0.000685  min_lr: 0.000001  loss: 1.7979 (1.9188)  class_acc: 0.7812 (0.7577)  loss_scale: 32768.0000 (33277.4414)  weight_decay: 0.0500 (0.0500)  time: 0.7898  data: 0.0003  max mem: 27222
Epoch: [13]  [1900/2502]  eta: 0:08:17  lr: 0.000685  min_lr: 0.000001  loss: 1.8398 (1.9178)  class_acc: 0.7812 (0.7580)  loss_scale: 32768.0000 (33526.4387)  weight_decay: 0.0500 (0.0500)  time: 0.8009  data: 0.0003  max mem: 27222
Epoch: [13]  [2000/2502]  eta: 0:06:54  lr: 0.000685  min_lr: 0.000001  loss: 1.8438 (1.9196)  class_acc: 0.7344 (0.7575)  loss_scale: 32768.0000 (33488.5357)  weight_decay: 0.0500 (0.0500)  time: 0.7924  data: 0.0002  max mem: 27222
Epoch: [13]  [2100/2502]  eta: 0:05:31  lr: 0.000684  min_lr: 0.000001  loss: 1.9805 (1.9208)  class_acc: 0.7344 (0.7572)  loss_scale: 32768.0000 (33454.2408)  weight_decay: 0.0500 (0.0500)  time: 0.8383  data: 0.0003  max mem: 27222
Epoch: [13]  [2200/2502]  eta: 0:04:09  lr: 0.000684  min_lr: 0.000001  loss: 1.9434 (1.9221)  class_acc: 0.7500 (0.7571)  loss_scale: 32768.0000 (33423.0622)  weight_decay: 0.0500 (0.0500)  time: 0.8187  data: 0.0004  max mem: 27222
Epoch: [13]  [2300/2502]  eta: 0:02:46  lr: 0.000684  min_lr: 0.000001  loss: 1.9141 (1.9219)  class_acc: 0.7500 (0.7572)  loss_scale: 32768.0000 (33394.5937)  weight_decay: 0.0500 (0.0500)  time: 0.7932  data: 0.0003  max mem: 27222
Epoch: [13]  [2400/2502]  eta: 0:01:24  lr: 0.000683  min_lr: 0.000001  loss: 1.8594 (1.9219)  class_acc: 0.7656 (0.7571)  loss_scale: 32768.0000 (33423.0870)  weight_decay: 0.0500 (0.0500)  time: 0.7985  data: 0.0003  max mem: 27222
Epoch: [13]  [2500/2502]  eta: 0:00:01  lr: 0.000683  min_lr: 0.000001  loss: 1.9111 (1.9218)  class_acc: 0.7656 (0.7571)  loss_scale: 32768.0000 (33397.1456)  weight_decay: 0.0500 (0.0500)  time: 0.7869  data: 0.0027  max mem: 27222
Epoch: [13]  [2501/2502]  eta: 0:00:00  lr: 0.000683  min_lr: 0.000001  loss: 1.9111 (1.9218)  class_acc: 0.7656 (0.7571)  loss_scale: 32768.0000 (33397.1456)  weight_decay: 0.0500 (0.0500)  time: 0.7420  data: 0.0027  max mem: 27222
Epoch: [13] Total time: 0:34:20 (0.8234 s / it)
Averaged stats: lr: 0.000683  min_lr: 0.000001  loss: 1.9111 (1.9214)  class_acc: 0.7656 (0.7584)  loss_scale: 32768.0000 (33397.1456)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Test:  [ 0/98]  eta: 0:08:25  loss: 0.3234 (0.3234)  acc1: 93.7500 (93.7500)  acc5: 98.4375 (98.4375)  time: 5.1614  data: 4.7733  max mem: 27222
Test:  [97/98]  eta: 0:00:00  loss: 0.7729 (0.6855)  acc1: 82.8125 (83.7120)  acc5: 96.8750 (96.9920)  time: 0.3312  data: 0.0007  max mem: 27222
Test: Total time: 0:00:40 (0.4085 s / it)
* Acc@1 83.890 Acc@5 97.156 loss 0.683
Accuracy of the network on the 50000 test images: 83.9%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Test:  [ 0/98]  eta: 0:08:50  loss: 0.4158 (0.4158)  acc1: 90.6250 (90.6250)  acc5: 100.0000 (100.0000)  time: 5.4138  data: 5.0752  max mem: 27222
Test:  [97/98]  eta: 0:00:00  loss: 0.7520 (0.7223)  acc1: 82.8125 (84.5920)  acc5: 98.4375 (97.8240)  time: 0.3336  data: 0.0007  max mem: 27222
Test: Total time: 0:00:40 (0.4178 s / it)
* Acc@1 84.876 Acc@5 97.690 loss 0.721
EMA Accuracy of the network on the 50000 test images: 84.9%
Max accuracy: 84.88%
{"train_lr": 0.0006867976143724194, "train_min_lr": 8.970053492236856e-07, "train_loss": 1.92144970703125, "train_class_acc": 0.7583859375, "train_loss_scale": 33397.1456, "train_weight_decay": 0.04999999999999801, "test_loss": 0.7209102845161545, "test_acc1": 84.87600000427246, "test_acc5": 97.69000000854493, "epoch": 13, "n_parameters": 86860264}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Epoch: [14]  [   0/2502]  eta: 1 day, 5:55:17  lr: 0.000683  min_lr: 0.000001  loss: 1.8076 (1.8076)  class_acc: 0.7812 (0.7812)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 43.0525  data: 41.3713  max mem: 27222
Epoch: [14]  [ 100/2502]  eta: 0:48:09  lr: 0.000683  min_lr: 0.000001  loss: 1.9023 (1.8860)  class_acc: 0.7500 (0.7698)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7835  data: 0.0003  max mem: 27222
Epoch: [14]  [ 200/2502]  eta: 0:38:19  lr: 0.000682  min_lr: 0.000001  loss: 1.8516 (1.8867)  class_acc: 0.7500 (0.7672)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7999  data: 0.0002  max mem: 27222
Epoch: [14]  [ 300/2502]  eta: 0:34:14  lr: 0.000682  min_lr: 0.000001  loss: 1.8994 (1.8857)  class_acc: 0.7812 (0.7677)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7916  data: 0.0003  max mem: 27222
Epoch: [14]  [ 400/2502]  eta: 0:31:32  lr: 0.000682  min_lr: 0.000001  loss: 1.8672 (1.8882)  class_acc: 0.7656 (0.7681)  loss_scale: 65536.0000 (33748.5885)  weight_decay: 0.0500 (0.0500)  time: 0.8287  data: 0.0003  max mem: 27222
Epoch: [14]  [ 500/2502]  eta: 0:29:28  lr: 0.000681  min_lr: 0.000001  loss: 1.8320 (1.8920)  class_acc: 0.7656 (0.7660)  loss_scale: 32768.0000 (33552.8623)  weight_decay: 0.0500 (0.0500)  time: 0.8133  data: 0.0002  max mem: 27222
Epoch: [14]  [ 600/2502]  eta: 0:27:40  lr: 0.000681  min_lr: 0.000001  loss: 1.9209 (1.8927)  class_acc: 0.7500 (0.7655)  loss_scale: 32768.0000 (33422.2696)  weight_decay: 0.0500 (0.0500)  time: 0.7886  data: 0.0002  max mem: 27222
Epoch: [14]  [ 700/2502]  eta: 0:25:56  lr: 0.000680  min_lr: 0.000001  loss: 1.8320 (1.8944)  class_acc: 0.7656 (0.7649)  loss_scale: 32768.0000 (33328.9358)  weight_decay: 0.0500 (0.0500)  time: 0.8193  data: 0.0002  max mem: 27222
Epoch: [14]  [ 800/2502]  eta: 0:24:17  lr: 0.000680  min_lr: 0.000001  loss: 1.7568 (1.8932)  class_acc: 0.7500 (0.7653)  loss_scale: 32768.0000 (33258.9064)  weight_decay: 0.0500 (0.0500)  time: 0.7932  data: 0.0003  max mem: 27222
Epoch: [14]  [ 900/2502]  eta: 0:22:43  lr: 0.000680  min_lr: 0.000001  loss: 1.8984 (1.8923)  class_acc: 0.7500 (0.7651)  loss_scale: 32768.0000 (33204.4218)  weight_decay: 0.0500 (0.0500)  time: 0.8621  data: 0.0012  max mem: 27222
Epoch: [14]  [1000/2502]  eta: 0:21:11  lr: 0.000679  min_lr: 0.000001  loss: 1.7188 (1.8905)  class_acc: 0.7969 (0.7662)  loss_scale: 32768.0000 (33422.7053)  weight_decay: 0.0500 (0.0500)  time: 0.8040  data: 0.0004  max mem: 27222
Epoch: [14]  [1100/2502]  eta: 0:19:41  lr: 0.000679  min_lr: 0.000001  loss: 1.8936 (1.8918)  class_acc: 0.7656 (0.7659)  loss_scale: 32768.0000 (33363.2407)  weight_decay: 0.0500 (0.0500)  time: 0.7970  data: 0.0002  max mem: 27222
Epoch: [14]  [1200/2502]  eta: 0:18:13  lr: 0.000679  min_lr: 0.000001  loss: 1.8916 (1.8910)  class_acc: 0.7500 (0.7657)  loss_scale: 32768.0000 (33313.6786)  weight_decay: 0.0500 (0.0500)  time: 0.7993  data: 0.0002  max mem: 27222
Epoch: [14]  [1300/2502]  eta: 0:16:46  lr: 0.000678  min_lr: 0.000001  loss: 1.9170 (1.8921)  class_acc: 0.7500 (0.7657)  loss_scale: 32768.0000 (33271.7356)  weight_decay: 0.0500 (0.0500)  time: 0.8306  data: 0.0002  max mem: 27222
Epoch: [14]  [1400/2502]  eta: 0:15:20  lr: 0.000678  min_lr: 0.000001  loss: 1.8848 (1.8921)  class_acc: 0.7500 (0.7658)  loss_scale: 32768.0000 (33235.7802)  weight_decay: 0.0500 (0.0500)  time: 0.7977  data: 0.0002  max mem: 27222
Epoch: [14]  [1500/2502]  eta: 0:13:55  lr: 0.000677  min_lr: 0.000001  loss: 1.8350 (1.8917)  class_acc: 0.7656 (0.7659)  loss_scale: 32768.0000 (33379.2618)  weight_decay: 0.0500 (0.0500)  time: 0.7923  data: 0.0002  max mem: 27222
Epoch: [14]  [1600/2502]  eta: 0:12:30  lr: 0.000677  min_lr: 0.000001  loss: 1.8545 (1.8927)  class_acc: 0.7500 (0.7656)  loss_scale: 32768.0000 (33341.0818)  weight_decay: 0.0500 (0.0500)  time: 0.8242  data: 0.0002  max mem: 27222
Epoch: [14]  [1700/2502]  eta: 0:11:06  lr: 0.000677  min_lr: 0.000001  loss: 1.9541 (1.8936)  class_acc: 0.7500 (0.7655)  loss_scale: 32768.0000 (33307.3909)  weight_decay: 0.0500 (0.0500)  time: 0.8744  data: 0.0002  max mem: 27222
Epoch: [14]  [1800/2502]  eta: 0:09:42  lr: 0.000676  min_lr: 0.000001  loss: 1.8281 (1.8937)  class_acc: 0.7500 (0.7653)  loss_scale: 32768.0000 (33277.4414)  weight_decay: 0.0500 (0.0500)  time: 0.8013  data: 0.0002  max mem: 27222
Epoch: [14]  [1900/2502]  eta: 0:08:19  lr: 0.000676  min_lr: 0.000001  loss: 1.8926 (1.8926)  class_acc: 0.7656 (0.7652)  loss_scale: 32768.0000 (33250.6428)  weight_decay: 0.0500 (0.0500)  time: 0.8296  data: 0.0002  max mem: 27222
Epoch: [14]  [2000/2502]  eta: 0:06:55  lr: 0.000675  min_lr: 0.000001  loss: 1.8848 (1.8930)  class_acc: 0.7812 (0.7650)  loss_scale: 32768.0000 (33292.0260)  weight_decay: 0.0500 (0.0500)  time: 0.8345  data: 0.0002  max mem: 27222
Epoch: [14]  [2100/2502]  eta: 0:05:32  lr: 0.000675  min_lr: 0.000001  loss: 1.8633 (1.8932)  class_acc: 0.7812 (0.7647)  loss_scale: 32768.0000 (33267.0842)  weight_decay: 0.0500 (0.0500)  time: 0.8267  data: 0.0002  max mem: 27222
Epoch: [14]  [2200/2502]  eta: 0:04:10  lr: 0.000675  min_lr: 0.000001  loss: 1.9219 (1.8951)  class_acc: 0.7812 (0.7644)  loss_scale: 32768.0000 (33244.4089)  weight_decay: 0.0500 (0.0500)  time: 0.8078  data: 0.0002  max mem: 27222
Epoch: [14]  [2300/2502]  eta: 0:02:46  lr: 0.000674  min_lr: 0.000001  loss: 1.9170 (1.8951)  class_acc: 0.7656 (0.7644)  loss_scale: 32768.0000 (33223.7045)  weight_decay: 0.0500 (0.0500)  time: 0.7951  data: 0.0002  max mem: 27222
Epoch: [14]  [2400/2502]  eta: 0:01:24  lr: 0.000674  min_lr: 0.000001  loss: 1.8857 (1.8961)  class_acc: 0.7500 (0.7640)  loss_scale: 32768.0000 (33204.7247)  weight_decay: 0.0500 (0.0500)  time: 0.7954  data: 0.0002  max mem: 27222
Epoch: [14]  [2500/2502]  eta: 0:00:01  lr: 0.000673  min_lr: 0.000001  loss: 1.9062 (1.8977)  class_acc: 0.7500 (0.7638)  loss_scale: 32768.0000 (33239.8592)  weight_decay: 0.0500 (0.0500)  time: 0.7629  data: 0.0010  max mem: 27222
Epoch: [14]  [2501/2502]  eta: 0:00:00  lr: 0.000673  min_lr: 0.000001  loss: 1.9062 (1.8977)  class_acc: 0.7500 (0.7638)  loss_scale: 32768.0000 (33239.8592)  weight_decay: 0.0500 (0.0500)  time: 0.7229  data: 0.0010  max mem: 27222
Epoch: [14] Total time: 0:34:23 (0.8249 s / it)
Averaged stats: lr: 0.000673  min_lr: 0.000001  loss: 1.9062 (1.8971)  class_acc: 0.7500 (0.7643)  loss_scale: 32768.0000 (33239.8592)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Test:  [ 0/98]  eta: 0:09:37  loss: 0.3599 (0.3599)  acc1: 92.1875 (92.1875)  acc5: 98.4375 (98.4375)  time: 5.8879  data: 5.5321  max mem: 27222
Test:  [97/98]  eta: 0:00:00  loss: 0.7339 (0.6796)  acc1: 80.9524 (83.9360)  acc5: 96.8750 (97.1040)  time: 0.3324  data: 0.0002  max mem: 27222
Test: Total time: 0:00:41 (0.4199 s / it)
* Acc@1 83.866 Acc@5 97.114 loss 0.688
Accuracy of the network on the 50000 test images: 83.9%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Test:  [ 0/98]  eta: 0:10:00  loss: 0.3656 (0.3656)  acc1: 92.1875 (92.1875)  acc5: 100.0000 (100.0000)  time: 6.1248  data: 5.7844  max mem: 27222
Test:  [97/98]  eta: 0:00:00  loss: 0.6991 (0.6668)  acc1: 82.8125 (84.8320)  acc5: 98.4375 (97.8240)  time: 0.3343  data: 0.0010  max mem: 27222
Test: Total time: 0:00:40 (0.4160 s / it)
* Acc@1 85.178 Acc@5 97.774 loss 0.666
EMA Accuracy of the network on the 50000 test images: 85.2%
Max accuracy: 85.18%
{"train_lr": 0.0006783212000343419, "train_min_lr": 8.859345638214446e-07, "train_loss": 1.8970693359375, "train_class_acc": 0.764290625, "train_loss_scale": 33239.8592, "train_weight_decay": 0.04999999999999801, "test_loss": 0.6661199515267294, "test_acc1": 85.17800000305176, "test_acc5": 97.77400000732422, "epoch": 14, "n_parameters": 86860264}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Epoch: [15]  [   0/2502]  eta: 1 day, 5:43:40  lr: 0.000673  min_lr: 0.000001  loss: 1.7744 (1.7744)  class_acc: 0.7656 (0.7656)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 42.7739  data: 41.9062  max mem: 27222
Epoch: [15]  [ 100/2502]  eta: 0:48:00  lr: 0.000673  min_lr: 0.000001  loss: 1.9199 (1.8597)  class_acc: 0.7656 (0.7732)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7846  data: 0.0003  max mem: 27222
Epoch: [15]  [ 200/2502]  eta: 0:38:21  lr: 0.000673  min_lr: 0.000001  loss: 1.8389 (1.8569)  class_acc: 0.7812 (0.7726)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7916  data: 0.0003  max mem: 27222
Epoch: [15]  [ 300/2502]  eta: 0:34:19  lr: 0.000672  min_lr: 0.000001  loss: 1.8213 (1.8575)  class_acc: 0.7969 (0.7736)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7900  data: 0.0002  max mem: 27222
Epoch: [15]  [ 400/2502]  eta: 0:31:37  lr: 0.000672  min_lr: 0.000001  loss: 1.8945 (1.8619)  class_acc: 0.7812 (0.7735)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7853  data: 0.0003  max mem: 27222
Epoch: [15]  [ 500/2502]  eta: 0:29:28  lr: 0.000671  min_lr: 0.000001  loss: 1.8975 (1.8648)  class_acc: 0.7500 (0.7727)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7997  data: 0.0002  max mem: 27222
Epoch: [15]  [ 600/2502]  eta: 0:27:34  lr: 0.000671  min_lr: 0.000001  loss: 1.7607 (1.8653)  class_acc: 0.7812 (0.7723)  loss_scale: 32768.0000 (32986.0899)  weight_decay: 0.0500 (0.0500)  time: 0.8010  data: 0.0002  max mem: 27222
Epoch: [15]  [ 700/2502]  eta: 0:25:49  lr: 0.000670  min_lr: 0.000001  loss: 1.8672 (1.8642)  class_acc: 0.7500 (0.7725)  loss_scale: 32768.0000 (32954.9786)  weight_decay: 0.0500 (0.0500)  time: 0.7943  data: 0.0004  max mem: 27222
Epoch: [15]  [ 800/2502]  eta: 0:24:11  lr: 0.000670  min_lr: 0.000001  loss: 1.9160 (1.8680)  class_acc: 0.7656 (0.7717)  loss_scale: 32768.0000 (32931.6355)  weight_decay: 0.0500 (0.0500)  time: 0.7899  data: 0.0002  max mem: 27222
Epoch: [15]  [ 900/2502]  eta: 0:22:37  lr: 0.000669  min_lr: 0.000001  loss: 1.9639 (1.8690)  class_acc: 0.7344 (0.7707)  loss_scale: 32768.0000 (32913.4739)  weight_decay: 0.0500 (0.0500)  time: 0.7953  data: 0.0003  max mem: 27222
Epoch: [15]  [1000/2502]  eta: 0:21:04  lr: 0.000669  min_lr: 0.000001  loss: 1.7461 (1.8704)  class_acc: 0.7969 (0.7705)  loss_scale: 32768.0000 (32898.9411)  weight_decay: 0.0500 (0.0500)  time: 0.7977  data: 0.0002  max mem: 27222
Epoch: [15]  [1100/2502]  eta: 0:19:36  lr: 0.000669  min_lr: 0.000001  loss: 1.8545 (1.8713)  class_acc: 0.7656 (0.7704)  loss_scale: 32768.0000 (33006.0963)  weight_decay: 0.0500 (0.0500)  time: 0.8074  data: 0.0002  max mem: 27222
Epoch: [15]  [1200/2502]  eta: 0:18:09  lr: 0.000668  min_lr: 0.000001  loss: 1.8662 (1.8727)  class_acc: 0.7656 (0.7702)  loss_scale: 32768.0000 (32986.2714)  weight_decay: 0.0500 (0.0500)  time: 0.8157  data: 0.0003  max mem: 27222
Epoch: [15]  [1300/2502]  eta: 0:16:43  lr: 0.000668  min_lr: 0.000001  loss: 1.8506 (1.8738)  class_acc: 0.7656 (0.7700)  loss_scale: 32768.0000 (32969.4942)  weight_decay: 0.0500 (0.0500)  time: 0.7962  data: 0.0005  max mem: 27222
Epoch: [15]  [1400/2502]  eta: 0:15:17  lr: 0.000667  min_lr: 0.000001  loss: 1.8223 (1.8736)  class_acc: 0.7812 (0.7699)  loss_scale: 32768.0000 (32955.1121)  weight_decay: 0.0500 (0.0500)  time: 0.8129  data: 0.0002  max mem: 27222
Epoch: [15]  [1500/2502]  eta: 0:13:53  lr: 0.000667  min_lr: 0.000001  loss: 1.8340 (1.8723)  class_acc: 0.7812 (0.7700)  loss_scale: 32768.0000 (32942.6462)  weight_decay: 0.0500 (0.0500)  time: 0.8260  data: 0.0002  max mem: 27222
Epoch: [15]  [1600/2502]  eta: 0:12:29  lr: 0.000666  min_lr: 0.000001  loss: 1.8555 (1.8738)  class_acc: 0.7656 (0.7699)  loss_scale: 32768.0000 (33095.4753)  weight_decay: 0.0500 (0.0500)  time: 0.7918  data: 0.0025  max mem: 27222
Epoch: [15]  [1700/2502]  eta: 0:11:04  lr: 0.000666  min_lr: 0.000001  loss: 1.8555 (1.8740)  class_acc: 0.7500 (0.7697)  loss_scale: 32768.0000 (33076.2234)  weight_decay: 0.0500 (0.0500)  time: 0.8139  data: 0.0002  max mem: 27222
Epoch: [15]  [1800/2502]  eta: 0:09:41  lr: 0.000665  min_lr: 0.000001  loss: 1.8428 (1.8734)  class_acc: 0.7656 (0.7698)  loss_scale: 32768.0000 (33059.1094)  weight_decay: 0.0500 (0.0500)  time: 0.7887  data: 0.0002  max mem: 27222
Epoch: [15]  [1900/2502]  eta: 0:08:18  lr: 0.000665  min_lr: 0.000001  loss: 1.8643 (1.8740)  class_acc: 0.7812 (0.7698)  loss_scale: 32768.0000 (33043.7959)  weight_decay: 0.0500 (0.0500)  time: 0.8339  data: 0.0002  max mem: 27222
Epoch: [15]  [2000/2502]  eta: 0:06:54  lr: 0.000664  min_lr: 0.000001  loss: 1.8389 (1.8747)  class_acc: 0.7656 (0.7696)  loss_scale: 32768.0000 (33030.0130)  weight_decay: 0.0500 (0.0500)  time: 0.8076  data: 0.0002  max mem: 27222
Epoch: [15]  [2100/2502]  eta: 0:05:31  lr: 0.000664  min_lr: 0.000001  loss: 1.7959 (1.8753)  class_acc: 0.7812 (0.7695)  loss_scale: 32768.0000 (33142.3132)  weight_decay: 0.0500 (0.0500)  time: 0.8064  data: 0.0002  max mem: 27222
Epoch: [15]  [2200/2502]  eta: 0:04:09  lr: 0.000663  min_lr: 0.000001  loss: 1.8965 (1.8766)  class_acc: 0.7812 (0.7693)  loss_scale: 32768.0000 (33125.3067)  weight_decay: 0.0500 (0.0500)  time: 0.8200  data: 0.0002  max mem: 27222
Epoch: [15]  [2300/2502]  eta: 0:02:46  lr: 0.000663  min_lr: 0.000001  loss: 1.8799 (1.8789)  class_acc: 0.7500 (0.7688)  loss_scale: 32768.0000 (33109.7784)  weight_decay: 0.0500 (0.0500)  time: 0.7950  data: 0.0003  max mem: 27222
Epoch: [15]  [2400/2502]  eta: 0:01:23  lr: 0.000662  min_lr: 0.000001  loss: 1.8301 (1.8779)  class_acc: 0.7812 (0.7688)  loss_scale: 32768.0000 (33095.5435)  weight_decay: 0.0500 (0.0500)  time: 0.8020  data: 0.0002  max mem: 27222
Epoch: [15]  [2500/2502]  eta: 0:00:01  lr: 0.000662  min_lr: 0.000001  loss: 1.9609 (1.8787)  class_acc: 0.7656 (0.7686)  loss_scale: 32768.0000 (33082.5728)  weight_decay: 0.0500 (0.0500)  time: 0.7478  data: 0.0009  max mem: 27222
Epoch: [15]  [2501/2502]  eta: 0:00:00  lr: 0.000662  min_lr: 0.000001  loss: 1.9609 (1.8787)  class_acc: 0.7656 (0.7686)  loss_scale: 32768.0000 (33082.5728)  weight_decay: 0.0500 (0.0500)  time: 0.7094  data: 0.0009  max mem: 27222
Epoch: [15] Total time: 0:34:16 (0.8220 s / it)
Averaged stats: lr: 0.000662  min_lr: 0.000001  loss: 1.9609 (1.8763)  class_acc: 0.7656 (0.7696)  loss_scale: 32768.0000 (33082.5728)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Test:  [ 0/98]  eta: 0:08:20  loss: 0.4452 (0.4452)  acc1: 89.0625 (89.0625)  acc5: 98.4375 (98.4375)  time: 5.1027  data: 4.7557  max mem: 27222
Test:  [97/98]  eta: 0:00:00  loss: 0.7464 (0.6919)  acc1: 81.2500 (83.4560)  acc5: 96.8750 (97.4080)  time: 0.3352  data: 0.0002  max mem: 27222
Test: Total time: 0:00:40 (0.4150 s / it)
* Acc@1 83.736 Acc@5 97.166 loss 0.691
Accuracy of the network on the 50000 test images: 83.7%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Test:  [ 0/98]  eta: 0:05:04  loss: 0.3345 (0.3345)  acc1: 92.1875 (92.1875)  acc5: 100.0000 (100.0000)  time: 3.1101  data: 2.7706  max mem: 27222
Test:  [97/98]  eta: 0:00:00  loss: 0.6702 (0.6299)  acc1: 82.8125 (85.0080)  acc5: 98.4375 (97.9840)  time: 0.3381  data: 0.0004  max mem: 27222
Test: Total time: 0:00:41 (0.4185 s / it)
* Acc@1 85.372 Acc@5 97.834 loss 0.630
EMA Accuracy of the network on the 50000 test images: 85.4%
Max accuracy: 85.37%
{"train_lr": 0.000667823658986441, "train_min_lr": 8.722240466667466e-07, "train_loss": 1.876301513671875, "train_class_acc": 0.769590625, "train_loss_scale": 33082.5728, "train_weight_decay": 0.04999999999999801, "test_loss": 0.6296244440030079, "test_acc1": 85.37199999908447, "test_acc5": 97.83400000732422, "epoch": 15, "n_parameters": 86860264}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Epoch: [16]  [   0/2502]  eta: 1 day, 4:54:16  lr: 0.000662  min_lr: 0.000001  loss: 1.6631 (1.6631)  class_acc: 0.8438 (0.8438)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 41.5891  data: 32.3808  max mem: 27222
Epoch: [16]  [ 100/2502]  eta: 0:48:05  lr: 0.000661  min_lr: 0.000001  loss: 1.7812 (1.8513)  class_acc: 0.7656 (0.7706)  loss_scale: 32768.0000 (35363.4851)  weight_decay: 0.0500 (0.0500)  time: 0.7852  data: 0.0005  max mem: 27222
Epoch: [16]  [ 200/2502]  eta: 0:38:23  lr: 0.000661  min_lr: 0.000001  loss: 1.7842 (1.8405)  class_acc: 0.7812 (0.7769)  loss_scale: 32768.0000 (34072.1990)  weight_decay: 0.0500 (0.0500)  time: 0.7889  data: 0.0002  max mem: 27222
Epoch: [16]  [ 300/2502]  eta: 0:34:16  lr: 0.000660  min_lr: 0.000001  loss: 1.8486 (1.8448)  class_acc: 0.7812 (0.7767)  loss_scale: 32768.0000 (33638.9103)  weight_decay: 0.0500 (0.0500)  time: 0.8072  data: 0.0002  max mem: 27222
Epoch: [16]  [ 400/2502]  eta: 0:31:34  lr: 0.000660  min_lr: 0.000001  loss: 1.8311 (1.8448)  class_acc: 0.7812 (0.7773)  loss_scale: 32768.0000 (33421.7257)  weight_decay: 0.0500 (0.0500)  time: 0.8024  data: 0.0002  max mem: 27222
Epoch: [16]  [ 500/2502]  eta: 0:29:26  lr: 0.000659  min_lr: 0.000001  loss: 1.8506 (1.8456)  class_acc: 0.7656 (0.7776)  loss_scale: 32768.0000 (33291.2415)  weight_decay: 0.0500 (0.0500)  time: 0.7955  data: 0.0009  max mem: 27222
Epoch: [16]  [ 600/2502]  eta: 0:27:33  lr: 0.000659  min_lr: 0.000001  loss: 1.8398 (1.8494)  class_acc: 0.7812 (0.7768)  loss_scale: 32768.0000 (33204.1797)  weight_decay: 0.0500 (0.0500)  time: 0.7952  data: 0.0003  max mem: 27222
Epoch: [16]  [ 700/2502]  eta: 0:25:50  lr: 0.000658  min_lr: 0.000001  loss: 1.9014 (1.8505)  class_acc: 0.7656 (0.7765)  loss_scale: 32768.0000 (34263.8288)  weight_decay: 0.0500 (0.0500)  time: 0.7943  data: 0.0002  max mem: 27222
Epoch: [16]  [ 800/2502]  eta: 0:24:16  lr: 0.000658  min_lr: 0.000001  loss: 1.8633 (1.8517)  class_acc: 0.7812 (0.7767)  loss_scale: 32768.0000 (34077.0836)  weight_decay: 0.0500 (0.0500)  time: 0.8399  data: 0.0002  max mem: 27222
Epoch: [16]  [ 900/2502]  eta: 0:22:43  lr: 0.000657  min_lr: 0.000001  loss: 1.8428 (1.8509)  class_acc: 0.7656 (0.7763)  loss_scale: 32768.0000 (33931.7913)  weight_decay: 0.0500 (0.0500)  time: 0.7997  data: 0.0002  max mem: 27222
Epoch: [16]  [1000/2502]  eta: 0:21:12  lr: 0.000657  min_lr: 0.000001  loss: 1.8604 (1.8526)  class_acc: 0.7656 (0.7759)  loss_scale: 32768.0000 (33815.5285)  weight_decay: 0.0500 (0.0500)  time: 0.8377  data: 0.0004  max mem: 27222
Epoch: [16]  [1100/2502]  eta: 0:19:43  lr: 0.000656  min_lr: 0.000001  loss: 1.8652 (1.8550)  class_acc: 0.7812 (0.7753)  loss_scale: 32768.0000 (33720.3851)  weight_decay: 0.0500 (0.0500)  time: 0.8026  data: 0.0002  max mem: 27222
Epoch: [16]  [1200/2502]  eta: 0:18:14  lr: 0.000656  min_lr: 0.000001  loss: 1.8047 (1.8551)  class_acc: 0.7812 (0.7752)  loss_scale: 32768.0000 (33750.2215)  weight_decay: 0.0500 (0.0500)  time: 0.7953  data: 0.0002  max mem: 27222
Epoch: [16]  [1300/2502]  eta: 0:16:47  lr: 0.000655  min_lr: 0.000001  loss: 1.8652 (1.8541)  class_acc: 0.7812 (0.7747)  loss_scale: 32768.0000 (33674.7241)  weight_decay: 0.0500 (0.0500)  time: 0.7986  data: 0.0002  max mem: 27222
Epoch: [16]  [1400/2502]  eta: 0:15:21  lr: 0.000655  min_lr: 0.000001  loss: 1.8564 (1.8551)  class_acc: 0.7656 (0.7744)  loss_scale: 32768.0000 (33610.0043)  weight_decay: 0.0500 (0.0500)  time: 0.7956  data: 0.0002  max mem: 27222
Epoch: [16]  [1500/2502]  eta: 0:13:56  lr: 0.000654  min_lr: 0.000001  loss: 1.8379 (1.8554)  class_acc: 0.7812 (0.7744)  loss_scale: 32768.0000 (33553.9081)  weight_decay: 0.0500 (0.0500)  time: 0.8320  data: 0.0002  max mem: 27222
Epoch: [16]  [1600/2502]  eta: 0:12:31  lr: 0.000654  min_lr: 0.000001  loss: 1.7393 (1.8551)  class_acc: 0.7812 (0.7744)  loss_scale: 32768.0000 (33504.8195)  weight_decay: 0.0500 (0.0500)  time: 0.8431  data: 0.0003  max mem: 27222
Epoch: [16]  [1700/2502]  eta: 0:11:06  lr: 0.000653  min_lr: 0.000001  loss: 1.8359 (1.8549)  class_acc: 0.7969 (0.7746)  loss_scale: 32768.0000 (33615.6143)  weight_decay: 0.0500 (0.0500)  time: 0.7990  data: 0.0002  max mem: 27222
Epoch: [16]  [1800/2502]  eta: 0:09:42  lr: 0.000652  min_lr: 0.000001  loss: 1.9492 (1.8560)  class_acc: 0.7500 (0.7742)  loss_scale: 32768.0000 (33568.5508)  weight_decay: 0.0500 (0.0500)  time: 0.8097  data: 0.0002  max mem: 27222
Epoch: [16]  [1900/2502]  eta: 0:08:18  lr: 0.000652  min_lr: 0.000001  loss: 1.9043 (1.8571)  class_acc: 0.7656 (0.7738)  loss_scale: 32768.0000 (33526.4387)  weight_decay: 0.0500 (0.0500)  time: 0.8051  data: 0.0002  max mem: 27222
Epoch: [16]  [2000/2502]  eta: 0:06:55  lr: 0.000651  min_lr: 0.000001  loss: 1.7705 (1.8570)  class_acc: 0.7656 (0.7738)  loss_scale: 32768.0000 (33488.5357)  weight_decay: 0.0500 (0.0500)  time: 0.8386  data: 0.0002  max mem: 27222
Epoch: [16]  [2100/2502]  eta: 0:05:32  lr: 0.000651  min_lr: 0.000001  loss: 1.8076 (1.8566)  class_acc: 0.7812 (0.7737)  loss_scale: 32768.0000 (33454.2408)  weight_decay: 0.0500 (0.0500)  time: 0.8333  data: 0.0002  max mem: 27222
Epoch: [16]  [2200/2502]  eta: 0:04:09  lr: 0.000650  min_lr: 0.000001  loss: 1.8193 (1.8573)  class_acc: 0.7812 (0.7737)  loss_scale: 32768.0000 (33452.8378)  weight_decay: 0.0500 (0.0500)  time: 0.7986  data: 0.0002  max mem: 27222
Epoch: [16]  [2300/2502]  eta: 0:02:46  lr: 0.000650  min_lr: 0.000001  loss: 1.9141 (1.8571)  class_acc: 0.7500 (0.7737)  loss_scale: 32768.0000 (33451.5567)  weight_decay: 0.0500 (0.0500)  time: 0.8179  data: 0.0002  max mem: 27222
Epoch: [16]  [2400/2502]  eta: 0:01:24  lr: 0.000649  min_lr: 0.000001  loss: 1.8545 (1.8573)  class_acc: 0.7500 (0.7735)  loss_scale: 32768.0000 (33423.0870)  weight_decay: 0.0500 (0.0500)  time: 0.7971  data: 0.0002  max mem: 27222
Epoch: [16]  [2500/2502]  eta: 0:00:01  lr: 0.000649  min_lr: 0.000001  loss: 1.8203 (1.8571)  class_acc: 0.7812 (0.7735)  loss_scale: 32768.0000 (33397.1456)  weight_decay: 0.0500 (0.0500)  time: 0.7559  data: 0.0012  max mem: 27222
Epoch: [16]  [2501/2502]  eta: 0:00:00  lr: 0.000649  min_lr: 0.000001  loss: 1.8203 (1.8571)  class_acc: 0.7812 (0.7735)  loss_scale: 32768.0000 (33397.1456)  weight_decay: 0.0500 (0.0500)  time: 0.7162  data: 0.0012  max mem: 27222
Epoch: [16] Total time: 0:34:23 (0.8246 s / it)
Averaged stats: lr: 0.000649  min_lr: 0.000001  loss: 1.8203 (1.8542)  class_acc: 0.7812 (0.7751)  loss_scale: 32768.0000 (33397.1456)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Test:  [ 0/98]  eta: 0:06:04  loss: 0.3672 (0.3672)  acc1: 90.6250 (90.6250)  acc5: 100.0000 (100.0000)  time: 3.7149  data: 3.3748  max mem: 27222
Test:  [97/98]  eta: 0:00:00  loss: 0.6756 (0.6793)  acc1: 81.2500 (83.6960)  acc5: 96.8750 (97.3120)  time: 0.3331  data: 0.0002  max mem: 27222
Test: Total time: 0:00:40 (0.4165 s / it)
* Acc@1 84.050 Acc@5 97.210 loss 0.675
Accuracy of the network on the 50000 test images: 84.1%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Test:  [ 0/98]  eta: 0:11:46  loss: 0.3072 (0.3072)  acc1: 92.1875 (92.1875)  acc5: 100.0000 (100.0000)  time: 7.2110  data: 6.8697  max mem: 27222
Test:  [97/98]  eta: 0:00:00  loss: 0.6515 (0.6057)  acc1: 84.3750 (85.2480)  acc5: 98.4375 (98.0000)  time: 0.3334  data: 0.0002  max mem: 27222
Test: Total time: 0:00:41 (0.4222 s / it)
* Acc@1 85.618 Acc@5 97.870 loss 0.605
EMA Accuracy of the network on the 50000 test images: 85.6%
Max accuracy: 85.62%
{"train_lr": 0.0006553697120600687, "train_min_lr": 8.559583276570587e-07, "train_loss": 1.854233056640625, "train_class_acc": 0.77508125, "train_loss_scale": 33397.1456, "train_weight_decay": 0.04999999999999801, "test_loss": 0.6047348130722435, "test_acc1": 85.61800000183105, "test_acc5": 97.87000000732422, "epoch": 16, "n_parameters": 86860264}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Epoch: [17]  [   0/2502]  eta: 1 day, 4:54:03  lr: 0.000648  min_lr: 0.000001  loss: 1.5205 (1.5205)  class_acc: 0.8125 (0.8125)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 41.5843  data: 32.0960  max mem: 27222
Epoch: [17]  [ 100/2502]  eta: 0:47:34  lr: 0.000648  min_lr: 0.000001  loss: 1.8838 (1.8259)  class_acc: 0.7656 (0.7825)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7869  data: 0.0002  max mem: 27222
Epoch: [17]  [ 200/2502]  eta: 0:38:12  lr: 0.000647  min_lr: 0.000001  loss: 1.8027 (1.8271)  class_acc: 0.7656 (0.7830)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8013  data: 0.0002  max mem: 27222
Epoch: [17]  [ 300/2502]  eta: 0:34:16  lr: 0.000647  min_lr: 0.000001  loss: 1.7891 (1.8232)  class_acc: 0.7812 (0.7858)  loss_scale: 32768.0000 (34074.3654)  weight_decay: 0.0500 (0.0500)  time: 0.8112  data: 0.0002  max mem: 27222
Epoch: [17]  [ 400/2502]  eta: 0:31:36  lr: 0.000646  min_lr: 0.000001  loss: 1.9023 (1.8286)  class_acc: 0.7500 (0.7844)  loss_scale: 32768.0000 (33748.5885)  weight_decay: 0.0500 (0.0500)  time: 0.7979  data: 0.0002  max mem: 27222
Epoch: [17]  [ 500/2502]  eta: 0:29:28  lr: 0.000646  min_lr: 0.000001  loss: 1.7686 (1.8296)  class_acc: 0.7969 (0.7839)  loss_scale: 32768.0000 (33552.8623)  weight_decay: 0.0500 (0.0500)  time: 0.7902  data: 0.0002  max mem: 27222
Epoch: [17]  [ 600/2502]  eta: 0:27:34  lr: 0.000645  min_lr: 0.000001  loss: 1.7998 (1.8289)  class_acc: 0.7812 (0.7833)  loss_scale: 32768.0000 (33422.2696)  weight_decay: 0.0500 (0.0500)  time: 0.7933  data: 0.0003  max mem: 27222
Epoch: [17]  [ 700/2502]  eta: 0:25:54  lr: 0.000644  min_lr: 0.000001  loss: 1.8584 (1.8318)  class_acc: 0.7656 (0.7825)  loss_scale: 32768.0000 (33328.9358)  weight_decay: 0.0500 (0.0500)  time: 0.8247  data: 0.0002  max mem: 27222
Epoch: [17]  [ 800/2502]  eta: 0:24:14  lr: 0.000644  min_lr: 0.000001  loss: 1.7695 (1.8333)  class_acc: 0.7812 (0.7819)  loss_scale: 32768.0000 (33749.8127)  weight_decay: 0.0500 (0.0500)  time: 0.7928  data: 0.0002  max mem: 27222
Epoch: [17]  [ 900/2502]  eta: 0:22:40  lr: 0.000643  min_lr: 0.000001  loss: 1.7383 (1.8321)  class_acc: 0.8125 (0.7821)  loss_scale: 32768.0000 (33640.8435)  weight_decay: 0.0500 (0.0500)  time: 0.7951  data: 0.0002  max mem: 27222
Epoch: [17]  [1000/2502]  eta: 0:21:11  lr: 0.000643  min_lr: 0.000001  loss: 1.9141 (1.8324)  class_acc: 0.7656 (0.7818)  loss_scale: 32768.0000 (33553.6464)  weight_decay: 0.0500 (0.0500)  time: 0.7947  data: 0.0002  max mem: 27222
Epoch: [17]  [1100/2502]  eta: 0:19:43  lr: 0.000642  min_lr: 0.000001  loss: 1.7695 (1.8362)  class_acc: 0.7812 (0.7807)  loss_scale: 32768.0000 (33482.2888)  weight_decay: 0.0500 (0.0500)  time: 0.8259  data: 0.0002  max mem: 27222
Epoch: [17]  [1200/2502]  eta: 0:18:15  lr: 0.000641  min_lr: 0.000001  loss: 1.8086 (1.8383)  class_acc: 0.7812 (0.7805)  loss_scale: 32768.0000 (33422.8143)  weight_decay: 0.0500 (0.0500)  time: 0.8002  data: 0.0002  max mem: 27222
Epoch: [17]  [1300/2502]  eta: 0:16:47  lr: 0.000641  min_lr: 0.000001  loss: 1.7881 (1.8368)  class_acc: 0.7812 (0.7805)  loss_scale: 32768.0000 (33674.7241)  weight_decay: 0.0500 (0.0500)  time: 0.7949  data: 0.0002  max mem: 27222
Epoch: [17]  [1400/2502]  eta: 0:15:21  lr: 0.000640  min_lr: 0.000001  loss: 1.7490 (1.8367)  class_acc: 0.7969 (0.7802)  loss_scale: 32768.0000 (33610.0043)  weight_decay: 0.0500 (0.0500)  time: 0.8043  data: 0.0003  max mem: 27222
Epoch: [17]  [1500/2502]  eta: 0:13:56  lr: 0.000640  min_lr: 0.000001  loss: 1.9150 (1.8380)  class_acc: 0.7500 (0.7800)  loss_scale: 32768.0000 (33553.9081)  weight_decay: 0.0500 (0.0500)  time: 0.8015  data: 0.0002  max mem: 27222
Epoch: [17]  [1600/2502]  eta: 0:12:30  lr: 0.000639  min_lr: 0.000001  loss: 1.7832 (1.8363)  class_acc: 0.7812 (0.7807)  loss_scale: 32768.0000 (33504.8195)  weight_decay: 0.0500 (0.0500)  time: 0.7923  data: 0.0002  max mem: 27222
Epoch: [17]  [1700/2502]  eta: 0:11:06  lr: 0.000638  min_lr: 0.000001  loss: 1.9033 (1.8369)  class_acc: 0.7500 (0.7803)  loss_scale: 32768.0000 (33461.5026)  weight_decay: 0.0500 (0.0500)  time: 0.8093  data: 0.0002  max mem: 27222
Epoch: [17]  [1800/2502]  eta: 0:09:42  lr: 0.000638  min_lr: 0.000001  loss: 1.6826 (1.8368)  class_acc: 0.8125 (0.7801)  loss_scale: 32768.0000 (33422.9961)  weight_decay: 0.0500 (0.0500)  time: 0.8003  data: 0.0002  max mem: 27222
Epoch: [17]  [1900/2502]  eta: 0:08:18  lr: 0.000637  min_lr: 0.000001  loss: 1.9170 (1.8372)  class_acc: 0.7656 (0.7801)  loss_scale: 32768.0000 (33457.4897)  weight_decay: 0.0500 (0.0500)  time: 0.8253  data: 0.0002  max mem: 27222
Epoch: [17]  [2000/2502]  eta: 0:06:55  lr: 0.000636  min_lr: 0.000001  loss: 1.8682 (1.8371)  class_acc: 0.7969 (0.7799)  loss_scale: 32768.0000 (33423.0325)  weight_decay: 0.0500 (0.0500)  time: 0.7912  data: 0.0002  max mem: 27222
Epoch: [17]  [2100/2502]  eta: 0:05:31  lr: 0.000636  min_lr: 0.000001  loss: 1.8906 (1.8373)  class_acc: 0.7656 (0.7800)  loss_scale: 32768.0000 (33391.8553)  weight_decay: 0.0500 (0.0500)  time: 0.8212  data: 0.0002  max mem: 27222
Epoch: [17]  [2200/2502]  eta: 0:04:09  lr: 0.000635  min_lr: 0.000001  loss: 1.8154 (1.8371)  class_acc: 0.7656 (0.7799)  loss_scale: 32768.0000 (33363.5111)  weight_decay: 0.0500 (0.0500)  time: 0.7961  data: 0.0004  max mem: 27222
Epoch: [17]  [2300/2502]  eta: 0:02:46  lr: 0.000635  min_lr: 0.000001  loss: 1.7646 (1.8367)  class_acc: 0.7812 (0.7800)  loss_scale: 32768.0000 (33337.6306)  weight_decay: 0.0500 (0.0500)  time: 0.7939  data: 0.0002  max mem: 27222
Epoch: [17]  [2400/2502]  eta: 0:01:23  lr: 0.000634  min_lr: 0.000001  loss: 1.8486 (1.8355)  class_acc: 0.7812 (0.7802)  loss_scale: 32768.0000 (33423.0870)  weight_decay: 0.0500 (0.0500)  time: 0.8393  data: 0.0002  max mem: 27222
Epoch: [17]  [2500/2502]  eta: 0:00:01  lr: 0.000633  min_lr: 0.000001  loss: 1.8086 (1.8353)  class_acc: 0.7969 (0.7803)  loss_scale: 32768.0000 (33397.1456)  weight_decay: 0.0500 (0.0500)  time: 0.7528  data: 0.0012  max mem: 27222
Epoch: [17]  [2501/2502]  eta: 0:00:00  lr: 0.000633  min_lr: 0.000001  loss: 1.8086 (1.8353)  class_acc: 0.7969 (0.7803)  loss_scale: 32768.0000 (33397.1456)  weight_decay: 0.0500 (0.0500)  time: 0.7153  data: 0.0012  max mem: 27222
Epoch: [17] Total time: 0:34:17 (0.8223 s / it)
Averaged stats: lr: 0.000633  min_lr: 0.000001  loss: 1.8086 (1.8357)  class_acc: 0.7969 (0.7798)  loss_scale: 32768.0000 (33397.1456)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Test:  [ 0/98]  eta: 0:07:21  loss: 0.3125 (0.3125)  acc1: 92.1875 (92.1875)  acc5: 100.0000 (100.0000)  time: 4.5072  data: 4.1508  max mem: 27222
Test:  [97/98]  eta: 0:00:00  loss: 0.7011 (0.6749)  acc1: 84.3750 (84.3360)  acc5: 96.8750 (97.3920)  time: 0.3370  data: 0.0011  max mem: 27222
Test: Total time: 0:00:41 (0.4184 s / it)
* Acc@1 84.240 Acc@5 97.286 loss 0.671
Accuracy of the network on the 50000 test images: 84.2%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Test:  [ 0/98]  eta: 0:10:29  loss: 0.2857 (0.2857)  acc1: 93.7500 (93.7500)  acc5: 100.0000 (100.0000)  time: 6.4192  data: 6.0678  max mem: 27222
Test:  [97/98]  eta: 0:00:00  loss: 0.6421 (0.5894)  acc1: 84.3750 (85.3920)  acc5: 98.4375 (97.9840)  time: 0.3380  data: 0.0007  max mem: 27222
Test: Total time: 0:00:40 (0.4149 s / it)
* Acc@1 85.838 Acc@5 97.900 loss 0.588
EMA Accuracy of the network on the 50000 test images: 85.8%
Max accuracy: 85.84%
{"train_lr": 0.0006410361419793789, "train_min_lr": 8.372376903589817e-07, "train_loss": 1.83569931640625, "train_class_acc": 0.779821875, "train_loss_scale": 33397.1456, "train_weight_decay": 0.04999999999999801, "test_loss": 0.5880314527573635, "test_acc1": 85.83800000305176, "test_acc5": 97.90000000732422, "epoch": 17, "n_parameters": 86860264}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Epoch: [18]  [   0/2502]  eta: 1 day, 4:13:55  lr: 0.000633  min_lr: 0.000001  loss: 1.8750 (1.8750)  class_acc: 0.7344 (0.7344)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 40.6217  data: 30.9397  max mem: 27222
Epoch: [18]  [ 100/2502]  eta: 0:47:16  lr: 0.000633  min_lr: 0.000001  loss: 1.8271 (1.8101)  class_acc: 0.7656 (0.7789)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7870  data: 0.0002  max mem: 27222
Epoch: [18]  [ 200/2502]  eta: 0:38:02  lr: 0.000632  min_lr: 0.000001  loss: 1.7881 (1.8032)  class_acc: 0.7969 (0.7831)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7966  data: 0.0003  max mem: 27222
Epoch: [18]  [ 300/2502]  eta: 0:34:05  lr: 0.000631  min_lr: 0.000001  loss: 1.7803 (1.8108)  class_acc: 0.7812 (0.7832)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7943  data: 0.0002  max mem: 27222
Epoch: [18]  [ 400/2502]  eta: 0:31:31  lr: 0.000631  min_lr: 0.000001  loss: 1.8057 (1.8069)  class_acc: 0.7969 (0.7855)  loss_scale: 32768.0000 (33094.8628)  weight_decay: 0.0500 (0.0500)  time: 0.7952  data: 0.0002  max mem: 27222
Epoch: [18]  [ 500/2502]  eta: 0:29:23  lr: 0.000630  min_lr: 0.000001  loss: 1.7549 (1.8111)  class_acc: 0.7969 (0.7853)  loss_scale: 32768.0000 (33029.6208)  weight_decay: 0.0500 (0.0500)  time: 0.8037  data: 0.0002  max mem: 27222
Epoch: [18]  [ 600/2502]  eta: 0:27:29  lr: 0.000629  min_lr: 0.000001  loss: 1.6719 (1.8099)  class_acc: 0.8125 (0.7859)  loss_scale: 32768.0000 (32986.0899)  weight_decay: 0.0500 (0.0500)  time: 0.7932  data: 0.0003  max mem: 27222
Epoch: [18]  [ 700/2502]  eta: 0:25:47  lr: 0.000629  min_lr: 0.000001  loss: 1.8105 (1.8129)  class_acc: 0.7812 (0.7847)  loss_scale: 32768.0000 (32954.9786)  weight_decay: 0.0500 (0.0500)  time: 0.7917  data: 0.0002  max mem: 27222
Epoch: [18]  [ 800/2502]  eta: 0:24:10  lr: 0.000628  min_lr: 0.000001  loss: 1.7920 (1.8109)  class_acc: 0.7812 (0.7853)  loss_scale: 32768.0000 (32931.6355)  weight_decay: 0.0500 (0.0500)  time: 0.7938  data: 0.0002  max mem: 27222
Epoch: [18]  [ 900/2502]  eta: 0:22:38  lr: 0.000627  min_lr: 0.000001  loss: 1.8184 (1.8123)  class_acc: 0.7656 (0.7850)  loss_scale: 32768.0000 (33058.9478)  weight_decay: 0.0500 (0.0500)  time: 0.8091  data: 0.0002  max mem: 27222
Epoch: [18]  [1000/2502]  eta: 0:21:08  lr: 0.000627  min_lr: 0.000001  loss: 1.7969 (1.8126)  class_acc: 0.7656 (0.7847)  loss_scale: 32768.0000 (33029.8821)  weight_decay: 0.0500 (0.0500)  time: 0.7928  data: 0.0002  max mem: 27222
Epoch: [18]  [1100/2502]  eta: 0:19:40  lr: 0.000626  min_lr: 0.000001  loss: 1.8223 (1.8134)  class_acc: 0.7812 (0.7850)  loss_scale: 32768.0000 (33006.0963)  weight_decay: 0.0500 (0.0500)  time: 0.7998  data: 0.0002  max mem: 27222
Epoch: [18]  [1200/2502]  eta: 0:18:12  lr: 0.000625  min_lr: 0.000001  loss: 1.7920 (1.8116)  class_acc: 0.7812 (0.7854)  loss_scale: 32768.0000 (32986.2714)  weight_decay: 0.0500 (0.0500)  time: 0.8043  data: 0.0003  max mem: 27222
Epoch: [18]  [1300/2502]  eta: 0:16:46  lr: 0.000625  min_lr: 0.000001  loss: 1.7959 (1.8132)  class_acc: 0.7969 (0.7856)  loss_scale: 32768.0000 (32969.4942)  weight_decay: 0.0500 (0.0500)  time: 0.8232  data: 0.0002  max mem: 27222
Epoch: [18]  [1400/2502]  eta: 0:15:19  lr: 0.000624  min_lr: 0.000001  loss: 1.7686 (1.8091)  class_acc: 0.8125 (0.7866)  loss_scale: 32768.0000 (33048.6681)  weight_decay: 0.0500 (0.0500)  time: 0.7957  data: 0.0002  max mem: 27222
Epoch: [18]  [1500/2502]  eta: 0:13:54  lr: 0.000623  min_lr: 0.000001  loss: 1.7461 (1.8115)  class_acc: 0.7812 (0.7859)  loss_scale: 32768.0000 (33029.9694)  weight_decay: 0.0500 (0.0500)  time: 0.8410  data: 0.0002  max mem: 27222
Epoch: [18]  [1600/2502]  eta: 0:12:30  lr: 0.000623  min_lr: 0.000001  loss: 1.8018 (1.8122)  class_acc: 0.7812 (0.7856)  loss_scale: 32768.0000 (33013.6065)  weight_decay: 0.0500 (0.0500)  time: 0.7993  data: 0.0003  max mem: 27222
Epoch: [18]  [1700/2502]  eta: 0:11:06  lr: 0.000622  min_lr: 0.000001  loss: 1.7812 (1.8127)  class_acc: 0.7812 (0.7853)  loss_scale: 32768.0000 (32999.1675)  weight_decay: 0.0500 (0.0500)  time: 0.8364  data: 0.0003  max mem: 27222
Epoch: [18]  [1800/2502]  eta: 0:09:41  lr: 0.000621  min_lr: 0.000001  loss: 1.7881 (1.8137)  class_acc: 0.7812 (0.7850)  loss_scale: 32768.0000 (32986.3320)  weight_decay: 0.0500 (0.0500)  time: 0.8056  data: 0.0002  max mem: 27222
Epoch: [18]  [1900/2502]  eta: 0:08:18  lr: 0.000620  min_lr: 0.000001  loss: 1.8408 (1.8150)  class_acc: 0.7656 (0.7848)  loss_scale: 32768.0000 (32974.8469)  weight_decay: 0.0500 (0.0500)  time: 0.8268  data: 0.0002  max mem: 27222
Epoch: [18]  [2000/2502]  eta: 0:06:55  lr: 0.000620  min_lr: 0.000001  loss: 1.8916 (1.8156)  class_acc: 0.7812 (0.7848)  loss_scale: 32768.0000 (33161.0195)  weight_decay: 0.0500 (0.0500)  time: 0.8394  data: 0.0002  max mem: 27222
Epoch: [18]  [2100/2502]  eta: 0:05:32  lr: 0.000619  min_lr: 0.000001  loss: 1.8525 (1.8159)  class_acc: 0.7812 (0.7851)  loss_scale: 16384.0000 (32845.9819)  weight_decay: 0.0500 (0.0500)  time: 0.8277  data: 0.0015  max mem: 27222
Epoch: [18]  [2200/2502]  eta: 0:04:09  lr: 0.000618  min_lr: 0.000001  loss: 1.8447 (1.8168)  class_acc: 0.7969 (0.7847)  loss_scale: 16384.0000 (32098.0500)  weight_decay: 0.0500 (0.0500)  time: 0.8108  data: 0.0002  max mem: 27222
Epoch: [18]  [2300/2502]  eta: 0:02:46  lr: 0.000618  min_lr: 0.000001  loss: 1.7158 (1.8165)  class_acc: 0.7969 (0.7849)  loss_scale: 16384.0000 (31415.1273)  weight_decay: 0.0500 (0.0500)  time: 0.8066  data: 0.0002  max mem: 27222
Epoch: [18]  [2400/2502]  eta: 0:01:24  lr: 0.000617  min_lr: 0.000001  loss: 1.8691 (1.8167)  class_acc: 0.7812 (0.7849)  loss_scale: 16384.0000 (30789.0912)  weight_decay: 0.0500 (0.0500)  time: 0.8035  data: 0.0002  max mem: 27222
Epoch: [18]  [2500/2502]  eta: 0:00:01  lr: 0.000616  min_lr: 0.000001  loss: 1.8105 (1.8169)  class_acc: 0.7812 (0.7846)  loss_scale: 16384.0000 (30218.6496)  weight_decay: 0.0500 (0.0500)  time: 0.7483  data: 0.0009  max mem: 27222
Epoch: [18]  [2501/2502]  eta: 0:00:00  lr: 0.000616  min_lr: 0.000001  loss: 1.8105 (1.8169)  class_acc: 0.7812 (0.7846)  loss_scale: 16384.0000 (30218.6496)  weight_decay: 0.0500 (0.0500)  time: 0.7112  data: 0.0009  max mem: 27222
Epoch: [18] Total time: 0:34:19 (0.8232 s / it)
Averaged stats: lr: 0.000616  min_lr: 0.000001  loss: 1.8105 (1.8157)  class_acc: 0.7812 (0.7853)  loss_scale: 16384.0000 (30218.6496)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Test:  [ 0/98]  eta: 0:05:15  loss: 0.3308 (0.3308)  acc1: 93.7500 (93.7500)  acc5: 100.0000 (100.0000)  time: 3.2208  data: 2.8815  max mem: 27222
Test:  [97/98]  eta: 0:00:00  loss: 0.6926 (0.6706)  acc1: 82.8125 (84.4000)  acc5: 96.8750 (97.2640)  time: 0.3399  data: 0.0006  max mem: 27222
Test: Total time: 0:00:40 (0.4134 s / it)
* Acc@1 84.180 Acc@5 97.226 loss 0.677
Accuracy of the network on the 50000 test images: 84.2%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Test:  [ 0/98]  eta: 0:08:58  loss: 0.2680 (0.2680)  acc1: 93.7500 (93.7500)  acc5: 100.0000 (100.0000)  time: 5.4928  data: 5.1571  max mem: 27222
Test:  [97/98]  eta: 0:00:00  loss: 0.6282 (0.5782)  acc1: 84.3750 (85.6640)  acc5: 98.4375 (98.0320)  time: 0.3406  data: 0.0040  max mem: 27222
Test: Total time: 0:00:41 (0.4210 s / it)
* Acc@1 85.964 Acc@5 97.928 loss 0.576
EMA Accuracy of the network on the 50000 test images: 86.0%
Max accuracy: 85.96%
{"train_lr": 0.0006249113199703138, "train_min_lr": 8.161775537266896e-07, "train_loss": 1.815709326171875, "train_class_acc": 0.785325, "train_loss_scale": 30218.6496, "train_weight_decay": 0.04999999999999801, "test_loss": 0.576413142095719, "test_acc1": 85.96400000823975, "test_acc5": 97.92800000732421, "epoch": 18, "n_parameters": 86860264}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Epoch: [19]  [   0/2502]  eta: 1 day, 4:57:41  lr: 0.000616  min_lr: 0.000001  loss: 1.8213 (1.8213)  class_acc: 0.8125 (0.8125)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 41.6713  data: 33.0670  max mem: 27222
Epoch: [19]  [ 100/2502]  eta: 0:47:50  lr: 0.000616  min_lr: 0.000001  loss: 1.8574 (1.7929)  class_acc: 0.7812 (0.7959)  loss_scale: 32768.0000 (19952.7921)  weight_decay: 0.0500 (0.0500)  time: 0.7847  data: 0.0002  max mem: 27222
Epoch: [19]  [ 200/2502]  eta: 0:38:23  lr: 0.000615  min_lr: 0.000001  loss: 1.6846 (1.7706)  class_acc: 0.8281 (0.7972)  loss_scale: 32768.0000 (26328.5174)  weight_decay: 0.0500 (0.0500)  time: 0.7912  data: 0.0002  max mem: 27222
Epoch: [19]  [ 300/2502]  eta: 0:34:17  lr: 0.000614  min_lr: 0.000001  loss: 1.8252 (1.7742)  class_acc: 0.7812 (0.7974)  loss_scale: 32768.0000 (28467.8804)  weight_decay: 0.0500 (0.0500)  time: 0.8092  data: 0.0002  max mem: 27222
Epoch: [19]  [ 400/2502]  eta: 0:31:38  lr: 0.000613  min_lr: 0.000001  loss: 1.7793 (1.7769)  class_acc: 0.7969 (0.7970)  loss_scale: 32768.0000 (29540.2294)  weight_decay: 0.0500 (0.0500)  time: 0.7952  data: 0.0002  max mem: 27222
Epoch: [19]  [ 500/2502]  eta: 0:29:32  lr: 0.000613  min_lr: 0.000001  loss: 1.7266 (1.7783)  class_acc: 0.8125 (0.7977)  loss_scale: 32768.0000 (30184.4950)  weight_decay: 0.0500 (0.0500)  time: 0.7968  data: 0.0002  max mem: 27222
Epoch: [19]  [ 600/2502]  eta: 0:27:43  lr: 0.000612  min_lr: 0.000001  loss: 1.7764 (1.7772)  class_acc: 0.8125 (0.7980)  loss_scale: 32768.0000 (30832.4526)  weight_decay: 0.0500 (0.0500)  time: 0.7943  data: 0.0002  max mem: 27222
Epoch: [19]  [ 700/2502]  eta: 0:25:57  lr: 0.000611  min_lr: 0.000001  loss: 1.8037 (1.7812)  class_acc: 0.7812 (0.7974)  loss_scale: 32768.0000 (31108.5649)  weight_decay: 0.0500 (0.0500)  time: 0.7981  data: 0.0002  max mem: 27222
Epoch: [19]  [ 800/2502]  eta: 0:24:18  lr: 0.000610  min_lr: 0.000001  loss: 1.6982 (1.7801)  class_acc: 0.8125 (0.7968)  loss_scale: 32768.0000 (31315.7353)  weight_decay: 0.0500 (0.0500)  time: 0.8020  data: 0.0004  max mem: 27222
Epoch: [19]  [ 900/2502]  eta: 0:22:45  lr: 0.000610  min_lr: 0.000001  loss: 1.8135 (1.7844)  class_acc: 0.7969 (0.7955)  loss_scale: 32768.0000 (31476.9190)  weight_decay: 0.0500 (0.0500)  time: 0.7942  data: 0.0002  max mem: 27222
Epoch: [19]  [1000/2502]  eta: 0:21:12  lr: 0.000609  min_lr: 0.000001  loss: 1.7861 (1.7858)  class_acc: 0.7812 (0.7948)  loss_scale: 32768.0000 (31605.8981)  weight_decay: 0.0500 (0.0500)  time: 0.8093  data: 0.0002  max mem: 27222
Epoch: [19]  [1100/2502]  eta: 0:19:43  lr: 0.000608  min_lr: 0.000001  loss: 1.7939 (1.7875)  class_acc: 0.7812 (0.7944)  loss_scale: 32768.0000 (31711.4478)  weight_decay: 0.0500 (0.0500)  time: 0.8433  data: 0.0002  max mem: 27222
Epoch: [19]  [1200/2502]  eta: 0:18:15  lr: 0.000608  min_lr: 0.000001  loss: 1.6562 (1.7874)  class_acc: 0.8125 (0.7942)  loss_scale: 32768.0000 (32126.8276)  weight_decay: 0.0500 (0.0500)  time: 0.8249  data: 0.0003  max mem: 27222
Epoch: [19]  [1300/2502]  eta: 0:16:48  lr: 0.000607  min_lr: 0.000001  loss: 1.7461 (1.7907)  class_acc: 0.7812 (0.7934)  loss_scale: 32768.0000 (32176.1107)  weight_decay: 0.0500 (0.0500)  time: 0.7945  data: 0.0002  max mem: 27222
Epoch: [19]  [1400/2502]  eta: 0:15:22  lr: 0.000606  min_lr: 0.000001  loss: 1.8262 (1.7906)  class_acc: 0.7656 (0.7935)  loss_scale: 32768.0000 (32218.3583)  weight_decay: 0.0500 (0.0500)  time: 0.7919  data: 0.0002  max mem: 27222
Epoch: [19]  [1500/2502]  eta: 0:13:57  lr: 0.000605  min_lr: 0.000001  loss: 1.6973 (1.7904)  class_acc: 0.7969 (0.7931)  loss_scale: 32768.0000 (32254.9767)  weight_decay: 0.0500 (0.0500)  time: 0.8082  data: 0.0002  max mem: 27222
Epoch: [19]  [1600/2502]  eta: 0:12:32  lr: 0.000605  min_lr: 0.000001  loss: 1.8477 (1.7916)  class_acc: 0.7656 (0.7926)  loss_scale: 32768.0000 (32287.0206)  weight_decay: 0.0500 (0.0500)  time: 0.8063  data: 0.0002  max mem: 27222
Epoch: [19]  [1700/2502]  eta: 0:11:07  lr: 0.000604  min_lr: 0.000001  loss: 1.7861 (1.7912)  class_acc: 0.8125 (0.7926)  loss_scale: 32768.0000 (32469.4086)  weight_decay: 0.0500 (0.0500)  time: 0.8155  data: 0.0004  max mem: 27222
Epoch: [19]  [1800/2502]  eta: 0:09:43  lr: 0.000603  min_lr: 0.000001  loss: 1.8184 (1.7933)  class_acc: 0.7812 (0.7920)  loss_scale: 32768.0000 (32485.9878)  weight_decay: 0.0500 (0.0500)  time: 0.7946  data: 0.0002  max mem: 27222
Epoch: [19]  [1900/2502]  eta: 0:08:19  lr: 0.000602  min_lr: 0.000001  loss: 1.7979 (1.7941)  class_acc: 0.7812 (0.7916)  loss_scale: 32768.0000 (32500.8227)  weight_decay: 0.0500 (0.0500)  time: 0.8008  data: 0.0002  max mem: 27222
Epoch: [19]  [2000/2502]  eta: 0:06:55  lr: 0.000601  min_lr: 0.000001  loss: 1.7715 (1.7955)  class_acc: 0.7969 (0.7914)  loss_scale: 32768.0000 (32514.1749)  weight_decay: 0.0500 (0.0500)  time: 0.8122  data: 0.0002  max mem: 27222
Epoch: [19]  [2100/2502]  eta: 0:05:32  lr: 0.000601  min_lr: 0.000001  loss: 1.7988 (1.7952)  class_acc: 0.7656 (0.7913)  loss_scale: 32768.0000 (32526.2561)  weight_decay: 0.0500 (0.0500)  time: 0.7927  data: 0.0003  max mem: 27222
Epoch: [19]  [2200/2502]  eta: 0:04:09  lr: 0.000600  min_lr: 0.000001  loss: 1.7148 (1.7932)  class_acc: 0.8125 (0.7918)  loss_scale: 32768.0000 (32775.4439)  weight_decay: 0.0500 (0.0500)  time: 0.8050  data: 0.0002  max mem: 27222
Epoch: [19]  [2300/2502]  eta: 0:02:46  lr: 0.000599  min_lr: 0.000001  loss: 1.7900 (1.7927)  class_acc: 0.7656 (0.7922)  loss_scale: 32768.0000 (32775.1204)  weight_decay: 0.0500 (0.0500)  time: 0.8290  data: 0.0002  max mem: 27222
Epoch: [19]  [2400/2502]  eta: 0:01:24  lr: 0.000598  min_lr: 0.000001  loss: 1.7744 (1.7929)  class_acc: 0.7812 (0.7920)  loss_scale: 32768.0000 (32774.8238)  weight_decay: 0.0500 (0.0500)  time: 0.8387  data: 0.0002  max mem: 27222
Epoch: [19]  [2500/2502]  eta: 0:00:01  lr: 0.000598  min_lr: 0.000001  loss: 1.7900 (1.7937)  class_acc: 0.7812 (0.7917)  loss_scale: 32768.0000 (32774.5536)  weight_decay: 0.0500 (0.0500)  time: 0.7558  data: 0.0009  max mem: 27222
Epoch: [19]  [2501/2502]  eta: 0:00:00  lr: 0.000598  min_lr: 0.000001  loss: 1.7900 (1.7937)  class_acc: 0.7812 (0.7917)  loss_scale: 32768.0000 (32774.5536)  weight_decay: 0.0500 (0.0500)  time: 0.7178  data: 0.0009  max mem: 27222
Epoch: [19] Total time: 0:34:23 (0.8246 s / it)
Averaged stats: lr: 0.000598  min_lr: 0.000001  loss: 1.7900 (1.7947)  class_acc: 0.7812 (0.7910)  loss_scale: 32768.0000 (32774.5536)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Test:  [ 0/98]  eta: 0:08:57  loss: 0.3007 (0.3007)  acc1: 93.7500 (93.7500)  acc5: 98.4375 (98.4375)  time: 5.4860  data: 5.1310  max mem: 27222
Test:  [97/98]  eta: 0:00:00  loss: 0.7310 (0.6615)  acc1: 81.2500 (84.6720)  acc5: 96.8750 (97.4400)  time: 0.3367  data: 0.0009  max mem: 27222
Test: Total time: 0:00:39 (0.4074 s / it)
* Acc@1 84.724 Acc@5 97.246 loss 0.660
Accuracy of the network on the 50000 test images: 84.7%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Test:  [ 0/98]  eta: 0:10:10  loss: 0.2562 (0.2562)  acc1: 93.7500 (93.7500)  acc5: 100.0000 (100.0000)  time: 6.2339  data: 5.8715  max mem: 27222
Test:  [97/98]  eta: 0:00:00  loss: 0.6224 (0.5708)  acc1: 83.3333 (85.8400)  acc5: 98.4375 (98.0320)  time: 0.3365  data: 0.0002  max mem: 27222
Test: Total time: 0:00:41 (0.4223 s / it)
* Acc@1 86.086 Acc@5 97.938 loss 0.568
EMA Accuracy of the network on the 50000 test images: 86.1%
Max accuracy: 86.09%
{"train_lr": 0.0006070946609226076, "train_min_lr": 7.929077605057412e-07, "train_loss": 1.794741015625, "train_class_acc": 0.7910421875, "train_loss_scale": 32774.5536, "train_weight_decay": 0.04999999999999801, "test_loss": 0.5683577597445372, "test_acc1": 86.08600000549316, "test_acc5": 97.93800000854492, "epoch": 19, "n_parameters": 86860264}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Epoch: [20]  [   0/2502]  eta: 1 day, 5:02:54  lr: 0.000598  min_lr: 0.000001  loss: 1.6758 (1.6758)  class_acc: 0.8438 (0.8438)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 41.7965  data: 37.8073  max mem: 27222
Epoch: [20]  [ 100/2502]  eta: 0:47:51  lr: 0.000597  min_lr: 0.000001  loss: 1.6914 (1.7665)  class_acc: 0.8125 (0.7994)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7976  data: 0.0002  max mem: 27222
Epoch: [20]  [ 200/2502]  eta: 0:38:17  lr: 0.000596  min_lr: 0.000001  loss: 1.7041 (1.7444)  class_acc: 0.7969 (0.8028)  loss_scale: 32768.0000 (33746.1493)  weight_decay: 0.0500 (0.0500)  time: 0.7971  data: 0.0002  max mem: 27222
Epoch: [20]  [ 300/2502]  eta: 0:34:17  lr: 0.000595  min_lr: 0.000001  loss: 1.8779 (1.7466)  class_acc: 0.7812 (0.8022)  loss_scale: 32768.0000 (33638.9103)  weight_decay: 0.0500 (0.0500)  time: 0.7916  data: 0.0002  max mem: 27222
Epoch: [20]  [ 400/2502]  eta: 0:31:36  lr: 0.000595  min_lr: 0.000001  loss: 1.6855 (1.7497)  class_acc: 0.7969 (0.8013)  loss_scale: 32768.0000 (33421.7257)  weight_decay: 0.0500 (0.0500)  time: 0.8156  data: 0.0003  max mem: 27222
Epoch: [20]  [ 500/2502]  eta: 0:29:28  lr: 0.000594  min_lr: 0.000001  loss: 1.8359 (1.7571)  class_acc: 0.7656 (0.7985)  loss_scale: 32768.0000 (33291.2415)  weight_decay: 0.0500 (0.0500)  time: 0.7933  data: 0.0002  max mem: 27222
Epoch: [20]  [ 600/2502]  eta: 0:27:36  lr: 0.000593  min_lr: 0.000001  loss: 1.8018 (1.7618)  class_acc: 0.7812 (0.7983)  loss_scale: 32768.0000 (33204.1797)  weight_decay: 0.0500 (0.0500)  time: 0.7946  data: 0.0002  max mem: 27222
Epoch: [20]  [ 700/2502]  eta: 0:25:52  lr: 0.000592  min_lr: 0.000001  loss: 1.7549 (1.7618)  class_acc: 0.7812 (0.7983)  loss_scale: 32768.0000 (33141.9572)  weight_decay: 0.0500 (0.0500)  time: 0.8174  data: 0.0002  max mem: 27222
Epoch: [20]  [ 800/2502]  eta: 0:24:13  lr: 0.000591  min_lr: 0.000001  loss: 1.6582 (1.7604)  class_acc: 0.7969 (0.7985)  loss_scale: 32768.0000 (33586.1773)  weight_decay: 0.0500 (0.0500)  time: 0.8070  data: 0.0002  max mem: 27222
Epoch: [20]  [ 900/2502]  eta: 0:22:38  lr: 0.000591  min_lr: 0.000001  loss: 1.7275 (1.7604)  class_acc: 0.7969 (0.7983)  loss_scale: 32768.0000 (33495.3696)  weight_decay: 0.0500 (0.0500)  time: 0.7917  data: 0.0002  max mem: 27222
Epoch: [20]  [1000/2502]  eta: 0:21:05  lr: 0.000590  min_lr: 0.000001  loss: 1.7285 (1.7625)  class_acc: 0.8125 (0.7979)  loss_scale: 32768.0000 (33422.7053)  weight_decay: 0.0500 (0.0500)  time: 0.7953  data: 0.0002  max mem: 27222
Epoch: [20]  [1100/2502]  eta: 0:19:36  lr: 0.000589  min_lr: 0.000001  loss: 1.7793 (1.7661)  class_acc: 0.7969 (0.7969)  loss_scale: 32768.0000 (33363.2407)  weight_decay: 0.0500 (0.0500)  time: 0.7913  data: 0.0002  max mem: 27222
Epoch: [20]  [1200/2502]  eta: 0:18:09  lr: 0.000588  min_lr: 0.000001  loss: 1.8066 (1.7687)  class_acc: 0.7656 (0.7961)  loss_scale: 32768.0000 (33313.6786)  weight_decay: 0.0500 (0.0500)  time: 0.7950  data: 0.0002  max mem: 27222
Epoch: [20]  [1300/2502]  eta: 0:16:43  lr: 0.000587  min_lr: 0.000001  loss: 1.7793 (1.7686)  class_acc: 0.7812 (0.7962)  loss_scale: 32768.0000 (33372.4827)  weight_decay: 0.0500 (0.0500)  time: 0.7985  data: 0.0002  max mem: 27222
Epoch: [20]  [1400/2502]  eta: 0:15:18  lr: 0.000587  min_lr: 0.000001  loss: 1.8721 (1.7696)  class_acc: 0.7812 (0.7960)  loss_scale: 32768.0000 (33329.3362)  weight_decay: 0.0500 (0.0500)  time: 0.8083  data: 0.0002  max mem: 27222
Epoch: [20]  [1500/2502]  eta: 0:13:53  lr: 0.000586  min_lr: 0.000001  loss: 1.7871 (1.7717)  class_acc: 0.7812 (0.7951)  loss_scale: 32768.0000 (33291.9387)  weight_decay: 0.0500 (0.0500)  time: 0.8014  data: 0.0002  max mem: 27222
Epoch: [20]  [1600/2502]  eta: 0:12:28  lr: 0.000585  min_lr: 0.000001  loss: 1.7539 (1.7723)  class_acc: 0.7969 (0.7953)  loss_scale: 32768.0000 (33259.2130)  weight_decay: 0.0500 (0.0500)  time: 0.8012  data: 0.0002  max mem: 27222
Epoch: [20]  [1700/2502]  eta: 0:11:04  lr: 0.000584  min_lr: 0.000001  loss: 1.8018 (1.7738)  class_acc: 0.7812 (0.7950)  loss_scale: 32768.0000 (33230.3351)  weight_decay: 0.0500 (0.0500)  time: 0.8283  data: 0.0002  max mem: 27222
Epoch: [20]  [1800/2502]  eta: 0:09:41  lr: 0.000583  min_lr: 0.000001  loss: 1.7441 (1.7752)  class_acc: 0.7812 (0.7948)  loss_scale: 32768.0000 (33350.2188)  weight_decay: 0.0500 (0.0500)  time: 0.8498  data: 0.0004  max mem: 27222
Epoch: [20]  [1900/2502]  eta: 0:08:17  lr: 0.000582  min_lr: 0.000001  loss: 1.7461 (1.7763)  class_acc: 0.7969 (0.7946)  loss_scale: 32768.0000 (33319.5918)  weight_decay: 0.0500 (0.0500)  time: 0.7966  data: 0.0002  max mem: 27222
Epoch: [20]  [2000/2502]  eta: 0:06:54  lr: 0.000582  min_lr: 0.000001  loss: 1.7129 (1.7753)  class_acc: 0.7969 (0.7949)  loss_scale: 32768.0000 (33292.0260)  weight_decay: 0.0500 (0.0500)  time: 0.7992  data: 0.0002  max mem: 27222
Epoch: [20]  [2100/2502]  eta: 0:05:31  lr: 0.000581  min_lr: 0.000001  loss: 1.8135 (1.7774)  class_acc: 0.7812 (0.7943)  loss_scale: 32768.0000 (33267.0842)  weight_decay: 0.0500 (0.0500)  time: 0.7989  data: 0.0002  max mem: 27222
Epoch: [20]  [2200/2502]  eta: 0:04:08  lr: 0.000580  min_lr: 0.000001  loss: 1.6934 (1.7776)  class_acc: 0.8125 (0.7942)  loss_scale: 32768.0000 (33244.4089)  weight_decay: 0.0500 (0.0500)  time: 0.7996  data: 0.0002  max mem: 27222
Epoch: [20]  [2300/2502]  eta: 0:02:46  lr: 0.000579  min_lr: 0.000001  loss: 1.6875 (1.7777)  class_acc: 0.8125 (0.7942)  loss_scale: 32768.0000 (33280.6675)  weight_decay: 0.0500 (0.0500)  time: 0.8265  data: 0.0002  max mem: 27222
Epoch: [20]  [2400/2502]  eta: 0:01:23  lr: 0.000578  min_lr: 0.000001  loss: 1.8262 (1.7795)  class_acc: 0.7812 (0.7939)  loss_scale: 32768.0000 (33259.3153)  weight_decay: 0.0500 (0.0500)  time: 0.7951  data: 0.0002  max mem: 27222
Epoch: [20]  [2500/2502]  eta: 0:00:01  lr: 0.000578  min_lr: 0.000001  loss: 1.7617 (1.7792)  class_acc: 0.7969 (0.7938)  loss_scale: 32768.0000 (33239.8592)  weight_decay: 0.0500 (0.0500)  time: 0.7491  data: 0.0012  max mem: 27222
Epoch: [20]  [2501/2502]  eta: 0:00:00  lr: 0.000578  min_lr: 0.000001  loss: 1.7617 (1.7792)  class_acc: 0.7969 (0.7938)  loss_scale: 32768.0000 (33239.8592)  weight_decay: 0.0500 (0.0500)  time: 0.7099  data: 0.0012  max mem: 27222
Epoch: [20] Total time: 0:34:14 (0.8213 s / it)
Averaged stats: lr: 0.000578  min_lr: 0.000001  loss: 1.7617 (1.7768)  class_acc: 0.7969 (0.7951)  loss_scale: 32768.0000 (33239.8592)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Test:  [ 0/98]  eta: 0:07:24  loss: 0.2904 (0.2904)  acc1: 93.7500 (93.7500)  acc5: 98.4375 (98.4375)  time: 4.5406  data: 4.2043  max mem: 27222
Test:  [97/98]  eta: 0:00:00  loss: 0.7426 (0.6613)  acc1: 80.9524 (84.4160)  acc5: 96.8750 (97.2960)  time: 0.3387  data: 0.0011  max mem: 27222
Test: Total time: 0:00:40 (0.4122 s / it)
* Acc@1 84.436 Acc@5 97.226 loss 0.668
Accuracy of the network on the 50000 test images: 84.4%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Test:  [ 0/98]  eta: 0:08:36  loss: 0.2480 (0.2480)  acc1: 93.7500 (93.7500)  acc5: 100.0000 (100.0000)  time: 5.2704  data: 4.9179  max mem: 27222
Test:  [97/98]  eta: 0:00:00  loss: 0.6160 (0.5656)  acc1: 82.8125 (85.9200)  acc5: 98.4375 (97.9840)  time: 0.3382  data: 0.0008  max mem: 27222
Test: Total time: 0:00:39 (0.4077 s / it)
* Acc@1 86.194 Acc@5 97.970 loss 0.563
EMA Accuracy of the network on the 50000 test images: 86.2%
Max accuracy: 86.19%
{"train_lr": 0.000587696010463931, "train_min_lr": 7.675717767093316e-07, "train_loss": 1.776771484375, "train_class_acc": 0.79511015625, "train_loss_scale": 33239.8592, "train_weight_decay": 0.04999999999999801, "test_loss": 0.5626179216695684, "test_acc1": 86.19400000549317, "test_acc5": 97.97000000732422, "epoch": 20, "n_parameters": 86860264}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Epoch: [21]  [   0/2502]  eta: 1 day, 4:42:04  lr: 0.000577  min_lr: 0.000001  loss: 1.7793 (1.7793)  class_acc: 0.8125 (0.8125)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 41.2966  data: 39.3266  max mem: 27222
Epoch: [21]  [ 100/2502]  eta: 0:47:55  lr: 0.000577  min_lr: 0.000001  loss: 1.6797 (1.7292)  class_acc: 0.8125 (0.8069)  loss_scale: 16384.0000 (29848.0792)  weight_decay: 0.0500 (0.0500)  time: 0.7821  data: 0.0002  max mem: 27222
Epoch: [21]  [ 200/2502]  eta: 0:38:31  lr: 0.000576  min_lr: 0.000001  loss: 1.7480 (1.7441)  class_acc: 0.8125 (0.8062)  loss_scale: 16384.0000 (23149.5323)  weight_decay: 0.0500 (0.0500)  time: 0.8039  data: 0.0002  max mem: 27222
Epoch: [21]  [ 300/2502]  eta: 0:34:24  lr: 0.000575  min_lr: 0.000001  loss: 1.7617 (1.7453)  class_acc: 0.7969 (0.8044)  loss_scale: 16384.0000 (20901.8472)  weight_decay: 0.0500 (0.0500)  time: 0.7929  data: 0.0002  max mem: 27222
Epoch: [21]  [ 400/2502]  eta: 0:31:43  lr: 0.000574  min_lr: 0.000001  loss: 1.6377 (1.7461)  class_acc: 0.8125 (0.8045)  loss_scale: 16384.0000 (19775.2020)  weight_decay: 0.0500 (0.0500)  time: 0.7916  data: 0.0002  max mem: 27222
Epoch: [21]  [ 500/2502]  eta: 0:29:35  lr: 0.000573  min_lr: 0.000001  loss: 1.7500 (1.7424)  class_acc: 0.7969 (0.8046)  loss_scale: 16384.0000 (19098.3154)  weight_decay: 0.0500 (0.0500)  time: 0.7916  data: 0.0002  max mem: 27222
Epoch: [21]  [ 600/2502]  eta: 0:27:43  lr: 0.000572  min_lr: 0.000001  loss: 1.7285 (1.7404)  class_acc: 0.7969 (0.8043)  loss_scale: 16384.0000 (18701.2047)  weight_decay: 0.0500 (0.0500)  time: 0.7981  data: 0.0002  max mem: 27222
Epoch: [21]  [ 700/2502]  eta: 0:25:57  lr: 0.000572  min_lr: 0.000001  loss: 1.7812 (1.7474)  class_acc: 0.7969 (0.8024)  loss_scale: 32768.0000 (20707.8802)  weight_decay: 0.0500 (0.0500)  time: 0.7966  data: 0.0002  max mem: 27222
Epoch: [21]  [ 800/2502]  eta: 0:24:17  lr: 0.000571  min_lr: 0.000001  loss: 1.7402 (1.7493)  class_acc: 0.7969 (0.8018)  loss_scale: 32768.0000 (22213.5131)  weight_decay: 0.0500 (0.0500)  time: 0.7905  data: 0.0004  max mem: 27222
Epoch: [21]  [ 900/2502]  eta: 0:22:41  lr: 0.000570  min_lr: 0.000001  loss: 1.7266 (1.7523)  class_acc: 0.7969 (0.8010)  loss_scale: 32768.0000 (23384.9323)  weight_decay: 0.0500 (0.0500)  time: 0.7905  data: 0.0002  max mem: 27222
Epoch: [21]  [1000/2502]  eta: 0:21:11  lr: 0.000569  min_lr: 0.000001  loss: 1.6875 (1.7560)  class_acc: 0.7969 (0.8000)  loss_scale: 32768.0000 (24322.3017)  weight_decay: 0.0500 (0.0500)  time: 0.8426  data: 0.0002  max mem: 27222
Epoch: [21]  [1100/2502]  eta: 0:19:41  lr: 0.000568  min_lr: 0.000001  loss: 1.7158 (1.7564)  class_acc: 0.7969 (0.8002)  loss_scale: 32768.0000 (25089.3951)  weight_decay: 0.0500 (0.0500)  time: 0.8292  data: 0.0002  max mem: 27222
Epoch: [21]  [1200/2502]  eta: 0:18:15  lr: 0.000567  min_lr: 0.000001  loss: 1.7139 (1.7570)  class_acc: 0.7969 (0.8001)  loss_scale: 32768.0000 (25947.0175)  weight_decay: 0.0500 (0.0500)  time: 0.8327  data: 0.0003  max mem: 27222
Epoch: [21]  [1300/2502]  eta: 0:16:47  lr: 0.000566  min_lr: 0.000001  loss: 1.7158 (1.7576)  class_acc: 0.8125 (0.7999)  loss_scale: 16384.0000 (25942.3828)  weight_decay: 0.0500 (0.0500)  time: 0.8274  data: 0.0002  max mem: 27222
Epoch: [21]  [1400/2502]  eta: 0:15:21  lr: 0.000566  min_lr: 0.000001  loss: 1.7002 (1.7574)  class_acc: 0.8125 (0.8003)  loss_scale: 16384.0000 (25260.1285)  weight_decay: 0.0500 (0.0500)  time: 0.8341  data: 0.0002  max mem: 27222
Epoch: [21]  [1500/2502]  eta: 0:13:55  lr: 0.000565  min_lr: 0.000001  loss: 1.7900 (1.7565)  class_acc: 0.7969 (0.8006)  loss_scale: 16384.0000 (24668.7808)  weight_decay: 0.0500 (0.0500)  time: 0.7988  data: 0.0002  max mem: 27222
Epoch: [21]  [1600/2502]  eta: 0:12:30  lr: 0.000564  min_lr: 0.000001  loss: 1.7959 (1.7572)  class_acc: 0.7969 (0.8006)  loss_scale: 16384.0000 (24151.3054)  weight_decay: 0.0500 (0.0500)  time: 0.8092  data: 0.0002  max mem: 27222
Epoch: [21]  [1700/2502]  eta: 0:11:05  lr: 0.000563  min_lr: 0.000001  loss: 1.7773 (1.7578)  class_acc: 0.7969 (0.8005)  loss_scale: 16384.0000 (23694.6737)  weight_decay: 0.0500 (0.0500)  time: 0.7956  data: 0.0002  max mem: 27222
Epoch: [21]  [1800/2502]  eta: 0:09:41  lr: 0.000562  min_lr: 0.000001  loss: 1.6504 (1.7581)  class_acc: 0.8125 (0.8005)  loss_scale: 32768.0000 (23525.2771)  weight_decay: 0.0500 (0.0500)  time: 0.7989  data: 0.0002  max mem: 27222
Epoch: [21]  [1900/2502]  eta: 0:08:17  lr: 0.000561  min_lr: 0.000001  loss: 1.7061 (1.7580)  class_acc: 0.7969 (0.8004)  loss_scale: 32768.0000 (24011.4803)  weight_decay: 0.0500 (0.0500)  time: 0.8354  data: 0.0002  max mem: 27222
Epoch: [21]  [2000/2502]  eta: 0:06:54  lr: 0.000560  min_lr: 0.000001  loss: 1.6699 (1.7582)  class_acc: 0.8125 (0.8003)  loss_scale: 32768.0000 (24449.0875)  weight_decay: 0.0500 (0.0500)  time: 0.8359  data: 0.0002  max mem: 27222
Epoch: [21]  [2100/2502]  eta: 0:05:31  lr: 0.000559  min_lr: 0.000001  loss: 1.7969 (1.7575)  class_acc: 0.7812 (0.8004)  loss_scale: 32768.0000 (24845.0376)  weight_decay: 0.0500 (0.0500)  time: 0.8078  data: 0.0002  max mem: 27222
Epoch: [21]  [2200/2502]  eta: 0:04:08  lr: 0.000559  min_lr: 0.000001  loss: 1.6572 (1.7573)  class_acc: 0.8125 (0.8005)  loss_scale: 32768.0000 (25205.0086)  weight_decay: 0.0500 (0.0500)  time: 0.7949  data: 0.0002  max mem: 27222
Epoch: [21]  [2300/2502]  eta: 0:02:46  lr: 0.000558  min_lr: 0.000001  loss: 1.8164 (1.7592)  class_acc: 0.7812 (0.8000)  loss_scale: 65536.0000 (25733.0621)  weight_decay: 0.0500 (0.0500)  time: 0.8265  data: 0.0002  max mem: 27222
Epoch: [21]  [2400/2502]  eta: 0:01:23  lr: 0.000557  min_lr: 0.000001  loss: 1.7402 (1.7582)  class_acc: 0.7969 (0.8001)  loss_scale: 32768.0000 (26653.8542)  weight_decay: 0.0500 (0.0500)  time: 0.8101  data: 0.0002  max mem: 27222
Epoch: [21]  [2500/2502]  eta: 0:00:01  lr: 0.000556  min_lr: 0.000001  loss: 1.7021 (1.7590)  class_acc: 0.8125 (0.7998)  loss_scale: 32768.0000 (26895.9744)  weight_decay: 0.0500 (0.0500)  time: 0.7477  data: 0.0011  max mem: 27222
Epoch: [21]  [2501/2502]  eta: 0:00:00  lr: 0.000556  min_lr: 0.000001  loss: 1.7021 (1.7590)  class_acc: 0.8125 (0.7998)  loss_scale: 32768.0000 (26895.9744)  weight_decay: 0.0500 (0.0500)  time: 0.7088  data: 0.0011  max mem: 27222
Epoch: [21] Total time: 0:34:17 (0.8222 s / it)
Averaged stats: lr: 0.000556  min_lr: 0.000001  loss: 1.7021 (1.7580)  class_acc: 0.8125 (0.8006)  loss_scale: 32768.0000 (26895.9744)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Test:  [ 0/98]  eta: 0:05:04  loss: 0.2495 (0.2495)  acc1: 96.8750 (96.8750)  acc5: 98.4375 (98.4375)  time: 3.1118  data: 2.7644  max mem: 27222
Test:  [97/98]  eta: 0:00:00  loss: 0.7650 (0.6637)  acc1: 82.8125 (84.6880)  acc5: 96.8750 (97.4240)  time: 0.3374  data: 0.0004  max mem: 27222
Test: Total time: 0:00:39 (0.4055 s / it)
* Acc@1 84.656 Acc@5 97.284 loss 0.669
Accuracy of the network on the 50000 test images: 84.7%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Test:  [ 0/98]  eta: 0:07:09  loss: 0.2386 (0.2386)  acc1: 92.1875 (92.1875)  acc5: 100.0000 (100.0000)  time: 4.3779  data: 4.0430  max mem: 27222
Test:  [97/98]  eta: 0:00:00  loss: 0.6157 (0.5619)  acc1: 84.3750 (85.9680)  acc5: 98.4375 (97.9840)  time: 0.3408  data: 0.0002  max mem: 27222
Test: Total time: 0:00:41 (0.4205 s / it)
* Acc@1 86.306 Acc@5 97.994 loss 0.559
EMA Accuracy of the network on the 50000 test images: 86.3%
Max accuracy: 86.31%
{"train_lr": 0.0005668349677250647, "train_min_lr": 7.403258071026293e-07, "train_loss": 1.7580333984375, "train_class_acc": 0.8006109375, "train_loss_scale": 26895.9744, "train_weight_decay": 0.04999999999999801, "test_loss": 0.5587674686495139, "test_acc1": 86.30600000946045, "test_acc5": 97.99400000732422, "epoch": 21, "n_parameters": 86860264}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Epoch: [22]  [   0/2502]  eta: 1 day, 3:27:30  lr: 0.000556  min_lr: 0.000001  loss: 1.7998 (1.7998)  class_acc: 0.8125 (0.8125)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 39.5085  data: 34.9689  max mem: 27222
Epoch: [22]  [ 100/2502]  eta: 0:47:37  lr: 0.000555  min_lr: 0.000001  loss: 1.7793 (1.7657)  class_acc: 0.7969 (0.7997)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7861  data: 0.0002  max mem: 27222
Epoch: [22]  [ 200/2502]  eta: 0:38:28  lr: 0.000554  min_lr: 0.000001  loss: 1.6387 (1.7491)  class_acc: 0.7969 (0.8004)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8579  data: 0.0002  max mem: 27222
Epoch: [22]  [ 300/2502]  eta: 0:34:23  lr: 0.000553  min_lr: 0.000001  loss: 1.6855 (1.7476)  class_acc: 0.7812 (0.7999)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8267  data: 0.0002  max mem: 27222
Epoch: [22]  [ 400/2502]  eta: 0:31:40  lr: 0.000552  min_lr: 0.000001  loss: 1.6826 (1.7439)  class_acc: 0.8125 (0.8011)  loss_scale: 32768.0000 (34729.1771)  weight_decay: 0.0500 (0.0500)  time: 0.7882  data: 0.0002  max mem: 27222
Epoch: [22]  [ 500/2502]  eta: 0:29:37  lr: 0.000551  min_lr: 0.000001  loss: 1.6748 (1.7429)  class_acc: 0.8125 (0.8033)  loss_scale: 32768.0000 (34337.7246)  weight_decay: 0.0500 (0.0500)  time: 0.7944  data: 0.0010  max mem: 27222
Epoch: [22]  [ 600/2502]  eta: 0:27:45  lr: 0.000551  min_lr: 0.000001  loss: 1.7217 (1.7410)  class_acc: 0.8125 (0.8043)  loss_scale: 32768.0000 (34076.5391)  weight_decay: 0.0500 (0.0500)  time: 0.8398  data: 0.0002  max mem: 27222
Epoch: [22]  [ 700/2502]  eta: 0:25:59  lr: 0.000550  min_lr: 0.000001  loss: 1.6348 (1.7396)  class_acc: 0.8281 (0.8045)  loss_scale: 32768.0000 (33889.8716)  weight_decay: 0.0500 (0.0500)  time: 0.8000  data: 0.0002  max mem: 27222
Epoch: [22]  [ 800/2502]  eta: 0:24:19  lr: 0.000549  min_lr: 0.000001  loss: 1.7861 (1.7371)  class_acc: 0.7969 (0.8050)  loss_scale: 32768.0000 (33749.8127)  weight_decay: 0.0500 (0.0500)  time: 0.7986  data: 0.0002  max mem: 27222
Epoch: [22]  [ 900/2502]  eta: 0:22:44  lr: 0.000548  min_lr: 0.000001  loss: 1.7598 (1.7381)  class_acc: 0.7969 (0.8052)  loss_scale: 32768.0000 (33640.8435)  weight_decay: 0.0500 (0.0500)  time: 0.8039  data: 0.0002  max mem: 27222
Epoch: [22]  [1000/2502]  eta: 0:21:15  lr: 0.000547  min_lr: 0.000001  loss: 1.7021 (1.7380)  class_acc: 0.8281 (0.8056)  loss_scale: 32768.0000 (33684.5874)  weight_decay: 0.0500 (0.0500)  time: 0.8439  data: 0.0002  max mem: 27222
Epoch: [22]  [1100/2502]  eta: 0:19:46  lr: 0.000546  min_lr: 0.000001  loss: 1.6855 (1.7387)  class_acc: 0.8281 (0.8053)  loss_scale: 32768.0000 (33601.3370)  weight_decay: 0.0500 (0.0500)  time: 0.8225  data: 0.0002  max mem: 27222
Epoch: [22]  [1200/2502]  eta: 0:18:17  lr: 0.000545  min_lr: 0.000001  loss: 1.7422 (1.7383)  class_acc: 0.7969 (0.8050)  loss_scale: 32768.0000 (33531.9500)  weight_decay: 0.0500 (0.0500)  time: 0.8048  data: 0.0002  max mem: 27222
Epoch: [22]  [1300/2502]  eta: 0:16:49  lr: 0.000544  min_lr: 0.000001  loss: 1.7383 (1.7366)  class_acc: 0.8281 (0.8055)  loss_scale: 32768.0000 (33473.2298)  weight_decay: 0.0500 (0.0500)  time: 0.8192  data: 0.0002  max mem: 27222
Epoch: [22]  [1400/2502]  eta: 0:15:23  lr: 0.000543  min_lr: 0.000001  loss: 1.7285 (1.7354)  class_acc: 0.8281 (0.8062)  loss_scale: 32768.0000 (33422.8922)  weight_decay: 0.0500 (0.0500)  time: 0.7947  data: 0.0003  max mem: 27222
Epoch: [22]  [1500/2502]  eta: 0:13:57  lr: 0.000542  min_lr: 0.000001  loss: 1.7334 (1.7369)  class_acc: 0.7812 (0.8055)  loss_scale: 32768.0000 (33466.5849)  weight_decay: 0.0500 (0.0500)  time: 0.7962  data: 0.0003  max mem: 27222
Epoch: [22]  [1600/2502]  eta: 0:12:31  lr: 0.000541  min_lr: 0.000001  loss: 1.7490 (1.7377)  class_acc: 0.8125 (0.8054)  loss_scale: 32768.0000 (33422.9507)  weight_decay: 0.0500 (0.0500)  time: 0.8005  data: 0.0002  max mem: 27222
Epoch: [22]  [1700/2502]  eta: 0:11:07  lr: 0.000541  min_lr: 0.000001  loss: 1.7354 (1.7380)  class_acc: 0.7812 (0.8054)  loss_scale: 32768.0000 (33384.4468)  weight_decay: 0.0500 (0.0500)  time: 0.8104  data: 0.0002  max mem: 27222
Epoch: [22]  [1800/2502]  eta: 0:09:43  lr: 0.000540  min_lr: 0.000001  loss: 1.7637 (1.7382)  class_acc: 0.7969 (0.8054)  loss_scale: 32768.0000 (33350.2188)  weight_decay: 0.0500 (0.0500)  time: 0.7931  data: 0.0003  max mem: 27222
Epoch: [22]  [1900/2502]  eta: 0:08:19  lr: 0.000539  min_lr: 0.000001  loss: 1.7314 (1.7394)  class_acc: 0.8125 (0.8050)  loss_scale: 32768.0000 (33319.5918)  weight_decay: 0.0500 (0.0500)  time: 0.8278  data: 0.0002  max mem: 27222
Epoch: [22]  [2000/2502]  eta: 0:06:56  lr: 0.000538  min_lr: 0.000001  loss: 1.6855 (1.7398)  class_acc: 0.7969 (0.8048)  loss_scale: 32768.0000 (33488.5357)  weight_decay: 0.0500 (0.0500)  time: 0.8029  data: 0.0002  max mem: 27222
Epoch: [22]  [2100/2502]  eta: 0:05:32  lr: 0.000537  min_lr: 0.000001  loss: 1.7412 (1.7403)  class_acc: 0.7812 (0.8046)  loss_scale: 32768.0000 (33454.2408)  weight_decay: 0.0500 (0.0500)  time: 0.7957  data: 0.0003  max mem: 27222
Epoch: [22]  [2200/2502]  eta: 0:04:09  lr: 0.000536  min_lr: 0.000001  loss: 1.7402 (1.7410)  class_acc: 0.7969 (0.8043)  loss_scale: 32768.0000 (33423.0622)  weight_decay: 0.0500 (0.0500)  time: 0.8313  data: 0.0002  max mem: 27222
Epoch: [22]  [2300/2502]  eta: 0:02:46  lr: 0.000535  min_lr: 0.000001  loss: 1.7764 (1.7417)  class_acc: 0.7969 (0.8043)  loss_scale: 32768.0000 (33394.5937)  weight_decay: 0.0500 (0.0500)  time: 0.7936  data: 0.0003  max mem: 27222
Epoch: [22]  [2400/2502]  eta: 0:01:24  lr: 0.000534  min_lr: 0.000001  loss: 1.7490 (1.7417)  class_acc: 0.7969 (0.8041)  loss_scale: 32768.0000 (33368.4965)  weight_decay: 0.0500 (0.0500)  time: 0.8293  data: 0.0002  max mem: 27222
Epoch: [22]  [2500/2502]  eta: 0:00:01  lr: 0.000533  min_lr: 0.000001  loss: 1.7168 (1.7426)  class_acc: 0.7812 (0.8039)  loss_scale: 32768.0000 (33397.1456)  weight_decay: 0.0500 (0.0500)  time: 0.7492  data: 0.0015  max mem: 27222
Epoch: [22]  [2501/2502]  eta: 0:00:00  lr: 0.000533  min_lr: 0.000001  loss: 1.7168 (1.7426)  class_acc: 0.7812 (0.8039)  loss_scale: 32768.0000 (33397.1456)  weight_decay: 0.0500 (0.0500)  time: 0.7102  data: 0.0015  max mem: 27222
Epoch: [22] Total time: 0:34:22 (0.8242 s / it)
Averaged stats: lr: 0.000533  min_lr: 0.000001  loss: 1.7168 (1.7414)  class_acc: 0.7812 (0.8047)  loss_scale: 32768.0000 (33397.1456)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Test:  [ 0/98]  eta: 0:08:14  loss: 0.3104 (0.3104)  acc1: 92.1875 (92.1875)  acc5: 98.4375 (98.4375)  time: 5.0504  data: 4.6982  max mem: 27222
Test:  [97/98]  eta: 0:00:00  loss: 0.8119 (0.6732)  acc1: 81.2500 (84.8960)  acc5: 98.4375 (97.1520)  time: 0.3332  data: 0.0002  max mem: 27222
Test: Total time: 0:00:40 (0.4103 s / it)
* Acc@1 84.604 Acc@5 97.254 loss 0.673
Accuracy of the network on the 50000 test images: 84.6%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Test:  [ 0/98]  eta: 0:09:11  loss: 0.2336 (0.2336)  acc1: 92.1875 (92.1875)  acc5: 100.0000 (100.0000)  time: 5.6316  data: 5.2946  max mem: 27222
Test:  [97/98]  eta: 0:00:00  loss: 0.6121 (0.5598)  acc1: 82.8125 (85.9200)  acc5: 98.4375 (97.9520)  time: 0.3376  data: 0.0027  max mem: 27222
Test: Total time: 0:00:41 (0.4201 s / it)
* Acc@1 86.404 Acc@5 97.990 loss 0.556
EMA Accuracy of the network on the 50000 test images: 86.4%
Max accuracy: 86.40%
{"train_lr": 0.0005446401479715018, "train_min_lr": 7.113378321484744e-07, "train_loss": 1.74141689453125, "train_class_acc": 0.80468671875, "train_loss_scale": 33397.1456, "train_weight_decay": 0.04999999999999801, "test_loss": 0.5561202646670292, "test_acc1": 86.40400000946045, "test_acc5": 97.99000000732421, "epoch": 22, "n_parameters": 86860264}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Epoch: [23]  [   0/2502]  eta: 1 day, 5:05:25  lr: 0.000533  min_lr: 0.000001  loss: 1.7031 (1.7031)  class_acc: 0.8125 (0.8125)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 41.8569  data: 27.6672  max mem: 27222
Epoch: [23]  [ 100/2502]  eta: 0:47:42  lr: 0.000532  min_lr: 0.000001  loss: 1.7119 (1.7144)  class_acc: 0.8125 (0.8153)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7867  data: 0.0003  max mem: 27222
Epoch: [23]  [ 200/2502]  eta: 0:38:10  lr: 0.000531  min_lr: 0.000001  loss: 1.7285 (1.6923)  class_acc: 0.8125 (0.8212)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8104  data: 0.0002  max mem: 27222
Epoch: [23]  [ 300/2502]  eta: 0:34:07  lr: 0.000530  min_lr: 0.000001  loss: 1.6885 (1.6961)  class_acc: 0.8125 (0.8182)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7915  data: 0.0002  max mem: 27222
Epoch: [23]  [ 400/2502]  eta: 0:31:25  lr: 0.000529  min_lr: 0.000001  loss: 1.7246 (1.7028)  class_acc: 0.7969 (0.8166)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8116  data: 0.0003  max mem: 27222
Epoch: [23]  [ 500/2502]  eta: 0:29:16  lr: 0.000528  min_lr: 0.000001  loss: 1.7363 (1.7088)  class_acc: 0.8125 (0.8148)  loss_scale: 32768.0000 (33291.2415)  weight_decay: 0.0500 (0.0500)  time: 0.8219  data: 0.0002  max mem: 27222
Epoch: [23]  [ 600/2502]  eta: 0:27:24  lr: 0.000527  min_lr: 0.000001  loss: 1.6709 (1.7095)  class_acc: 0.8125 (0.8138)  loss_scale: 32768.0000 (33204.1797)  weight_decay: 0.0500 (0.0500)  time: 0.7902  data: 0.0002  max mem: 27222
Epoch: [23]  [ 700/2502]  eta: 0:25:41  lr: 0.000527  min_lr: 0.000001  loss: 1.7129 (1.7115)  class_acc: 0.8125 (0.8137)  loss_scale: 32768.0000 (33141.9572)  weight_decay: 0.0500 (0.0500)  time: 0.7917  data: 0.0002  max mem: 27222
Epoch: [23]  [ 800/2502]  eta: 0:24:06  lr: 0.000526  min_lr: 0.000001  loss: 1.6855 (1.7151)  class_acc: 0.8125 (0.8130)  loss_scale: 32768.0000 (33095.2709)  weight_decay: 0.0500 (0.0500)  time: 0.8127  data: 0.0002  max mem: 27222
Epoch: [23]  [ 900/2502]  eta: 0:22:32  lr: 0.000525  min_lr: 0.000001  loss: 1.6650 (1.7177)  class_acc: 0.8125 (0.8122)  loss_scale: 32768.0000 (33058.9478)  weight_decay: 0.0500 (0.0500)  time: 0.7940  data: 0.0002  max mem: 27222
Epoch: [23]  [1000/2502]  eta: 0:21:03  lr: 0.000524  min_lr: 0.000001  loss: 1.7432 (1.7195)  class_acc: 0.8125 (0.8118)  loss_scale: 32768.0000 (33029.8821)  weight_decay: 0.0500 (0.0500)  time: 0.8049  data: 0.0002  max mem: 27222
Epoch: [23]  [1100/2502]  eta: 0:19:34  lr: 0.000523  min_lr: 0.000001  loss: 1.6982 (1.7182)  class_acc: 0.8125 (0.8117)  loss_scale: 32768.0000 (33244.1926)  weight_decay: 0.0500 (0.0500)  time: 0.8095  data: 0.0002  max mem: 27222
Epoch: [23]  [1200/2502]  eta: 0:18:07  lr: 0.000522  min_lr: 0.000001  loss: 1.6895 (1.7205)  class_acc: 0.8125 (0.8112)  loss_scale: 32768.0000 (33204.5429)  weight_decay: 0.0500 (0.0500)  time: 0.8462  data: 0.0002  max mem: 27222
Epoch: [23]  [1300/2502]  eta: 0:16:42  lr: 0.000521  min_lr: 0.000001  loss: 1.7510 (1.7201)  class_acc: 0.7969 (0.8111)  loss_scale: 32768.0000 (33170.9885)  weight_decay: 0.0500 (0.0500)  time: 0.7920  data: 0.0002  max mem: 27222
Epoch: [23]  [1400/2502]  eta: 0:15:16  lr: 0.000520  min_lr: 0.000001  loss: 1.6367 (1.7225)  class_acc: 0.8125 (0.8107)  loss_scale: 32768.0000 (33142.2241)  weight_decay: 0.0500 (0.0500)  time: 0.8062  data: 0.0003  max mem: 27222
Epoch: [23]  [1500/2502]  eta: 0:13:51  lr: 0.000519  min_lr: 0.000001  loss: 1.7100 (1.7224)  class_acc: 0.7969 (0.8106)  loss_scale: 32768.0000 (33117.2925)  weight_decay: 0.0500 (0.0500)  time: 0.7959  data: 0.0003  max mem: 27222
Epoch: [23]  [1600/2502]  eta: 0:12:28  lr: 0.000518  min_lr: 0.000001  loss: 1.7451 (1.7224)  class_acc: 0.8125 (0.8106)  loss_scale: 32768.0000 (33177.3442)  weight_decay: 0.0500 (0.0500)  time: 0.8325  data: 0.0005  max mem: 27222
Epoch: [23]  [1700/2502]  eta: 0:11:04  lr: 0.000517  min_lr: 0.000001  loss: 1.7080 (1.7219)  class_acc: 0.7969 (0.8104)  loss_scale: 32768.0000 (33153.2792)  weight_decay: 0.0500 (0.0500)  time: 0.8114  data: 0.0002  max mem: 27222
Epoch: [23]  [1800/2502]  eta: 0:09:40  lr: 0.000516  min_lr: 0.000001  loss: 1.7344 (1.7226)  class_acc: 0.8125 (0.8099)  loss_scale: 32768.0000 (33131.8867)  weight_decay: 0.0500 (0.0500)  time: 0.7906  data: 0.0003  max mem: 27222
Epoch: [23]  [1900/2502]  eta: 0:08:17  lr: 0.000515  min_lr: 0.000001  loss: 1.6963 (1.7206)  class_acc: 0.8125 (0.8105)  loss_scale: 32768.0000 (33112.7449)  weight_decay: 0.0500 (0.0500)  time: 0.8007  data: 0.0002  max mem: 27222
Epoch: [23]  [2000/2502]  eta: 0:06:54  lr: 0.000514  min_lr: 0.000001  loss: 1.6729 (1.7201)  class_acc: 0.8125 (0.8106)  loss_scale: 32768.0000 (33095.5162)  weight_decay: 0.0500 (0.0500)  time: 0.8141  data: 0.0002  max mem: 27222
Epoch: [23]  [2100/2502]  eta: 0:05:31  lr: 0.000513  min_lr: 0.000001  loss: 1.7705 (1.7209)  class_acc: 0.8125 (0.8104)  loss_scale: 32768.0000 (33329.4698)  weight_decay: 0.0500 (0.0500)  time: 0.7992  data: 0.0002  max mem: 27222
Epoch: [23]  [2200/2502]  eta: 0:04:08  lr: 0.000512  min_lr: 0.000001  loss: 1.6973 (1.7210)  class_acc: 0.8125 (0.8104)  loss_scale: 32768.0000 (33303.9600)  weight_decay: 0.0500 (0.0500)  time: 0.7925  data: 0.0002  max mem: 27222
Epoch: [23]  [2300/2502]  eta: 0:02:46  lr: 0.000511  min_lr: 0.000001  loss: 1.6846 (1.7216)  class_acc: 0.8281 (0.8100)  loss_scale: 32768.0000 (33280.6675)  weight_decay: 0.0500 (0.0500)  time: 0.7934  data: 0.0003  max mem: 27222
Epoch: [23]  [2400/2502]  eta: 0:01:23  lr: 0.000510  min_lr: 0.000001  loss: 1.7373 (1.7240)  class_acc: 0.8125 (0.8094)  loss_scale: 32768.0000 (33259.3153)  weight_decay: 0.0500 (0.0500)  time: 0.8393  data: 0.0002  max mem: 27222
Epoch: [23]  [2500/2502]  eta: 0:00:01  lr: 0.000509  min_lr: 0.000001  loss: 1.7080 (1.7251)  class_acc: 0.7969 (0.8090)  loss_scale: 32768.0000 (33239.8592)  weight_decay: 0.0500 (0.0500)  time: 0.7514  data: 0.0010  max mem: 27222
Epoch: [23]  [2501/2502]  eta: 0:00:00  lr: 0.000509  min_lr: 0.000001  loss: 1.7080 (1.7251)  class_acc: 0.7969 (0.8090)  loss_scale: 32768.0000 (33239.8592)  weight_decay: 0.0500 (0.0500)  time: 0.7129  data: 0.0010  max mem: 27222
Epoch: [23] Total time: 0:34:14 (0.8213 s / it)
Averaged stats: lr: 0.000509  min_lr: 0.000001  loss: 1.7080 (1.7210)  class_acc: 0.7969 (0.8103)  loss_scale: 32768.0000 (33239.8592)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Test:  [ 0/98]  eta: 0:06:10  loss: 0.3296 (0.3296)  acc1: 93.7500 (93.7500)  acc5: 98.4375 (98.4375)  time: 3.7787  data: 3.4369  max mem: 27222
Test:  [97/98]  eta: 0:00:00  loss: 0.6857 (0.6658)  acc1: 81.2500 (84.5920)  acc5: 96.8750 (97.4880)  time: 0.3331  data: 0.0014  max mem: 27222
Test: Total time: 0:00:43 (0.4396 s / it)
* Acc@1 84.882 Acc@5 97.336 loss 0.663
Accuracy of the network on the 50000 test images: 84.9%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Test:  [ 0/98]  eta: 0:07:35  loss: 0.2317 (0.2317)  acc1: 92.1875 (92.1875)  acc5: 100.0000 (100.0000)  time: 4.6450  data: 4.2879  max mem: 27222
Test:  [97/98]  eta: 0:00:00  loss: 0.6140 (0.5590)  acc1: 82.8125 (86.0480)  acc5: 98.4375 (97.9200)  time: 0.3360  data: 0.0003  max mem: 27222
Test: Total time: 0:00:39 (0.4027 s / it)
* Acc@1 86.452 Acc@5 98.002 loss 0.555
EMA Accuracy of the network on the 50000 test images: 86.5%
Max accuracy: 86.45%
{"train_lr": 0.0005212483896475374, "train_min_lr": 6.807865723519243e-07, "train_loss": 1.72095478515625, "train_class_acc": 0.8103078125, "train_loss_scale": 33239.8592, "train_weight_decay": 0.04999999999999801, "test_loss": 0.5546171187746282, "test_acc1": 86.45200000823975, "test_acc5": 98.00200000610351, "epoch": 23, "n_parameters": 86860264}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Epoch: [24]  [   0/2502]  eta: 1 day, 4:34:31  lr: 0.000509  min_lr: 0.000001  loss: 1.5469 (1.5469)  class_acc: 0.8750 (0.8750)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 41.1156  data: 34.9568  max mem: 27222
Epoch: [24]  [ 100/2502]  eta: 0:47:33  lr: 0.000508  min_lr: 0.000001  loss: 1.6572 (1.7010)  class_acc: 0.8281 (0.8157)  loss_scale: 32768.0000 (34065.7426)  weight_decay: 0.0500 (0.0500)  time: 0.7872  data: 0.0003  max mem: 27222
Epoch: [24]  [ 200/2502]  eta: 0:38:17  lr: 0.000507  min_lr: 0.000001  loss: 1.6006 (1.6916)  class_acc: 0.8281 (0.8157)  loss_scale: 32768.0000 (33420.0995)  weight_decay: 0.0500 (0.0500)  time: 0.7881  data: 0.0002  max mem: 27222
Epoch: [24]  [ 300/2502]  eta: 0:34:14  lr: 0.000506  min_lr: 0.000001  loss: 1.6709 (1.6908)  class_acc: 0.8125 (0.8184)  loss_scale: 32768.0000 (33203.4551)  weight_decay: 0.0500 (0.0500)  time: 0.8170  data: 0.0002  max mem: 27222
Epoch: [24]  [ 400/2502]  eta: 0:31:35  lr: 0.000505  min_lr: 0.000001  loss: 1.6309 (1.6937)  class_acc: 0.8281 (0.8179)  loss_scale: 32768.0000 (33094.8628)  weight_decay: 0.0500 (0.0500)  time: 0.8208  data: 0.0002  max mem: 27222
Epoch: [24]  [ 500/2502]  eta: 0:29:23  lr: 0.000504  min_lr: 0.000001  loss: 1.6719 (1.6988)  class_acc: 0.7969 (0.8166)  loss_scale: 32768.0000 (33029.6208)  weight_decay: 0.0500 (0.0500)  time: 0.7948  data: 0.0002  max mem: 27222
Epoch: [24]  [ 600/2502]  eta: 0:27:27  lr: 0.000503  min_lr: 0.000001  loss: 1.6924 (1.6995)  class_acc: 0.8438 (0.8169)  loss_scale: 32768.0000 (32986.0899)  weight_decay: 0.0500 (0.0500)  time: 0.7923  data: 0.0002  max mem: 27222
Epoch: [24]  [ 700/2502]  eta: 0:25:47  lr: 0.000502  min_lr: 0.000001  loss: 1.7080 (1.7005)  class_acc: 0.8281 (0.8169)  loss_scale: 32768.0000 (33141.9572)  weight_decay: 0.0500 (0.0500)  time: 0.8159  data: 0.0002  max mem: 27222
Epoch: [24]  [ 800/2502]  eta: 0:24:09  lr: 0.000501  min_lr: 0.000001  loss: 1.7139 (1.7002)  class_acc: 0.8125 (0.8163)  loss_scale: 32768.0000 (33095.2709)  weight_decay: 0.0500 (0.0500)  time: 0.8201  data: 0.0006  max mem: 27222
Epoch: [24]  [ 900/2502]  eta: 0:22:37  lr: 0.000500  min_lr: 0.000001  loss: 1.6562 (1.7005)  class_acc: 0.8125 (0.8158)  loss_scale: 32768.0000 (33058.9478)  weight_decay: 0.0500 (0.0500)  time: 0.7880  data: 0.0002  max mem: 27222
Epoch: [24]  [1000/2502]  eta: 0:21:08  lr: 0.000499  min_lr: 0.000001  loss: 1.6924 (1.7017)  class_acc: 0.8281 (0.8152)  loss_scale: 32768.0000 (33029.8821)  weight_decay: 0.0500 (0.0500)  time: 0.8299  data: 0.0002  max mem: 27222
Epoch: [24]  [1100/2502]  eta: 0:19:39  lr: 0.000498  min_lr: 0.000001  loss: 1.6768 (1.7018)  class_acc: 0.7969 (0.8154)  loss_scale: 32768.0000 (33006.0963)  weight_decay: 0.0500 (0.0500)  time: 0.7961  data: 0.0002  max mem: 27222
Epoch: [24]  [1200/2502]  eta: 0:18:10  lr: 0.000497  min_lr: 0.000001  loss: 1.6221 (1.7008)  class_acc: 0.8281 (0.8156)  loss_scale: 32768.0000 (33422.8143)  weight_decay: 0.0500 (0.0500)  time: 0.8008  data: 0.0002  max mem: 27222
Epoch: [24]  [1300/2502]  eta: 0:16:43  lr: 0.000496  min_lr: 0.000001  loss: 1.7217 (1.7006)  class_acc: 0.7812 (0.8156)  loss_scale: 32768.0000 (33372.4827)  weight_decay: 0.0500 (0.0500)  time: 0.7971  data: 0.0002  max mem: 27222
Epoch: [24]  [1400/2502]  eta: 0:15:18  lr: 0.000495  min_lr: 0.000001  loss: 1.6855 (1.7012)  class_acc: 0.8125 (0.8157)  loss_scale: 32768.0000 (33329.3362)  weight_decay: 0.0500 (0.0500)  time: 0.8425  data: 0.0003  max mem: 27222
Epoch: [24]  [1500/2502]  eta: 0:13:52  lr: 0.000494  min_lr: 0.000001  loss: 1.6523 (1.7004)  class_acc: 0.8125 (0.8157)  loss_scale: 32768.0000 (33291.9387)  weight_decay: 0.0500 (0.0500)  time: 0.7974  data: 0.0002  max mem: 27222
Epoch: [24]  [1600/2502]  eta: 0:12:27  lr: 0.000493  min_lr: 0.000001  loss: 1.6494 (1.7015)  class_acc: 0.8281 (0.8153)  loss_scale: 32768.0000 (33259.2130)  weight_decay: 0.0500 (0.0500)  time: 0.7908  data: 0.0002  max mem: 27222
Epoch: [24]  [1700/2502]  eta: 0:11:03  lr: 0.000492  min_lr: 0.000001  loss: 1.7285 (1.7036)  class_acc: 0.8125 (0.8146)  loss_scale: 32768.0000 (33384.4468)  weight_decay: 0.0500 (0.0500)  time: 0.7945  data: 0.0002  max mem: 27222
Epoch: [24]  [1800/2502]  eta: 0:09:39  lr: 0.000491  min_lr: 0.000001  loss: 1.7656 (1.7036)  class_acc: 0.8125 (0.8147)  loss_scale: 32768.0000 (33350.2188)  weight_decay: 0.0500 (0.0500)  time: 0.7947  data: 0.0002  max mem: 27222
Epoch: [24]  [1900/2502]  eta: 0:08:16  lr: 0.000490  min_lr: 0.000001  loss: 1.7490 (1.7043)  class_acc: 0.7969 (0.8144)  loss_scale: 32768.0000 (33319.5918)  weight_decay: 0.0500 (0.0500)  time: 0.7990  data: 0.0004  max mem: 27222
Epoch: [24]  [2000/2502]  eta: 0:06:53  lr: 0.000489  min_lr: 0.000001  loss: 1.6367 (1.7043)  class_acc: 0.8281 (0.8142)  loss_scale: 32768.0000 (33292.0260)  weight_decay: 0.0500 (0.0500)  time: 0.7957  data: 0.0002  max mem: 27222
Epoch: [24]  [2100/2502]  eta: 0:05:30  lr: 0.000488  min_lr: 0.000001  loss: 1.6562 (1.7041)  class_acc: 0.7969 (0.8144)  loss_scale: 32768.0000 (33267.0842)  weight_decay: 0.0500 (0.0500)  time: 0.7879  data: 0.0004  max mem: 27222
Epoch: [24]  [2200/2502]  eta: 0:04:08  lr: 0.000487  min_lr: 0.000001  loss: 1.7148 (1.7045)  class_acc: 0.7969 (0.8141)  loss_scale: 32768.0000 (33363.5111)  weight_decay: 0.0500 (0.0500)  time: 0.8273  data: 0.0002  max mem: 27222
Epoch: [24]  [2300/2502]  eta: 0:02:46  lr: 0.000486  min_lr: 0.000001  loss: 1.7334 (1.7046)  class_acc: 0.7969 (0.8140)  loss_scale: 32768.0000 (33337.6306)  weight_decay: 0.0500 (0.0500)  time: 0.7976  data: 0.0002  max mem: 27222
Epoch: [24]  [2400/2502]  eta: 0:01:23  lr: 0.000485  min_lr: 0.000001  loss: 1.6973 (1.7046)  class_acc: 0.8281 (0.8141)  loss_scale: 32768.0000 (33313.9059)  weight_decay: 0.0500 (0.0500)  time: 0.8328  data: 0.0003  max mem: 27222
Epoch: [24]  [2500/2502]  eta: 0:00:01  lr: 0.000484  min_lr: 0.000001  loss: 1.6768 (1.7047)  class_acc: 0.7969 (0.8141)  loss_scale: 32768.0000 (33292.2880)  weight_decay: 0.0500 (0.0500)  time: 0.7688  data: 0.0010  max mem: 27222
Epoch: [24]  [2501/2502]  eta: 0:00:00  lr: 0.000484  min_lr: 0.000001  loss: 1.6768 (1.7047)  class_acc: 0.7969 (0.8141)  loss_scale: 32768.0000 (33292.2880)  weight_decay: 0.0500 (0.0500)  time: 0.7306  data: 0.0009  max mem: 27222
Epoch: [24] Total time: 0:34:16 (0.8219 s / it)
Averaged stats: lr: 0.000484  min_lr: 0.000001  loss: 1.6768 (1.7049)  class_acc: 0.7969 (0.8150)  loss_scale: 32768.0000 (33292.2880)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Test:  [ 0/98]  eta: 0:09:34  loss: 0.3582 (0.3582)  acc1: 93.7500 (93.7500)  acc5: 98.4375 (98.4375)  time: 5.8603  data: 5.5034  max mem: 27222
Test:  [97/98]  eta: 0:00:00  loss: 0.7561 (0.6741)  acc1: 82.8125 (84.6880)  acc5: 96.8750 (97.2160)  time: 0.3342  data: 0.0017  max mem: 27222
Test: Total time: 0:00:40 (0.4160 s / it)
* Acc@1 84.702 Acc@5 97.184 loss 0.678
Accuracy of the network on the 50000 test images: 84.7%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Test:  [ 0/98]  eta: 0:07:22  loss: 0.2334 (0.2334)  acc1: 92.1875 (92.1875)  acc5: 100.0000 (100.0000)  time: 4.5169  data: 4.1582  max mem: 27222
Test:  [97/98]  eta: 0:00:00  loss: 0.6139 (0.5587)  acc1: 84.3750 (86.2400)  acc5: 98.4375 (97.9040)  time: 0.3375  data: 0.0005  max mem: 27222
Test: Total time: 0:00:39 (0.4058 s / it)
* Acc@1 86.526 Acc@5 98.020 loss 0.554
EMA Accuracy of the network on the 50000 test images: 86.5%
Max accuracy: 86.53%
{"train_lr": 0.0004968039107218005, "train_min_lr": 6.488603863889624e-07, "train_loss": 1.704872216796875, "train_class_acc": 0.81500390625, "train_loss_scale": 33292.288, "train_weight_decay": 0.04999999999999801, "test_loss": 0.5541112438908645, "test_acc1": 86.52600001342773, "test_acc5": 98.02000000610352, "epoch": 24, "n_parameters": 86860264}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Epoch: [25]  [   0/2502]  eta: 1 day, 4:33:29  lr: 0.000484  min_lr: 0.000001  loss: 1.8945 (1.8945)  class_acc: 0.7656 (0.7656)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 41.0909  data: 34.9277  max mem: 27222
Epoch: [25]  [ 100/2502]  eta: 0:47:28  lr: 0.000483  min_lr: 0.000001  loss: 1.6025 (1.7042)  class_acc: 0.8281 (0.8145)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7832  data: 0.0003  max mem: 27222
Epoch: [25]  [ 200/2502]  eta: 0:37:59  lr: 0.000482  min_lr: 0.000001  loss: 1.6963 (1.7039)  class_acc: 0.7969 (0.8154)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8054  data: 0.0003  max mem: 27222
Epoch: [25]  [ 300/2502]  eta: 0:34:06  lr: 0.000481  min_lr: 0.000001  loss: 1.7510 (1.6909)  class_acc: 0.7969 (0.8188)  loss_scale: 32768.0000 (39299.8272)  weight_decay: 0.0500 (0.0500)  time: 0.7996  data: 0.0002  max mem: 27222
Epoch: [25]  [ 400/2502]  eta: 0:31:28  lr: 0.000480  min_lr: 0.000001  loss: 1.7168 (1.6875)  class_acc: 0.7969 (0.8198)  loss_scale: 32768.0000 (37670.9426)  weight_decay: 0.0500 (0.0500)  time: 0.8155  data: 0.0002  max mem: 27222
Epoch: [25]  [ 500/2502]  eta: 0:29:21  lr: 0.000479  min_lr: 0.000001  loss: 1.6172 (1.6885)  class_acc: 0.8125 (0.8193)  loss_scale: 32768.0000 (36692.3114)  weight_decay: 0.0500 (0.0500)  time: 0.8209  data: 0.0002  max mem: 27222
Epoch: [25]  [ 600/2502]  eta: 0:27:29  lr: 0.000478  min_lr: 0.000001  loss: 1.6641 (1.6881)  class_acc: 0.8281 (0.8198)  loss_scale: 32768.0000 (36039.3478)  weight_decay: 0.0500 (0.0500)  time: 0.7954  data: 0.0003  max mem: 27222
Epoch: [25]  [ 700/2502]  eta: 0:25:47  lr: 0.000477  min_lr: 0.000001  loss: 1.7178 (1.6927)  class_acc: 0.8281 (0.8186)  loss_scale: 32768.0000 (35572.6790)  weight_decay: 0.0500 (0.0500)  time: 0.7947  data: 0.0002  max mem: 27222
Epoch: [25]  [ 800/2502]  eta: 0:24:10  lr: 0.000476  min_lr: 0.000001  loss: 1.7012 (1.6956)  class_acc: 0.8125 (0.8179)  loss_scale: 32768.0000 (35386.1673)  weight_decay: 0.0500 (0.0500)  time: 0.7948  data: 0.0002  max mem: 27222
Epoch: [25]  [ 900/2502]  eta: 0:22:35  lr: 0.000475  min_lr: 0.000001  loss: 1.5908 (1.6979)  class_acc: 0.8438 (0.8175)  loss_scale: 32768.0000 (35095.5827)  weight_decay: 0.0500 (0.0500)  time: 0.8053  data: 0.0002  max mem: 27222
Epoch: [25]  [1000/2502]  eta: 0:21:03  lr: 0.000474  min_lr: 0.000001  loss: 1.6553 (1.6970)  class_acc: 0.8281 (0.8180)  loss_scale: 32768.0000 (34863.0569)  weight_decay: 0.0500 (0.0500)  time: 0.7937  data: 0.0003  max mem: 27222
Epoch: [25]  [1100/2502]  eta: 0:19:35  lr: 0.000473  min_lr: 0.000001  loss: 1.7158 (1.6957)  class_acc: 0.7969 (0.8180)  loss_scale: 32768.0000 (34672.7702)  weight_decay: 0.0500 (0.0500)  time: 0.7935  data: 0.0002  max mem: 27222
Epoch: [25]  [1200/2502]  eta: 0:18:08  lr: 0.000472  min_lr: 0.000001  loss: 1.7734 (1.6944)  class_acc: 0.8125 (0.8179)  loss_scale: 32768.0000 (34514.1715)  weight_decay: 0.0500 (0.0500)  time: 0.8137  data: 0.0003  max mem: 27222
Epoch: [25]  [1300/2502]  eta: 0:16:43  lr: 0.000471  min_lr: 0.000001  loss: 1.6465 (1.6926)  class_acc: 0.8281 (0.8183)  loss_scale: 32768.0000 (34379.9539)  weight_decay: 0.0500 (0.0500)  time: 0.7975  data: 0.0002  max mem: 27222
Epoch: [25]  [1400/2502]  eta: 0:15:17  lr: 0.000470  min_lr: 0.000001  loss: 1.6221 (1.6934)  class_acc: 0.8125 (0.8182)  loss_scale: 32768.0000 (34639.1206)  weight_decay: 0.0500 (0.0500)  time: 0.7882  data: 0.0002  max mem: 27222
Epoch: [25]  [1500/2502]  eta: 0:13:52  lr: 0.000469  min_lr: 0.000001  loss: 1.7012 (1.6931)  class_acc: 0.7969 (0.8182)  loss_scale: 32768.0000 (34514.4624)  weight_decay: 0.0500 (0.0500)  time: 0.8182  data: 0.0002  max mem: 27222
Epoch: [25]  [1600/2502]  eta: 0:12:28  lr: 0.000468  min_lr: 0.000001  loss: 1.6172 (1.6927)  class_acc: 0.8281 (0.8184)  loss_scale: 32768.0000 (34405.3766)  weight_decay: 0.0500 (0.0500)  time: 0.8279  data: 0.0002  max mem: 27222
Epoch: [25]  [1700/2502]  eta: 0:11:04  lr: 0.000467  min_lr: 0.000001  loss: 1.6436 (1.6923)  class_acc: 0.8281 (0.8186)  loss_scale: 32768.0000 (34309.1170)  weight_decay: 0.0500 (0.0500)  time: 0.8090  data: 0.0002  max mem: 27222
Epoch: [25]  [1800/2502]  eta: 0:09:41  lr: 0.000466  min_lr: 0.000001  loss: 1.6377 (1.6911)  class_acc: 0.8438 (0.8187)  loss_scale: 32768.0000 (34223.5469)  weight_decay: 0.0500 (0.0500)  time: 0.8131  data: 0.0003  max mem: 27222
Epoch: [25]  [1900/2502]  eta: 0:08:17  lr: 0.000465  min_lr: 0.000001  loss: 1.6260 (1.6908)  class_acc: 0.8281 (0.8184)  loss_scale: 32768.0000 (34284.8774)  weight_decay: 0.0500 (0.0500)  time: 0.8089  data: 0.0002  max mem: 27222
Epoch: [25]  [2000/2502]  eta: 0:06:54  lr: 0.000464  min_lr: 0.000001  loss: 1.6318 (1.6903)  class_acc: 0.8125 (0.8183)  loss_scale: 32768.0000 (34209.0715)  weight_decay: 0.0500 (0.0500)  time: 0.8299  data: 0.0003  max mem: 27222
Epoch: [25]  [2100/2502]  eta: 0:05:31  lr: 0.000463  min_lr: 0.000001  loss: 1.6621 (1.6908)  class_acc: 0.8281 (0.8181)  loss_scale: 32768.0000 (34140.4817)  weight_decay: 0.0500 (0.0500)  time: 0.8172  data: 0.0002  max mem: 27222
Epoch: [25]  [2200/2502]  eta: 0:04:09  lr: 0.000462  min_lr: 0.000001  loss: 1.6973 (1.6912)  class_acc: 0.8125 (0.8181)  loss_scale: 32768.0000 (34078.1245)  weight_decay: 0.0500 (0.0500)  time: 0.8310  data: 0.0002  max mem: 27222
Epoch: [25]  [2300/2502]  eta: 0:02:46  lr: 0.000461  min_lr: 0.000001  loss: 1.6572 (1.6921)  class_acc: 0.8125 (0.8178)  loss_scale: 32768.0000 (34021.1873)  weight_decay: 0.0500 (0.0500)  time: 0.7919  data: 0.0010  max mem: 27222
Epoch: [25]  [2400/2502]  eta: 0:01:23  lr: 0.000460  min_lr: 0.000001  loss: 1.6318 (1.6921)  class_acc: 0.8281 (0.8178)  loss_scale: 32768.0000 (34296.5364)  weight_decay: 0.0500 (0.0500)  time: 0.7928  data: 0.0003  max mem: 27222
Epoch: [25]  [2500/2502]  eta: 0:00:01  lr: 0.000459  min_lr: 0.000001  loss: 1.6104 (1.6914)  class_acc: 0.8281 (0.8181)  loss_scale: 32768.0000 (34236.0064)  weight_decay: 0.0500 (0.0500)  time: 0.7546  data: 0.0010  max mem: 27222
Epoch: [25]  [2501/2502]  eta: 0:00:00  lr: 0.000459  min_lr: 0.000001  loss: 1.6104 (1.6914)  class_acc: 0.8281 (0.8181)  loss_scale: 32768.0000 (34236.0064)  weight_decay: 0.0500 (0.0500)  time: 0.7146  data: 0.0009  max mem: 27222
Epoch: [25] Total time: 0:34:16 (0.8219 s / it)
Averaged stats: lr: 0.000459  min_lr: 0.000001  loss: 1.6104 (1.6899)  class_acc: 0.8281 (0.8188)  loss_scale: 32768.0000 (34236.0064)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Test:  [ 0/98]  eta: 0:09:39  loss: 0.3603 (0.3603)  acc1: 93.7500 (93.7500)  acc5: 98.4375 (98.4375)  time: 5.9149  data: 5.5642  max mem: 27222
Test:  [97/98]  eta: 0:00:00  loss: 0.6994 (0.6681)  acc1: 81.2500 (84.7520)  acc5: 96.8750 (97.2320)  time: 0.3341  data: 0.0006  max mem: 27222
Test: Total time: 0:00:40 (0.4167 s / it)
* Acc@1 84.790 Acc@5 97.208 loss 0.670
Accuracy of the network on the 50000 test images: 84.8%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Test:  [ 0/98]  eta: 0:10:12  loss: 0.2352 (0.2352)  acc1: 92.1875 (92.1875)  acc5: 100.0000 (100.0000)  time: 6.2510  data: 5.9141  max mem: 27222
Test:  [97/98]  eta: 0:00:00  loss: 0.6171 (0.5597)  acc1: 84.3750 (86.4160)  acc5: 98.4375 (97.9200)  time: 0.3333  data: 0.0002  max mem: 27222
Test: Total time: 0:00:41 (0.4276 s / it)
* Acc@1 86.584 Acc@5 98.008 loss 0.555
EMA Accuracy of the network on the 50000 test images: 86.6%
Max accuracy: 86.58%
{"train_lr": 0.0004714574195354769, "train_min_lr": 6.157561098125737e-07, "train_loss": 1.68992197265625, "train_class_acc": 0.81882734375, "train_loss_scale": 34236.0064, "train_weight_decay": 0.04999999999999801, "test_loss": 0.5548417166027487, "test_acc1": 86.58400001464844, "test_acc5": 98.00800000732421, "epoch": 25, "n_parameters": 86860264}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Epoch: [26]  [   0/2502]  eta: 1 day, 6:25:44  lr: 0.000459  min_lr: 0.000001  loss: 1.4766 (1.4766)  class_acc: 0.8750 (0.8750)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 43.7826  data: 40.1023  max mem: 27222
Epoch: [26]  [ 100/2502]  eta: 0:48:30  lr: 0.000457  min_lr: 0.000001  loss: 1.6260 (1.6452)  class_acc: 0.8125 (0.8325)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7913  data: 0.0002  max mem: 27222
Epoch: [26]  [ 200/2502]  eta: 0:38:33  lr: 0.000456  min_lr: 0.000001  loss: 1.6982 (1.6726)  class_acc: 0.8125 (0.8252)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7894  data: 0.0002  max mem: 27222
Epoch: [26]  [ 300/2502]  eta: 0:34:27  lr: 0.000455  min_lr: 0.000001  loss: 1.6221 (1.6742)  class_acc: 0.8281 (0.8250)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8083  data: 0.0002  max mem: 27222
Epoch: [26]  [ 400/2502]  eta: 0:31:42  lr: 0.000454  min_lr: 0.000001  loss: 1.6191 (1.6658)  class_acc: 0.8438 (0.8273)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7887  data: 0.0003  max mem: 27222
Epoch: [26]  [ 500/2502]  eta: 0:29:27  lr: 0.000453  min_lr: 0.000001  loss: 1.6689 (1.6663)  class_acc: 0.8438 (0.8275)  loss_scale: 32768.0000 (34076.1038)  weight_decay: 0.0500 (0.0500)  time: 0.7930  data: 0.0002  max mem: 27222
Epoch: [26]  [ 600/2502]  eta: 0:27:34  lr: 0.000452  min_lr: 0.000001  loss: 1.6680 (1.6680)  class_acc: 0.8125 (0.8263)  loss_scale: 32768.0000 (33858.4493)  weight_decay: 0.0500 (0.0500)  time: 0.8273  data: 0.0002  max mem: 27222
Epoch: [26]  [ 700/2502]  eta: 0:25:53  lr: 0.000451  min_lr: 0.000001  loss: 1.6514 (1.6664)  class_acc: 0.8125 (0.8262)  loss_scale: 32768.0000 (33702.8930)  weight_decay: 0.0500 (0.0500)  time: 0.8106  data: 0.0003  max mem: 27222
Epoch: [26]  [ 800/2502]  eta: 0:24:14  lr: 0.000450  min_lr: 0.000001  loss: 1.6416 (1.6647)  class_acc: 0.8125 (0.8266)  loss_scale: 32768.0000 (33586.1773)  weight_decay: 0.0500 (0.0500)  time: 0.8202  data: 0.0002  max mem: 27222
Epoch: [26]  [ 900/2502]  eta: 0:22:41  lr: 0.000449  min_lr: 0.000001  loss: 1.6016 (1.6626)  class_acc: 0.8438 (0.8271)  loss_scale: 32768.0000 (33495.3696)  weight_decay: 0.0500 (0.0500)  time: 0.8053  data: 0.0002  max mem: 27222
Epoch: [26]  [1000/2502]  eta: 0:21:08  lr: 0.000448  min_lr: 0.000001  loss: 1.6562 (1.6631)  class_acc: 0.8125 (0.8266)  loss_scale: 32768.0000 (33553.6464)  weight_decay: 0.0500 (0.0500)  time: 0.8012  data: 0.0003  max mem: 27222
Epoch: [26]  [1100/2502]  eta: 0:19:38  lr: 0.000447  min_lr: 0.000001  loss: 1.6523 (1.6637)  class_acc: 0.8281 (0.8268)  loss_scale: 32768.0000 (33482.2888)  weight_decay: 0.0500 (0.0500)  time: 0.7968  data: 0.0002  max mem: 27222
Epoch: [26]  [1200/2502]  eta: 0:18:10  lr: 0.000446  min_lr: 0.000001  loss: 1.6855 (1.6642)  class_acc: 0.8125 (0.8267)  loss_scale: 32768.0000 (33422.8143)  weight_decay: 0.0500 (0.0500)  time: 0.7896  data: 0.0002  max mem: 27222
Epoch: [26]  [1300/2502]  eta: 0:16:43  lr: 0.000445  min_lr: 0.000001  loss: 1.6221 (1.6668)  class_acc: 0.7969 (0.8261)  loss_scale: 32768.0000 (33372.4827)  weight_decay: 0.0500 (0.0500)  time: 0.7912  data: 0.0003  max mem: 27222
Epoch: [26]  [1400/2502]  eta: 0:15:17  lr: 0.000444  min_lr: 0.000001  loss: 1.6982 (1.6665)  class_acc: 0.8125 (0.8259)  loss_scale: 32768.0000 (33329.3362)  weight_decay: 0.0500 (0.0500)  time: 0.7893  data: 0.0002  max mem: 27222
Epoch: [26]  [1500/2502]  eta: 0:13:52  lr: 0.000443  min_lr: 0.000001  loss: 1.6338 (1.6675)  class_acc: 0.8281 (0.8257)  loss_scale: 32768.0000 (33641.2312)  weight_decay: 0.0500 (0.0500)  time: 0.8268  data: 0.0002  max mem: 27222
Epoch: [26]  [1600/2502]  eta: 0:12:27  lr: 0.000442  min_lr: 0.000001  loss: 1.6113 (1.6659)  class_acc: 0.8281 (0.8257)  loss_scale: 32768.0000 (33586.6883)  weight_decay: 0.0500 (0.0500)  time: 0.7940  data: 0.0002  max mem: 27222
Epoch: [26]  [1700/2502]  eta: 0:11:03  lr: 0.000441  min_lr: 0.000001  loss: 1.7432 (1.6659)  class_acc: 0.7969 (0.8255)  loss_scale: 32768.0000 (33538.5585)  weight_decay: 0.0500 (0.0500)  time: 0.8051  data: 0.0003  max mem: 27222
Epoch: [26]  [1800/2502]  eta: 0:09:39  lr: 0.000440  min_lr: 0.000001  loss: 1.6143 (1.6670)  class_acc: 0.8125 (0.8255)  loss_scale: 32768.0000 (33495.7735)  weight_decay: 0.0500 (0.0500)  time: 0.7875  data: 0.0002  max mem: 27222
Epoch: [26]  [1900/2502]  eta: 0:08:16  lr: 0.000438  min_lr: 0.000001  loss: 1.7197 (1.6683)  class_acc: 0.7969 (0.8251)  loss_scale: 32768.0000 (33457.4897)  weight_decay: 0.0500 (0.0500)  time: 0.7910  data: 0.0002  max mem: 27222
Epoch: [26]  [2000/2502]  eta: 0:06:53  lr: 0.000437  min_lr: 0.000001  loss: 1.6670 (1.6699)  class_acc: 0.7969 (0.8247)  loss_scale: 32768.0000 (33554.0390)  weight_decay: 0.0500 (0.0500)  time: 0.8158  data: 0.0002  max mem: 27222
Epoch: [26]  [2100/2502]  eta: 0:05:30  lr: 0.000436  min_lr: 0.000001  loss: 1.6338 (1.6719)  class_acc: 0.8125 (0.8243)  loss_scale: 32768.0000 (33516.6264)  weight_decay: 0.0500 (0.0500)  time: 0.7967  data: 0.0002  max mem: 27222
Epoch: [26]  [2200/2502]  eta: 0:04:08  lr: 0.000435  min_lr: 0.000001  loss: 1.6426 (1.6736)  class_acc: 0.8281 (0.8239)  loss_scale: 32768.0000 (33482.6134)  weight_decay: 0.0500 (0.0500)  time: 0.8369  data: 0.0002  max mem: 27222
Epoch: [26]  [2300/2502]  eta: 0:02:45  lr: 0.000434  min_lr: 0.000001  loss: 1.5957 (1.6722)  class_acc: 0.8438 (0.8242)  loss_scale: 32768.0000 (33451.5567)  weight_decay: 0.0500 (0.0500)  time: 0.8234  data: 0.0002  max mem: 27222
Epoch: [26]  [2400/2502]  eta: 0:01:23  lr: 0.000433  min_lr: 0.000001  loss: 1.6611 (1.6722)  class_acc: 0.8125 (0.8241)  loss_scale: 32768.0000 (33423.0870)  weight_decay: 0.0500 (0.0500)  time: 0.8036  data: 0.0002  max mem: 27222
Epoch: [26]  [2500/2502]  eta: 0:00:01  lr: 0.000432  min_lr: 0.000001  loss: 1.6328 (1.6716)  class_acc: 0.8281 (0.8242)  loss_scale: 32768.0000 (33397.1456)  weight_decay: 0.0500 (0.0500)  time: 0.8011  data: 0.0012  max mem: 27222
Epoch: [26]  [2501/2502]  eta: 0:00:00  lr: 0.000432  min_lr: 0.000001  loss: 1.6328 (1.6716)  class_acc: 0.8281 (0.8242)  loss_scale: 32768.0000 (33397.1456)  weight_decay: 0.0500 (0.0500)  time: 0.7613  data: 0.0012  max mem: 27222
Epoch: [26] Total time: 0:34:11 (0.8199 s / it)
Averaged stats: lr: 0.000432  min_lr: 0.000001  loss: 1.6328 (1.6719)  class_acc: 0.8281 (0.8236)  loss_scale: 32768.0000 (33397.1456)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Test:  [ 0/98]  eta: 0:07:57  loss: 0.3502 (0.3502)  acc1: 90.6250 (90.6250)  acc5: 98.4375 (98.4375)  time: 4.8680  data: 4.5326  max mem: 27222
Test:  [97/98]  eta: 0:00:00  loss: 0.7412 (0.6695)  acc1: 81.2500 (84.8160)  acc5: 96.8750 (97.0720)  time: 0.3388  data: 0.0002  max mem: 27222
Test: Total time: 0:00:41 (0.4185 s / it)
* Acc@1 84.786 Acc@5 97.186 loss 0.671
Accuracy of the network on the 50000 test images: 84.8%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Test:  [ 0/98]  eta: 0:07:48  loss: 0.2373 (0.2373)  acc1: 92.1875 (92.1875)  acc5: 100.0000 (100.0000)  time: 4.7768  data: 4.4415  max mem: 27222
Test:  [97/98]  eta: 0:00:00  loss: 0.6117 (0.5607)  acc1: 84.3750 (86.2720)  acc5: 98.4375 (97.9360)  time: 0.3404  data: 0.0005  max mem: 27222
Test: Total time: 0:00:39 (0.4043 s / it)
* Acc@1 86.604 Acc@5 97.990 loss 0.556
EMA Accuracy of the network on the 50000 test images: 86.6%
Max accuracy: 86.60%
{"train_lr": 0.00044536518563528837, "train_min_lr": 5.816778414961547e-07, "train_loss": 1.6718802734375, "train_class_acc": 0.82362890625, "train_loss_scale": 33397.1456, "train_weight_decay": 0.04999999999999801, "test_loss": 0.5561132238683651, "test_acc1": 86.60400000946045, "test_acc5": 97.99000000732421, "epoch": 26, "n_parameters": 86860264}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Epoch: [27]  [   0/2502]  eta: 1 day, 5:41:34  lr: 0.000432  min_lr: 0.000001  loss: 1.5752 (1.5752)  class_acc: 0.7969 (0.7969)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 42.7235  data: 38.0318  max mem: 27222
Epoch: [27]  [ 100/2502]  eta: 0:48:07  lr: 0.000431  min_lr: 0.000001  loss: 1.5947 (1.6441)  class_acc: 0.8438 (0.8263)  loss_scale: 32768.0000 (35363.4851)  weight_decay: 0.0500 (0.0500)  time: 0.7951  data: 0.0002  max mem: 27222
Epoch: [27]  [ 200/2502]  eta: 0:38:20  lr: 0.000430  min_lr: 0.000001  loss: 1.6113 (1.6292)  class_acc: 0.8594 (0.8340)  loss_scale: 16384.0000 (29344.4776)  weight_decay: 0.0500 (0.0500)  time: 0.7933  data: 0.0002  max mem: 27222
Epoch: [27]  [ 300/2502]  eta: 0:34:10  lr: 0.000429  min_lr: 0.000001  loss: 1.6113 (1.6291)  class_acc: 0.8438 (0.8353)  loss_scale: 16384.0000 (25038.6711)  weight_decay: 0.0500 (0.0500)  time: 0.7875  data: 0.0002  max mem: 27222
Epoch: [27]  [ 400/2502]  eta: 0:31:26  lr: 0.000428  min_lr: 0.000001  loss: 1.5928 (1.6321)  class_acc: 0.8281 (0.8339)  loss_scale: 16384.0000 (22880.3990)  weight_decay: 0.0500 (0.0500)  time: 0.7915  data: 0.0002  max mem: 27222
Epoch: [27]  [ 500/2502]  eta: 0:29:17  lr: 0.000427  min_lr: 0.000001  loss: 1.6465 (1.6364)  class_acc: 0.8281 (0.8332)  loss_scale: 16384.0000 (21583.7126)  weight_decay: 0.0500 (0.0500)  time: 0.8053  data: 0.0002  max mem: 27222
Epoch: [27]  [ 600/2502]  eta: 0:27:24  lr: 0.000426  min_lr: 0.000001  loss: 1.6152 (1.6388)  class_acc: 0.8438 (0.8328)  loss_scale: 16384.0000 (20718.5358)  weight_decay: 0.0500 (0.0500)  time: 0.8043  data: 0.0002  max mem: 27222
Epoch: [27]  [ 700/2502]  eta: 0:25:41  lr: 0.000425  min_lr: 0.000001  loss: 1.6289 (1.6387)  class_acc: 0.8438 (0.8330)  loss_scale: 32768.0000 (21081.8374)  weight_decay: 0.0500 (0.0500)  time: 0.7935  data: 0.0002  max mem: 27222
Epoch: [27]  [ 800/2502]  eta: 0:24:04  lr: 0.000424  min_lr: 0.000001  loss: 1.7656 (1.6435)  class_acc: 0.8125 (0.8320)  loss_scale: 32768.0000 (22540.7840)  weight_decay: 0.0500 (0.0500)  time: 0.8074  data: 0.0002  max mem: 27222
Epoch: [27]  [ 900/2502]  eta: 0:22:31  lr: 0.000422  min_lr: 0.000001  loss: 1.6982 (1.6453)  class_acc: 0.8281 (0.8321)  loss_scale: 32768.0000 (23675.8801)  weight_decay: 0.0500 (0.0500)  time: 0.7979  data: 0.0002  max mem: 27222
Epoch: [27]  [1000/2502]  eta: 0:21:02  lr: 0.000421  min_lr: 0.000001  loss: 1.6729 (1.6452)  class_acc: 0.8125 (0.8319)  loss_scale: 32768.0000 (24584.1838)  weight_decay: 0.0500 (0.0500)  time: 0.7991  data: 0.0002  max mem: 27222
Epoch: [27]  [1100/2502]  eta: 0:19:34  lr: 0.000420  min_lr: 0.000001  loss: 1.6143 (1.6436)  class_acc: 0.8281 (0.8317)  loss_scale: 32768.0000 (25327.4914)  weight_decay: 0.0500 (0.0500)  time: 0.7934  data: 0.0002  max mem: 27222
Epoch: [27]  [1200/2502]  eta: 0:18:08  lr: 0.000419  min_lr: 0.000001  loss: 1.6279 (1.6439)  class_acc: 0.8438 (0.8315)  loss_scale: 65536.0000 (26765.5354)  weight_decay: 0.0500 (0.0500)  time: 0.8352  data: 0.0002  max mem: 27222
Epoch: [27]  [1300/2502]  eta: 0:16:41  lr: 0.000418  min_lr: 0.000001  loss: 1.6699 (1.6448)  class_acc: 0.8281 (0.8314)  loss_scale: 32768.0000 (27478.7763)  weight_decay: 0.0500 (0.0500)  time: 0.8465  data: 0.0002  max mem: 27222
Epoch: [27]  [1400/2502]  eta: 0:15:16  lr: 0.000417  min_lr: 0.000001  loss: 1.6377 (1.6458)  class_acc: 0.8281 (0.8311)  loss_scale: 32768.0000 (27856.3084)  weight_decay: 0.0500 (0.0500)  time: 0.7993  data: 0.0002  max mem: 27222
Epoch: [27]  [1500/2502]  eta: 0:13:51  lr: 0.000416  min_lr: 0.000001  loss: 1.6387 (1.6467)  class_acc: 0.8281 (0.8307)  loss_scale: 32768.0000 (28183.5363)  weight_decay: 0.0500 (0.0500)  time: 0.7998  data: 0.0002  max mem: 27222
Epoch: [27]  [1600/2502]  eta: 0:12:27  lr: 0.000415  min_lr: 0.000001  loss: 1.6465 (1.6486)  class_acc: 0.8125 (0.8305)  loss_scale: 32768.0000 (28469.8863)  weight_decay: 0.0500 (0.0500)  time: 0.7937  data: 0.0002  max mem: 27222
Epoch: [27]  [1700/2502]  eta: 0:11:03  lr: 0.000414  min_lr: 0.000001  loss: 1.5811 (1.6492)  class_acc: 0.8125 (0.8306)  loss_scale: 32768.0000 (28722.5679)  weight_decay: 0.0500 (0.0500)  time: 0.7999  data: 0.0002  max mem: 27222
Epoch: [27]  [1800/2502]  eta: 0:09:39  lr: 0.000413  min_lr: 0.000001  loss: 1.6250 (1.6507)  class_acc: 0.8125 (0.8301)  loss_scale: 32768.0000 (29092.7440)  weight_decay: 0.0500 (0.0500)  time: 0.8225  data: 0.0002  max mem: 27222
Epoch: [27]  [1900/2502]  eta: 0:08:16  lr: 0.000412  min_lr: 0.000001  loss: 1.7188 (1.6523)  class_acc: 0.8281 (0.8300)  loss_scale: 32768.0000 (29286.0768)  weight_decay: 0.0500 (0.0500)  time: 0.7946  data: 0.0002  max mem: 27222
Epoch: [27]  [2000/2502]  eta: 0:06:52  lr: 0.000411  min_lr: 0.000001  loss: 1.7178 (1.6528)  class_acc: 0.8281 (0.8297)  loss_scale: 32768.0000 (29460.0860)  weight_decay: 0.0500 (0.0500)  time: 0.7918  data: 0.0002  max mem: 27222
Epoch: [27]  [2100/2502]  eta: 0:05:30  lr: 0.000410  min_lr: 0.000001  loss: 1.6924 (1.6534)  class_acc: 0.8125 (0.8294)  loss_scale: 32768.0000 (29617.5307)  weight_decay: 0.0500 (0.0500)  time: 0.8065  data: 0.0002  max mem: 27222
Epoch: [27]  [2200/2502]  eta: 0:04:07  lr: 0.000408  min_lr: 0.000001  loss: 1.6719 (1.6531)  class_acc: 0.8281 (0.8295)  loss_scale: 32768.0000 (29760.6688)  weight_decay: 0.0500 (0.0500)  time: 0.8022  data: 0.0002  max mem: 27222
Epoch: [27]  [2300/2502]  eta: 0:02:45  lr: 0.000407  min_lr: 0.000001  loss: 1.6455 (1.6534)  class_acc: 0.8281 (0.8297)  loss_scale: 32768.0000 (29948.3286)  weight_decay: 0.0500 (0.0500)  time: 0.7949  data: 0.0002  max mem: 27222
Epoch: [27]  [2400/2502]  eta: 0:01:23  lr: 0.000406  min_lr: 0.000001  loss: 1.5762 (1.6535)  class_acc: 0.8281 (0.8297)  loss_scale: 32768.0000 (30065.7659)  weight_decay: 0.0500 (0.0500)  time: 0.7924  data: 0.0002  max mem: 27222
Epoch: [27]  [2500/2502]  eta: 0:00:01  lr: 0.000405  min_lr: 0.000001  loss: 1.6699 (1.6545)  class_acc: 0.8281 (0.8294)  loss_scale: 32768.0000 (30172.7744)  weight_decay: 0.0500 (0.0500)  time: 0.7457  data: 0.0009  max mem: 27222
Epoch: [27]  [2501/2502]  eta: 0:00:00  lr: 0.000405  min_lr: 0.000001  loss: 1.6699 (1.6545)  class_acc: 0.8281 (0.8294)  loss_scale: 32768.0000 (30172.7744)  weight_decay: 0.0500 (0.0500)  time: 0.7074  data: 0.0009  max mem: 27222
Epoch: [27] Total time: 0:34:07 (0.8183 s / it)
Averaged stats: lr: 0.000405  min_lr: 0.000001  loss: 1.6699 (1.6535)  class_acc: 0.8281 (0.8289)  loss_scale: 32768.0000 (30172.7744)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Test:  [ 0/98]  eta: 0:08:27  loss: 0.3400 (0.3400)  acc1: 92.1875 (92.1875)  acc5: 98.4375 (98.4375)  time: 5.1815  data: 4.8362  max mem: 27222
Test:  [97/98]  eta: 0:00:00  loss: 0.7284 (0.6761)  acc1: 82.8125 (84.8800)  acc5: 97.6190 (97.2480)  time: 0.3381  data: 0.0023  max mem: 27222
Test: Total time: 0:00:40 (0.4128 s / it)
* Acc@1 84.856 Acc@5 97.198 loss 0.677
Accuracy of the network on the 50000 test images: 84.9%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Test:  [ 0/98]  eta: 0:07:32  loss: 0.2349 (0.2349)  acc1: 92.1875 (92.1875)  acc5: 100.0000 (100.0000)  time: 4.6128  data: 4.2780  max mem: 27222
Test:  [97/98]  eta: 0:00:00  loss: 0.6108 (0.5625)  acc1: 84.3750 (86.4000)  acc5: 98.4375 (97.9360)  time: 0.3347  data: 0.0008  max mem: 27222
Test: Total time: 0:00:39 (0.4072 s / it)
* Acc@1 86.644 Acc@5 97.982 loss 0.558
EMA Accuracy of the network on the 50000 test images: 86.6%
Max accuracy: 86.64%
{"train_lr": 0.00041868807631978633, "train_min_lr": 5.468356852960352e-07, "train_loss": 1.653488134765625, "train_class_acc": 0.8289390625, "train_loss_scale": 30172.7744, "train_weight_decay": 0.04999999999999801, "test_loss": 0.5580433193138059, "test_acc1": 86.64400000946046, "test_acc5": 97.98200000732422, "epoch": 27, "n_parameters": 86860264}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Epoch: [28]  [   0/2502]  eta: 1 day, 5:43:49  lr: 0.000405  min_lr: 0.000001  loss: 1.3574 (1.3574)  class_acc: 0.9062 (0.9062)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 42.7776  data: 32.6918  max mem: 27222
Epoch: [28]  [ 100/2502]  eta: 0:48:01  lr: 0.000404  min_lr: 0.000001  loss: 1.5391 (1.6176)  class_acc: 0.8438 (0.8410)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7847  data: 0.0003  max mem: 27222
Epoch: [28]  [ 200/2502]  eta: 0:38:29  lr: 0.000403  min_lr: 0.000001  loss: 1.5889 (1.6239)  class_acc: 0.8438 (0.8368)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8058  data: 0.0003  max mem: 27222
Epoch: [28]  [ 300/2502]  eta: 0:34:26  lr: 0.000402  min_lr: 0.000001  loss: 1.5361 (1.6250)  class_acc: 0.8594 (0.8365)  loss_scale: 32768.0000 (33203.4551)  weight_decay: 0.0500 (0.0500)  time: 0.8414  data: 0.0003  max mem: 27222
Epoch: [28]  [ 400/2502]  eta: 0:31:43  lr: 0.000401  min_lr: 0.000001  loss: 1.6377 (1.6212)  class_acc: 0.8438 (0.8367)  loss_scale: 32768.0000 (33094.8628)  weight_decay: 0.0500 (0.0500)  time: 0.7950  data: 0.0015  max mem: 27222
Epoch: [28]  [ 500/2502]  eta: 0:29:30  lr: 0.000400  min_lr: 0.000001  loss: 1.6406 (1.6231)  class_acc: 0.8281 (0.8362)  loss_scale: 32768.0000 (33029.6208)  weight_decay: 0.0500 (0.0500)  time: 0.7920  data: 0.0015  max mem: 27222
Epoch: [28]  [ 600/2502]  eta: 0:27:35  lr: 0.000399  min_lr: 0.000001  loss: 1.6777 (1.6261)  class_acc: 0.8281 (0.8352)  loss_scale: 32768.0000 (32986.0899)  weight_decay: 0.0500 (0.0500)  time: 0.8115  data: 0.0003  max mem: 27222
Epoch: [28]  [ 700/2502]  eta: 0:25:51  lr: 0.000398  min_lr: 0.000001  loss: 1.6484 (1.6287)  class_acc: 0.8281 (0.8346)  loss_scale: 32768.0000 (32954.9786)  weight_decay: 0.0500 (0.0500)  time: 0.7936  data: 0.0003  max mem: 27222
Epoch: [28]  [ 800/2502]  eta: 0:24:14  lr: 0.000396  min_lr: 0.000001  loss: 1.5889 (1.6281)  class_acc: 0.8281 (0.8348)  loss_scale: 32768.0000 (33095.2709)  weight_decay: 0.0500 (0.0500)  time: 0.7915  data: 0.0002  max mem: 27222
Epoch: [28]  [ 900/2502]  eta: 0:22:39  lr: 0.000395  min_lr: 0.000001  loss: 1.6377 (1.6271)  class_acc: 0.8125 (0.8351)  loss_scale: 32768.0000 (33058.9478)  weight_decay: 0.0500 (0.0500)  time: 0.8109  data: 0.0003  max mem: 27222
Epoch: [28]  [1000/2502]  eta: 0:21:07  lr: 0.000394  min_lr: 0.000001  loss: 1.6553 (1.6278)  class_acc: 0.8281 (0.8353)  loss_scale: 32768.0000 (33029.8821)  weight_decay: 0.0500 (0.0500)  time: 0.8296  data: 0.0003  max mem: 27222
Epoch: [28]  [1100/2502]  eta: 0:19:38  lr: 0.000393  min_lr: 0.000001  loss: 1.5645 (1.6290)  class_acc: 0.8438 (0.8351)  loss_scale: 32768.0000 (33006.0963)  weight_decay: 0.0500 (0.0500)  time: 0.7974  data: 0.0003  max mem: 27222
Epoch: [28]  [1200/2502]  eta: 0:18:10  lr: 0.000392  min_lr: 0.000001  loss: 1.6797 (1.6301)  class_acc: 0.8281 (0.8355)  loss_scale: 32768.0000 (32986.2714)  weight_decay: 0.0500 (0.0500)  time: 0.7974  data: 0.0005  max mem: 27222
Epoch: [28]  [1300/2502]  eta: 0:16:44  lr: 0.000391  min_lr: 0.000001  loss: 1.6650 (1.6301)  class_acc: 0.8125 (0.8352)  loss_scale: 32768.0000 (32969.4942)  weight_decay: 0.0500 (0.0500)  time: 0.8236  data: 0.0007  max mem: 27222
Epoch: [28]  [1400/2502]  eta: 0:15:18  lr: 0.000390  min_lr: 0.000001  loss: 1.6748 (1.6288)  class_acc: 0.8281 (0.8361)  loss_scale: 32768.0000 (33142.2241)  weight_decay: 0.0500 (0.0500)  time: 0.8607  data: 0.0003  max mem: 27222
Epoch: [28]  [1500/2502]  eta: 0:13:53  lr: 0.000389  min_lr: 0.000001  loss: 1.5527 (1.6276)  class_acc: 0.8438 (0.8362)  loss_scale: 32768.0000 (33117.2925)  weight_decay: 0.0500 (0.0500)  time: 0.7919  data: 0.0004  max mem: 27222
Epoch: [28]  [1600/2502]  eta: 0:12:28  lr: 0.000388  min_lr: 0.000001  loss: 1.5840 (1.6291)  class_acc: 0.8438 (0.8360)  loss_scale: 32768.0000 (33095.4753)  weight_decay: 0.0500 (0.0500)  time: 0.7946  data: 0.0002  max mem: 27222
Epoch: [28]  [1700/2502]  eta: 0:11:04  lr: 0.000387  min_lr: 0.000001  loss: 1.6133 (1.6288)  class_acc: 0.8438 (0.8362)  loss_scale: 32768.0000 (33076.2234)  weight_decay: 0.0500 (0.0500)  time: 0.8306  data: 0.0004  max mem: 27222
Epoch: [28]  [1800/2502]  eta: 0:09:40  lr: 0.000386  min_lr: 0.000001  loss: 1.6328 (1.6293)  class_acc: 0.8125 (0.8360)  loss_scale: 32768.0000 (33059.1094)  weight_decay: 0.0500 (0.0500)  time: 0.7976  data: 0.0011  max mem: 27222
Epoch: [28]  [1900/2502]  eta: 0:08:16  lr: 0.000384  min_lr: 0.000001  loss: 1.6748 (1.6305)  class_acc: 0.8125 (0.8356)  loss_scale: 32768.0000 (33595.3877)  weight_decay: 0.0500 (0.0500)  time: 0.8016  data: 0.0004  max mem: 27222
Epoch: [28]  [2000/2502]  eta: 0:06:53  lr: 0.000383  min_lr: 0.000001  loss: 1.6104 (1.6309)  class_acc: 0.8281 (0.8355)  loss_scale: 32768.0000 (33554.0390)  weight_decay: 0.0500 (0.0500)  time: 0.8147  data: 0.0003  max mem: 27222
Epoch: [28]  [2100/2502]  eta: 0:05:30  lr: 0.000382  min_lr: 0.000000  loss: 1.5596 (1.6304)  class_acc: 0.8594 (0.8356)  loss_scale: 32768.0000 (33516.6264)  weight_decay: 0.0500 (0.0500)  time: 0.7935  data: 0.0003  max mem: 27222
Epoch: [28]  [2200/2502]  eta: 0:04:08  lr: 0.000381  min_lr: 0.000000  loss: 1.4961 (1.6291)  class_acc: 0.8594 (0.8357)  loss_scale: 32768.0000 (33482.6134)  weight_decay: 0.0500 (0.0500)  time: 0.8033  data: 0.0004  max mem: 27222
Epoch: [28]  [2300/2502]  eta: 0:02:45  lr: 0.000380  min_lr: 0.000000  loss: 1.6250 (1.6296)  class_acc: 0.8281 (0.8356)  loss_scale: 32768.0000 (33451.5567)  weight_decay: 0.0500 (0.0500)  time: 0.8290  data: 0.0003  max mem: 27222
Epoch: [28]  [2400/2502]  eta: 0:01:23  lr: 0.000379  min_lr: 0.000000  loss: 1.6582 (1.6299)  class_acc: 0.8281 (0.8355)  loss_scale: 65536.0000 (33641.4494)  weight_decay: 0.0500 (0.0500)  time: 0.7936  data: 0.0003  max mem: 27222
Epoch: [28]  [2500/2502]  eta: 0:00:01  lr: 0.000378  min_lr: 0.000000  loss: 1.6367 (1.6299)  class_acc: 0.8281 (0.8355)  loss_scale: 32768.0000 (33606.8608)  weight_decay: 0.0500 (0.0500)  time: 0.7487  data: 0.0027  max mem: 27222
Epoch: [28]  [2501/2502]  eta: 0:00:00  lr: 0.000378  min_lr: 0.000000  loss: 1.6367 (1.6299)  class_acc: 0.8281 (0.8355)  loss_scale: 32768.0000 (33606.8608)  weight_decay: 0.0500 (0.0500)  time: 0.7100  data: 0.0027  max mem: 27222
Epoch: [28] Total time: 0:34:09 (0.8190 s / it)
Averaged stats: lr: 0.000378  min_lr: 0.000000  loss: 1.6367 (1.6379)  class_acc: 0.8281 (0.8339)  loss_scale: 32768.0000 (33606.8608)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Test:  [ 0/98]  eta: 0:10:13  loss: 0.3980 (0.3980)  acc1: 93.7500 (93.7500)  acc5: 98.4375 (98.4375)  time: 6.2622  data: 5.9251  max mem: 27222
Test:  [97/98]  eta: 0:00:00  loss: 0.7558 (0.6968)  acc1: 79.6875 (84.4640)  acc5: 97.6190 (97.0560)  time: 0.3324  data: 0.0002  max mem: 27222
Test: Total time: 0:00:42 (0.4289 s / it)
* Acc@1 84.730 Acc@5 97.088 loss 0.683
Accuracy of the network on the 50000 test images: 84.7%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Test:  [ 0/98]  eta: 0:07:04  loss: 0.2357 (0.2357)  acc1: 92.1875 (92.1875)  acc5: 100.0000 (100.0000)  time: 4.3275  data: 3.9779  max mem: 27222
Test:  [97/98]  eta: 0:00:00  loss: 0.6139 (0.5649)  acc1: 84.3750 (86.4320)  acc5: 98.4375 (97.9680)  time: 0.3367  data: 0.0013  max mem: 27222
Test: Total time: 0:00:40 (0.4114 s / it)
* Acc@1 86.626 Acc@5 97.978 loss 0.561
EMA Accuracy of the network on the 50000 test images: 86.6%
Max accuracy: 86.64%
{"train_lr": 0.00039159056483893234, "train_min_lr": 5.114444546913917e-07, "train_loss": 1.637948681640625, "train_class_acc": 0.83392734375, "train_loss_scale": 33606.8608, "train_weight_decay": 0.04999999999999801, "test_loss": 0.5605405590363911, "test_acc1": 86.62600000946045, "test_acc5": 97.97800000610351, "epoch": 28, "n_parameters": 86860264}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Epoch: [29]  [   0/2502]  eta: 1 day, 5:19:22  lr: 0.000378  min_lr: 0.000000  loss: 1.5449 (1.5449)  class_acc: 0.8281 (0.8281)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 42.1912  data: 34.2753  max mem: 27222
Epoch: [29]  [ 100/2502]  eta: 0:47:50  lr: 0.000377  min_lr: 0.000000  loss: 1.5264 (1.5923)  class_acc: 0.8438 (0.8450)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7869  data: 0.0003  max mem: 27222
Epoch: [29]  [ 200/2502]  eta: 0:38:11  lr: 0.000376  min_lr: 0.000000  loss: 1.5527 (1.6093)  class_acc: 0.8281 (0.8431)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7889  data: 0.0003  max mem: 27222
Epoch: [29]  [ 300/2502]  eta: 0:34:06  lr: 0.000375  min_lr: 0.000000  loss: 1.5498 (1.6151)  class_acc: 0.8594 (0.8420)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8034  data: 0.0002  max mem: 27222
Epoch: [29]  [ 400/2502]  eta: 0:31:23  lr: 0.000374  min_lr: 0.000000  loss: 1.6045 (1.6203)  class_acc: 0.8594 (0.8408)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7962  data: 0.0014  max mem: 27222
Epoch: [29]  [ 500/2502]  eta: 0:29:13  lr: 0.000372  min_lr: 0.000000  loss: 1.6670 (1.6171)  class_acc: 0.8125 (0.8404)  loss_scale: 32768.0000 (33029.6208)  weight_decay: 0.0500 (0.0500)  time: 0.7941  data: 0.0002  max mem: 27222
Epoch: [29]  [ 600/2502]  eta: 0:27:22  lr: 0.000371  min_lr: 0.000000  loss: 1.6104 (1.6157)  class_acc: 0.8281 (0.8408)  loss_scale: 32768.0000 (32986.0899)  weight_decay: 0.0500 (0.0500)  time: 0.7952  data: 0.0003  max mem: 27222
Epoch: [29]  [ 700/2502]  eta: 0:25:39  lr: 0.000370  min_lr: 0.000000  loss: 1.6738 (1.6188)  class_acc: 0.8125 (0.8405)  loss_scale: 32768.0000 (32954.9786)  weight_decay: 0.0500 (0.0500)  time: 0.8148  data: 0.0002  max mem: 27222
Epoch: [29]  [ 800/2502]  eta: 0:24:07  lr: 0.000369  min_lr: 0.000000  loss: 1.6660 (1.6204)  class_acc: 0.8125 (0.8394)  loss_scale: 32768.0000 (32931.6355)  weight_decay: 0.0500 (0.0500)  time: 0.8130  data: 0.0010  max mem: 27222
Epoch: [29]  [ 900/2502]  eta: 0:22:32  lr: 0.000368  min_lr: 0.000000  loss: 1.6270 (1.6194)  class_acc: 0.8438 (0.8397)  loss_scale: 32768.0000 (32913.4739)  weight_decay: 0.0500 (0.0500)  time: 0.7966  data: 0.0002  max mem: 27222
Epoch: [29]  [1000/2502]  eta: 0:21:01  lr: 0.000367  min_lr: 0.000000  loss: 1.5625 (1.6215)  class_acc: 0.8281 (0.8389)  loss_scale: 32768.0000 (33029.8821)  weight_decay: 0.0500 (0.0500)  time: 0.8143  data: 0.0002  max mem: 27222
Epoch: [29]  [1100/2502]  eta: 0:19:31  lr: 0.000366  min_lr: 0.000000  loss: 1.5840 (1.6212)  class_acc: 0.8438 (0.8389)  loss_scale: 32768.0000 (33006.0963)  weight_decay: 0.0500 (0.0500)  time: 0.7924  data: 0.0002  max mem: 27222
Epoch: [29]  [1200/2502]  eta: 0:18:03  lr: 0.000365  min_lr: 0.000000  loss: 1.6055 (1.6221)  class_acc: 0.8281 (0.8388)  loss_scale: 32768.0000 (32986.2714)  weight_decay: 0.0500 (0.0500)  time: 0.7917  data: 0.0003  max mem: 27222
Epoch: [29]  [1300/2502]  eta: 0:16:36  lr: 0.000364  min_lr: 0.000000  loss: 1.5293 (1.6220)  class_acc: 0.8594 (0.8390)  loss_scale: 32768.0000 (32969.4942)  weight_decay: 0.0500 (0.0500)  time: 0.7907  data: 0.0002  max mem: 27222
Epoch: [29]  [1400/2502]  eta: 0:15:11  lr: 0.000363  min_lr: 0.000000  loss: 1.6104 (1.6212)  class_acc: 0.8594 (0.8391)  loss_scale: 32768.0000 (32955.1121)  weight_decay: 0.0500 (0.0500)  time: 0.7902  data: 0.0002  max mem: 27222
Epoch: [29]  [1500/2502]  eta: 0:13:47  lr: 0.000361  min_lr: 0.000000  loss: 1.6123 (1.6218)  class_acc: 0.8281 (0.8393)  loss_scale: 32768.0000 (33117.2925)  weight_decay: 0.0500 (0.0500)  time: 0.8039  data: 0.0002  max mem: 27222
Epoch: [29]  [1600/2502]  eta: 0:12:23  lr: 0.000360  min_lr: 0.000000  loss: 1.5664 (1.6208)  class_acc: 0.8438 (0.8397)  loss_scale: 32768.0000 (33095.4753)  weight_decay: 0.0500 (0.0500)  time: 0.8073  data: 0.0002  max mem: 27222
Epoch: [29]  [1700/2502]  eta: 0:11:00  lr: 0.000359  min_lr: 0.000000  loss: 1.6406 (1.6227)  class_acc: 0.8281 (0.8390)  loss_scale: 32768.0000 (33076.2234)  weight_decay: 0.0500 (0.0500)  time: 0.7942  data: 0.0002  max mem: 27222
Epoch: [29]  [1800/2502]  eta: 0:09:37  lr: 0.000358  min_lr: 0.000000  loss: 1.4873 (1.6210)  class_acc: 0.8750 (0.8394)  loss_scale: 32768.0000 (33059.1094)  weight_decay: 0.0500 (0.0500)  time: 0.8002  data: 0.0002  max mem: 27222
Epoch: [29]  [1900/2502]  eta: 0:08:14  lr: 0.000357  min_lr: 0.000000  loss: 1.6045 (1.6215)  class_acc: 0.8438 (0.8393)  loss_scale: 32768.0000 (33043.7959)  weight_decay: 0.0500 (0.0500)  time: 0.8071  data: 0.0002  max mem: 27222
Epoch: [29]  [2000/2502]  eta: 0:06:51  lr: 0.000356  min_lr: 0.000000  loss: 1.6494 (1.6211)  class_acc: 0.8281 (0.8391)  loss_scale: 32768.0000 (33161.0195)  weight_decay: 0.0500 (0.0500)  time: 0.8385  data: 0.0002  max mem: 27222
Epoch: [29]  [2100/2502]  eta: 0:05:29  lr: 0.000355  min_lr: 0.000000  loss: 1.5918 (1.6220)  class_acc: 0.8281 (0.8388)  loss_scale: 32768.0000 (33142.3132)  weight_decay: 0.0500 (0.0500)  time: 0.8139  data: 0.0003  max mem: 27222
Epoch: [29]  [2200/2502]  eta: 0:04:07  lr: 0.000354  min_lr: 0.000000  loss: 1.6104 (1.6208)  class_acc: 0.8438 (0.8389)  loss_scale: 32768.0000 (33125.3067)  weight_decay: 0.0500 (0.0500)  time: 0.8227  data: 0.0004  max mem: 27222
Epoch: [29]  [2300/2502]  eta: 0:02:45  lr: 0.000353  min_lr: 0.000000  loss: 1.6064 (1.6208)  class_acc: 0.8438 (0.8389)  loss_scale: 32768.0000 (33109.7784)  weight_decay: 0.0500 (0.0500)  time: 0.7987  data: 0.0011  max mem: 27222
Epoch: [29]  [2400/2502]  eta: 0:01:23  lr: 0.000352  min_lr: 0.000000  loss: 1.6660 (1.6218)  class_acc: 0.8281 (0.8385)  loss_scale: 32768.0000 (33095.5435)  weight_decay: 0.0500 (0.0500)  time: 0.8465  data: 0.0003  max mem: 27222
Epoch: [29]  [2500/2502]  eta: 0:00:01  lr: 0.000351  min_lr: 0.000000  loss: 1.6328 (1.6222)  class_acc: 0.8281 (0.8383)  loss_scale: 32768.0000 (33082.5728)  weight_decay: 0.0500 (0.0500)  time: 0.7524  data: 0.0010  max mem: 27222
Epoch: [29]  [2501/2502]  eta: 0:00:00  lr: 0.000351  min_lr: 0.000000  loss: 1.6328 (1.6222)  class_acc: 0.8281 (0.8383)  loss_scale: 32768.0000 (33082.5728)  weight_decay: 0.0500 (0.0500)  time: 0.7144  data: 0.0010  max mem: 27222
Epoch: [29] Total time: 0:34:02 (0.8164 s / it)
Averaged stats: lr: 0.000351  min_lr: 0.000000  loss: 1.6328 (1.6227)  class_acc: 0.8281 (0.8380)  loss_scale: 32768.0000 (33082.5728)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Test:  [ 0/98]  eta: 0:09:29  loss: 0.3460 (0.3460)  acc1: 90.6250 (90.6250)  acc5: 98.4375 (98.4375)  time: 5.8132  data: 5.4621  max mem: 27222
Test:  [97/98]  eta: 0:00:00  loss: 0.7744 (0.6775)  acc1: 82.8125 (85.0240)  acc5: 98.4375 (97.2960)  time: 0.3319  data: 0.0002  max mem: 27222
Test: Total time: 0:00:39 (0.4080 s / it)
* Acc@1 84.874 Acc@5 97.180 loss 0.680
Accuracy of the network on the 50000 test images: 84.9%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Test:  [ 0/98]  eta: 0:10:17  loss: 0.2406 (0.2406)  acc1: 92.1875 (92.1875)  acc5: 98.4375 (98.4375)  time: 6.2983  data: 5.9343  max mem: 27222
Test:  [97/98]  eta: 0:00:00  loss: 0.6226 (0.5681)  acc1: 85.7143 (86.5760)  acc5: 98.4375 (97.9200)  time: 0.3391  data: 0.0005  max mem: 27222
Test: Total time: 0:00:41 (0.4243 s / it)
* Acc@1 86.614 Acc@5 97.962 loss 0.563
EMA Accuracy of the network on the 50000 test images: 86.6%
Max accuracy: 86.64%
{"train_lr": 0.0003642397163618527, "train_min_lr": 4.757223483876763e-07, "train_loss": 1.62267587890625, "train_class_acc": 0.83796875, "train_loss_scale": 33082.5728, "train_weight_decay": 0.04999999999999801, "test_loss": 0.5634502142819823, "test_acc1": 86.61400000946045, "test_acc5": 97.96200000488281, "epoch": 29, "n_parameters": 86860264}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Epoch: [30]  [   0/2502]  eta: 1 day, 5:31:53  lr: 0.000351  min_lr: 0.000000  loss: 1.3164 (1.3164)  class_acc: 0.9375 (0.9375)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 42.4914  data: 24.0155  max mem: 27222
Epoch: [30]  [ 100/2502]  eta: 0:47:57  lr: 0.000349  min_lr: 0.000000  loss: 1.5225 (1.5614)  class_acc: 0.8594 (0.8549)  loss_scale: 32768.0000 (39256.7129)  weight_decay: 0.0500 (0.0500)  time: 0.7849  data: 0.0004  max mem: 27222
Epoch: [30]  [ 200/2502]  eta: 0:38:22  lr: 0.000348  min_lr: 0.000000  loss: 1.5547 (1.5748)  class_acc: 0.8594 (0.8490)  loss_scale: 32768.0000 (36028.4975)  weight_decay: 0.0500 (0.0500)  time: 0.8105  data: 0.0003  max mem: 27222
Epoch: [30]  [ 300/2502]  eta: 0:34:20  lr: 0.000347  min_lr: 0.000000  loss: 1.5254 (1.5733)  class_acc: 0.8438 (0.8490)  loss_scale: 32768.0000 (34945.2757)  weight_decay: 0.0500 (0.0500)  time: 0.8223  data: 0.0002  max mem: 27222
Epoch: [30]  [ 400/2502]  eta: 0:31:40  lr: 0.000346  min_lr: 0.000000  loss: 1.5713 (1.5790)  class_acc: 0.8438 (0.8469)  loss_scale: 32768.0000 (34402.3142)  weight_decay: 0.0500 (0.0500)  time: 0.7973  data: 0.0003  max mem: 27222
Epoch: [30]  [ 500/2502]  eta: 0:29:27  lr: 0.000345  min_lr: 0.000000  loss: 1.5488 (1.5820)  class_acc: 0.8438 (0.8469)  loss_scale: 32768.0000 (34076.1038)  weight_decay: 0.0500 (0.0500)  time: 0.7906  data: 0.0004  max mem: 27222
Epoch: [30]  [ 600/2502]  eta: 0:27:31  lr: 0.000344  min_lr: 0.000000  loss: 1.5947 (1.5859)  class_acc: 0.8438 (0.8462)  loss_scale: 32768.0000 (34076.5391)  weight_decay: 0.0500 (0.0500)  time: 0.8028  data: 0.0002  max mem: 27222
Epoch: [30]  [ 700/2502]  eta: 0:25:47  lr: 0.000343  min_lr: 0.000000  loss: 1.6602 (1.5922)  class_acc: 0.8438 (0.8456)  loss_scale: 32768.0000 (33889.8716)  weight_decay: 0.0500 (0.0500)  time: 0.8254  data: 0.0003  max mem: 27222
Epoch: [30]  [ 800/2502]  eta: 0:24:08  lr: 0.000342  min_lr: 0.000000  loss: 1.6357 (1.5931)  class_acc: 0.8281 (0.8454)  loss_scale: 32768.0000 (33749.8127)  weight_decay: 0.0500 (0.0500)  time: 0.7854  data: 0.0002  max mem: 27222
Epoch: [30]  [ 900/2502]  eta: 0:22:34  lr: 0.000341  min_lr: 0.000000  loss: 1.5254 (1.5944)  class_acc: 0.8594 (0.8452)  loss_scale: 32768.0000 (33640.8435)  weight_decay: 0.0500 (0.0500)  time: 0.7903  data: 0.0010  max mem: 27222
Epoch: [30]  [1000/2502]  eta: 0:21:04  lr: 0.000340  min_lr: 0.000000  loss: 1.5879 (1.5924)  class_acc: 0.8438 (0.8457)  loss_scale: 32768.0000 (33553.6464)  weight_decay: 0.0500 (0.0500)  time: 0.8083  data: 0.0002  max mem: 27222
Epoch: [30]  [1100/2502]  eta: 0:19:34  lr: 0.000338  min_lr: 0.000000  loss: 1.5967 (1.5949)  class_acc: 0.8281 (0.8448)  loss_scale: 32768.0000 (34196.5777)  weight_decay: 0.0500 (0.0500)  time: 0.7915  data: 0.0003  max mem: 27222
Epoch: [30]  [1200/2502]  eta: 0:18:07  lr: 0.000337  min_lr: 0.000000  loss: 1.6045 (1.5976)  class_acc: 0.8281 (0.8442)  loss_scale: 32768.0000 (34077.6286)  weight_decay: 0.0500 (0.0500)  time: 0.8029  data: 0.0002  max mem: 27222
Epoch: [30]  [1300/2502]  eta: 0:16:40  lr: 0.000336  min_lr: 0.000000  loss: 1.6113 (1.5991)  class_acc: 0.8281 (0.8438)  loss_scale: 32768.0000 (33976.9654)  weight_decay: 0.0500 (0.0500)  time: 0.7943  data: 0.0003  max mem: 27222
Epoch: [30]  [1400/2502]  eta: 0:15:15  lr: 0.000335  min_lr: 0.000000  loss: 1.5537 (1.6007)  class_acc: 0.8438 (0.8433)  loss_scale: 32768.0000 (33890.6724)  weight_decay: 0.0500 (0.0500)  time: 0.7945  data: 0.0002  max mem: 27222
Epoch: [30]  [1500/2502]  eta: 0:13:50  lr: 0.000334  min_lr: 0.000000  loss: 1.5439 (1.6020)  class_acc: 0.8594 (0.8431)  loss_scale: 32768.0000 (33815.8774)  weight_decay: 0.0500 (0.0500)  time: 0.8034  data: 0.0003  max mem: 27222
Epoch: [30]  [1600/2502]  eta: 0:12:25  lr: 0.000333  min_lr: 0.000000  loss: 1.6152 (1.6028)  class_acc: 0.8438 (0.8429)  loss_scale: 32768.0000 (33791.3604)  weight_decay: 0.0500 (0.0500)  time: 0.8050  data: 0.0004  max mem: 27222
Epoch: [30]  [1700/2502]  eta: 0:11:02  lr: 0.000332  min_lr: 0.000000  loss: 1.5840 (1.6043)  class_acc: 0.8438 (0.8424)  loss_scale: 32768.0000 (33846.7819)  weight_decay: 0.0500 (0.0500)  time: 0.7881  data: 0.0003  max mem: 27222
Epoch: [30]  [1800/2502]  eta: 0:09:38  lr: 0.000331  min_lr: 0.000000  loss: 1.5605 (1.6051)  class_acc: 0.8438 (0.8423)  loss_scale: 32768.0000 (33786.8828)  weight_decay: 0.0500 (0.0500)  time: 0.8016  data: 0.0002  max mem: 27222
Epoch: [30]  [1900/2502]  eta: 0:08:15  lr: 0.000330  min_lr: 0.000000  loss: 1.5752 (1.6051)  class_acc: 0.8438 (0.8422)  loss_scale: 32768.0000 (33733.2856)  weight_decay: 0.0500 (0.0500)  time: 0.8348  data: 0.0002  max mem: 27222
Epoch: [30]  [2000/2502]  eta: 0:06:52  lr: 0.000329  min_lr: 0.000000  loss: 1.5703 (1.6055)  class_acc: 0.8281 (0.8420)  loss_scale: 32768.0000 (33685.0455)  weight_decay: 0.0500 (0.0500)  time: 0.7950  data: 0.0002  max mem: 27222
Epoch: [30]  [2100/2502]  eta: 0:05:29  lr: 0.000327  min_lr: 0.000000  loss: 1.5908 (1.6054)  class_acc: 0.8438 (0.8423)  loss_scale: 32768.0000 (33641.3974)  weight_decay: 0.0500 (0.0500)  time: 0.7920  data: 0.0003  max mem: 27222
Epoch: [30]  [2200/2502]  eta: 0:04:07  lr: 0.000326  min_lr: 0.000000  loss: 1.5576 (1.6055)  class_acc: 0.8438 (0.8424)  loss_scale: 32768.0000 (33720.8178)  weight_decay: 0.0500 (0.0500)  time: 0.8099  data: 0.0002  max mem: 27222
Epoch: [30]  [2300/2502]  eta: 0:02:45  lr: 0.000325  min_lr: 0.000000  loss: 1.6240 (1.6058)  class_acc: 0.8281 (0.8423)  loss_scale: 32768.0000 (33679.4090)  weight_decay: 0.0500 (0.0500)  time: 0.8073  data: 0.0003  max mem: 27222
Epoch: [30]  [2400/2502]  eta: 0:01:23  lr: 0.000324  min_lr: 0.000000  loss: 1.5303 (1.6067)  class_acc: 0.8594 (0.8422)  loss_scale: 32768.0000 (33641.4494)  weight_decay: 0.0500 (0.0500)  time: 0.7970  data: 0.0002  max mem: 27222
Epoch: [30]  [2500/2502]  eta: 0:00:01  lr: 0.000323  min_lr: 0.000000  loss: 1.5645 (1.6059)  class_acc: 0.8281 (0.8425)  loss_scale: 32768.0000 (33606.8608)  weight_decay: 0.0500 (0.0500)  time: 0.7450  data: 0.0010  max mem: 27222
Epoch: [30]  [2501/2502]  eta: 0:00:00  lr: 0.000323  min_lr: 0.000000  loss: 1.5645 (1.6059)  class_acc: 0.8281 (0.8425)  loss_scale: 32768.0000 (33606.8608)  weight_decay: 0.0500 (0.0500)  time: 0.7074  data: 0.0010  max mem: 27222
Epoch: [30] Total time: 0:34:05 (0.8174 s / it)
Averaged stats: lr: 0.000323  min_lr: 0.000000  loss: 1.5645 (1.6057)  class_acc: 0.8281 (0.8426)  loss_scale: 32768.0000 (33606.8608)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Test:  [ 0/98]  eta: 0:10:18  loss: 0.3342 (0.3342)  acc1: 89.0625 (89.0625)  acc5: 100.0000 (100.0000)  time: 6.3063  data: 5.9649  max mem: 27222
Test:  [97/98]  eta: 0:00:00  loss: 0.7939 (0.6854)  acc1: 81.2500 (84.8800)  acc5: 96.8750 (97.0400)  time: 0.3330  data: 0.0011  max mem: 27222
Test: Total time: 0:00:40 (0.4168 s / it)
* Acc@1 84.730 Acc@5 97.116 loss 0.690
Accuracy of the network on the 50000 test images: 84.7%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Test:  [ 0/98]  eta: 0:10:39  loss: 0.2460 (0.2460)  acc1: 92.1875 (92.1875)  acc5: 98.4375 (98.4375)  time: 6.5260  data: 6.1904  max mem: 27222
Test:  [97/98]  eta: 0:00:00  loss: 0.6364 (0.5714)  acc1: 84.3750 (86.4160)  acc5: 98.4375 (97.8240)  time: 0.3330  data: 0.0002  max mem: 27222
Test: Total time: 0:00:42 (0.4325 s / it)
* Acc@1 86.584 Acc@5 97.914 loss 0.567
EMA Accuracy of the network on the 50000 test images: 86.6%
Max accuracy: 86.64%
{"train_lr": 0.0003368041579644781, "train_min_lr": 4.3988960504905623e-07, "train_loss": 1.6056935546875, "train_class_acc": 0.84263671875, "train_loss_scale": 33606.8608, "train_weight_decay": 0.04999999999999801, "test_loss": 0.566851652778533, "test_acc1": 86.58400000946045, "test_acc5": 97.91400000366211, "epoch": 30, "n_parameters": 86860264}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Epoch: [31]  [   0/2502]  eta: 1 day, 4:42:39  lr: 0.000323  min_lr: 0.000000  loss: 1.4473 (1.4473)  class_acc: 0.8750 (0.8750)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 41.3107  data: 36.2979  max mem: 27222
Epoch: [31]  [ 100/2502]  eta: 0:47:27  lr: 0.000322  min_lr: 0.000000  loss: 1.5527 (1.5745)  class_acc: 0.8438 (0.8482)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7848  data: 0.0007  max mem: 27222
Epoch: [31]  [ 200/2502]  eta: 0:38:05  lr: 0.000321  min_lr: 0.000000  loss: 1.5156 (1.5573)  class_acc: 0.8594 (0.8541)  loss_scale: 32768.0000 (35376.3980)  weight_decay: 0.0500 (0.0500)  time: 0.7938  data: 0.0002  max mem: 27222
Epoch: [31]  [ 300/2502]  eta: 0:34:06  lr: 0.000320  min_lr: 0.000000  loss: 1.5693 (1.5640)  class_acc: 0.8594 (0.8521)  loss_scale: 32768.0000 (34509.8206)  weight_decay: 0.0500 (0.0500)  time: 0.8075  data: 0.0004  max mem: 27222
Epoch: [31]  [ 400/2502]  eta: 0:31:25  lr: 0.000319  min_lr: 0.000000  loss: 1.6416 (1.5752)  class_acc: 0.8438 (0.8501)  loss_scale: 32768.0000 (34075.4514)  weight_decay: 0.0500 (0.0500)  time: 0.7980  data: 0.0003  max mem: 27222
Epoch: [31]  [ 500/2502]  eta: 0:29:18  lr: 0.000318  min_lr: 0.000000  loss: 1.5820 (1.5762)  class_acc: 0.8281 (0.8509)  loss_scale: 32768.0000 (33814.4830)  weight_decay: 0.0500 (0.0500)  time: 0.7957  data: 0.0003  max mem: 27222
Epoch: [31]  [ 600/2502]  eta: 0:27:28  lr: 0.000317  min_lr: 0.000000  loss: 1.5713 (1.5808)  class_acc: 0.8281 (0.8499)  loss_scale: 32768.0000 (33640.3594)  weight_decay: 0.0500 (0.0500)  time: 0.8304  data: 0.0002  max mem: 27222
Epoch: [31]  [ 700/2502]  eta: 0:25:43  lr: 0.000315  min_lr: 0.000000  loss: 1.5518 (1.5810)  class_acc: 0.8438 (0.8495)  loss_scale: 65536.0000 (34263.8288)  weight_decay: 0.0500 (0.0500)  time: 0.7930  data: 0.0002  max mem: 27222
Epoch: [31]  [ 800/2502]  eta: 0:24:07  lr: 0.000314  min_lr: 0.000000  loss: 1.4766 (1.5790)  class_acc: 0.8594 (0.8497)  loss_scale: 32768.0000 (34077.0836)  weight_decay: 0.0500 (0.0500)  time: 0.8411  data: 0.0003  max mem: 27222
Epoch: [31]  [ 900/2502]  eta: 0:22:34  lr: 0.000313  min_lr: 0.000000  loss: 1.5186 (1.5790)  class_acc: 0.8594 (0.8499)  loss_scale: 32768.0000 (33931.7913)  weight_decay: 0.0500 (0.0500)  time: 0.8036  data: 0.0008  max mem: 27222
Epoch: [31]  [1000/2502]  eta: 0:21:04  lr: 0.000312  min_lr: 0.000000  loss: 1.6016 (1.5834)  class_acc: 0.8438 (0.8490)  loss_scale: 32768.0000 (33815.5285)  weight_decay: 0.0500 (0.0500)  time: 0.8057  data: 0.0002  max mem: 27222
Epoch: [31]  [1100/2502]  eta: 0:19:34  lr: 0.000311  min_lr: 0.000000  loss: 1.5791 (1.5820)  class_acc: 0.8438 (0.8494)  loss_scale: 32768.0000 (33720.3851)  weight_decay: 0.0500 (0.0500)  time: 0.8023  data: 0.0002  max mem: 27222
Epoch: [31]  [1200/2502]  eta: 0:18:08  lr: 0.000310  min_lr: 0.000000  loss: 1.5117 (1.5839)  class_acc: 0.8750 (0.8492)  loss_scale: 32768.0000 (33641.0858)  weight_decay: 0.0500 (0.0500)  time: 0.8035  data: 0.0003  max mem: 27222
Epoch: [31]  [1300/2502]  eta: 0:16:42  lr: 0.000309  min_lr: 0.000000  loss: 1.5469 (1.5831)  class_acc: 0.8438 (0.8492)  loss_scale: 32768.0000 (34379.9539)  weight_decay: 0.0500 (0.0500)  time: 0.8075  data: 0.0002  max mem: 27222
Epoch: [31]  [1400/2502]  eta: 0:15:16  lr: 0.000308  min_lr: 0.000000  loss: 1.5605 (1.5874)  class_acc: 0.8281 (0.8478)  loss_scale: 32768.0000 (34264.8965)  weight_decay: 0.0500 (0.0500)  time: 0.7891  data: 0.0002  max mem: 27222
Epoch: [31]  [1500/2502]  eta: 0:13:50  lr: 0.000307  min_lr: 0.000000  loss: 1.6016 (1.5892)  class_acc: 0.8594 (0.8473)  loss_scale: 32768.0000 (34165.1699)  weight_decay: 0.0500 (0.0500)  time: 0.7898  data: 0.0003  max mem: 27222
Epoch: [31]  [1600/2502]  eta: 0:12:26  lr: 0.000306  min_lr: 0.000000  loss: 1.5586 (1.5902)  class_acc: 0.8594 (0.8471)  loss_scale: 32768.0000 (34077.9013)  weight_decay: 0.0500 (0.0500)  time: 0.7921  data: 0.0003  max mem: 27222
Epoch: [31]  [1700/2502]  eta: 0:11:01  lr: 0.000305  min_lr: 0.000000  loss: 1.5703 (1.5902)  class_acc: 0.8594 (0.8470)  loss_scale: 32768.0000 (34000.8936)  weight_decay: 0.0500 (0.0500)  time: 0.7895  data: 0.0003  max mem: 27222
Epoch: [31]  [1800/2502]  eta: 0:09:38  lr: 0.000303  min_lr: 0.000000  loss: 1.6260 (1.5926)  class_acc: 0.8281 (0.8465)  loss_scale: 32768.0000 (34005.2149)  weight_decay: 0.0500 (0.0500)  time: 0.8309  data: 0.0002  max mem: 27222
Epoch: [31]  [1900/2502]  eta: 0:08:15  lr: 0.000302  min_lr: 0.000000  loss: 1.6172 (1.5923)  class_acc: 0.8438 (0.8467)  loss_scale: 32768.0000 (33940.1326)  weight_decay: 0.0500 (0.0500)  time: 0.8309  data: 0.0002  max mem: 27222
Epoch: [31]  [2000/2502]  eta: 0:06:52  lr: 0.000301  min_lr: 0.000000  loss: 1.5352 (1.5926)  class_acc: 0.8438 (0.8466)  loss_scale: 32768.0000 (33881.5552)  weight_decay: 0.0500 (0.0500)  time: 0.8026  data: 0.0002  max mem: 27222
Epoch: [31]  [2100/2502]  eta: 0:05:29  lr: 0.000300  min_lr: 0.000000  loss: 1.5674 (1.5924)  class_acc: 0.8438 (0.8466)  loss_scale: 32768.0000 (33828.5540)  weight_decay: 0.0500 (0.0500)  time: 0.7885  data: 0.0003  max mem: 27222
Epoch: [31]  [2200/2502]  eta: 0:04:07  lr: 0.000299  min_lr: 0.000000  loss: 1.5566 (1.5915)  class_acc: 0.8594 (0.8469)  loss_scale: 32768.0000 (33780.3689)  weight_decay: 0.0500 (0.0500)  time: 0.7907  data: 0.0002  max mem: 27222
Epoch: [31]  [2300/2502]  eta: 0:02:45  lr: 0.000298  min_lr: 0.000000  loss: 1.6299 (1.5919)  class_acc: 0.8281 (0.8468)  loss_scale: 65536.0000 (34049.6688)  weight_decay: 0.0500 (0.0500)  time: 0.8285  data: 0.0002  max mem: 27222
Epoch: [31]  [2400/2502]  eta: 0:01:23  lr: 0.000297  min_lr: 0.000000  loss: 1.5186 (1.5921)  class_acc: 0.8438 (0.8466)  loss_scale: 32768.0000 (34023.5835)  weight_decay: 0.0500 (0.0500)  time: 0.8086  data: 0.0003  max mem: 27222
Epoch: [31]  [2500/2502]  eta: 0:00:01  lr: 0.000296  min_lr: 0.000000  loss: 1.5439 (1.5922)  class_acc: 0.8594 (0.8464)  loss_scale: 32768.0000 (33973.8624)  weight_decay: 0.0500 (0.0500)  time: 0.7635  data: 0.0010  max mem: 27222
Epoch: [31]  [2501/2502]  eta: 0:00:00  lr: 0.000296  min_lr: 0.000000  loss: 1.5439 (1.5922)  class_acc: 0.8594 (0.8464)  loss_scale: 32768.0000 (33973.8624)  weight_decay: 0.0500 (0.0500)  time: 0.7253  data: 0.0010  max mem: 27222
Epoch: [31] Total time: 0:34:05 (0.8175 s / it)
Averaged stats: lr: 0.000296  min_lr: 0.000000  loss: 1.5439 (1.5914)  class_acc: 0.8594 (0.8466)  loss_scale: 32768.0000 (33973.8624)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Test:  [ 0/98]  eta: 0:09:30  loss: 0.2945 (0.2945)  acc1: 95.3125 (95.3125)  acc5: 100.0000 (100.0000)  time: 5.8213  data: 5.4740  max mem: 27222
Test:  [97/98]  eta: 0:00:00  loss: 0.7785 (0.6883)  acc1: 82.8125 (85.1200)  acc5: 96.8750 (96.9760)  time: 0.3356  data: 0.0010  max mem: 27222
Test: Total time: 0:00:39 (0.4068 s / it)
* Acc@1 84.958 Acc@5 97.156 loss 0.686
Accuracy of the network on the 50000 test images: 85.0%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Test:  [ 0/98]  eta: 0:08:42  loss: 0.2481 (0.2481)  acc1: 92.1875 (92.1875)  acc5: 98.4375 (98.4375)  time: 5.3349  data: 4.9993  max mem: 27222
Test:  [97/98]  eta: 0:00:00  loss: 0.6473 (0.5752)  acc1: 84.3750 (86.4320)  acc5: 98.4375 (97.8400)  time: 0.3335  data: 0.0008  max mem: 27222
Test: Total time: 0:00:41 (0.4283 s / it)
* Acc@1 86.574 Acc@5 97.910 loss 0.571
EMA Accuracy of the network on the 50000 test images: 86.6%
Max accuracy: 86.64%
{"train_lr": 0.00030945303898758595, "train_min_lr": 4.0416714545381677e-07, "train_loss": 1.59137529296875, "train_class_acc": 0.84656171875, "train_loss_scale": 33973.8624, "train_weight_decay": 0.04999999999999801, "test_loss": 0.5708126194242921, "test_acc1": 86.57400001190186, "test_acc5": 97.91000000488282, "epoch": 31, "n_parameters": 86860264}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Epoch: [32]  [   0/2502]  eta: 1 day, 4:43:25  lr: 0.000296  min_lr: 0.000000  loss: 1.6797 (1.6797)  class_acc: 0.8125 (0.8125)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 41.3289  data: 38.8216  max mem: 27222
Epoch: [32]  [ 100/2502]  eta: 0:48:01  lr: 0.000295  min_lr: 0.000000  loss: 1.5459 (1.5640)  class_acc: 0.8594 (0.8564)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7878  data: 0.0003  max mem: 27222
Epoch: [32]  [ 200/2502]  eta: 0:38:12  lr: 0.000294  min_lr: 0.000000  loss: 1.5137 (1.5551)  class_acc: 0.8594 (0.8588)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7956  data: 0.0002  max mem: 27222
Epoch: [32]  [ 300/2502]  eta: 0:34:26  lr: 0.000293  min_lr: 0.000000  loss: 1.4678 (1.5496)  class_acc: 0.8594 (0.8597)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8399  data: 0.0008  max mem: 27222
Epoch: [32]  [ 400/2502]  eta: 0:31:42  lr: 0.000291  min_lr: 0.000000  loss: 1.5674 (1.5509)  class_acc: 0.8594 (0.8591)  loss_scale: 32768.0000 (33094.8628)  weight_decay: 0.0500 (0.0500)  time: 0.7919  data: 0.0003  max mem: 27222
Epoch: [32]  [ 500/2502]  eta: 0:29:30  lr: 0.000290  min_lr: 0.000000  loss: 1.5850 (1.5571)  class_acc: 0.8750 (0.8579)  loss_scale: 32768.0000 (33029.6208)  weight_decay: 0.0500 (0.0500)  time: 0.7938  data: 0.0002  max mem: 27222
Epoch: [32]  [ 600/2502]  eta: 0:27:37  lr: 0.000289  min_lr: 0.000000  loss: 1.5781 (1.5569)  class_acc: 0.8281 (0.8569)  loss_scale: 32768.0000 (32986.0899)  weight_decay: 0.0500 (0.0500)  time: 0.8171  data: 0.0003  max mem: 27222
Epoch: [32]  [ 700/2502]  eta: 0:25:53  lr: 0.000288  min_lr: 0.000000  loss: 1.5244 (1.5594)  class_acc: 0.8438 (0.8559)  loss_scale: 32768.0000 (32954.9786)  weight_decay: 0.0500 (0.0500)  time: 0.7926  data: 0.0003  max mem: 27222
Epoch: [32]  [ 800/2502]  eta: 0:24:13  lr: 0.000287  min_lr: 0.000000  loss: 1.4873 (1.5610)  class_acc: 0.8750 (0.8558)  loss_scale: 32768.0000 (32931.6355)  weight_decay: 0.0500 (0.0500)  time: 0.7925  data: 0.0002  max mem: 27222
Epoch: [32]  [ 900/2502]  eta: 0:22:38  lr: 0.000286  min_lr: 0.000000  loss: 1.5693 (1.5618)  class_acc: 0.8594 (0.8557)  loss_scale: 32768.0000 (34077.2653)  weight_decay: 0.0500 (0.0500)  time: 0.8003  data: 0.0003  max mem: 27222
Epoch: [32]  [1000/2502]  eta: 0:21:08  lr: 0.000285  min_lr: 0.000000  loss: 1.5693 (1.5611)  class_acc: 0.8594 (0.8561)  loss_scale: 32768.0000 (33946.4695)  weight_decay: 0.0500 (0.0500)  time: 0.8145  data: 0.0005  max mem: 27222
Epoch: [32]  [1100/2502]  eta: 0:19:37  lr: 0.000284  min_lr: 0.000000  loss: 1.5176 (1.5630)  class_acc: 0.8594 (0.8557)  loss_scale: 32768.0000 (33839.4332)  weight_decay: 0.0500 (0.0500)  time: 0.7954  data: 0.0002  max mem: 27222
Epoch: [32]  [1200/2502]  eta: 0:18:11  lr: 0.000283  min_lr: 0.000000  loss: 1.5732 (1.5665)  class_acc: 0.8438 (0.8549)  loss_scale: 32768.0000 (33750.2215)  weight_decay: 0.0500 (0.0500)  time: 0.8342  data: 0.0003  max mem: 27222
Epoch: [32]  [1300/2502]  eta: 0:16:44  lr: 0.000282  min_lr: 0.000000  loss: 1.5664 (1.5687)  class_acc: 0.8438 (0.8543)  loss_scale: 32768.0000 (33674.7241)  weight_decay: 0.0500 (0.0500)  time: 0.7927  data: 0.0003  max mem: 27222
Epoch: [32]  [1400/2502]  eta: 0:15:18  lr: 0.000281  min_lr: 0.000000  loss: 1.5928 (1.5696)  class_acc: 0.8438 (0.8537)  loss_scale: 32768.0000 (33703.5603)  weight_decay: 0.0500 (0.0500)  time: 0.8350  data: 0.0003  max mem: 27222
Epoch: [32]  [1500/2502]  eta: 0:13:52  lr: 0.000280  min_lr: 0.000000  loss: 1.5029 (1.5675)  class_acc: 0.8750 (0.8539)  loss_scale: 32768.0000 (33641.2312)  weight_decay: 0.0500 (0.0500)  time: 0.7903  data: 0.0003  max mem: 27222
Epoch: [32]  [1600/2502]  eta: 0:12:28  lr: 0.000279  min_lr: 0.000000  loss: 1.5625 (1.5668)  class_acc: 0.8594 (0.8540)  loss_scale: 32768.0000 (33586.6883)  weight_decay: 0.0500 (0.0500)  time: 0.8101  data: 0.0002  max mem: 27222
Epoch: [32]  [1700/2502]  eta: 0:11:04  lr: 0.000277  min_lr: 0.000000  loss: 1.5137 (1.5663)  class_acc: 0.8594 (0.8544)  loss_scale: 32768.0000 (33538.5585)  weight_decay: 0.0500 (0.0500)  time: 0.8477  data: 0.0003  max mem: 27222
Epoch: [32]  [1800/2502]  eta: 0:09:40  lr: 0.000276  min_lr: 0.000000  loss: 1.5801 (1.5685)  class_acc: 0.8281 (0.8537)  loss_scale: 32768.0000 (33495.7735)  weight_decay: 0.0500 (0.0500)  time: 0.7953  data: 0.0002  max mem: 27222
Epoch: [32]  [1900/2502]  eta: 0:08:16  lr: 0.000275  min_lr: 0.000000  loss: 1.5859 (1.5704)  class_acc: 0.8438 (0.8532)  loss_scale: 32768.0000 (33457.4897)  weight_decay: 0.0500 (0.0500)  time: 0.7953  data: 0.0004  max mem: 27222
Epoch: [32]  [2000/2502]  eta: 0:06:53  lr: 0.000274  min_lr: 0.000000  loss: 1.5459 (1.5707)  class_acc: 0.8594 (0.8530)  loss_scale: 32768.0000 (33554.0390)  weight_decay: 0.0500 (0.0500)  time: 0.7946  data: 0.0007  max mem: 27222
Epoch: [32]  [2100/2502]  eta: 0:05:30  lr: 0.000273  min_lr: 0.000000  loss: 1.5312 (1.5715)  class_acc: 0.8438 (0.8527)  loss_scale: 32768.0000 (33516.6264)  weight_decay: 0.0500 (0.0500)  time: 0.7970  data: 0.0002  max mem: 27222
Epoch: [32]  [2200/2502]  eta: 0:04:08  lr: 0.000272  min_lr: 0.000000  loss: 1.5596 (1.5719)  class_acc: 0.8438 (0.8526)  loss_scale: 32768.0000 (33482.6134)  weight_decay: 0.0500 (0.0500)  time: 0.7930  data: 0.0004  max mem: 27222
Epoch: [32]  [2300/2502]  eta: 0:02:46  lr: 0.000271  min_lr: 0.000000  loss: 1.6113 (1.5724)  class_acc: 0.8438 (0.8523)  loss_scale: 32768.0000 (33451.5567)  weight_decay: 0.0500 (0.0500)  time: 0.8509  data: 0.0002  max mem: 27222
Epoch: [32]  [2400/2502]  eta: 0:01:23  lr: 0.000270  min_lr: 0.000000  loss: 1.5332 (1.5727)  class_acc: 0.8594 (0.8522)  loss_scale: 32768.0000 (33423.0870)  weight_decay: 0.0500 (0.0500)  time: 0.8042  data: 0.0003  max mem: 27222
Epoch: [32]  [2500/2502]  eta: 0:00:01  lr: 0.000269  min_lr: 0.000000  loss: 1.4619 (1.5721)  class_acc: 0.8750 (0.8522)  loss_scale: 32768.0000 (33554.4320)  weight_decay: 0.0500 (0.0500)  time: 0.7476  data: 0.0013  max mem: 27222
Epoch: [32]  [2501/2502]  eta: 0:00:00  lr: 0.000269  min_lr: 0.000000  loss: 1.4619 (1.5721)  class_acc: 0.8750 (0.8522)  loss_scale: 32768.0000 (33554.4320)  weight_decay: 0.0500 (0.0500)  time: 0.7092  data: 0.0013  max mem: 27222
Epoch: [32] Total time: 0:34:13 (0.8208 s / it)
Averaged stats: lr: 0.000269  min_lr: 0.000000  loss: 1.4619 (1.5747)  class_acc: 0.8750 (0.8515)  loss_scale: 32768.0000 (33554.4320)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Test:  [ 0/98]  eta: 0:09:56  loss: 0.2909 (0.2909)  acc1: 95.3125 (95.3125)  acc5: 100.0000 (100.0000)  time: 6.0907  data: 5.7561  max mem: 27222
Test:  [97/98]  eta: 0:00:00  loss: 0.7526 (0.6946)  acc1: 81.2500 (84.7360)  acc5: 96.8750 (97.2160)  time: 0.3358  data: 0.0002  max mem: 27222
Test: Total time: 0:00:40 (0.4127 s / it)
* Acc@1 84.688 Acc@5 97.174 loss 0.694
Accuracy of the network on the 50000 test images: 84.7%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
Test:  [ 0/98]  eta: 0:08:35  loss: 0.2491 (0.2491)  acc1: 92.1875 (92.1875)  acc5: 100.0000 (100.0000)  time: 5.2650  data: 4.9089  max mem: 27222
Test:  [97/98]  eta: 0:00:00  loss: 0.6635 (0.5798)  acc1: 84.3750 (86.5920)  acc5: 98.4375 (97.8880)  time: 0.3344  data: 0.0015  max mem: 27222
Test: Total time: 0:00:40 (0.4147 s / it)
* Acc@1 86.566 Acc@5 97.898 loss 0.576
EMA Accuracy of the network on the 50000 test images: 86.6%
Max accuracy: 86.64%
{"train_lr": 0.00028235498817483386, "train_min_lr": 3.687752104442794e-07, "train_loss": 1.574734765625, "train_class_acc": 0.85146796875, "train_loss_scale": 33554.432, "train_weight_decay": 0.04999999999999801, "test_loss": 0.5755703883739759, "test_acc1": 86.56600001190185, "test_acc5": 97.89800000488282, "epoch": 32, "n_parameters": 86860264}

