/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
**********find WORLD_SIZE 8 in env**********
**********find WORLD_SIZE 8 in env**********
**********find WORLD_SIZE 8 in env**********
**********find WORLD_SIZE 8 in env**********
**********find WORLD_SIZE 8 in env**********
**********find WORLD_SIZE 8 in env**********
**********find WORLD_SIZE 8 in env**********
**********find WORLD_SIZE 8 in env**********
************World_size is 8, current rank 0 ***********, local rank 0
************World_size is 8, current rank 7 ***********, local rank 7
************World_size is 8, current rank 3 ***********, local rank 3
************World_size is 8, current rank 2 ***********, local rank 2
************World_size is 8, current rank 6 ***********, local rank 6
************World_size is 8, current rank 1 ***********, local rank 1
************World_size is 8, current rank 4 ***********, local rank 4
************World_size is 8, current rank 5 ***********, local rank 5
speec363a0001JZ:77303:77303 [0] NCCL INFO Bootstrap : Using eth0:192.168.0.176<0>
speec363a0001JZ:77303:77303 [0] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v4 symbol.
speec363a0001JZ:77303:77303 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
speec363a0001JZ:77303:77303 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [4]mlx5_4:1/IB [5]mlx5_5:1/IB [6]mlx5_6:1/IB [7]mlx5_7:1/IB ; OOB eth0:192.168.0.176<0>
speec363a0001JZ:77303:77303 [0] NCCL INFO Using network IB
NCCL version 2.10.3+cuda11.3
speec363a0001JZ:77305:77305 [2] NCCL INFO Bootstrap : Using eth0:192.168.0.176<0>
speec363a0001JZ:77306:77306 [3] NCCL INFO Bootstrap : Using eth0:192.168.0.176<0>
speec363a0001JZ:77304:77304 [1] NCCL INFO Bootstrap : Using eth0:192.168.0.176<0>
speec363a0001JZ:77307:77307 [4] NCCL INFO Bootstrap : Using eth0:192.168.0.176<0>
speec363a0001JZ:77308:77308 [5] NCCL INFO Bootstrap : Using eth0:192.168.0.176<0>
speec363a0001JZ:77309:77309 [6] NCCL INFO Bootstrap : Using eth0:192.168.0.176<0>
speec363a0001JZ:77305:77305 [2] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v4 symbol.
speec363a0001JZ:77304:77304 [1] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v4 symbol.
speec363a0001JZ:77306:77306 [3] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v4 symbol.
speec363a0001JZ:77308:77308 [5] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v4 symbol.
speec363a0001JZ:77307:77307 [4] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v4 symbol.
speec363a0001JZ:77309:77309 [6] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v4 symbol.
speec363a0001JZ:77310:77310 [7] NCCL INFO Bootstrap : Using eth0:192.168.0.176<0>
speec363a0001JZ:77304:77304 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
speec363a0001JZ:77305:77305 [2] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
speec363a0001JZ:77306:77306 [3] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
speec363a0001JZ:77310:77310 [7] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v4 symbol.
speec363a0001JZ:77308:77308 [5] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
speec363a0001JZ:77307:77307 [4] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
speec363a0001JZ:77309:77309 [6] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
speec363a0001JZ:77310:77310 [7] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
speec363a0001JZ:77305:77305 [2] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [4]mlx5_4:1/IB [5]mlx5_5:1/IB [6]mlx5_6:1/IB [7]mlx5_7:1/IB ; OOB eth0:192.168.0.176<0>
speec363a0001JZ:77305:77305 [2] NCCL INFO Using network IB
speec363a0001JZ:77306:77306 [3] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [4]mlx5_4:1/IB [5]mlx5_5:1/IB [6]mlx5_6:1/IB [7]mlx5_7:1/IB ; OOB eth0:192.168.0.176<0>
speec363a0001JZ:77306:77306 [3] NCCL INFO Using network IB
speec363a0001JZ:77304:77304 [1] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [4]mlx5_4:1/IB [5]mlx5_5:1/IB [6]mlx5_6:1/IB [7]mlx5_7:1/IB ; OOB eth0:192.168.0.176<0>
speec363a0001JZ:77304:77304 [1] NCCL INFO Using network IB
speec363a0001JZ:77307:77307 [4] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [4]mlx5_4:1/IB [5]mlx5_5:1/IB [6]mlx5_6:1/IB [7]mlx5_7:1/IB ; OOB eth0:192.168.0.176<0>
speec363a0001JZ:77307:77307 [4] NCCL INFO Using network IB
speec363a0001JZ:77309:77309 [6] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [4]mlx5_4:1/IB [5]mlx5_5:1/IB [6]mlx5_6:1/IB [7]mlx5_7:1/IB ; OOB eth0:192.168.0.176<0>
speec363a0001JZ:77309:77309 [6] NCCL INFO Using network IB
speec363a0001JZ:77308:77308 [5] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [4]mlx5_4:1/IB [5]mlx5_5:1/IB [6]mlx5_6:1/IB [7]mlx5_7:1/IB ; OOB eth0:192.168.0.176<0>
speec363a0001JZ:77308:77308 [5] NCCL INFO Using network IB
speec363a0001JZ:77310:77310 [7] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [4]mlx5_4:1/IB [5]mlx5_5:1/IB [6]mlx5_6:1/IB [7]mlx5_7:1/IB ; OOB eth0:192.168.0.176<0>
speec363a0001JZ:77310:77310 [7] NCCL INFO Using network IB
speec363a0001JZ:77309:77623 [6] NCCL INFO NCCL_IB_TIMEOUT set by environment to 60.
speec363a0001JZ:77303:77562 [0] NCCL INFO NCCL_IB_TIMEOUT set by environment to 60.
speec363a0001JZ:77310:77624 [7] NCCL INFO NCCL_IB_TIMEOUT set by environment to 60.
speec363a0001JZ:77305:77618 [2] NCCL INFO NCCL_IB_TIMEOUT set by environment to 60.
speec363a0001JZ:77307:77622 [4] NCCL INFO NCCL_IB_TIMEOUT set by environment to 60.
speec363a0001JZ:77308:77625 [5] NCCL INFO NCCL_IB_TIMEOUT set by environment to 60.
speec363a0001JZ:77304:77621 [1] NCCL INFO NCCL_IB_TIMEOUT set by environment to 60.
speec363a0001JZ:77306:77619 [3] NCCL INFO NCCL_IB_TIMEOUT set by environment to 60.
speec363a0001JZ:77310:77624 [7] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
speec363a0001JZ:77303:77562 [0] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
speec363a0001JZ:77309:77623 [6] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
speec363a0001JZ:77308:77625 [5] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
speec363a0001JZ:77306:77619 [3] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
speec363a0001JZ:77305:77618 [2] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
speec363a0001JZ:77304:77621 [1] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
speec363a0001JZ:77307:77622 [4] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
speec363a0001JZ:77307:77622 [4] NCCL INFO Trees [0] 3/-1/-1->4->5 [1] 3/-1/-1->4->5 [2] 3/-1/-1->4->5 [3] 3/-1/-1->4->5 [4] 3/-1/-1->4->5 [5] 3/-1/-1->4->5 [6] 3/-1/-1->4->5 [7] 3/-1/-1->4->5 [8] 3/-1/-1->4->5 [9] 3/-1/-1->4->5 [10] 3/-1/-1->4->5 [11] 3/-1/-1->4->5 [12] 3/-1/-1->4->5 [13] 3/-1/-1->4->5 [14] 3/-1/-1->4->5 [15] 3/-1/-1->4->5 [16] 3/-1/-1->4->5 [17] 3/-1/-1->4->5 [18] 3/-1/-1->4->5 [19] 3/-1/-1->4->5 [20] 3/-1/-1->4->5 [21] 3/-1/-1->4->5 [22] 3/-1/-1->4->5 [23] 3/-1/-1->4->5
speec363a0001JZ:77306:77619 [3] NCCL INFO Trees [0] 2/-1/-1->3->4 [1] 2/-1/-1->3->4 [2] 2/-1/-1->3->4 [3] 2/-1/-1->3->4 [4] 2/-1/-1->3->4 [5] 2/-1/-1->3->4 [6] 2/-1/-1->3->4 [7] 2/-1/-1->3->4 [8] 2/-1/-1->3->4 [9] 2/-1/-1->3->4 [10] 2/-1/-1->3->4 [11] 2/-1/-1->3->4 [12] 2/-1/-1->3->4 [13] 2/-1/-1->3->4 [14] 2/-1/-1->3->4 [15] 2/-1/-1->3->4 [16] 2/-1/-1->3->4 [17] 2/-1/-1->3->4 [18] 2/-1/-1->3->4 [19] 2/-1/-1->3->4 [20] 2/-1/-1->3->4 [21] 2/-1/-1->3->4 [22] 2/-1/-1->3->4 [23] 2/-1/-1->3->4
speec363a0001JZ:77305:77618 [2] NCCL INFO Trees [0] 1/-1/-1->2->3 [1] 1/-1/-1->2->3 [2] 1/-1/-1->2->3 [3] 1/-1/-1->2->3 [4] 1/-1/-1->2->3 [5] 1/-1/-1->2->3 [6] 1/-1/-1->2->3 [7] 1/-1/-1->2->3 [8] 1/-1/-1->2->3 [9] 1/-1/-1->2->3 [10] 1/-1/-1->2->3 [11] 1/-1/-1->2->3 [12] 1/-1/-1->2->3 [13] 1/-1/-1->2->3 [14] 1/-1/-1->2->3 [15] 1/-1/-1->2->3 [16] 1/-1/-1->2->3 [17] 1/-1/-1->2->3 [18] 1/-1/-1->2->3 [19] 1/-1/-1->2->3 [20] 1/-1/-1->2->3 [21] 1/-1/-1->2->3 [22] 1/-1/-1->2->3 [23] 1/-1/-1->2->3
speec363a0001JZ:77304:77621 [1] NCCL INFO Trees [0] 0/-1/-1->1->2 [1] 0/-1/-1->1->2 [2] 0/-1/-1->1->2 [3] 0/-1/-1->1->2 [4] 0/-1/-1->1->2 [5] 0/-1/-1->1->2 [6] 0/-1/-1->1->2 [7] 0/-1/-1->1->2 [8] 0/-1/-1->1->2 [9] 0/-1/-1->1->2 [10] 0/-1/-1->1->2 [11] 0/-1/-1->1->2 [12] 0/-1/-1->1->2 [13] 0/-1/-1->1->2 [14] 0/-1/-1->1->2 [15] 0/-1/-1->1->2 [16] 0/-1/-1->1->2 [17] 0/-1/-1->1->2 [18] 0/-1/-1->1->2 [19] 0/-1/-1->1->2 [20] 0/-1/-1->1->2 [21] 0/-1/-1->1->2 [22] 0/-1/-1->1->2 [23] 0/-1/-1->1->2
speec363a0001JZ:77305:77618 [2] NCCL INFO Setting affinity for GPU 5 to ffff,0000ffff
speec363a0001JZ:77308:77625 [5] NCCL INFO Trees [0] 4/-1/-1->5->6 [1] 4/-1/-1->5->6 [2] 4/-1/-1->5->6 [3] 4/-1/-1->5->6 [4] 4/-1/-1->5->6 [5] 4/-1/-1->5->6 [6] 4/-1/-1->5->6 [7] 4/-1/-1->5->6 [8] 4/-1/-1->5->6 [9] 4/-1/-1->5->6 [10] 4/-1/-1->5->6 [11] 4/-1/-1->5->6 [12] 4/-1/-1->5->6 [13] 4/-1/-1->5->6 [14] 4/-1/-1->5->6 [15] 4/-1/-1->5->6 [16] 4/-1/-1->5->6 [17] 4/-1/-1->5->6 [18] 4/-1/-1->5->6 [19] 4/-1/-1->5->6 [20] 4/-1/-1->5->6 [21] 4/-1/-1->5->6 [22] 4/-1/-1->5->6 [23] 4/-1/-1->5->6
speec363a0001JZ:77306:77619 [3] NCCL INFO Setting affinity for GPU 4 to ffff,0000ffff
speec363a0001JZ:77309:77623 [6] NCCL INFO Trees [0] 5/-1/-1->6->7 [1] 5/-1/-1->6->7 [2] 5/-1/-1->6->7 [3] 5/-1/-1->6->7 [4] 5/-1/-1->6->7 [5] 5/-1/-1->6->7 [6] 5/-1/-1->6->7 [7] 5/-1/-1->6->7 [8] 5/-1/-1->6->7 [9] 5/-1/-1->6->7 [10] 5/-1/-1->6->7 [11] 5/-1/-1->6->7 [12] 5/-1/-1->6->7 [13] 5/-1/-1->6->7 [14] 5/-1/-1->6->7 [15] 5/-1/-1->6->7 [16] 5/-1/-1->6->7 [17] 5/-1/-1->6->7 [18] 5/-1/-1->6->7 [19] 5/-1/-1->6->7 [20] 5/-1/-1->6->7 [21] 5/-1/-1->6->7 [22] 5/-1/-1->6->7 [23] 5/-1/-1->6->7
speec363a0001JZ:77304:77621 [1] NCCL INFO Setting affinity for GPU 6 to ffff,0000ffff
speec363a0001JZ:77308:77625 [5] NCCL INFO Setting affinity for GPU 2 to ffff,0000ffff
speec363a0001JZ:77309:77623 [6] NCCL INFO Setting affinity for GPU 1 to ffff,0000ffff
speec363a0001JZ:77307:77622 [4] NCCL INFO Setting affinity for GPU 3 to ffff,0000ffff
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 00/24 :    0   7   6   5   4   3   2   1
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 01/24 :    0   7   6   5   4   3   2   1
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 02/24 :    0   7   6   5   4   3   2   1
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 03/24 :    0   7   6   5   4   3   2   1
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 04/24 :    0   7   6   5   4   3   2   1
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 05/24 :    0   7   6   5   4   3   2   1
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 06/24 :    0   7   6   5   4   3   2   1
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 07/24 :    0   7   6   5   4   3   2   1
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 08/24 :    0   7   6   5   4   3   2   1
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 09/24 :    0   7   6   5   4   3   2   1
speec363a0001JZ:77310:77624 [7] NCCL INFO Trees [0] 6/-1/-1->7->-1 [1] 6/-1/-1->7->-1 [2] 6/-1/-1->7->-1 [3] 6/-1/-1->7->-1 [4] 6/-1/-1->7->-1 [5] 6/-1/-1->7->-1 [6] 6/-1/-1->7->-1 [7] 6/-1/-1->7->-1 [8] 6/-1/-1->7->-1 [9] 6/-1/-1->7->-1 [10] 6/-1/-1->7->-1 [11] 6/-1/-1->7->-1 [12] 6/-1/-1->7->-1 [13] 6/-1/-1->7->-1 [14] 6/-1/-1->7->-1 [15] 6/-1/-1->7->-1 [16] 6/-1/-1->7->-1 [17] 6/-1/-1->7->-1 [18] 6/-1/-1->7->-1 [19] 6/-1/-1->7->-1 [20] 6/-1/-1->7->-1 [21] 6/-1/-1->7->-1 [22] 6/-1/-1->7->-1 [23] 6/-1/-1->7->-1
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 10/24 :    0   7   6   5   4   3   2   1
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 11/24 :    0   7   6   5   4   3   2   1
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 12/24 :    0   7   6   5   4   3   2   1
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 13/24 :    0   7   6   5   4   3   2   1
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 14/24 :    0   7   6   5   4   3   2   1
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 15/24 :    0   7   6   5   4   3   2   1
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 16/24 :    0   7   6   5   4   3   2   1
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 17/24 :    0   7   6   5   4   3   2   1
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 18/24 :    0   7   6   5   4   3   2   1
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 19/24 :    0   7   6   5   4   3   2   1
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 20/24 :    0   7   6   5   4   3   2   1
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 21/24 :    0   7   6   5   4   3   2   1
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 22/24 :    0   7   6   5   4   3   2   1
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 23/24 :    0   7   6   5   4   3   2   1
speec363a0001JZ:77303:77562 [0] NCCL INFO Trees [0] -1/-1/-1->0->1 [1] -1/-1/-1->0->1 [2] -1/-1/-1->0->1 [3] -1/-1/-1->0->1 [4] -1/-1/-1->0->1 [5] -1/-1/-1->0->1 [6] -1/-1/-1->0->1 [7] -1/-1/-1->0->1 [8] -1/-1/-1->0->1 [9] -1/-1/-1->0->1 [10] -1/-1/-1->0->1 [11] -1/-1/-1->0->1 [12] -1/-1/-1->0->1 [13] -1/-1/-1->0->1 [14] -1/-1/-1->0->1 [15] -1/-1/-1->0->1 [16] -1/-1/-1->0->1 [17] -1/-1/-1->0->1 [18] -1/-1/-1->0->1 [19] -1/-1/-1->0->1 [20] -1/-1/-1->0->1 [21] -1/-1/-1->0->1 [22] -1/-1/-1->0->1 [23] -1/-1/-1->0->1
speec363a0001JZ:77303:77562 [0] NCCL INFO Setting affinity for GPU 7 to ffff,0000ffff
speec363a0001JZ:77310:77624 [7] NCCL INFO Setting affinity for GPU 0 to ffff,0000ffff
speec363a0001JZ:77306:77619 [3] NCCL INFO Channel 00 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77304:77621 [1] NCCL INFO Channel 00 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 00 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77306:77619 [3] NCCL INFO Channel 01 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77304:77621 [1] NCCL INFO Channel 01 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 01 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77310:77624 [7] NCCL INFO Channel 00 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77306:77619 [3] NCCL INFO Channel 02 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77308:77625 [5] NCCL INFO Channel 00 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77309:77623 [6] NCCL INFO Channel 00 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77304:77621 [1] NCCL INFO Channel 02 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JZ:77305:77618 [2] NCCL INFO Channel 00 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 02 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77307:77622 [4] NCCL INFO Channel 00 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77310:77624 [7] NCCL INFO Channel 01 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77306:77619 [3] NCCL INFO Channel 03 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77308:77625 [5] NCCL INFO Channel 01 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77304:77621 [1] NCCL INFO Channel 03 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JZ:77309:77623 [6] NCCL INFO Channel 01 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77305:77618 [2] NCCL INFO Channel 01 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 03 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77307:77622 [4] NCCL INFO Channel 01 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77310:77624 [7] NCCL INFO Channel 02 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77306:77619 [3] NCCL INFO Channel 04 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77308:77625 [5] NCCL INFO Channel 02 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77304:77621 [1] NCCL INFO Channel 04 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JZ:77309:77623 [6] NCCL INFO Channel 02 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 04 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77305:77618 [2] NCCL INFO Channel 02 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77310:77624 [7] NCCL INFO Channel 03 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77307:77622 [4] NCCL INFO Channel 02 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77306:77619 [3] NCCL INFO Channel 05 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77308:77625 [5] NCCL INFO Channel 03 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77304:77621 [1] NCCL INFO Channel 05 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JZ:77309:77623 [6] NCCL INFO Channel 03 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 05 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77305:77618 [2] NCCL INFO Channel 03 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77310:77624 [7] NCCL INFO Channel 04 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77307:77622 [4] NCCL INFO Channel 03 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77306:77619 [3] NCCL INFO Channel 06 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77308:77625 [5] NCCL INFO Channel 04 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77304:77621 [1] NCCL INFO Channel 06 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JZ:77309:77623 [6] NCCL INFO Channel 04 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 06 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77305:77618 [2] NCCL INFO Channel 04 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77307:77622 [4] NCCL INFO Channel 04 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77310:77624 [7] NCCL INFO Channel 05 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77306:77619 [3] NCCL INFO Channel 07 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77308:77625 [5] NCCL INFO Channel 05 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77309:77623 [6] NCCL INFO Channel 05 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77304:77621 [1] NCCL INFO Channel 07 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 07 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77305:77618 [2] NCCL INFO Channel 05 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77307:77622 [4] NCCL INFO Channel 05 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77310:77624 [7] NCCL INFO Channel 06 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77306:77619 [3] NCCL INFO Channel 08 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77308:77625 [5] NCCL INFO Channel 06 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77309:77623 [6] NCCL INFO Channel 06 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77304:77621 [1] NCCL INFO Channel 08 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JZ:77305:77618 [2] NCCL INFO Channel 06 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 08 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77307:77622 [4] NCCL INFO Channel 06 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77310:77624 [7] NCCL INFO Channel 07 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77306:77619 [3] NCCL INFO Channel 09 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77308:77625 [5] NCCL INFO Channel 07 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77304:77621 [1] NCCL INFO Channel 09 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JZ:77309:77623 [6] NCCL INFO Channel 07 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 09 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77305:77618 [2] NCCL INFO Channel 07 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77310:77624 [7] NCCL INFO Channel 08 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77307:77622 [4] NCCL INFO Channel 07 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77306:77619 [3] NCCL INFO Channel 10 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77308:77625 [5] NCCL INFO Channel 08 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77304:77621 [1] NCCL INFO Channel 10 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JZ:77309:77623 [6] NCCL INFO Channel 08 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 10 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77305:77618 [2] NCCL INFO Channel 08 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77310:77624 [7] NCCL INFO Channel 09 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77307:77622 [4] NCCL INFO Channel 08 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77306:77619 [3] NCCL INFO Channel 11 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77308:77625 [5] NCCL INFO Channel 09 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77304:77621 [1] NCCL INFO Channel 11 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JZ:77309:77623 [6] NCCL INFO Channel 09 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 11 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77305:77618 [2] NCCL INFO Channel 09 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77310:77624 [7] NCCL INFO Channel 10 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77307:77622 [4] NCCL INFO Channel 09 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77306:77619 [3] NCCL INFO Channel 12 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77308:77625 [5] NCCL INFO Channel 10 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77309:77623 [6] NCCL INFO Channel 10 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77304:77621 [1] NCCL INFO Channel 12 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 12 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77305:77618 [2] NCCL INFO Channel 10 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77310:77624 [7] NCCL INFO Channel 11 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77307:77622 [4] NCCL INFO Channel 10 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77306:77619 [3] NCCL INFO Channel 13 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77308:77625 [5] NCCL INFO Channel 11 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77309:77623 [6] NCCL INFO Channel 11 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77304:77621 [1] NCCL INFO Channel 13 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 13 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77305:77618 [2] NCCL INFO Channel 11 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77310:77624 [7] NCCL INFO Channel 12 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77307:77622 [4] NCCL INFO Channel 11 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77306:77619 [3] NCCL INFO Channel 14 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77308:77625 [5] NCCL INFO Channel 12 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77304:77621 [1] NCCL INFO Channel 14 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JZ:77309:77623 [6] NCCL INFO Channel 12 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 14 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77305:77618 [2] NCCL INFO Channel 12 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77310:77624 [7] NCCL INFO Channel 13 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77307:77622 [4] NCCL INFO Channel 12 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77306:77619 [3] NCCL INFO Channel 15 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77308:77625 [5] NCCL INFO Channel 13 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77304:77621 [1] NCCL INFO Channel 15 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JZ:77309:77623 [6] NCCL INFO Channel 13 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 15 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77305:77618 [2] NCCL INFO Channel 13 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77310:77624 [7] NCCL INFO Channel 14 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77307:77622 [4] NCCL INFO Channel 13 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77306:77619 [3] NCCL INFO Channel 16 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77308:77625 [5] NCCL INFO Channel 14 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77304:77621 [1] NCCL INFO Channel 16 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JZ:77309:77623 [6] NCCL INFO Channel 14 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77305:77618 [2] NCCL INFO Channel 14 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 16 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77310:77624 [7] NCCL INFO Channel 15 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77307:77622 [4] NCCL INFO Channel 14 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77306:77619 [3] NCCL INFO Channel 17 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77308:77625 [5] NCCL INFO Channel 15 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77304:77621 [1] NCCL INFO Channel 17 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JZ:77309:77623 [6] NCCL INFO Channel 15 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77305:77618 [2] NCCL INFO Channel 15 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 17 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77310:77624 [7] NCCL INFO Channel 16 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77307:77622 [4] NCCL INFO Channel 15 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77306:77619 [3] NCCL INFO Channel 18 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77308:77625 [5] NCCL INFO Channel 16 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77304:77621 [1] NCCL INFO Channel 18 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JZ:77309:77623 [6] NCCL INFO Channel 16 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77305:77618 [2] NCCL INFO Channel 16 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 18 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77307:77622 [4] NCCL INFO Channel 16 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77310:77624 [7] NCCL INFO Channel 17 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77306:77619 [3] NCCL INFO Channel 19 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77308:77625 [5] NCCL INFO Channel 17 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77304:77621 [1] NCCL INFO Channel 19 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JZ:77309:77623 [6] NCCL INFO Channel 17 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 19 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77305:77618 [2] NCCL INFO Channel 17 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77310:77624 [7] NCCL INFO Channel 18 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77307:77622 [4] NCCL INFO Channel 17 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77306:77619 [3] NCCL INFO Channel 20 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77308:77625 [5] NCCL INFO Channel 18 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77304:77621 [1] NCCL INFO Channel 20 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JZ:77309:77623 [6] NCCL INFO Channel 18 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77305:77618 [2] NCCL INFO Channel 18 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 20 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77310:77624 [7] NCCL INFO Channel 19 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77307:77622 [4] NCCL INFO Channel 18 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77306:77619 [3] NCCL INFO Channel 21 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77308:77625 [5] NCCL INFO Channel 19 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77304:77621 [1] NCCL INFO Channel 21 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JZ:77309:77623 [6] NCCL INFO Channel 19 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77305:77618 [2] NCCL INFO Channel 19 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 21 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77310:77624 [7] NCCL INFO Channel 20 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77307:77622 [4] NCCL INFO Channel 19 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77308:77625 [5] NCCL INFO Channel 20 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77306:77619 [3] NCCL INFO Channel 22 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77309:77623 [6] NCCL INFO Channel 20 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77304:77621 [1] NCCL INFO Channel 22 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JZ:77305:77618 [2] NCCL INFO Channel 20 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 22 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77310:77624 [7] NCCL INFO Channel 21 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77307:77622 [4] NCCL INFO Channel 20 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77308:77625 [5] NCCL INFO Channel 21 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77306:77619 [3] NCCL INFO Channel 23 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77309:77623 [6] NCCL INFO Channel 21 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77304:77621 [1] NCCL INFO Channel 23 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JZ:77305:77618 [2] NCCL INFO Channel 21 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 23 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77307:77622 [4] NCCL INFO Channel 21 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77310:77624 [7] NCCL INFO Channel 22 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77308:77625 [5] NCCL INFO Channel 22 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77309:77623 [6] NCCL INFO Channel 22 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77305:77618 [2] NCCL INFO Channel 22 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77310:77624 [7] NCCL INFO Channel 23 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77307:77622 [4] NCCL INFO Channel 22 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77308:77625 [5] NCCL INFO Channel 23 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77309:77623 [6] NCCL INFO Channel 23 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77305:77618 [2] NCCL INFO Channel 23 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77307:77622 [4] NCCL INFO Channel 23 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77303:77562 [0] NCCL INFO Connected all rings
speec363a0001JZ:77310:77624 [7] NCCL INFO Connected all rings
speec363a0001JZ:77309:77623 [6] NCCL INFO Connected all rings
speec363a0001JZ:77305:77618 [2] NCCL INFO Connected all rings
speec363a0001JZ:77304:77621 [1] NCCL INFO Connected all rings
speec363a0001JZ:77306:77619 [3] NCCL INFO Connected all rings
speec363a0001JZ:77307:77622 [4] NCCL INFO Connected all rings
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 00 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77308:77625 [5] NCCL INFO Connected all rings
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 01 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 02 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 03 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 04 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 05 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 06 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 07 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 08 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 09 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 10 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 11 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 12 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 13 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 14 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 15 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 16 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 17 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 18 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 19 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 20 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 21 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 22 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77303:77562 [0] NCCL INFO Channel 23 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77309:77623 [6] NCCL INFO Channel 00 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77305:77618 [2] NCCL INFO Channel 00 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77304:77621 [1] NCCL INFO Channel 00 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77306:77619 [3] NCCL INFO Channel 00 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77307:77622 [4] NCCL INFO Channel 00 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77308:77625 [5] NCCL INFO Channel 00 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77305:77618 [2] NCCL INFO Channel 01 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77309:77623 [6] NCCL INFO Channel 01 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77304:77621 [1] NCCL INFO Channel 01 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77306:77619 [3] NCCL INFO Channel 01 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77307:77622 [4] NCCL INFO Channel 01 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77308:77625 [5] NCCL INFO Channel 01 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77305:77618 [2] NCCL INFO Channel 02 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77309:77623 [6] NCCL INFO Channel 02 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77304:77621 [1] NCCL INFO Channel 02 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77306:77619 [3] NCCL INFO Channel 02 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77307:77622 [4] NCCL INFO Channel 02 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77308:77625 [5] NCCL INFO Channel 02 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77305:77618 [2] NCCL INFO Channel 03 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77309:77623 [6] NCCL INFO Channel 03 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77304:77621 [1] NCCL INFO Channel 03 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77306:77619 [3] NCCL INFO Channel 03 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77307:77622 [4] NCCL INFO Channel 03 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77308:77625 [5] NCCL INFO Channel 03 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77305:77618 [2] NCCL INFO Channel 04 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77309:77623 [6] NCCL INFO Channel 04 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77304:77621 [1] NCCL INFO Channel 04 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77306:77619 [3] NCCL INFO Channel 04 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77307:77622 [4] NCCL INFO Channel 04 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77308:77625 [5] NCCL INFO Channel 04 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77305:77618 [2] NCCL INFO Channel 05 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77309:77623 [6] NCCL INFO Channel 05 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77304:77621 [1] NCCL INFO Channel 05 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77306:77619 [3] NCCL INFO Channel 05 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77307:77622 [4] NCCL INFO Channel 05 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77308:77625 [5] NCCL INFO Channel 05 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77305:77618 [2] NCCL INFO Channel 06 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77309:77623 [6] NCCL INFO Channel 06 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77304:77621 [1] NCCL INFO Channel 06 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77306:77619 [3] NCCL INFO Channel 06 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77307:77622 [4] NCCL INFO Channel 06 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77308:77625 [5] NCCL INFO Channel 06 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77305:77618 [2] NCCL INFO Channel 07 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77309:77623 [6] NCCL INFO Channel 07 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77304:77621 [1] NCCL INFO Channel 07 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77306:77619 [3] NCCL INFO Channel 07 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77307:77622 [4] NCCL INFO Channel 07 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77308:77625 [5] NCCL INFO Channel 07 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77305:77618 [2] NCCL INFO Channel 08 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77309:77623 [6] NCCL INFO Channel 08 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77304:77621 [1] NCCL INFO Channel 08 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77306:77619 [3] NCCL INFO Channel 08 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77307:77622 [4] NCCL INFO Channel 08 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77308:77625 [5] NCCL INFO Channel 08 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77305:77618 [2] NCCL INFO Channel 09 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77309:77623 [6] NCCL INFO Channel 09 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77304:77621 [1] NCCL INFO Channel 09 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77306:77619 [3] NCCL INFO Channel 09 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77307:77622 [4] NCCL INFO Channel 09 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77308:77625 [5] NCCL INFO Channel 09 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77305:77618 [2] NCCL INFO Channel 10 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77309:77623 [6] NCCL INFO Channel 10 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77304:77621 [1] NCCL INFO Channel 10 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77306:77619 [3] NCCL INFO Channel 10 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77307:77622 [4] NCCL INFO Channel 10 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77308:77625 [5] NCCL INFO Channel 10 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77305:77618 [2] NCCL INFO Channel 11 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77309:77623 [6] NCCL INFO Channel 11 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77304:77621 [1] NCCL INFO Channel 11 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77306:77619 [3] NCCL INFO Channel 11 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77307:77622 [4] NCCL INFO Channel 11 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77308:77625 [5] NCCL INFO Channel 11 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77305:77618 [2] NCCL INFO Channel 12 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77309:77623 [6] NCCL INFO Channel 12 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77304:77621 [1] NCCL INFO Channel 12 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77306:77619 [3] NCCL INFO Channel 12 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77307:77622 [4] NCCL INFO Channel 12 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77308:77625 [5] NCCL INFO Channel 12 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77305:77618 [2] NCCL INFO Channel 13 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77309:77623 [6] NCCL INFO Channel 13 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77304:77621 [1] NCCL INFO Channel 13 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77306:77619 [3] NCCL INFO Channel 13 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77307:77622 [4] NCCL INFO Channel 13 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77308:77625 [5] NCCL INFO Channel 13 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77305:77618 [2] NCCL INFO Channel 14 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77309:77623 [6] NCCL INFO Channel 14 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77304:77621 [1] NCCL INFO Channel 14 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77306:77619 [3] NCCL INFO Channel 14 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77307:77622 [4] NCCL INFO Channel 14 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77308:77625 [5] NCCL INFO Channel 14 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77305:77618 [2] NCCL INFO Channel 15 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77309:77623 [6] NCCL INFO Channel 15 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77304:77621 [1] NCCL INFO Channel 15 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77306:77619 [3] NCCL INFO Channel 15 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77307:77622 [4] NCCL INFO Channel 15 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77308:77625 [5] NCCL INFO Channel 15 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77305:77618 [2] NCCL INFO Channel 16 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77309:77623 [6] NCCL INFO Channel 16 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77304:77621 [1] NCCL INFO Channel 16 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77306:77619 [3] NCCL INFO Channel 16 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77307:77622 [4] NCCL INFO Channel 16 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77308:77625 [5] NCCL INFO Channel 16 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77305:77618 [2] NCCL INFO Channel 17 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77309:77623 [6] NCCL INFO Channel 17 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77304:77621 [1] NCCL INFO Channel 17 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77306:77619 [3] NCCL INFO Channel 17 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77307:77622 [4] NCCL INFO Channel 17 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77308:77625 [5] NCCL INFO Channel 17 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77305:77618 [2] NCCL INFO Channel 18 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77309:77623 [6] NCCL INFO Channel 18 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77304:77621 [1] NCCL INFO Channel 18 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77306:77619 [3] NCCL INFO Channel 18 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77307:77622 [4] NCCL INFO Channel 18 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77308:77625 [5] NCCL INFO Channel 18 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77305:77618 [2] NCCL INFO Channel 19 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77309:77623 [6] NCCL INFO Channel 19 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77304:77621 [1] NCCL INFO Channel 19 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77307:77622 [4] NCCL INFO Channel 19 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77306:77619 [3] NCCL INFO Channel 19 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77308:77625 [5] NCCL INFO Channel 19 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77305:77618 [2] NCCL INFO Channel 20 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77309:77623 [6] NCCL INFO Channel 20 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77304:77621 [1] NCCL INFO Channel 20 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77306:77619 [3] NCCL INFO Channel 20 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77307:77622 [4] NCCL INFO Channel 20 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77308:77625 [5] NCCL INFO Channel 20 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77305:77618 [2] NCCL INFO Channel 21 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77309:77623 [6] NCCL INFO Channel 21 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77304:77621 [1] NCCL INFO Channel 21 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77306:77619 [3] NCCL INFO Channel 21 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77307:77622 [4] NCCL INFO Channel 21 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77308:77625 [5] NCCL INFO Channel 21 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77305:77618 [2] NCCL INFO Channel 22 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77309:77623 [6] NCCL INFO Channel 22 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77304:77621 [1] NCCL INFO Channel 22 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77306:77619 [3] NCCL INFO Channel 22 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77307:77622 [4] NCCL INFO Channel 22 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77308:77625 [5] NCCL INFO Channel 22 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77305:77618 [2] NCCL INFO Channel 23 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77309:77623 [6] NCCL INFO Channel 23 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77304:77621 [1] NCCL INFO Channel 23 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77306:77619 [3] NCCL INFO Channel 23 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77307:77622 [4] NCCL INFO Channel 23 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77308:77625 [5] NCCL INFO Channel 23 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77310:77624 [7] NCCL INFO Connected all trees
speec363a0001JZ:77310:77624 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
speec363a0001JZ:77303:77562 [0] NCCL INFO Connected all trees
speec363a0001JZ:77303:77562 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
speec363a0001JZ:77303:77562 [0] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
speec363a0001JZ:77310:77624 [7] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
speec363a0001JZ:77305:77618 [2] NCCL INFO Connected all trees
speec363a0001JZ:77305:77618 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
speec363a0001JZ:77304:77621 [1] NCCL INFO Connected all trees
speec363a0001JZ:77304:77621 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
speec363a0001JZ:77306:77619 [3] NCCL INFO Connected all trees
speec363a0001JZ:77306:77619 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
speec363a0001JZ:77307:77622 [4] NCCL INFO Connected all trees
speec363a0001JZ:77307:77622 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
speec363a0001JZ:77308:77625 [5] NCCL INFO Connected all trees
speec363a0001JZ:77308:77625 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
speec363a0001JZ:77309:77623 [6] NCCL INFO Connected all trees
speec363a0001JZ:77309:77623 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
speec363a0001JZ:77304:77621 [1] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
speec363a0001JZ:77305:77618 [2] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
speec363a0001JZ:77306:77619 [3] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
speec363a0001JZ:77307:77622 [4] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
speec363a0001JZ:77308:77625 [5] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
speec363a0001JZ:77309:77623 [6] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
speec363a0001JZ:77309:77623 [6] NCCL INFO comm 0x7f340c002fb0 rank 6 nranks 8 cudaDev 6 busId 200000 - Init COMPLETE
speec363a0001JZ:77304:77621 [1] NCCL INFO comm 0x7fa050002fb0 rank 1 nranks 8 cudaDev 1 busId d00000 - Init COMPLETE
speec363a0001JZ:77303:77562 [0] NCCL INFO comm 0x7f22d4002fb0 rank 0 nranks 8 cudaDev 0 busId e00000 - Init COMPLETE
speec363a0001JZ:77310:77624 [7] NCCL INFO comm 0x7efbd0002fb0 rank 7 nranks 8 cudaDev 7 busId 100000 - Init COMPLETE
speec363a0001JZ:77303:77303 [0] NCCL INFO Launch mode Parallel
speec363a0001JZ:77308:77625 [5] NCCL INFO comm 0x7f9050002fb0 rank 5 nranks 8 cudaDev 5 busId 300000 - Init COMPLETE
speec363a0001JZ:77305:77618 [2] NCCL INFO comm 0x7fc91c002fb0 rank 2 nranks 8 cudaDev 2 busId c00000 - Init COMPLETE
speec363a0001JZ:77307:77622 [4] NCCL INFO comm 0x7fd7bc002fb0 rank 4 nranks 8 cudaDev 4 busId 400000 - Init COMPLETE
speec363a0001JZ:77306:77619 [3] NCCL INFO comm 0x7f88c0002fb0 rank 3 nranks 8 cudaDev 3 busId b00000 - Init COMPLETE
Namespace(aa='rand-m9-mstd0.5-inc1', abs_pos_emb=True, attn_drop_rate=0.0, auto_resume=True, backbone_decay=1.0, batch_size=256, clip_grad=None, clip_mean_and_std=True, color_jitter=0.4, crop_pct=None, cutmix=0.0, cutmix_minmax=None, data='imagenet', data_path='/tmp/DATASET/imagenet', data_set='IMNET', deepscale=False, deepscale_config=None, deepspeed=False, deepspeed_config=None, deepspeed_mpi=False, device='cuda:0', disable_eval_during_finetuning=False, disable_weight_decay_on_rel_pos_bias=False, dist_eval=True, dist_on_itp=False, dist_url='env://', distributed=True, drop=0.0, drop_path=0.0, enable_deepspeed=True, epochs=50, eval=False, eval_all=True, eval_data_path=None, finetune='OUTPUT/SLIP/declip_model/clip_vitb16.pth', imagenet_default_mean_and_std=False, init_scale=0.001, input_size=224, layer_decay=0.6, layer_scale_init_value=0.0, local_rank=0, log_dir=None, lr=0.0006, min_lr=1e-06, mixup=0.0, mixup_mode='batch', mixup_prob=1.0, mixup_switch_prob=0.5, model='declip_B16', model_ema=True, model_ema_decay=0.9998, model_ema_force_cpu=False, model_key='state', model_prefix='visual.', momentum=0.9, nb_classes=1000, num_gpu=1, num_workers=8, opt='adamw', opt_betas=None, opt_eps=1e-08, output_dir='OUTPUT/SLIP/maskclip_ft/CLIP_openai_P16/FT50_60E5_D06_EMA98_DPR00', pin_mem=True, rank=0, recount=1, rel_pos_bias=False, remode='pixel', reprob=0.25, resplit=False, resume='', save_ckpt=True, save_ckpt_freq=5, seed=0, smoothing=0.1, start_epoch=0, train_interpolation='bicubic', train_set='train', update_freq=1, use_mean_pooling=True, warmup_epochs=10, warmup_lr=1e-06, warmup_steps=-1, weight_decay=0.05, weight_decay_end=None, world_size=8)
Transform = 
RandomResizedCropAndInterpolation(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=PIL.Image.BICUBIC)
RandomHorizontalFlip(p=0.5)
<timm.data.auto_augment.RandAugment object at 0x7f2439c0fd90>
ToTensor()
Normalize(mean=tensor([0.4815, 0.4578, 0.4082]), std=tensor([0.2686, 0.2613, 0.2758]))
<timm.data.random_erasing.RandomErasing object at 0x7f2439c24130>
---------------------------
FAILED TO LOAD JPEG, TRY PIL
USE PIL DATALOADER
Number of the class = 1000
Set crop pct to 1 for clip task
Transform = 
Resize(size=224, interpolation=bicubic, max_size=None, antialias=None)
CenterCrop(size=(224, 224))
ToTensor()
Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------------------------
USE PIL DATALOADER
Number of the class = 1000
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f2439c0f610>
No Layer Scale
{'num_classes': 1000, 'drop_rate': 0.0, 'drop_path_rate': 0.0, 'attn_drop_rate': 0.0, 'use_mean_pooling': True, 'init_scale': 0.001, 'use_rel_pos_bias': False, 'use_abs_pos_emb': True}
drop_path_rate: 0.0
layer_scale: False
Using DPR [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Patch size = (16, 16)
Load ckpt from OUTPUT/SLIP/declip_model/clip_vitb16.pth
Weights of VisualTransformer not initialized from pretrained model: ['visual.patch_embed.proj.weight', 'visual.patch_embed.proj.bias', 'visual.fc_norm.weight', 'visual.fc_norm.bias', 'visual.head.weight', 'visual.head.bias']
Weights from pretrained model not used in VisualTransformer: ['visual.proj', 'visual.ln_post.weight', 'visual.ln_post.bias']
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Using EMA with decay = 0.99980000
Model = VisualTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
  )
  (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
  (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (transformer): Transformer(
    (dropout): Dropout(p=0, inplace=False)
    (resblocks): Sequential(
      (0): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (2): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (3): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (4): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (5): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (6): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (7): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (8): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (9): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (10): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (11): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
    )
  )
  (fc_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
number of params: 85978600
LR = 0.00060000
Batch size = 2048
Update frequent = 1
Number of training examples = 1281167
Number of training training per epoch = 625
Assigned values = [0.0013060694015999993, 0.002176782335999999, 0.0036279705599999985, 0.006046617599999997, 0.010077695999999997, 0.016796159999999994, 0.027993599999999993, 0.04665599999999999, 0.07775999999999998, 0.1296, 0.21599999999999997, 0.36, 0.6, 1.0]
Param groups = {
  "layer_0_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "class_embedding",
      "positional_embedding",
      "ln_pre.weight",
      "ln_pre.bias"
    ],
    "lr_scale": 0.0013060694015999993
  },
  "layer_1_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.0.attn.in_proj_weight",
      "transformer.resblocks.0.attn.out_proj.weight",
      "transformer.resblocks.0.mlp.c_fc.weight",
      "transformer.resblocks.0.mlp.c_proj.weight"
    ],
    "lr_scale": 0.002176782335999999
  },
  "layer_1_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.0.attn.in_proj_bias",
      "transformer.resblocks.0.attn.out_proj.bias",
      "transformer.resblocks.0.ln_1.weight",
      "transformer.resblocks.0.ln_1.bias",
      "transformer.resblocks.0.mlp.c_fc.bias",
      "transformer.resblocks.0.mlp.c_proj.bias",
      "transformer.resblocks.0.ln_2.weight",
      "transformer.resblocks.0.ln_2.bias"
    ],
    "lr_scale": 0.002176782335999999
  },
  "layer_2_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.1.attn.in_proj_weight",
      "transformer.resblocks.1.attn.out_proj.weight",
      "transformer.resblocks.1.mlp.c_fc.weight",
      "transformer.resblocks.1.mlp.c_proj.weight"
    ],
    "lr_scale": 0.0036279705599999985
  },
  "layer_2_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.1.attn.in_proj_bias",
      "transformer.resblocks.1.attn.out_proj.bias",
      "transformer.resblocks.1.ln_1.weight",
      "transformer.resblocks.1.ln_1.bias",
      "transformer.resblocks.1.mlp.c_fc.bias",
      "transformer.resblocks.1.mlp.c_proj.bias",
      "transformer.resblocks.1.ln_2.weight",
      "transformer.resblocks.1.ln_2.bias"
    ],
    "lr_scale": 0.0036279705599999985
  },
  "layer_3_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.2.attn.in_proj_weight",
      "transformer.resblocks.2.attn.out_proj.weight",
      "transformer.resblocks.2.mlp.c_fc.weight",
      "transformer.resblocks.2.mlp.c_proj.weight"
    ],
    "lr_scale": 0.006046617599999997
  },
  "layer_3_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.2.attn.in_proj_bias",
      "transformer.resblocks.2.attn.out_proj.bias",
      "transformer.resblocks.2.ln_1.weight",
      "transformer.resblocks.2.ln_1.bias",
      "transformer.resblocks.2.mlp.c_fc.bias",
      "transformer.resblocks.2.mlp.c_proj.bias",
      "transformer.resblocks.2.ln_2.weight",
      "transformer.resblocks.2.ln_2.bias"
    ],
    "lr_scale": 0.006046617599999997
  },
  "layer_4_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.3.attn.in_proj_weight",
      "transformer.resblocks.3.attn.out_proj.weight",
      "transformer.resblocks.3.mlp.c_fc.weight",
      "transformer.resblocks.3.mlp.c_proj.weight"
    ],
    "lr_scale": 0.010077695999999997
  },
  "layer_4_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.3.attn.in_proj_bias",
      "transformer.resblocks.3.attn.out_proj.bias",
      "transformer.resblocks.3.ln_1.weight",
      "transformer.resblocks.3.ln_1.bias",
      "transformer.resblocks.3.mlp.c_fc.bias",
      "transformer.resblocks.3.mlp.c_proj.bias",
      "transformer.resblocks.3.ln_2.weight",
      "transformer.resblocks.3.ln_2.bias"
    ],
    "lr_scale": 0.010077695999999997
  },
  "layer_5_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.4.attn.in_proj_weight",
      "transformer.resblocks.4.attn.out_proj.weight",
      "transformer.resblocks.4.mlp.c_fc.weight",
      "transformer.resblocks.4.mlp.c_proj.weight"
    ],
    "lr_scale": 0.016796159999999994
  },
  "layer_5_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.4.attn.in_proj_bias",
      "transformer.resblocks.4.attn.out_proj.bias",
      "transformer.resblocks.4.ln_1.weight",
      "transformer.resblocks.4.ln_1.bias",
      "transformer.resblocks.4.mlp.c_fc.bias",
      "transformer.resblocks.4.mlp.c_proj.bias",
      "transformer.resblocks.4.ln_2.weight",
      "transformer.resblocks.4.ln_2.bias"
    ],
    "lr_scale": 0.016796159999999994
  },
  "layer_6_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.5.attn.in_proj_weight",
      "transformer.resblocks.5.attn.out_proj.weight",
      "transformer.resblocks.5.mlp.c_fc.weight",
      "transformer.resblocks.5.mlp.c_proj.weight"
    ],
    "lr_scale": 0.027993599999999993
  },
  "layer_6_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.5.attn.in_proj_bias",
      "transformer.resblocks.5.attn.out_proj.bias",
      "transformer.resblocks.5.ln_1.weight",
      "transformer.resblocks.5.ln_1.bias",
      "transformer.resblocks.5.mlp.c_fc.bias",
      "transformer.resblocks.5.mlp.c_proj.bias",
      "transformer.resblocks.5.ln_2.weight",
      "transformer.resblocks.5.ln_2.bias"
    ],
    "lr_scale": 0.027993599999999993
  },
  "layer_7_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.6.attn.in_proj_weight",
      "transformer.resblocks.6.attn.out_proj.weight",
      "transformer.resblocks.6.mlp.c_fc.weight",
      "transformer.resblocks.6.mlp.c_proj.weight"
    ],
    "lr_scale": 0.04665599999999999
  },
  "layer_7_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.6.attn.in_proj_bias",
      "transformer.resblocks.6.attn.out_proj.bias",
      "transformer.resblocks.6.ln_1.weight",
      "transformer.resblocks.6.ln_1.bias",
      "transformer.resblocks.6.mlp.c_fc.bias",
      "transformer.resblocks.6.mlp.c_proj.bias",
      "transformer.resblocks.6.ln_2.weight",
      "transformer.resblocks.6.ln_2.bias"
    ],
    "lr_scale": 0.04665599999999999
  },
  "layer_8_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.7.attn.in_proj_weight",
      "transformer.resblocks.7.attn.out_proj.weight",
      "transformer.resblocks.7.mlp.c_fc.weight",
      "transformer.resblocks.7.mlp.c_proj.weight"
    ],
    "lr_scale": 0.07775999999999998
  },
  "layer_8_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.7.attn.in_proj_bias",
      "transformer.resblocks.7.attn.out_proj.bias",
      "transformer.resblocks.7.ln_1.weight",
      "transformer.resblocks.7.ln_1.bias",
      "transformer.resblocks.7.mlp.c_fc.bias",
      "transformer.resblocks.7.mlp.c_proj.bias",
      "transformer.resblocks.7.ln_2.weight",
      "transformer.resblocks.7.ln_2.bias"
    ],
    "lr_scale": 0.07775999999999998
  },
  "layer_9_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.8.attn.in_proj_weight",
      "transformer.resblocks.8.attn.out_proj.weight",
      "transformer.resblocks.8.mlp.c_fc.weight",
      "transformer.resblocks.8.mlp.c_proj.weight"
    ],
    "lr_scale": 0.1296
  },
  "layer_9_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.8.attn.in_proj_bias",
      "transformer.resblocks.8.attn.out_proj.bias",
      "transformer.resblocks.8.ln_1.weight",
      "transformer.resblocks.8.ln_1.bias",
      "transformer.resblocks.8.mlp.c_fc.bias",
      "transformer.resblocks.8.mlp.c_proj.bias",
      "transformer.resblocks.8.ln_2.weight",
      "transformer.resblocks.8.ln_2.bias"
    ],
    "lr_scale": 0.1296
  },
  "layer_10_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.9.attn.in_proj_weight",
      "transformer.resblocks.9.attn.out_proj.weight",
      "transformer.resblocks.9.mlp.c_fc.weight",
      "transformer.resblocks.9.mlp.c_proj.weight"
    ],
    "lr_scale": 0.21599999999999997
  },
  "layer_10_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.9.attn.in_proj_bias",
      "transformer.resblocks.9.attn.out_proj.bias",
      "transformer.resblocks.9.ln_1.weight",
      "transformer.resblocks.9.ln_1.bias",
      "transformer.resblocks.9.mlp.c_fc.bias",
      "transformer.resblocks.9.mlp.c_proj.bias",
      "transformer.resblocks.9.ln_2.weight",
      "transformer.resblocks.9.ln_2.bias"
    ],
    "lr_scale": 0.21599999999999997
  },
  "layer_11_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.10.attn.in_proj_weight",
      "transformer.resblocks.10.attn.out_proj.weight",
      "transformer.resblocks.10.mlp.c_fc.weight",
      "transformer.resblocks.10.mlp.c_proj.weight"
    ],
    "lr_scale": 0.36
  },
  "layer_11_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.10.attn.in_proj_bias",
      "transformer.resblocks.10.attn.out_proj.bias",
      "transformer.resblocks.10.ln_1.weight",
      "transformer.resblocks.10.ln_1.bias",
      "transformer.resblocks.10.mlp.c_fc.bias",
      "transformer.resblocks.10.mlp.c_proj.bias",
      "transformer.resblocks.10.ln_2.weight",
      "transformer.resblocks.10.ln_2.bias"
    ],
    "lr_scale": 0.36
  },
  "layer_12_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.11.attn.in_proj_weight",
      "transformer.resblocks.11.attn.out_proj.weight",
      "transformer.resblocks.11.mlp.c_fc.weight",
      "transformer.resblocks.11.mlp.c_proj.weight"
    ],
    "lr_scale": 0.6
  },
  "layer_12_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.11.attn.in_proj_bias",
      "transformer.resblocks.11.attn.out_proj.bias",
      "transformer.resblocks.11.ln_1.weight",
      "transformer.resblocks.11.ln_1.bias",
      "transformer.resblocks.11.mlp.c_fc.bias",
      "transformer.resblocks.11.mlp.c_proj.bias",
      "transformer.resblocks.11.ln_2.weight",
      "transformer.resblocks.11.ln_2.bias"
    ],
    "lr_scale": 0.6
  },
  "layer_13_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "fc_norm.weight",
      "fc_norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  },
  "layer_13_decay": {
    "weight_decay": 0.05,
    "params": [
      "head.weight"
    ],
    "lr_scale": 1.0
  }
}
speec363a0001JZ:77305:77679 [2] NCCL INFO Trees [0] 1/-1/-1->2->3 [1] 1/-1/-1->2->3 [2] 1/-1/-1->2->3 [3] 1/-1/-1->2->3 [4] 1/-1/-1->2->3 [5] 1/-1/-1->2->3 [6] 1/-1/-1->2->3 [7] 1/-1/-1->2->3 [8] 1/-1/-1->2->3 [9] 1/-1/-1->2->3 [10] 1/-1/-1->2->3 [11] 1/-1/-1->2->3 [12] 1/-1/-1->2->3 [13] 1/-1/-1->2->3 [14] 1/-1/-1->2->3 [15] 1/-1/-1->2->3 [16] 1/-1/-1->2->3 [17] 1/-1/-1->2->3 [18] 1/-1/-1->2->3 [19] 1/-1/-1->2->3 [20] 1/-1/-1->2->3 [21] 1/-1/-1->2->3 [22] 1/-1/-1->2->3 [23] 1/-1/-1->2->3
speec363a0001JZ:77305:77679 [2] NCCL INFO Setting affinity for GPU 5 to ffff,0000ffff
speec363a0001JZ:77306:77676 [3] NCCL INFO Trees [0] 2/-1/-1->3->4 [1] 2/-1/-1->3->4 [2] 2/-1/-1->3->4 [3] 2/-1/-1->3->4 [4] 2/-1/-1->3->4 [5] 2/-1/-1->3->4 [6] 2/-1/-1->3->4 [7] 2/-1/-1->3->4 [8] 2/-1/-1->3->4 [9] 2/-1/-1->3->4 [10] 2/-1/-1->3->4 [11] 2/-1/-1->3->4 [12] 2/-1/-1->3->4 [13] 2/-1/-1->3->4 [14] 2/-1/-1->3->4 [15] 2/-1/-1->3->4 [16] 2/-1/-1->3->4 [17] 2/-1/-1->3->4 [18] 2/-1/-1->3->4 [19] 2/-1/-1->3->4 [20] 2/-1/-1->3->4 [21] 2/-1/-1->3->4 [22] 2/-1/-1->3->4 [23] 2/-1/-1->3->4
speec363a0001JZ:77307:77681 [4] NCCL INFO Trees [0] 3/-1/-1->4->5 [1] 3/-1/-1->4->5 [2] 3/-1/-1->4->5 [3] 3/-1/-1->4->5 [4] 3/-1/-1->4->5 [5] 3/-1/-1->4->5 [6] 3/-1/-1->4->5 [7] 3/-1/-1->4->5 [8] 3/-1/-1->4->5 [9] 3/-1/-1->4->5 [10] 3/-1/-1->4->5 [11] 3/-1/-1->4->5 [12] 3/-1/-1->4->5 [13] 3/-1/-1->4->5 [14] 3/-1/-1->4->5 [15] 3/-1/-1->4->5 [16] 3/-1/-1->4->5 [17] 3/-1/-1->4->5 [18] 3/-1/-1->4->5 [19] 3/-1/-1->4->5 [20] 3/-1/-1->4->5 [21] 3/-1/-1->4->5 [22] 3/-1/-1->4->5 [23] 3/-1/-1->4->5
speec363a0001JZ:77307:77681 [4] NCCL INFO Setting affinity for GPU 3 to ffff,0000ffff
speec363a0001JZ:77306:77676 [3] NCCL INFO Setting affinity for GPU 4 to ffff,0000ffff
speec363a0001JZ:77309:77680 [6] NCCL INFO Trees [0] 5/-1/-1->6->7 [1] 5/-1/-1->6->7 [2] 5/-1/-1->6->7 [3] 5/-1/-1->6->7 [4] 5/-1/-1->6->7 [5] 5/-1/-1->6->7 [6] 5/-1/-1->6->7 [7] 5/-1/-1->6->7 [8] 5/-1/-1->6->7 [9] 5/-1/-1->6->7 [10] 5/-1/-1->6->7 [11] 5/-1/-1->6->7 [12] 5/-1/-1->6->7 [13] 5/-1/-1->6->7 [14] 5/-1/-1->6->7 [15] 5/-1/-1->6->7 [16] 5/-1/-1->6->7 [17] 5/-1/-1->6->7 [18] 5/-1/-1->6->7 [19] 5/-1/-1->6->7 [20] 5/-1/-1->6->7 [21] 5/-1/-1->6->7 [22] 5/-1/-1->6->7 [23] 5/-1/-1->6->7
speec363a0001JZ:77309:77680 [6] NCCL INFO Setting affinity for GPU 1 to ffff,0000ffff
speec363a0001JZ:77308:77675 [5] NCCL INFO Trees [0] 4/-1/-1->5->6 [1] 4/-1/-1->5->6 [2] 4/-1/-1->5->6 [3] 4/-1/-1->5->6 [4] 4/-1/-1->5->6 [5] 4/-1/-1->5->6 [6] 4/-1/-1->5->6 [7] 4/-1/-1->5->6 [8] 4/-1/-1->5->6 [9] 4/-1/-1->5->6 [10] 4/-1/-1->5->6 [11] 4/-1/-1->5->6 [12] 4/-1/-1->5->6 [13] 4/-1/-1->5->6 [14] 4/-1/-1->5->6 [15] 4/-1/-1->5->6 [16] 4/-1/-1->5->6 [17] 4/-1/-1->5->6 [18] 4/-1/-1->5->6 [19] 4/-1/-1->5->6 [20] 4/-1/-1->5->6 [21] 4/-1/-1->5->6 [22] 4/-1/-1->5->6 [23] 4/-1/-1->5->6
speec363a0001JZ:77308:77675 [5] NCCL INFO Setting affinity for GPU 2 to ffff,0000ffff
speec363a0001JZ:77304:77678 [1] NCCL INFO Trees [0] 0/-1/-1->1->2 [1] 0/-1/-1->1->2 [2] 0/-1/-1->1->2 [3] 0/-1/-1->1->2 [4] 0/-1/-1->1->2 [5] 0/-1/-1->1->2 [6] 0/-1/-1->1->2 [7] 0/-1/-1->1->2 [8] 0/-1/-1->1->2 [9] 0/-1/-1->1->2 [10] 0/-1/-1->1->2 [11] 0/-1/-1->1->2 [12] 0/-1/-1->1->2 [13] 0/-1/-1->1->2 [14] 0/-1/-1->1->2 [15] 0/-1/-1->1->2 [16] 0/-1/-1->1->2 [17] 0/-1/-1->1->2 [18] 0/-1/-1->1->2 [19] 0/-1/-1->1->2 [20] 0/-1/-1->1->2 [21] 0/-1/-1->1->2 [22] 0/-1/-1->1->2 [23] 0/-1/-1->1->2
speec363a0001JZ:77304:77678 [1] NCCL INFO Setting affinity for GPU 6 to ffff,0000ffff
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 00/24 :    0   7   6   5   4   3   2   1
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 01/24 :    0   7   6   5   4   3   2   1
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 02/24 :    0   7   6   5   4   3   2   1
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 03/24 :    0   7   6   5   4   3   2   1
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 04/24 :    0   7   6   5   4   3   2   1
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 05/24 :    0   7   6   5   4   3   2   1
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 06/24 :    0   7   6   5   4   3   2   1
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 07/24 :    0   7   6   5   4   3   2   1
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 08/24 :    0   7   6   5   4   3   2   1
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 09/24 :    0   7   6   5   4   3   2   1
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 10/24 :    0   7   6   5   4   3   2   1
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 11/24 :    0   7   6   5   4   3   2   1
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 12/24 :    0   7   6   5   4   3   2   1
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 13/24 :    0   7   6   5   4   3   2   1
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 14/24 :    0   7   6   5   4   3   2   1
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 15/24 :    0   7   6   5   4   3   2   1
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 16/24 :    0   7   6   5   4   3   2   1
speec363a0001JZ:77310:77677 [7] NCCL INFO Trees [0] 6/-1/-1->7->-1 [1] 6/-1/-1->7->-1 [2] 6/-1/-1->7->-1 [3] 6/-1/-1->7->-1 [4] 6/-1/-1->7->-1 [5] 6/-1/-1->7->-1 [6] 6/-1/-1->7->-1 [7] 6/-1/-1->7->-1 [8] 6/-1/-1->7->-1 [9] 6/-1/-1->7->-1 [10] 6/-1/-1->7->-1 [11] 6/-1/-1->7->-1 [12] 6/-1/-1->7->-1 [13] 6/-1/-1->7->-1 [14] 6/-1/-1->7->-1 [15] 6/-1/-1->7->-1 [16] 6/-1/-1->7->-1 [17] 6/-1/-1->7->-1 [18] 6/-1/-1->7->-1 [19] 6/-1/-1->7->-1 [20] 6/-1/-1->7->-1 [21] 6/-1/-1->7->-1 [22] 6/-1/-1->7->-1 [23] 6/-1/-1->7->-1
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 17/24 :    0   7   6   5   4   3   2   1
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 18/24 :    0   7   6   5   4   3   2   1
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 19/24 :    0   7   6   5   4   3   2   1
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 20/24 :    0   7   6   5   4   3   2   1
speec363a0001JZ:77310:77677 [7] NCCL INFO Setting affinity for GPU 0 to ffff,0000ffff
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 21/24 :    0   7   6   5   4   3   2   1
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 22/24 :    0   7   6   5   4   3   2   1
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 23/24 :    0   7   6   5   4   3   2   1
speec363a0001JZ:77303:77674 [0] NCCL INFO Trees [0] -1/-1/-1->0->1 [1] -1/-1/-1->0->1 [2] -1/-1/-1->0->1 [3] -1/-1/-1->0->1 [4] -1/-1/-1->0->1 [5] -1/-1/-1->0->1 [6] -1/-1/-1->0->1 [7] -1/-1/-1->0->1 [8] -1/-1/-1->0->1 [9] -1/-1/-1->0->1 [10] -1/-1/-1->0->1 [11] -1/-1/-1->0->1 [12] -1/-1/-1->0->1 [13] -1/-1/-1->0->1 [14] -1/-1/-1->0->1 [15] -1/-1/-1->0->1 [16] -1/-1/-1->0->1 [17] -1/-1/-1->0->1 [18] -1/-1/-1->0->1 [19] -1/-1/-1->0->1 [20] -1/-1/-1->0->1 [21] -1/-1/-1->0->1 [22] -1/-1/-1->0->1 [23] -1/-1/-1->0->1
speec363a0001JZ:77303:77674 [0] NCCL INFO Setting affinity for GPU 7 to ffff,0000ffff
speec363a0001JZ:77306:77676 [3] NCCL INFO Channel 00 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77308:77675 [5] NCCL INFO Channel 00 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 00 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77304:77678 [1] NCCL INFO Channel 00 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JZ:77309:77680 [6] NCCL INFO Channel 00 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77307:77681 [4] NCCL INFO Channel 00 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77305:77679 [2] NCCL INFO Channel 00 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77306:77676 [3] NCCL INFO Channel 01 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77308:77675 [5] NCCL INFO Channel 01 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77310:77677 [7] NCCL INFO Channel 00 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 01 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77304:77678 [1] NCCL INFO Channel 01 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JZ:77309:77680 [6] NCCL INFO Channel 01 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77307:77681 [4] NCCL INFO Channel 01 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77305:77679 [2] NCCL INFO Channel 01 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77306:77676 [3] NCCL INFO Channel 02 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77308:77675 [5] NCCL INFO Channel 02 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77310:77677 [7] NCCL INFO Channel 01 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 02 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77304:77678 [1] NCCL INFO Channel 02 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JZ:77309:77680 [6] NCCL INFO Channel 02 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77307:77681 [4] NCCL INFO Channel 02 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77305:77679 [2] NCCL INFO Channel 02 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77306:77676 [3] NCCL INFO Channel 03 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77308:77675 [5] NCCL INFO Channel 03 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77310:77677 [7] NCCL INFO Channel 02 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 03 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77304:77678 [1] NCCL INFO Channel 03 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JZ:77307:77681 [4] NCCL INFO Channel 03 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77309:77680 [6] NCCL INFO Channel 03 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77305:77679 [2] NCCL INFO Channel 03 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77306:77676 [3] NCCL INFO Channel 04 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77308:77675 [5] NCCL INFO Channel 04 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77310:77677 [7] NCCL INFO Channel 03 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 04 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77304:77678 [1] NCCL INFO Channel 04 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JZ:77307:77681 [4] NCCL INFO Channel 04 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77309:77680 [6] NCCL INFO Channel 04 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77305:77679 [2] NCCL INFO Channel 04 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77306:77676 [3] NCCL INFO Channel 05 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77308:77675 [5] NCCL INFO Channel 05 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77310:77677 [7] NCCL INFO Channel 04 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 05 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77304:77678 [1] NCCL INFO Channel 05 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JZ:77307:77681 [4] NCCL INFO Channel 05 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77309:77680 [6] NCCL INFO Channel 05 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77305:77679 [2] NCCL INFO Channel 05 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77306:77676 [3] NCCL INFO Channel 06 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77308:77675 [5] NCCL INFO Channel 06 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77310:77677 [7] NCCL INFO Channel 05 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 06 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77304:77678 [1] NCCL INFO Channel 06 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JZ:77307:77681 [4] NCCL INFO Channel 06 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77309:77680 [6] NCCL INFO Channel 06 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77305:77679 [2] NCCL INFO Channel 06 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77306:77676 [3] NCCL INFO Channel 07 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77308:77675 [5] NCCL INFO Channel 07 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77310:77677 [7] NCCL INFO Channel 06 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 07 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77304:77678 [1] NCCL INFO Channel 07 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JZ:77307:77681 [4] NCCL INFO Channel 07 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77309:77680 [6] NCCL INFO Channel 07 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77305:77679 [2] NCCL INFO Channel 07 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77306:77676 [3] NCCL INFO Channel 08 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77308:77675 [5] NCCL INFO Channel 08 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77310:77677 [7] NCCL INFO Channel 07 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 08 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77304:77678 [1] NCCL INFO Channel 08 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JZ:77307:77681 [4] NCCL INFO Channel 08 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77309:77680 [6] NCCL INFO Channel 08 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77305:77679 [2] NCCL INFO Channel 08 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77306:77676 [3] NCCL INFO Channel 09 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77308:77675 [5] NCCL INFO Channel 09 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77310:77677 [7] NCCL INFO Channel 08 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 09 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77304:77678 [1] NCCL INFO Channel 09 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JZ:77307:77681 [4] NCCL INFO Channel 09 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77309:77680 [6] NCCL INFO Channel 09 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77305:77679 [2] NCCL INFO Channel 09 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77306:77676 [3] NCCL INFO Channel 10 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77308:77675 [5] NCCL INFO Channel 10 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77310:77677 [7] NCCL INFO Channel 09 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 10 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77304:77678 [1] NCCL INFO Channel 10 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JZ:77307:77681 [4] NCCL INFO Channel 10 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77309:77680 [6] NCCL INFO Channel 10 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77305:77679 [2] NCCL INFO Channel 10 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77306:77676 [3] NCCL INFO Channel 11 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77308:77675 [5] NCCL INFO Channel 11 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77310:77677 [7] NCCL INFO Channel 10 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 11 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77304:77678 [1] NCCL INFO Channel 11 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JZ:77307:77681 [4] NCCL INFO Channel 11 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77309:77680 [6] NCCL INFO Channel 11 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77305:77679 [2] NCCL INFO Channel 11 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77306:77676 [3] NCCL INFO Channel 12 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77308:77675 [5] NCCL INFO Channel 12 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 12 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77310:77677 [7] NCCL INFO Channel 11 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77304:77678 [1] NCCL INFO Channel 12 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JZ:77307:77681 [4] NCCL INFO Channel 12 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77309:77680 [6] NCCL INFO Channel 12 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77305:77679 [2] NCCL INFO Channel 12 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77306:77676 [3] NCCL INFO Channel 13 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77308:77675 [5] NCCL INFO Channel 13 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 13 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77310:77677 [7] NCCL INFO Channel 12 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77304:77678 [1] NCCL INFO Channel 13 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JZ:77307:77681 [4] NCCL INFO Channel 13 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77309:77680 [6] NCCL INFO Channel 13 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77305:77679 [2] NCCL INFO Channel 13 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77306:77676 [3] NCCL INFO Channel 14 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77308:77675 [5] NCCL INFO Channel 14 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 14 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77310:77677 [7] NCCL INFO Channel 13 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77304:77678 [1] NCCL INFO Channel 14 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JZ:77307:77681 [4] NCCL INFO Channel 14 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77309:77680 [6] NCCL INFO Channel 14 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77305:77679 [2] NCCL INFO Channel 14 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77306:77676 [3] NCCL INFO Channel 15 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77308:77675 [5] NCCL INFO Channel 15 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 15 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77310:77677 [7] NCCL INFO Channel 14 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77304:77678 [1] NCCL INFO Channel 15 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JZ:77307:77681 [4] NCCL INFO Channel 15 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77309:77680 [6] NCCL INFO Channel 15 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77305:77679 [2] NCCL INFO Channel 15 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77306:77676 [3] NCCL INFO Channel 16 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77308:77675 [5] NCCL INFO Channel 16 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 16 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77310:77677 [7] NCCL INFO Channel 15 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77304:77678 [1] NCCL INFO Channel 16 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JZ:77307:77681 [4] NCCL INFO Channel 16 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77309:77680 [6] NCCL INFO Channel 16 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77305:77679 [2] NCCL INFO Channel 16 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77306:77676 [3] NCCL INFO Channel 17 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77308:77675 [5] NCCL INFO Channel 17 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 17 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77310:77677 [7] NCCL INFO Channel 16 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77304:77678 [1] NCCL INFO Channel 17 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JZ:77307:77681 [4] NCCL INFO Channel 17 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77309:77680 [6] NCCL INFO Channel 17 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77305:77679 [2] NCCL INFO Channel 17 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77306:77676 [3] NCCL INFO Channel 18 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77308:77675 [5] NCCL INFO Channel 18 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 18 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77310:77677 [7] NCCL INFO Channel 17 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77304:77678 [1] NCCL INFO Channel 18 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JZ:77307:77681 [4] NCCL INFO Channel 18 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77309:77680 [6] NCCL INFO Channel 18 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77305:77679 [2] NCCL INFO Channel 18 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77306:77676 [3] NCCL INFO Channel 19 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77308:77675 [5] NCCL INFO Channel 19 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 19 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77310:77677 [7] NCCL INFO Channel 18 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77304:77678 [1] NCCL INFO Channel 19 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JZ:77307:77681 [4] NCCL INFO Channel 19 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77309:77680 [6] NCCL INFO Channel 19 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77305:77679 [2] NCCL INFO Channel 19 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77306:77676 [3] NCCL INFO Channel 20 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77308:77675 [5] NCCL INFO Channel 20 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 20 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77310:77677 [7] NCCL INFO Channel 19 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77304:77678 [1] NCCL INFO Channel 20 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JZ:77307:77681 [4] NCCL INFO Channel 20 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77309:77680 [6] NCCL INFO Channel 20 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77305:77679 [2] NCCL INFO Channel 20 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77306:77676 [3] NCCL INFO Channel 21 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77308:77675 [5] NCCL INFO Channel 21 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 21 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77310:77677 [7] NCCL INFO Channel 20 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77304:77678 [1] NCCL INFO Channel 21 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JZ:77307:77681 [4] NCCL INFO Channel 21 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77309:77680 [6] NCCL INFO Channel 21 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77305:77679 [2] NCCL INFO Channel 21 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77306:77676 [3] NCCL INFO Channel 22 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77308:77675 [5] NCCL INFO Channel 22 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 22 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77310:77677 [7] NCCL INFO Channel 21 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77304:77678 [1] NCCL INFO Channel 22 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JZ:77307:77681 [4] NCCL INFO Channel 22 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77309:77680 [6] NCCL INFO Channel 22 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77305:77679 [2] NCCL INFO Channel 22 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77306:77676 [3] NCCL INFO Channel 23 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77308:77675 [5] NCCL INFO Channel 23 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 23 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77310:77677 [7] NCCL INFO Channel 22 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77304:77678 [1] NCCL INFO Channel 23 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JZ:77307:77681 [4] NCCL INFO Channel 23 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77309:77680 [6] NCCL INFO Channel 23 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77305:77679 [2] NCCL INFO Channel 23 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77310:77677 [7] NCCL INFO Channel 23 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77304:77678 [1] NCCL INFO Connected all rings
speec363a0001JZ:77308:77675 [5] NCCL INFO Connected all rings
speec363a0001JZ:77306:77676 [3] NCCL INFO Connected all rings
speec363a0001JZ:77303:77674 [0] NCCL INFO Connected all rings
speec363a0001JZ:77307:77681 [4] NCCL INFO Connected all rings
speec363a0001JZ:77305:77679 [2] NCCL INFO Connected all rings
speec363a0001JZ:77310:77677 [7] NCCL INFO Connected all rings
speec363a0001JZ:77309:77680 [6] NCCL INFO Connected all rings
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 00 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 01 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 02 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 03 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 04 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 05 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 06 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 07 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 08 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 09 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 10 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 11 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 12 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 13 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 14 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 15 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 16 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 17 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 18 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 19 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 20 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 21 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 22 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77303:77674 [0] NCCL INFO Channel 23 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JZ:77304:77678 [1] NCCL INFO Channel 00 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77308:77675 [5] NCCL INFO Channel 00 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77306:77676 [3] NCCL INFO Channel 00 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77305:77679 [2] NCCL INFO Channel 00 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77307:77681 [4] NCCL INFO Channel 00 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77309:77680 [6] NCCL INFO Channel 00 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77304:77678 [1] NCCL INFO Channel 01 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77308:77675 [5] NCCL INFO Channel 01 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77305:77679 [2] NCCL INFO Channel 01 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77306:77676 [3] NCCL INFO Channel 01 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77307:77681 [4] NCCL INFO Channel 01 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77309:77680 [6] NCCL INFO Channel 01 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77304:77678 [1] NCCL INFO Channel 02 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77308:77675 [5] NCCL INFO Channel 02 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77305:77679 [2] NCCL INFO Channel 02 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77306:77676 [3] NCCL INFO Channel 02 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77307:77681 [4] NCCL INFO Channel 02 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77309:77680 [6] NCCL INFO Channel 02 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77304:77678 [1] NCCL INFO Channel 03 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77308:77675 [5] NCCL INFO Channel 03 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77305:77679 [2] NCCL INFO Channel 03 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77306:77676 [3] NCCL INFO Channel 03 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77307:77681 [4] NCCL INFO Channel 03 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77309:77680 [6] NCCL INFO Channel 03 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77304:77678 [1] NCCL INFO Channel 04 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77308:77675 [5] NCCL INFO Channel 04 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77305:77679 [2] NCCL INFO Channel 04 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77306:77676 [3] NCCL INFO Channel 04 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77307:77681 [4] NCCL INFO Channel 04 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77309:77680 [6] NCCL INFO Channel 04 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77304:77678 [1] NCCL INFO Channel 05 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77308:77675 [5] NCCL INFO Channel 05 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77305:77679 [2] NCCL INFO Channel 05 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77306:77676 [3] NCCL INFO Channel 05 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77307:77681 [4] NCCL INFO Channel 05 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77309:77680 [6] NCCL INFO Channel 05 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77304:77678 [1] NCCL INFO Channel 06 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77308:77675 [5] NCCL INFO Channel 06 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77305:77679 [2] NCCL INFO Channel 06 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77306:77676 [3] NCCL INFO Channel 06 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77307:77681 [4] NCCL INFO Channel 06 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77309:77680 [6] NCCL INFO Channel 06 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77304:77678 [1] NCCL INFO Channel 07 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77308:77675 [5] NCCL INFO Channel 07 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77305:77679 [2] NCCL INFO Channel 07 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77306:77676 [3] NCCL INFO Channel 07 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77307:77681 [4] NCCL INFO Channel 07 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77309:77680 [6] NCCL INFO Channel 07 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77304:77678 [1] NCCL INFO Channel 08 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77308:77675 [5] NCCL INFO Channel 08 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77305:77679 [2] NCCL INFO Channel 08 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77306:77676 [3] NCCL INFO Channel 08 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77307:77681 [4] NCCL INFO Channel 08 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77309:77680 [6] NCCL INFO Channel 08 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77304:77678 [1] NCCL INFO Channel 09 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77308:77675 [5] NCCL INFO Channel 09 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77305:77679 [2] NCCL INFO Channel 09 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77306:77676 [3] NCCL INFO Channel 09 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77307:77681 [4] NCCL INFO Channel 09 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77309:77680 [6] NCCL INFO Channel 09 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77304:77678 [1] NCCL INFO Channel 10 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77308:77675 [5] NCCL INFO Channel 10 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77305:77679 [2] NCCL INFO Channel 10 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77306:77676 [3] NCCL INFO Channel 10 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77307:77681 [4] NCCL INFO Channel 10 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77309:77680 [6] NCCL INFO Channel 10 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77304:77678 [1] NCCL INFO Channel 11 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77308:77675 [5] NCCL INFO Channel 11 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77305:77679 [2] NCCL INFO Channel 11 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77306:77676 [3] NCCL INFO Channel 11 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77307:77681 [4] NCCL INFO Channel 11 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77309:77680 [6] NCCL INFO Channel 11 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77304:77678 [1] NCCL INFO Channel 12 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77308:77675 [5] NCCL INFO Channel 12 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77305:77679 [2] NCCL INFO Channel 12 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77306:77676 [3] NCCL INFO Channel 12 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77307:77681 [4] NCCL INFO Channel 12 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77309:77680 [6] NCCL INFO Channel 12 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77304:77678 [1] NCCL INFO Channel 13 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77308:77675 [5] NCCL INFO Channel 13 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77305:77679 [2] NCCL INFO Channel 13 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77306:77676 [3] NCCL INFO Channel 13 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77307:77681 [4] NCCL INFO Channel 13 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77309:77680 [6] NCCL INFO Channel 13 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77304:77678 [1] NCCL INFO Channel 14 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77308:77675 [5] NCCL INFO Channel 14 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77305:77679 [2] NCCL INFO Channel 14 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77306:77676 [3] NCCL INFO Channel 14 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77307:77681 [4] NCCL INFO Channel 14 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77309:77680 [6] NCCL INFO Channel 14 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77308:77675 [5] NCCL INFO Channel 15 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77304:77678 [1] NCCL INFO Channel 15 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77305:77679 [2] NCCL INFO Channel 15 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77306:77676 [3] NCCL INFO Channel 15 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77307:77681 [4] NCCL INFO Channel 15 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77309:77680 [6] NCCL INFO Channel 15 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77308:77675 [5] NCCL INFO Channel 16 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77304:77678 [1] NCCL INFO Channel 16 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77305:77679 [2] NCCL INFO Channel 16 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77306:77676 [3] NCCL INFO Channel 16 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77307:77681 [4] NCCL INFO Channel 16 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77309:77680 [6] NCCL INFO Channel 16 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77308:77675 [5] NCCL INFO Channel 17 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77304:77678 [1] NCCL INFO Channel 17 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77305:77679 [2] NCCL INFO Channel 17 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77306:77676 [3] NCCL INFO Channel 17 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77307:77681 [4] NCCL INFO Channel 17 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77309:77680 [6] NCCL INFO Channel 17 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77308:77675 [5] NCCL INFO Channel 18 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77304:77678 [1] NCCL INFO Channel 18 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77305:77679 [2] NCCL INFO Channel 18 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77306:77676 [3] NCCL INFO Channel 18 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77307:77681 [4] NCCL INFO Channel 18 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77309:77680 [6] NCCL INFO Channel 18 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77308:77675 [5] NCCL INFO Channel 19 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77304:77678 [1] NCCL INFO Channel 19 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77305:77679 [2] NCCL INFO Channel 19 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77306:77676 [3] NCCL INFO Channel 19 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77307:77681 [4] NCCL INFO Channel 19 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77309:77680 [6] NCCL INFO Channel 19 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77308:77675 [5] NCCL INFO Channel 20 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77304:77678 [1] NCCL INFO Channel 20 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77305:77679 [2] NCCL INFO Channel 20 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77306:77676 [3] NCCL INFO Channel 20 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77307:77681 [4] NCCL INFO Channel 20 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77309:77680 [6] NCCL INFO Channel 20 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77308:77675 [5] NCCL INFO Channel 21 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77304:77678 [1] NCCL INFO Channel 21 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77305:77679 [2] NCCL INFO Channel 21 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77306:77676 [3] NCCL INFO Channel 21 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77307:77681 [4] NCCL INFO Channel 21 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77309:77680 [6] NCCL INFO Channel 21 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77308:77675 [5] NCCL INFO Channel 22 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77304:77678 [1] NCCL INFO Channel 22 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77305:77679 [2] NCCL INFO Channel 22 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77306:77676 [3] NCCL INFO Channel 22 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77307:77681 [4] NCCL INFO Channel 22 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77309:77680 [6] NCCL INFO Channel 22 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77308:77675 [5] NCCL INFO Channel 23 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JZ:77304:77678 [1] NCCL INFO Channel 23 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JZ:77305:77679 [2] NCCL INFO Channel 23 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JZ:77306:77676 [3] NCCL INFO Channel 23 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JZ:77307:77681 [4] NCCL INFO Channel 23 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JZ:77309:77680 [6] NCCL INFO Channel 23 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JZ:77303:77674 [0] NCCL INFO Connected all trees
speec363a0001JZ:77303:77674 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
speec363a0001JZ:77310:77677 [7] NCCL INFO Connected all trees
speec363a0001JZ:77310:77677 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
speec363a0001JZ:77303:77674 [0] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
speec363a0001JZ:77310:77677 [7] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
speec363a0001JZ:77304:77678 [1] NCCL INFO Connected all trees
speec363a0001JZ:77304:77678 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
speec363a0001JZ:77306:77676 [3] NCCL INFO Connected all trees
speec363a0001JZ:77306:77676 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
speec363a0001JZ:77305:77679 [2] NCCL INFO Connected all trees
speec363a0001JZ:77305:77679 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
speec363a0001JZ:77308:77675 [5] NCCL INFO Connected all trees
speec363a0001JZ:77308:77675 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
speec363a0001JZ:77309:77680 [6] NCCL INFO Connected all trees
speec363a0001JZ:77309:77680 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
speec363a0001JZ:77307:77681 [4] NCCL INFO Connected all trees
speec363a0001JZ:77307:77681 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
speec363a0001JZ:77304:77678 [1] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
speec363a0001JZ:77306:77676 [3] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
speec363a0001JZ:77305:77679 [2] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
speec363a0001JZ:77308:77675 [5] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
speec363a0001JZ:77309:77680 [6] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
speec363a0001JZ:77307:77681 [4] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
speec363a0001JZ:77310:77677 [7] NCCL INFO comm 0x7efb10002fb0 rank 7 nranks 8 cudaDev 7 busId 100000 - Init COMPLETE
speec363a0001JZ:77308:77675 [5] NCCL INFO comm 0x7f8f7c002fb0 rank 5 nranks 8 cudaDev 5 busId 300000 - Init COMPLETE
speec363a0001JZ:77303:77674 [0] NCCL INFO comm 0x7f2214002fb0 rank 0 nranks 8 cudaDev 0 busId e00000 - Init COMPLETE
speec363a0001JZ:77309:77680 [6] NCCL INFO comm 0x7f3338002fb0 rank 6 nranks 8 cudaDev 6 busId 200000 - Init COMPLETE
speec363a0001JZ:77304:77678 [1] NCCL INFO comm 0x7f9f7c002fb0 rank 1 nranks 8 cudaDev 1 busId d00000 - Init COMPLETE
speec363a0001JZ:77305:77679 [2] NCCL INFO comm 0x7fc844002fb0 rank 2 nranks 8 cudaDev 2 busId c00000 - Init COMPLETE
speec363a0001JZ:77303:77303 [0] NCCL INFO Launch mode Parallel
speec363a0001JZ:77306:77676 [3] NCCL INFO comm 0x7f87f0002fb0 rank 3 nranks 8 cudaDev 3 busId b00000 - Init COMPLETE
speec363a0001JZ:77307:77681 [4] NCCL INFO comm 0x7fd6e8002fb0 rank 4 nranks 8 cudaDev 4 busId 400000 - Init COMPLETE
Using /home/dochen/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.6032202243804932 seconds
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
    "stage": 0, 
    "contiguous_gradients": false, 
    "reduce_scatter": true, 
    "reduce_bucket_size": 5.000000e+08, 
    "allgather_partitions": true, 
    "allgather_bucket_size": 5.000000e+08, 
    "overlap_comm": false, 
    "load_from_fp32_weights": true, 
    "elastic_checkpoint": true, 
    "offload_param": null, 
    "offload_optimizer": null, 
    "sub_group_size": 1.000000e+12, 
    "prefetch_bucket_size": 5.000000e+07, 
    "param_persistence_threshold": 1.000000e+05, 
    "max_live_parameters": 1.000000e+09, 
    "max_reuse_distance": 1.000000e+09, 
    "gather_fp16_weights_on_model_save": false, 
    "ignore_unused_parameters": true, 
    "legacy_stage1": false
}
    "train_batch_size": 2.048000e+03, 
    "train_micro_batch_size_per_gpu": 256, 
    "steps_per_print": 1000, 
    "optimizer": {
        "type": "Adam", 
        "adam_w_mode": true, 
        "params": {
            "lr": 0.0006, 
            "weight_decay": 0.05, 
            "bias_correction": true, 
            "betas": [0.9, 0.999], 
            "eps": 1e-08
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 7, 
        "loss_scale_window": 128
    }
}
Using /home/dochen/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.501833438873291 seconds
model.gradient_accumulation_steps() = 1
Use step level LR scheduler!
Set warmup steps = 6250
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
criterion = LabelSmoothingCrossEntropy()
latest_ckpt: -1
Start training for 50 epochs
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Epoch: [0]  [  0/625]  eta: 0:58:45  lr: 0.000000  min_lr: 0.000000  loss: 6.9062 (6.9062)  class_acc: 0.0039 (0.0039)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 5.6405  data: 3.1108  max mem: 25578
Epoch: [0]  [100/625]  eta: 0:05:41  lr: 0.000010  min_lr: 0.000000  loss: 6.8320 (6.8862)  class_acc: 0.1172 (0.0674)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6027  data: 0.0003  max mem: 25582
Epoch: [0]  [200/625]  eta: 0:04:25  lr: 0.000019  min_lr: 0.000000  loss: 6.2930 (6.7050)  class_acc: 0.0781 (0.0710)  loss_scale: 256.0000 (174.4876)  weight_decay: 0.0500 (0.0500)  time: 0.6001  data: 0.0003  max mem: 25582
Epoch: [0]  [300/625]  eta: 0:03:20  lr: 0.000029  min_lr: 0.000000  loss: 5.6172 (6.4384)  class_acc: 0.1016 (0.0794)  loss_scale: 512.0000 (239.8405)  weight_decay: 0.0500 (0.0500)  time: 0.5999  data: 0.0003  max mem: 25582
Epoch: [0]  [400/625]  eta: 0:02:17  lr: 0.000038  min_lr: 0.000000  loss: 4.9570 (6.1329)  class_acc: 0.1914 (0.0964)  loss_scale: 1024.0000 (329.4165)  weight_decay: 0.0500 (0.0500)  time: 0.6001  data: 0.0003  max mem: 25582
Epoch: [0]  [500/625]  eta: 0:01:16  lr: 0.000048  min_lr: 0.000000  loss: 4.2969 (5.8191)  class_acc: 0.2734 (0.1249)  loss_scale: 1024.0000 (468.0559)  weight_decay: 0.0500 (0.0500)  time: 0.5992  data: 0.0003  max mem: 25582
Epoch: [0]  [600/625]  eta: 0:00:15  lr: 0.000058  min_lr: 0.000000  loss: 3.7871 (5.5131)  class_acc: 0.3750 (0.1601)  loss_scale: 1024.0000 (569.0782)  weight_decay: 0.0500 (0.0500)  time: 0.6000  data: 0.0003  max mem: 25582
Epoch: [0]  [624/625]  eta: 0:00:00  lr: 0.000060  min_lr: 0.000000  loss: 3.6719 (5.4426)  class_acc: 0.3984 (0.1690)  loss_scale: 1024.0000 (586.5472)  weight_decay: 0.0500 (0.0500)  time: 0.5895  data: 0.0008  max mem: 25582
Epoch: [0] Total time: 0:06:20 (0.6085 s / it)
Averaged stats: lr: 0.000060  min_lr: 0.000000  loss: 3.6719 (5.4416)  class_acc: 0.3984 (0.1691)  loss_scale: 1024.0000 (586.5472)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:01  loss: 3.0099 (3.0099)  acc1: 35.9375 (35.9375)  acc5: 68.7500 (68.7500)  time: 2.4444  data: 2.1728  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 2.3807 (2.5158)  acc1: 54.2969 (52.1760)  acc5: 87.5000 (85.2000)  time: 0.2156  data: 0.0002  max mem: 25582
Test: Total time: 0:00:07 (0.3078 s / it)
* Acc@1 52.116 Acc@5 85.118 loss 2.529
Accuracy of the network on the 50000 test images: 52.1%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:07  loss: 6.8633 (6.8633)  acc1: 10.1562 (10.1562)  acc5: 25.0000 (25.0000)  time: 2.6926  data: 2.4414  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 6.8527 (6.8556)  acc1: 21.8750 (20.2880)  acc5: 46.4844 (43.9360)  time: 0.2111  data: 0.0033  max mem: 25582
Test: Total time: 0:00:07 (0.3146 s / it)
* Acc@1 20.044 Acc@5 44.272 loss 6.856
EMA Accuracy of the network on the 50000 test images: 20.0%
Max accuracy: 20.04%
{"train_lr": 2.99567930868939e-05, "train_min_lr": 3.912565082085452e-08, "train_loss": 5.441641015625, "train_class_acc": 0.16905078125, "train_loss_scale": 586.5472, "train_weight_decay": 0.050000000000000495, "test_loss": 6.855536937713623, "test_acc1": 20.044000002441408, "test_acc5": 44.27200002960205, "epoch": 0, "n_parameters": 85978600}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Epoch: [1]  [  0/625]  eta: 0:26:21  lr: 0.000060  min_lr: 0.000000  loss: 3.5605 (3.5605)  class_acc: 0.4531 (0.4531)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  time: 2.5301  data: 1.8561  max mem: 25582
Epoch: [1]  [100/625]  eta: 0:05:23  lr: 0.000070  min_lr: 0.000000  loss: 3.2676 (3.4018)  class_acc: 0.4727 (0.4457)  loss_scale: 2048.0000 (1753.9802)  weight_decay: 0.0500 (0.0500)  time: 0.5992  data: 0.0002  max mem: 25582
Epoch: [1]  [200/625]  eta: 0:04:18  lr: 0.000079  min_lr: 0.000000  loss: 2.9805 (3.2451)  class_acc: 0.5078 (0.4749)  loss_scale: 1024.0000 (1390.8060)  weight_decay: 0.0500 (0.0500)  time: 0.6017  data: 0.0002  max mem: 25582
Epoch: [1]  [300/625]  eta: 0:03:16  lr: 0.000089  min_lr: 0.000000  loss: 2.8125 (3.1132)  class_acc: 0.5508 (0.4989)  loss_scale: 2048.0000 (1537.7010)  weight_decay: 0.0500 (0.0500)  time: 0.5992  data: 0.0003  max mem: 25582
Epoch: [1]  [400/625]  eta: 0:02:15  lr: 0.000098  min_lr: 0.000000  loss: 2.7305 (3.0183)  class_acc: 0.5664 (0.5161)  loss_scale: 2048.0000 (1695.6010)  weight_decay: 0.0500 (0.0500)  time: 0.6015  data: 0.0002  max mem: 25582
Epoch: [1]  [500/625]  eta: 0:01:15  lr: 0.000108  min_lr: 0.000000  loss: 2.5879 (2.9439)  class_acc: 0.5898 (0.5301)  loss_scale: 2048.0000 (1770.0279)  weight_decay: 0.0500 (0.0500)  time: 0.5975  data: 0.0002  max mem: 25582
Epoch: [1]  [600/625]  eta: 0:00:15  lr: 0.000118  min_lr: 0.000000  loss: 2.5449 (2.8803)  class_acc: 0.6055 (0.5426)  loss_scale: 2048.0000 (1816.2795)  weight_decay: 0.0500 (0.0500)  time: 0.6027  data: 0.0002  max mem: 25582
Epoch: [1]  [624/625]  eta: 0:00:00  lr: 0.000120  min_lr: 0.000000  loss: 2.5293 (2.8666)  class_acc: 0.6172 (0.5453)  loss_scale: 2048.0000 (1857.9456)  weight_decay: 0.0500 (0.0500)  time: 0.5882  data: 0.0007  max mem: 25582
Epoch: [1] Total time: 0:06:16 (0.6025 s / it)
Averaged stats: lr: 0.000120  min_lr: 0.000000  loss: 2.5293 (2.8677)  class_acc: 0.6172 (0.5454)  loss_scale: 2048.0000 (1857.9456)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:06  loss: 1.1539 (1.1539)  acc1: 72.2656 (72.2656)  acc5: 94.5312 (94.5312)  time: 2.6543  data: 2.4151  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.9688 (1.0082)  acc1: 76.5625 (76.0800)  acc5: 95.3125 (94.8960)  time: 0.2070  data: 0.0002  max mem: 25582
Test: Total time: 0:00:07 (0.3091 s / it)
* Acc@1 76.212 Acc@5 94.830 loss 1.006
Accuracy of the network on the 50000 test images: 76.2%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:07  loss: 6.6315 (6.6315)  acc1: 29.2969 (29.2969)  acc5: 57.0312 (57.0312)  time: 2.7028  data: 2.4570  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 6.5963 (6.6041)  acc1: 41.7969 (41.6640)  acc5: 73.8281 (71.7600)  time: 0.2067  data: 0.0002  max mem: 25582
Test: Total time: 0:00:07 (0.3162 s / it)
* Acc@1 41.858 Acc@5 71.524 loss 6.604
EMA Accuracy of the network on the 50000 test images: 41.9%
Max accuracy: 41.86%
{"train_lr": 8.996639462313973e-05, "train_min_lr": 1.175023551895535e-07, "train_loss": 2.86768125, "train_class_acc": 0.54538046875, "train_loss_scale": 1857.9456, "train_weight_decay": 0.050000000000000495, "test_loss": 6.604437830448151, "test_acc1": 41.85800002700805, "test_acc5": 71.52400004577636, "epoch": 1, "n_parameters": 85978600}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Epoch: [2]  [  0/625]  eta: 0:26:42  lr: 0.000120  min_lr: 0.000000  loss: 2.4316 (2.4316)  class_acc: 0.6562 (0.6562)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  time: 2.5633  data: 1.9563  max mem: 25582
Epoch: [2]  [100/625]  eta: 0:05:25  lr: 0.000130  min_lr: 0.000000  loss: 2.4531 (2.4871)  class_acc: 0.6250 (0.6217)  loss_scale: 4096.0000 (4014.8911)  weight_decay: 0.0500 (0.0500)  time: 0.6001  data: 0.0002  max mem: 25582
Epoch: [2]  [200/625]  eta: 0:04:19  lr: 0.000139  min_lr: 0.000000  loss: 2.4434 (2.4639)  class_acc: 0.6289 (0.6262)  loss_scale: 2048.0000 (3036.3383)  weight_decay: 0.0500 (0.0500)  time: 0.6033  data: 0.0002  max mem: 25582
Epoch: [2]  [300/625]  eta: 0:03:17  lr: 0.000149  min_lr: 0.000000  loss: 2.3848 (2.4493)  class_acc: 0.6328 (0.6303)  loss_scale: 4096.0000 (3218.2857)  weight_decay: 0.0500 (0.0500)  time: 0.5971  data: 0.0002  max mem: 25582
Epoch: [2]  [400/625]  eta: 0:02:16  lr: 0.000158  min_lr: 0.000000  loss: 2.3633 (2.4339)  class_acc: 0.6484 (0.6323)  loss_scale: 4096.0000 (3518.8828)  weight_decay: 0.0500 (0.0500)  time: 0.6009  data: 0.0002  max mem: 25582
Epoch: [2]  [500/625]  eta: 0:01:15  lr: 0.000168  min_lr: 0.000000  loss: 2.3945 (2.4199)  class_acc: 0.6289 (0.6358)  loss_scale: 4096.0000 (3715.8323)  weight_decay: 0.0500 (0.0500)  time: 0.5990  data: 0.0002  max mem: 25582
Epoch: [2]  [600/625]  eta: 0:00:15  lr: 0.000178  min_lr: 0.000000  loss: 2.2969 (2.4076)  class_acc: 0.6719 (0.6392)  loss_scale: 4096.0000 (3833.6106)  weight_decay: 0.0500 (0.0500)  time: 0.6056  data: 0.0002  max mem: 25582
Epoch: [2]  [624/625]  eta: 0:00:00  lr: 0.000180  min_lr: 0.000000  loss: 2.3672 (2.4057)  class_acc: 0.6406 (0.6394)  loss_scale: 4096.0000 (3843.6864)  weight_decay: 0.0500 (0.0500)  time: 0.5899  data: 0.0006  max mem: 25582
Epoch: [2] Total time: 0:06:17 (0.6044 s / it)
Averaged stats: lr: 0.000180  min_lr: 0.000000  loss: 2.3672 (2.4169)  class_acc: 0.6406 (0.6365)  loss_scale: 4096.0000 (3843.6864)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:07  loss: 0.9116 (0.9116)  acc1: 79.2969 (79.2969)  acc5: 95.7031 (95.7031)  time: 2.6935  data: 2.4420  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.8034 (0.8444)  acc1: 80.0781 (79.6000)  acc5: 96.4844 (96.0000)  time: 0.2079  data: 0.0002  max mem: 25582
Test: Total time: 0:00:07 (0.3113 s / it)
* Acc@1 79.160 Acc@5 95.830 loss 0.855
Accuracy of the network on the 50000 test images: 79.2%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:05  loss: 6.2038 (6.2038)  acc1: 43.3594 (43.3594)  acc5: 73.0469 (73.0469)  time: 2.6302  data: 2.3940  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 6.1396 (6.1544)  acc1: 60.1562 (57.4560)  acc5: 85.5469 (84.5280)  time: 0.2087  data: 0.0024  max mem: 25582
Test: Total time: 0:00:07 (0.3099 s / it)
* Acc@1 57.496 Acc@5 84.766 loss 6.155
EMA Accuracy of the network on the 50000 test images: 57.5%
Max accuracy: 57.50%
{"train_lr": 0.00014997599615938545, "train_min_lr": 1.9587905955825232e-07, "train_loss": 2.416944921875, "train_class_acc": 0.636496875, "train_loss_scale": 3843.6864, "train_weight_decay": 0.050000000000000495, "test_loss": 6.154796228408814, "test_acc1": 57.49600001739502, "test_acc5": 84.76600002929688, "epoch": 2, "n_parameters": 85978600}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Epoch: [3]  [  0/625]  eta: 0:25:07  lr: 0.000180  min_lr: 0.000000  loss: 2.2363 (2.2363)  class_acc: 0.6797 (0.6797)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  time: 2.4122  data: 1.7935  max mem: 25582
Epoch: [3]  [100/625]  eta: 0:05:26  lr: 0.000190  min_lr: 0.000000  loss: 2.2852 (2.3151)  class_acc: 0.6602 (0.6604)  loss_scale: 8192.0000 (7664.7921)  weight_decay: 0.0500 (0.0500)  time: 0.6031  data: 0.0002  max mem: 25582
Epoch: [3]  [200/625]  eta: 0:04:19  lr: 0.000199  min_lr: 0.000000  loss: 2.2363 (2.3071)  class_acc: 0.6758 (0.6626)  loss_scale: 16384.0000 (10372.4577)  weight_decay: 0.0500 (0.0500)  time: 0.6010  data: 0.0002  max mem: 25582
Epoch: [3]  [300/625]  eta: 0:03:17  lr: 0.000209  min_lr: 0.000000  loss: 2.2695 (2.3045)  class_acc: 0.6602 (0.6626)  loss_scale: 8192.0000 (11335.4419)  weight_decay: 0.0500 (0.0500)  time: 0.6047  data: 0.0002  max mem: 25582
Epoch: [3]  [400/625]  eta: 0:02:16  lr: 0.000218  min_lr: 0.000000  loss: 2.2656 (2.2985)  class_acc: 0.6719 (0.6648)  loss_scale: 4096.0000 (9856.9576)  weight_decay: 0.0500 (0.0500)  time: 0.6035  data: 0.0002  max mem: 25582
Epoch: [3]  [500/625]  eta: 0:01:15  lr: 0.000228  min_lr: 0.000000  loss: 2.2520 (2.2948)  class_acc: 0.6719 (0.6654)  loss_scale: 8192.0000 (9025.9162)  weight_decay: 0.0500 (0.0500)  time: 0.6003  data: 0.0002  max mem: 25582
Epoch: [3]  [600/625]  eta: 0:00:15  lr: 0.000238  min_lr: 0.000000  loss: 2.2480 (2.2931)  class_acc: 0.6641 (0.6654)  loss_scale: 16384.0000 (9037.0982)  weight_decay: 0.0500 (0.0500)  time: 0.6010  data: 0.0002  max mem: 25582
Epoch: [3]  [624/625]  eta: 0:00:00  lr: 0.000240  min_lr: 0.000000  loss: 2.2617 (2.2920)  class_acc: 0.6680 (0.6655)  loss_scale: 16384.0000 (9319.2192)  weight_decay: 0.0500 (0.0500)  time: 0.5878  data: 0.0006  max mem: 25582
Epoch: [3] Total time: 0:06:18 (0.6053 s / it)
Averaged stats: lr: 0.000240  min_lr: 0.000000  loss: 2.2617 (2.2940)  class_acc: 0.6680 (0.6655)  loss_scale: 16384.0000 (9319.2192)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:03  loss: 0.8018 (0.8018)  acc1: 79.2969 (79.2969)  acc5: 96.8750 (96.8750)  time: 2.5568  data: 2.3095  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.7549 (0.7922)  acc1: 82.4219 (81.0880)  acc5: 96.4844 (96.0960)  time: 0.2072  data: 0.0002  max mem: 25582
Test: Total time: 0:00:07 (0.3047 s / it)
* Acc@1 80.696 Acc@5 96.054 loss 0.806
Accuracy of the network on the 50000 test images: 80.7%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:06  loss: 5.5831 (5.5831)  acc1: 54.6875 (54.6875)  acc5: 86.7188 (86.7188)  time: 2.6714  data: 2.4206  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 5.4940 (5.5184)  acc1: 68.7500 (67.1360)  acc5: 92.5781 (90.9600)  time: 0.2096  data: 0.0002  max mem: 25582
Test: Total time: 0:00:07 (0.3131 s / it)
* Acc@1 67.408 Acc@5 90.860 loss 5.519
EMA Accuracy of the network on the 50000 test images: 67.4%
Max accuracy: 67.41%
{"train_lr": 0.00020998559769563134, "train_min_lr": 2.7425576392695136e-07, "train_loss": 2.29400625, "train_class_acc": 0.66549296875, "train_loss_scale": 9319.2192, "train_weight_decay": 0.050000000000000495, "test_loss": 5.51872014760971, "test_acc1": 67.40800000244141, "test_acc5": 90.86000002319336, "epoch": 3, "n_parameters": 85978600}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Epoch: [4]  [  0/625]  eta: 0:24:29  lr: 0.000240  min_lr: 0.000000  loss: 2.3027 (2.3027)  class_acc: 0.6289 (0.6289)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 2.3505  data: 1.7571  max mem: 25582
Epoch: [4]  [100/625]  eta: 0:05:26  lr: 0.000250  min_lr: 0.000000  loss: 2.2188 (2.2290)  class_acc: 0.6953 (0.6823)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6096  data: 0.0003  max mem: 25582
Epoch: [4]  [200/625]  eta: 0:04:20  lr: 0.000259  min_lr: 0.000000  loss: 2.2871 (2.2279)  class_acc: 0.6758 (0.6826)  loss_scale: 8192.0000 (8884.8557)  weight_decay: 0.0500 (0.0500)  time: 0.6046  data: 0.0002  max mem: 25582
Epoch: [4]  [300/625]  eta: 0:03:17  lr: 0.000269  min_lr: 0.000000  loss: 2.2285 (2.2280)  class_acc: 0.6719 (0.6821)  loss_scale: 8192.0000 (8763.5349)  weight_decay: 0.0500 (0.0500)  time: 0.6012  data: 0.0002  max mem: 25582
Epoch: [4]  [400/625]  eta: 0:02:16  lr: 0.000278  min_lr: 0.000000  loss: 2.2227 (2.2269)  class_acc: 0.6797 (0.6820)  loss_scale: 8192.0000 (8621.0075)  weight_decay: 0.0500 (0.0500)  time: 0.6000  data: 0.0002  max mem: 25582
Epoch: [4]  [500/625]  eta: 0:01:15  lr: 0.000288  min_lr: 0.000000  loss: 2.1816 (2.2232)  class_acc: 0.6836 (0.6823)  loss_scale: 16384.0000 (10056.0479)  weight_decay: 0.0500 (0.0500)  time: 0.6060  data: 0.0002  max mem: 25582
Epoch: [4]  [600/625]  eta: 0:00:15  lr: 0.000298  min_lr: 0.000000  loss: 2.2188 (2.2199)  class_acc: 0.6914 (0.6832)  loss_scale: 16384.0000 (11217.9967)  weight_decay: 0.0500 (0.0500)  time: 0.6042  data: 0.0002  max mem: 25582
Epoch: [4]  [624/625]  eta: 0:00:00  lr: 0.000300  min_lr: 0.000000  loss: 2.2246 (2.2198)  class_acc: 0.6836 (0.6833)  loss_scale: 8192.0000 (11114.9056)  weight_decay: 0.0500 (0.0500)  time: 0.5919  data: 0.0006  max mem: 25582
Epoch: [4] Total time: 0:06:18 (0.6059 s / it)
Averaged stats: lr: 0.000300  min_lr: 0.000000  loss: 2.2246 (2.2236)  class_acc: 0.6836 (0.6824)  loss_scale: 8192.0000 (11114.9056)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:00:58  loss: 0.8199 (0.8199)  acc1: 81.2500 (81.2500)  acc5: 96.0938 (96.0938)  time: 2.3266  data: 2.0976  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.7630 (0.7757)  acc1: 82.4219 (81.3600)  acc5: 96.4844 (96.4160)  time: 0.2056  data: 0.0002  max mem: 25582
Test: Total time: 0:00:07 (0.2954 s / it)
* Acc@1 81.182 Acc@5 96.246 loss 0.789
Accuracy of the network on the 50000 test images: 81.2%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:03  loss: 4.8216 (4.8216)  acc1: 65.2344 (65.2344)  acc5: 90.2344 (90.2344)  time: 2.5463  data: 2.3052  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 4.7172 (4.7502)  acc1: 73.8281 (72.8320)  acc5: 93.7500 (93.3120)  time: 0.2163  data: 0.0075  max mem: 25582
Test: Total time: 0:00:07 (0.3131 s / it)
* Acc@1 73.140 Acc@5 93.482 loss 4.751
EMA Accuracy of the network on the 50000 test images: 73.1%
Max accuracy: 73.14%
{"train_lr": 0.00026999519923187707, "train_min_lr": 3.526324682956502e-07, "train_loss": 2.2235859375, "train_class_acc": 0.68236796875, "train_loss_scale": 11114.9056, "train_weight_decay": 0.050000000000000495, "test_loss": 4.750632240772247, "test_acc1": 73.14000003540039, "test_acc5": 93.4820000189209, "epoch": 4, "n_parameters": 85978600}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Epoch: [5]  [  0/625]  eta: 0:27:17  lr: 0.000300  min_lr: 0.000000  loss: 2.2246 (2.2246)  class_acc: 0.6914 (0.6914)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 2.6196  data: 1.9891  max mem: 25582
Epoch: [5]  [100/625]  eta: 0:05:25  lr: 0.000310  min_lr: 0.000000  loss: 2.1699 (2.1646)  class_acc: 0.7109 (0.7001)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5996  data: 0.0002  max mem: 25582
Epoch: [5]  [200/625]  eta: 0:04:19  lr: 0.000319  min_lr: 0.000000  loss: 2.1445 (2.1712)  class_acc: 0.6992 (0.6978)  loss_scale: 16384.0000 (12063.8408)  weight_decay: 0.0500 (0.0500)  time: 0.6023  data: 0.0013  max mem: 25582
Epoch: [5]  [300/625]  eta: 0:03:17  lr: 0.000329  min_lr: 0.000000  loss: 2.1621 (2.1748)  class_acc: 0.6953 (0.6958)  loss_scale: 8192.0000 (11022.4585)  weight_decay: 0.0500 (0.0500)  time: 0.6047  data: 0.0002  max mem: 25582
Epoch: [5]  [400/625]  eta: 0:02:16  lr: 0.000338  min_lr: 0.000000  loss: 2.1406 (2.1734)  class_acc: 0.6914 (0.6953)  loss_scale: 8192.0000 (11072.4788)  weight_decay: 0.0500 (0.0500)  time: 0.6005  data: 0.0002  max mem: 25582
Epoch: [5]  [500/625]  eta: 0:01:15  lr: 0.000348  min_lr: 0.000000  loss: 2.1758 (2.1757)  class_acc: 0.6914 (0.6952)  loss_scale: 8192.0000 (10497.5329)  weight_decay: 0.0500 (0.0500)  time: 0.6041  data: 0.0002  max mem: 25582
Epoch: [5]  [600/625]  eta: 0:00:15  lr: 0.000358  min_lr: 0.000000  loss: 2.1465 (2.1726)  class_acc: 0.7070 (0.6962)  loss_scale: 16384.0000 (11422.4559)  weight_decay: 0.0500 (0.0500)  time: 0.6013  data: 0.0002  max mem: 25582
Epoch: [5]  [624/625]  eta: 0:00:00  lr: 0.000360  min_lr: 0.000000  loss: 2.1602 (2.1721)  class_acc: 0.6953 (0.6962)  loss_scale: 16384.0000 (11612.9792)  weight_decay: 0.0500 (0.0500)  time: 0.5876  data: 0.0006  max mem: 25582
Epoch: [5] Total time: 0:06:17 (0.6032 s / it)
Averaged stats: lr: 0.000360  min_lr: 0.000000  loss: 2.1602 (2.1789)  class_acc: 0.6953 (0.6935)  loss_scale: 16384.0000 (11612.9792)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:04  loss: 0.7520 (0.7520)  acc1: 83.9844 (83.9844)  acc5: 96.8750 (96.8750)  time: 2.5863  data: 2.3520  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.7028 (0.7653)  acc1: 82.0312 (81.4080)  acc5: 96.8750 (96.5920)  time: 0.2180  data: 0.0095  max mem: 25582
Test: Total time: 0:00:07 (0.3150 s / it)
* Acc@1 81.348 Acc@5 96.488 loss 0.773
Accuracy of the network on the 50000 test images: 81.3%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:07  loss: 4.0092 (4.0092)  acc1: 69.5312 (69.5312)  acc5: 93.7500 (93.7500)  time: 2.7074  data: 2.4630  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 3.8933 (3.9358)  acc1: 78.3019 (75.9840)  acc5: 95.3125 (94.8000)  time: 0.2099  data: 0.0002  max mem: 25582
Test: Total time: 0:00:07 (0.3155 s / it)
* Acc@1 76.434 Acc@5 94.956 loss 3.937
EMA Accuracy of the network on the 50000 test images: 76.4%
Max accuracy: 76.43%
{"train_lr": 0.00033000480076812277, "train_min_lr": 4.310091726643493e-07, "train_loss": 2.1788927734375, "train_class_acc": 0.6935421875, "train_loss_scale": 11612.9792, "train_weight_decay": 0.050000000000000495, "test_loss": 3.936923360824585, "test_acc1": 76.43400001800538, "test_acc5": 94.95600001708985, "epoch": 5, "n_parameters": 85978600}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Epoch: [6]  [  0/625]  eta: 0:27:43  lr: 0.000360  min_lr: 0.000000  loss: 2.3848 (2.3848)  class_acc: 0.6328 (0.6328)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 2.6613  data: 2.0511  max mem: 25582
Epoch: [6]  [100/625]  eta: 0:05:27  lr: 0.000370  min_lr: 0.000000  loss: 2.1602 (2.1541)  class_acc: 0.7070 (0.7014)  loss_scale: 16384.0000 (18006.1782)  weight_decay: 0.0500 (0.0500)  time: 0.6039  data: 0.0002  max mem: 25582
Epoch: [6]  [200/625]  eta: 0:04:21  lr: 0.000379  min_lr: 0.000000  loss: 2.1133 (2.1360)  class_acc: 0.7109 (0.7049)  loss_scale: 8192.0000 (13449.5522)  weight_decay: 0.0500 (0.0500)  time: 0.6052  data: 0.0002  max mem: 25582
Epoch: [6]  [300/625]  eta: 0:03:18  lr: 0.000389  min_lr: 0.000001  loss: 2.0879 (2.1327)  class_acc: 0.7148 (0.7053)  loss_scale: 4096.0000 (10695.8671)  weight_decay: 0.0500 (0.0500)  time: 0.6054  data: 0.0002  max mem: 25582
Epoch: [6]  [400/625]  eta: 0:02:17  lr: 0.000398  min_lr: 0.000001  loss: 2.1406 (2.1353)  class_acc: 0.6953 (0.7033)  loss_scale: 8192.0000 (9509.6658)  weight_decay: 0.0500 (0.0500)  time: 0.6074  data: 0.0002  max mem: 25582
Epoch: [6]  [500/625]  eta: 0:01:16  lr: 0.000408  min_lr: 0.000001  loss: 2.1660 (2.1414)  class_acc: 0.6875 (0.7021)  loss_scale: 16384.0000 (9524.6307)  weight_decay: 0.0500 (0.0500)  time: 0.6013  data: 0.0002  max mem: 25582
Epoch: [6]  [600/625]  eta: 0:00:15  lr: 0.000418  min_lr: 0.000001  loss: 2.1699 (2.1440)  class_acc: 0.6953 (0.7016)  loss_scale: 8192.0000 (10093.4709)  weight_decay: 0.0500 (0.0500)  time: 0.6023  data: 0.0002  max mem: 25582
Epoch: [6]  [624/625]  eta: 0:00:00  lr: 0.000420  min_lr: 0.000001  loss: 2.1504 (2.1437)  class_acc: 0.6953 (0.7015)  loss_scale: 8192.0000 (10020.4544)  weight_decay: 0.0500 (0.0500)  time: 0.5896  data: 0.0006  max mem: 25582
Epoch: [6] Total time: 0:06:20 (0.6080 s / it)
Averaged stats: lr: 0.000420  min_lr: 0.000001  loss: 2.1504 (2.1425)  class_acc: 0.6953 (0.7024)  loss_scale: 8192.0000 (10020.4544)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:03  loss: 0.7762 (0.7762)  acc1: 81.2500 (81.2500)  acc5: 96.8750 (96.8750)  time: 2.5262  data: 2.2849  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.6876 (0.7513)  acc1: 82.8125 (82.0800)  acc5: 97.2656 (96.5440)  time: 0.2090  data: 0.0002  max mem: 25582
Test: Total time: 0:00:07 (0.3060 s / it)
* Acc@1 81.464 Acc@5 96.566 loss 0.764
Accuracy of the network on the 50000 test images: 81.5%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:08  loss: 3.2270 (3.2270)  acc1: 75.3906 (75.3906)  acc5: 94.9219 (94.9219)  time: 2.7492  data: 2.5025  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 3.0928 (3.1558)  acc1: 79.6875 (78.1920)  acc5: 96.0938 (95.6480)  time: 0.2091  data: 0.0038  max mem: 25582
Test: Total time: 0:00:07 (0.3154 s / it)
* Acc@1 78.752 Acc@5 95.700 loss 3.158
EMA Accuracy of the network on the 50000 test images: 78.8%
Max accuracy: 78.75%
{"train_lr": 0.00039001440230436863, "train_min_lr": 5.093858770330484e-07, "train_loss": 2.1424705078125, "train_class_acc": 0.70240859375, "train_loss_scale": 10020.4544, "train_weight_decay": 0.050000000000000495, "test_loss": 3.1583117711544038, "test_acc1": 78.75200003540039, "test_acc5": 95.70000001739501, "epoch": 6, "n_parameters": 85978600}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Epoch: [7]  [  0/625]  eta: 0:25:26  lr: 0.000420  min_lr: 0.000001  loss: 2.0977 (2.0977)  class_acc: 0.7266 (0.7266)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 2.4419  data: 1.8328  max mem: 25582
Epoch: [7]  [100/625]  eta: 0:05:25  lr: 0.000430  min_lr: 0.000001  loss: 2.1055 (2.0925)  class_acc: 0.7070 (0.7136)  loss_scale: 8192.0000 (8840.8713)  weight_decay: 0.0500 (0.0500)  time: 0.6037  data: 0.0002  max mem: 25582
Epoch: [7]  [200/625]  eta: 0:04:19  lr: 0.000439  min_lr: 0.000001  loss: 2.1191 (2.0991)  class_acc: 0.7031 (0.7116)  loss_scale: 8192.0000 (8558.8060)  weight_decay: 0.0500 (0.0500)  time: 0.6017  data: 0.0002  max mem: 25582
Epoch: [7]  [300/625]  eta: 0:03:17  lr: 0.000449  min_lr: 0.000001  loss: 2.0625 (2.1023)  class_acc: 0.7188 (0.7113)  loss_scale: 16384.0000 (11158.5382)  weight_decay: 0.0500 (0.0500)  time: 0.6009  data: 0.0002  max mem: 25582
Epoch: [7]  [400/625]  eta: 0:02:16  lr: 0.000458  min_lr: 0.000001  loss: 2.1230 (2.1094)  class_acc: 0.7070 (0.7099)  loss_scale: 8192.0000 (10582.1845)  weight_decay: 0.0500 (0.0500)  time: 0.6091  data: 0.0002  max mem: 25582
Epoch: [7]  [500/625]  eta: 0:01:15  lr: 0.000468  min_lr: 0.000001  loss: 2.1172 (2.1148)  class_acc: 0.7070 (0.7087)  loss_scale: 16384.0000 (11135.2335)  weight_decay: 0.0500 (0.0500)  time: 0.5992  data: 0.0002  max mem: 25582
Epoch: [7]  [600/625]  eta: 0:00:15  lr: 0.000478  min_lr: 0.000001  loss: 2.1035 (2.1148)  class_acc: 0.7109 (0.7083)  loss_scale: 8192.0000 (10904.4925)  weight_decay: 0.0500 (0.0500)  time: 0.6025  data: 0.0002  max mem: 25582
Epoch: [7]  [624/625]  eta: 0:00:00  lr: 0.000480  min_lr: 0.000001  loss: 2.1113 (2.1147)  class_acc: 0.7031 (0.7083)  loss_scale: 8192.0000 (10800.3328)  weight_decay: 0.0500 (0.0500)  time: 0.5904  data: 0.0006  max mem: 25582
Epoch: [7] Total time: 0:06:18 (0.6053 s / it)
Averaged stats: lr: 0.000480  min_lr: 0.000001  loss: 2.1113 (2.1152)  class_acc: 0.7031 (0.7091)  loss_scale: 8192.0000 (10800.3328)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:05  loss: 0.7615 (0.7615)  acc1: 82.8125 (82.8125)  acc5: 97.2656 (97.2656)  time: 2.6050  data: 2.3582  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.6759 (0.7382)  acc1: 82.4219 (82.1120)  acc5: 97.2656 (96.8640)  time: 0.2100  data: 0.0023  max mem: 25582
Test: Total time: 0:00:07 (0.3108 s / it)
* Acc@1 81.676 Acc@5 96.590 loss 0.750
Accuracy of the network on the 50000 test images: 81.7%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:05  loss: 2.5313 (2.5313)  acc1: 77.3438 (77.3438)  acc5: 96.0938 (96.0938)  time: 2.6366  data: 2.3825  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 2.4066 (2.4625)  acc1: 81.6406 (79.8240)  acc5: 96.4844 (96.1760)  time: 0.2106  data: 0.0013  max mem: 25582
Test: Total time: 0:00:07 (0.3115 s / it)
* Acc@1 80.188 Acc@5 96.216 loss 2.467
EMA Accuracy of the network on the 50000 test images: 80.2%
Max accuracy: 80.19%
{"train_lr": 0.00045002400384061466, "train_min_lr": 5.877625814017474e-07, "train_loss": 2.1152021484375, "train_class_acc": 0.70909296875, "train_loss_scale": 10800.3328, "train_weight_decay": 0.050000000000000495, "test_loss": 2.4665425074100495, "test_acc1": 80.18800003601075, "test_acc5": 96.21600001220703, "epoch": 7, "n_parameters": 85978600}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Epoch: [8]  [  0/625]  eta: 0:25:38  lr: 0.000480  min_lr: 0.000001  loss: 1.9443 (1.9443)  class_acc: 0.7344 (0.7344)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 2.4614  data: 1.8421  max mem: 25582
Epoch: [8]  [100/625]  eta: 0:05:25  lr: 0.000490  min_lr: 0.000001  loss: 2.0508 (2.0688)  class_acc: 0.7148 (0.7219)  loss_scale: 16384.0000 (14437.3861)  weight_decay: 0.0500 (0.0500)  time: 0.6045  data: 0.0002  max mem: 25582
Epoch: [8]  [200/625]  eta: 0:04:20  lr: 0.000499  min_lr: 0.000001  loss: 2.0723 (2.0767)  class_acc: 0.7227 (0.7205)  loss_scale: 8192.0000 (11656.2786)  weight_decay: 0.0500 (0.0500)  time: 0.6071  data: 0.0002  max mem: 25582
Epoch: [8]  [300/625]  eta: 0:03:17  lr: 0.000509  min_lr: 0.000001  loss: 2.0938 (2.0808)  class_acc: 0.7188 (0.7191)  loss_scale: 16384.0000 (12219.9601)  weight_decay: 0.0500 (0.0500)  time: 0.6017  data: 0.0002  max mem: 25582
Epoch: [8]  [400/625]  eta: 0:02:16  lr: 0.000518  min_lr: 0.000001  loss: 2.1133 (2.0900)  class_acc: 0.7148 (0.7169)  loss_scale: 8192.0000 (12502.5037)  weight_decay: 0.0500 (0.0500)  time: 0.6033  data: 0.0002  max mem: 25582
Epoch: [8]  [500/625]  eta: 0:01:15  lr: 0.000528  min_lr: 0.000001  loss: 2.1152 (2.0949)  class_acc: 0.7031 (0.7156)  loss_scale: 8192.0000 (11772.9341)  weight_decay: 0.0500 (0.0500)  time: 0.6022  data: 0.0003  max mem: 25582
Epoch: [8]  [600/625]  eta: 0:00:15  lr: 0.000538  min_lr: 0.000001  loss: 2.0859 (2.0945)  class_acc: 0.7148 (0.7153)  loss_scale: 16384.0000 (12485.6439)  weight_decay: 0.0500 (0.0500)  time: 0.6039  data: 0.0002  max mem: 25582
Epoch: [8]  [624/625]  eta: 0:00:00  lr: 0.000540  min_lr: 0.000001  loss: 2.1426 (2.0970)  class_acc: 0.7148 (0.7148)  loss_scale: 8192.0000 (12320.7680)  weight_decay: 0.0500 (0.0500)  time: 0.5879  data: 0.0007  max mem: 25582
Epoch: [8] Total time: 0:06:19 (0.6066 s / it)
Averaged stats: lr: 0.000540  min_lr: 0.000001  loss: 2.1426 (2.0942)  class_acc: 0.7148 (0.7146)  loss_scale: 8192.0000 (12320.7680)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:01  loss: 0.7598 (0.7598)  acc1: 81.2500 (81.2500)  acc5: 96.8750 (96.8750)  time: 2.4458  data: 2.2056  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.7241 (0.7546)  acc1: 82.4219 (81.9840)  acc5: 96.8750 (96.5920)  time: 0.2113  data: 0.0017  max mem: 25582
Test: Total time: 0:00:07 (0.3050 s / it)
* Acc@1 81.774 Acc@5 96.470 loss 0.762
Accuracy of the network on the 50000 test images: 81.8%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:07  loss: 1.9531 (1.9531)  acc1: 78.5156 (78.5156)  acc5: 97.2656 (97.2656)  time: 2.7041  data: 2.4514  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 1.8283 (1.8904)  acc1: 83.2031 (80.9440)  acc5: 96.4844 (96.4800)  time: 0.2097  data: 0.0007  max mem: 25582
Test: Total time: 0:00:07 (0.3145 s / it)
* Acc@1 81.314 Acc@5 96.582 loss 1.896
EMA Accuracy of the network on the 50000 test images: 81.3%
Max accuracy: 81.31%
{"train_lr": 0.0005100336053768605, "train_min_lr": 6.661392857704459e-07, "train_loss": 2.094219140625, "train_class_acc": 0.7145578125, "train_loss_scale": 12320.768, "train_weight_decay": 0.050000000000000495, "test_loss": 1.896189449429512, "test_acc1": 81.31400003356934, "test_acc5": 96.58200000976562, "epoch": 8, "n_parameters": 85978600}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Epoch: [9]  [  0/625]  eta: 0:25:15  lr: 0.000540  min_lr: 0.000001  loss: 1.9678 (1.9678)  class_acc: 0.7539 (0.7539)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 2.4247  data: 1.8153  max mem: 25582
Epoch: [9]  [100/625]  eta: 0:05:25  lr: 0.000550  min_lr: 0.000001  loss: 2.0566 (2.0596)  class_acc: 0.7188 (0.7227)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6029  data: 0.0002  max mem: 25582
Epoch: [9]  [200/625]  eta: 0:04:19  lr: 0.000559  min_lr: 0.000001  loss: 2.1250 (2.0702)  class_acc: 0.7109 (0.7207)  loss_scale: 8192.0000 (8721.8308)  weight_decay: 0.0500 (0.0500)  time: 0.6021  data: 0.0002  max mem: 25582
Epoch: [9]  [300/625]  eta: 0:03:17  lr: 0.000569  min_lr: 0.000001  loss: 2.0684 (2.0692)  class_acc: 0.7109 (0.7208)  loss_scale: 16384.0000 (10124.3322)  weight_decay: 0.0500 (0.0500)  time: 0.6038  data: 0.0002  max mem: 25582
Epoch: [9]  [400/625]  eta: 0:02:16  lr: 0.000578  min_lr: 0.000001  loss: 2.0742 (2.0695)  class_acc: 0.7227 (0.7209)  loss_scale: 16384.0000 (11767.0623)  weight_decay: 0.0500 (0.0500)  time: 0.6029  data: 0.0002  max mem: 25582
Epoch: [9]  [500/625]  eta: 0:01:15  lr: 0.000588  min_lr: 0.000001  loss: 2.0820 (2.0687)  class_acc: 0.7031 (0.7208)  loss_scale: 8192.0000 (11789.2854)  weight_decay: 0.0500 (0.0500)  time: 0.6027  data: 0.0002  max mem: 25582
Epoch: [9]  [600/625]  eta: 0:00:15  lr: 0.000598  min_lr: 0.000001  loss: 2.1133 (2.0700)  class_acc: 0.7070 (0.7209)  loss_scale: 8192.0000 (11313.4110)  weight_decay: 0.0500 (0.0500)  time: 0.6043  data: 0.0002  max mem: 25582
Epoch: [9]  [624/625]  eta: 0:00:00  lr: 0.000600  min_lr: 0.000001  loss: 2.0645 (2.0710)  class_acc: 0.7109 (0.7205)  loss_scale: 8192.0000 (11193.5488)  weight_decay: 0.0500 (0.0500)  time: 0.5887  data: 0.0006  max mem: 25582
Epoch: [9] Total time: 0:06:18 (0.6053 s / it)
Averaged stats: lr: 0.000600  min_lr: 0.000001  loss: 2.0645 (2.0776)  class_acc: 0.7109 (0.7183)  loss_scale: 8192.0000 (11193.5488)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:00:59  loss: 0.7210 (0.7210)  acc1: 83.2031 (83.2031)  acc5: 97.2656 (97.2656)  time: 2.3929  data: 2.1617  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.7153 (0.7472)  acc1: 83.0189 (82.1440)  acc5: 97.6562 (97.0080)  time: 0.2052  data: 0.0001  max mem: 25582
Test: Total time: 0:00:07 (0.2987 s / it)
* Acc@1 81.752 Acc@5 96.500 loss 0.770
Accuracy of the network on the 50000 test images: 81.8%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:08  loss: 1.5148 (1.5148)  acc1: 79.2969 (79.2969)  acc5: 97.2656 (97.2656)  time: 2.7274  data: 2.4888  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 1.4049 (1.4585)  acc1: 83.9844 (81.8560)  acc5: 96.8750 (96.7040)  time: 0.2146  data: 0.0041  max mem: 25582
Test: Total time: 0:00:07 (0.3193 s / it)
* Acc@1 82.090 Acc@5 96.856 loss 1.466
EMA Accuracy of the network on the 50000 test images: 82.1%
Max accuracy: 82.09%
{"train_lr": 0.000570043206913106, "train_min_lr": 7.445159901391449e-07, "train_loss": 2.0776224609375, "train_class_acc": 0.71828046875, "train_loss_scale": 11193.5488, "train_weight_decay": 0.050000000000000495, "test_loss": 1.465642710328102, "test_acc1": 82.09000003112793, "test_acc5": 96.85600001525879, "epoch": 9, "n_parameters": 85978600}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Epoch: [10]  [  0/625]  eta: 0:26:46  lr: 0.000600  min_lr: 0.000001  loss: 2.0605 (2.0605)  class_acc: 0.7227 (0.7227)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 2.5699  data: 1.9562  max mem: 25582
Epoch: [10]  [100/625]  eta: 0:05:26  lr: 0.000600  min_lr: 0.000001  loss: 2.0156 (2.0384)  class_acc: 0.7344 (0.7297)  loss_scale: 16384.0000 (9246.4158)  weight_decay: 0.0500 (0.0500)  time: 0.6028  data: 0.0002  max mem: 25582
Epoch: [10]  [200/625]  eta: 0:04:20  lr: 0.000600  min_lr: 0.000001  loss: 2.0605 (2.0511)  class_acc: 0.7188 (0.7258)  loss_scale: 16384.0000 (12797.4527)  weight_decay: 0.0500 (0.0500)  time: 0.5985  data: 0.0003  max mem: 25582
Epoch: [10]  [300/625]  eta: 0:03:17  lr: 0.000600  min_lr: 0.000001  loss: 2.0332 (2.0512)  class_acc: 0.7188 (0.7252)  loss_scale: 16384.0000 (14043.4286)  weight_decay: 0.0500 (0.0500)  time: 0.5994  data: 0.0002  max mem: 25582
Epoch: [10]  [400/625]  eta: 0:02:16  lr: 0.000600  min_lr: 0.000001  loss: 2.0547 (2.0506)  class_acc: 0.7227 (0.7257)  loss_scale: 16384.0000 (15239.9800)  weight_decay: 0.0500 (0.0500)  time: 0.6042  data: 0.0002  max mem: 25582
Epoch: [10]  [500/625]  eta: 0:01:15  lr: 0.000599  min_lr: 0.000001  loss: 2.0566 (2.0532)  class_acc: 0.7188 (0.7254)  loss_scale: 8192.0000 (14797.9242)  weight_decay: 0.0500 (0.0500)  time: 0.6033  data: 0.0002  max mem: 25582
Epoch: [10]  [600/625]  eta: 0:00:15  lr: 0.000599  min_lr: 0.000001  loss: 2.0488 (2.0516)  class_acc: 0.7305 (0.7259)  loss_scale: 16384.0000 (13862.3361)  weight_decay: 0.0500 (0.0500)  time: 0.5963  data: 0.0002  max mem: 25582
Epoch: [10]  [624/625]  eta: 0:00:00  lr: 0.000599  min_lr: 0.000001  loss: 2.0059 (2.0511)  class_acc: 0.7188 (0.7258)  loss_scale: 8192.0000 (13670.8096)  weight_decay: 0.0500 (0.0500)  time: 0.5875  data: 0.0006  max mem: 25582
Epoch: [10] Total time: 0:06:17 (0.6041 s / it)
Averaged stats: lr: 0.000599  min_lr: 0.000001  loss: 2.0059 (2.0553)  class_acc: 0.7188 (0.7240)  loss_scale: 8192.0000 (13670.8096)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:09  loss: 0.7552 (0.7552)  acc1: 82.0312 (82.0312)  acc5: 96.4844 (96.4844)  time: 2.7932  data: 2.5385  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.7220 (0.7424)  acc1: 82.4219 (82.1440)  acc5: 97.2656 (96.7040)  time: 0.2092  data: 0.0002  max mem: 25582
Test: Total time: 0:00:07 (0.3179 s / it)
* Acc@1 82.018 Acc@5 96.516 loss 0.755
Accuracy of the network on the 50000 test images: 82.0%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:05  loss: 1.2056 (1.2056)  acc1: 79.6875 (79.6875)  acc5: 96.8750 (96.8750)  time: 2.6142  data: 2.3812  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 1.1012 (1.1573)  acc1: 83.5938 (82.6240)  acc5: 97.2656 (96.9600)  time: 0.2154  data: 0.0081  max mem: 25582
Test: Total time: 0:00:07 (0.3171 s / it)
* Acc@1 82.734 Acc@5 97.048 loss 1.165
EMA Accuracy of the network on the 50000 test images: 82.7%
Max accuracy: 82.73%
{"train_lr": 0.0005996929220710539, "train_min_lr": 7.832405758730968e-07, "train_loss": 2.0552744140625, "train_class_acc": 0.7240265625, "train_loss_scale": 13670.8096, "train_weight_decay": 0.050000000000000495, "test_loss": 1.1651768630743027, "test_acc1": 82.73400006103516, "test_acc5": 97.04800002868653, "epoch": 10, "n_parameters": 85978600}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Epoch: [11]  [  0/625]  eta: 0:25:06  lr: 0.000599  min_lr: 0.000001  loss: 2.1113 (2.1113)  class_acc: 0.7109 (0.7109)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 2.4097  data: 1.7969  max mem: 25582
Epoch: [11]  [100/625]  eta: 0:05:25  lr: 0.000599  min_lr: 0.000001  loss: 2.0215 (2.0210)  class_acc: 0.7305 (0.7321)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6024  data: 0.0002  max mem: 25582
Epoch: [11]  [200/625]  eta: 0:04:19  lr: 0.000598  min_lr: 0.000001  loss: 1.9971 (2.0149)  class_acc: 0.7305 (0.7353)  loss_scale: 16384.0000 (11737.7910)  weight_decay: 0.0500 (0.0500)  time: 0.6017  data: 0.0002  max mem: 25582
Epoch: [11]  [300/625]  eta: 0:03:17  lr: 0.000598  min_lr: 0.000001  loss: 2.0254 (2.0137)  class_acc: 0.7227 (0.7353)  loss_scale: 8192.0000 (10559.7874)  weight_decay: 0.0500 (0.0500)  time: 0.6010  data: 0.0002  max mem: 25582
Epoch: [11]  [400/625]  eta: 0:02:16  lr: 0.000598  min_lr: 0.000001  loss: 2.0195 (2.0171)  class_acc: 0.7383 (0.7346)  loss_scale: 8192.0000 (10541.3267)  weight_decay: 0.0500 (0.0500)  time: 0.6031  data: 0.0002  max mem: 25582
Epoch: [11]  [500/625]  eta: 0:01:15  lr: 0.000597  min_lr: 0.000001  loss: 2.0371 (2.0198)  class_acc: 0.7266 (0.7339)  loss_scale: 16384.0000 (10415.7764)  weight_decay: 0.0500 (0.0500)  time: 0.5985  data: 0.0002  max mem: 25582
Epoch: [11]  [600/625]  eta: 0:00:15  lr: 0.000596  min_lr: 0.000001  loss: 2.0430 (2.0210)  class_acc: 0.7227 (0.7333)  loss_scale: 16384.0000 (11408.8253)  weight_decay: 0.0500 (0.0500)  time: 0.5995  data: 0.0002  max mem: 25582
Epoch: [11]  [624/625]  eta: 0:00:00  lr: 0.000596  min_lr: 0.000001  loss: 1.9795 (2.0204)  class_acc: 0.7344 (0.7333)  loss_scale: 16384.0000 (11678.5152)  weight_decay: 0.0500 (0.0500)  time: 0.5864  data: 0.0006  max mem: 25582
Epoch: [11] Total time: 0:06:17 (0.6044 s / it)
Averaged stats: lr: 0.000596  min_lr: 0.000001  loss: 1.9795 (2.0258)  class_acc: 0.7344 (0.7315)  loss_scale: 16384.0000 (11678.5152)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:08  loss: 0.7752 (0.7752)  acc1: 80.0781 (80.0781)  acc5: 97.2656 (97.2656)  time: 2.7503  data: 2.5119  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.6689 (0.7111)  acc1: 84.7656 (83.0240)  acc5: 97.2656 (96.7680)  time: 0.2094  data: 0.0011  max mem: 25582
Test: Total time: 0:00:07 (0.3152 s / it)
* Acc@1 82.474 Acc@5 96.612 loss 0.732
Accuracy of the network on the 50000 test images: 82.5%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:10  loss: 0.9974 (0.9974)  acc1: 81.2500 (81.2500)  acc5: 96.8750 (96.8750)  time: 2.8256  data: 2.5793  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.8930 (0.9563)  acc1: 83.2031 (83.2320)  acc5: 97.2656 (97.1680)  time: 0.2112  data: 0.0043  max mem: 25582
Test: Total time: 0:00:08 (0.3215 s / it)
* Acc@1 83.248 Acc@5 97.218 loss 0.965
EMA Accuracy of the network on the 50000 test images: 83.2%
Max accuracy: 83.25%
{"train_lr": 0.0005978497754284178, "train_min_lr": 7.808332984404872e-07, "train_loss": 2.0257587890625, "train_class_acc": 0.7315375, "train_loss_scale": 11678.5152, "train_weight_decay": 0.050000000000000495, "test_loss": 0.9651677197217942, "test_acc1": 83.24800005828857, "test_acc5": 97.2180000314331, "epoch": 11, "n_parameters": 85978600}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Epoch: [12]  [  0/625]  eta: 0:24:29  lr: 0.000596  min_lr: 0.000001  loss: 2.1113 (2.1113)  class_acc: 0.6797 (0.6797)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 2.3506  data: 1.7302  max mem: 25582
Epoch: [12]  [100/625]  eta: 0:05:24  lr: 0.000596  min_lr: 0.000001  loss: 1.9404 (1.9882)  class_acc: 0.7461 (0.7416)  loss_scale: 8192.0000 (13464.0792)  weight_decay: 0.0500 (0.0500)  time: 0.6000  data: 0.0002  max mem: 25582
Epoch: [12]  [200/625]  eta: 0:04:18  lr: 0.000595  min_lr: 0.000001  loss: 2.0137 (1.9929)  class_acc: 0.7422 (0.7401)  loss_scale: 8192.0000 (11126.4478)  weight_decay: 0.0500 (0.0500)  time: 0.5994  data: 0.0003  max mem: 25582
Epoch: [12]  [300/625]  eta: 0:03:17  lr: 0.000594  min_lr: 0.000001  loss: 1.9717 (1.9956)  class_acc: 0.7422 (0.7391)  loss_scale: 8192.0000 (11539.5615)  weight_decay: 0.0500 (0.0500)  time: 0.5994  data: 0.0002  max mem: 25582
Epoch: [12]  [400/625]  eta: 0:02:16  lr: 0.000594  min_lr: 0.000001  loss: 1.9775 (2.0004)  class_acc: 0.7305 (0.7384)  loss_scale: 16384.0000 (10949.9052)  weight_decay: 0.0500 (0.0500)  time: 0.6023  data: 0.0002  max mem: 25582
Epoch: [12]  [500/625]  eta: 0:01:15  lr: 0.000593  min_lr: 0.000001  loss: 2.0098 (2.0012)  class_acc: 0.7344 (0.7382)  loss_scale: 8192.0000 (10399.4251)  weight_decay: 0.0500 (0.0500)  time: 0.6050  data: 0.0002  max mem: 25582
Epoch: [12]  [600/625]  eta: 0:00:15  lr: 0.000592  min_lr: 0.000001  loss: 1.9854 (2.0009)  class_acc: 0.7461 (0.7379)  loss_scale: 8192.0000 (10359.2679)  weight_decay: 0.0500 (0.0500)  time: 0.6014  data: 0.0003  max mem: 25582
Epoch: [12]  [624/625]  eta: 0:00:00  lr: 0.000592  min_lr: 0.000001  loss: 2.0020 (2.0010)  class_acc: 0.7266 (0.7376)  loss_scale: 8192.0000 (10276.0448)  weight_decay: 0.0500 (0.0500)  time: 0.5889  data: 0.0007  max mem: 25582
Epoch: [12] Total time: 0:06:17 (0.6042 s / it)
Averaged stats: lr: 0.000592  min_lr: 0.000001  loss: 2.0020 (1.9982)  class_acc: 0.7266 (0.7381)  loss_scale: 8192.0000 (10276.0448)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:10  loss: 0.6659 (0.6659)  acc1: 81.6406 (81.6406)  acc5: 98.0469 (98.0469)  time: 2.8051  data: 2.5696  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.6915 (0.7139)  acc1: 82.4219 (82.4800)  acc5: 97.2656 (96.9120)  time: 0.2079  data: 0.0015  max mem: 25582
Test: Total time: 0:00:07 (0.3176 s / it)
* Acc@1 82.410 Acc@5 96.654 loss 0.732
Accuracy of the network on the 50000 test images: 82.4%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:10  loss: 0.8604 (0.8604)  acc1: 82.4219 (82.4219)  acc5: 96.8750 (96.8750)  time: 2.8066  data: 2.5476  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.7621 (0.8271)  acc1: 83.5938 (83.5680)  acc5: 97.6562 (97.2800)  time: 0.2105  data: 0.0042  max mem: 25582
Test: Total time: 0:00:07 (0.3180 s / it)
* Acc@1 83.622 Acc@5 97.278 loss 0.836
EMA Accuracy of the network on the 50000 test images: 83.6%
Max accuracy: 83.62%
{"train_lr": 0.0005941733685414326, "train_min_lr": 7.760316558975646e-07, "train_loss": 1.998215625, "train_class_acc": 0.7380921875, "train_loss_scale": 10276.0448, "train_weight_decay": 0.050000000000000495, "test_loss": 0.8364692470431327, "test_acc1": 83.62200003417969, "test_acc5": 97.27800003143311, "epoch": 12, "n_parameters": 85978600}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Epoch: [13]  [  0/625]  eta: 0:26:21  lr: 0.000592  min_lr: 0.000001  loss: 1.8223 (1.8223)  class_acc: 0.7812 (0.7812)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 2.5299  data: 1.9316  max mem: 25582
Epoch: [13]  [100/625]  eta: 0:05:25  lr: 0.000591  min_lr: 0.000001  loss: 1.9600 (1.9601)  class_acc: 0.7422 (0.7490)  loss_scale: 8192.0000 (10868.5941)  weight_decay: 0.0500 (0.0500)  time: 0.6027  data: 0.0002  max mem: 25582
Epoch: [13]  [200/625]  eta: 0:04:20  lr: 0.000590  min_lr: 0.000001  loss: 1.9912 (1.9665)  class_acc: 0.7344 (0.7458)  loss_scale: 8192.0000 (9536.9552)  weight_decay: 0.0500 (0.0500)  time: 0.6038  data: 0.0002  max mem: 25582
Epoch: [13]  [300/625]  eta: 0:03:17  lr: 0.000589  min_lr: 0.000001  loss: 1.9814 (1.9684)  class_acc: 0.7461 (0.7462)  loss_scale: 8192.0000 (11049.6744)  weight_decay: 0.0500 (0.0500)  time: 0.5996  data: 0.0002  max mem: 25582
Epoch: [13]  [400/625]  eta: 0:02:16  lr: 0.000588  min_lr: 0.000001  loss: 1.9316 (1.9668)  class_acc: 0.7422 (0.7462)  loss_scale: 8192.0000 (10337.0374)  weight_decay: 0.0500 (0.0500)  time: 0.6030  data: 0.0002  max mem: 25582
Epoch: [13]  [500/625]  eta: 0:01:15  lr: 0.000587  min_lr: 0.000001  loss: 1.9521 (1.9683)  class_acc: 0.7422 (0.7459)  loss_scale: 8192.0000 (10464.8303)  weight_decay: 0.0500 (0.0500)  time: 0.6063  data: 0.0002  max mem: 25582
Epoch: [13]  [600/625]  eta: 0:00:15  lr: 0.000586  min_lr: 0.000001  loss: 2.0449 (1.9712)  class_acc: 0.7305 (0.7452)  loss_scale: 4096.0000 (9405.1248)  weight_decay: 0.0500 (0.0500)  time: 0.6013  data: 0.0002  max mem: 25582
Epoch: [13]  [624/625]  eta: 0:00:00  lr: 0.000585  min_lr: 0.000001  loss: 2.0117 (1.9732)  class_acc: 0.7383 (0.7448)  loss_scale: 4096.0000 (9220.9152)  weight_decay: 0.0500 (0.0500)  time: 0.5888  data: 0.0006  max mem: 25582
Epoch: [13] Total time: 0:06:18 (0.6052 s / it)
Averaged stats: lr: 0.000585  min_lr: 0.000001  loss: 2.0117 (1.9745)  class_acc: 0.7383 (0.7443)  loss_scale: 4096.0000 (9220.9152)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:05  loss: 0.6918 (0.6918)  acc1: 83.5938 (83.5938)  acc5: 97.2656 (97.2656)  time: 2.6147  data: 2.3811  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.6776 (0.7184)  acc1: 83.5938 (82.9280)  acc5: 96.8750 (96.7360)  time: 0.2093  data: 0.0019  max mem: 25582
Test: Total time: 0:00:07 (0.3098 s / it)
* Acc@1 82.648 Acc@5 96.724 loss 0.732
Accuracy of the network on the 50000 test images: 82.6%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:07  loss: 0.7662 (0.7662)  acc1: 83.5938 (83.5938)  acc5: 96.8750 (96.8750)  time: 2.7170  data: 2.4673  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.6814 (0.7430)  acc1: 84.3750 (84.0800)  acc5: 97.6562 (97.3600)  time: 0.2085  data: 0.0002  max mem: 25582
Test: Total time: 0:00:07 (0.3145 s / it)
* Acc@1 83.982 Acc@5 97.334 loss 0.753
EMA Accuracy of the network on the 50000 test images: 84.0%
Max accuracy: 83.98%
{"train_lr": 0.000588686367681084, "train_min_lr": 7.688652519673107e-07, "train_loss": 1.9745375, "train_class_acc": 0.744303125, "train_loss_scale": 9220.9152, "train_weight_decay": 0.050000000000000495, "test_loss": 0.7531073850393295, "test_acc1": 83.98200004760743, "test_acc5": 97.33400003417968, "epoch": 13, "n_parameters": 85978600}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Epoch: [14]  [  0/625]  eta: 0:26:53  lr: 0.000585  min_lr: 0.000001  loss: 2.1035 (2.1035)  class_acc: 0.7109 (0.7109)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 2.5821  data: 1.9588  max mem: 25582
Epoch: [14]  [100/625]  eta: 0:05:26  lr: 0.000584  min_lr: 0.000001  loss: 1.9541 (1.9373)  class_acc: 0.7500 (0.7543)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6039  data: 0.0002  max mem: 25582
Epoch: [14]  [200/625]  eta: 0:04:20  lr: 0.000583  min_lr: 0.000001  loss: 1.9355 (1.9443)  class_acc: 0.7578 (0.7527)  loss_scale: 16384.0000 (11289.4726)  weight_decay: 0.0500 (0.0500)  time: 0.6004  data: 0.0002  max mem: 25582
Epoch: [14]  [300/625]  eta: 0:03:17  lr: 0.000582  min_lr: 0.000001  loss: 1.9668 (1.9539)  class_acc: 0.7422 (0.7501)  loss_scale: 8192.0000 (10614.2193)  weight_decay: 0.0500 (0.0500)  time: 0.6010  data: 0.0002  max mem: 25582
Epoch: [14]  [400/625]  eta: 0:02:16  lr: 0.000580  min_lr: 0.000001  loss: 1.9932 (1.9559)  class_acc: 0.7422 (0.7494)  loss_scale: 8192.0000 (10398.3242)  weight_decay: 0.0500 (0.0500)  time: 0.6010  data: 0.0003  max mem: 25582
Epoch: [14]  [500/625]  eta: 0:01:15  lr: 0.000579  min_lr: 0.000001  loss: 1.9297 (1.9554)  class_acc: 0.7383 (0.7494)  loss_scale: 8192.0000 (10121.4531)  weight_decay: 0.0500 (0.0500)  time: 0.5988  data: 0.0002  max mem: 25582
Epoch: [14]  [600/625]  eta: 0:00:15  lr: 0.000578  min_lr: 0.000001  loss: 1.9502 (1.9560)  class_acc: 0.7461 (0.7491)  loss_scale: 8192.0000 (10441.0516)  weight_decay: 0.0500 (0.0500)  time: 0.6017  data: 0.0003  max mem: 25582
Epoch: [14]  [624/625]  eta: 0:00:00  lr: 0.000577  min_lr: 0.000001  loss: 1.9746 (1.9572)  class_acc: 0.7383 (0.7487)  loss_scale: 8192.0000 (10354.6880)  weight_decay: 0.0500 (0.0500)  time: 0.5880  data: 0.0006  max mem: 25582
Epoch: [14] Total time: 0:06:17 (0.6041 s / it)
Averaged stats: lr: 0.000577  min_lr: 0.000001  loss: 1.9746 (1.9509)  class_acc: 0.7383 (0.7498)  loss_scale: 8192.0000 (10354.6880)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:00:56  loss: 0.7227 (0.7227)  acc1: 82.4219 (82.4219)  acc5: 97.6562 (97.6562)  time: 2.2714  data: 2.0164  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.6796 (0.7054)  acc1: 83.0189 (83.6160)  acc5: 97.2656 (96.8160)  time: 0.2043  data: 0.0001  max mem: 25582
Test: Total time: 0:00:07 (0.2924 s / it)
* Acc@1 82.906 Acc@5 96.730 loss 0.725
Accuracy of the network on the 50000 test images: 82.9%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:03  loss: 0.7033 (0.7033)  acc1: 84.3750 (84.3750)  acc5: 96.8750 (96.8750)  time: 2.5348  data: 2.3047  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.6313 (0.6881)  acc1: 84.7656 (84.3040)  acc5: 98.0469 (97.4400)  time: 0.2111  data: 0.0012  max mem: 25582
Test: Total time: 0:00:07 (0.3084 s / it)
* Acc@1 84.254 Acc@5 97.422 loss 0.699
EMA Accuracy of the network on the 50000 test images: 84.3%
Max accuracy: 84.25%
{"train_lr": 0.00058142260203229, "train_min_lr": 7.593782699130269e-07, "train_loss": 1.9509283203125, "train_class_acc": 0.74983828125, "train_loss_scale": 10354.688, "train_weight_decay": 0.050000000000000495, "test_loss": 0.6986417564749717, "test_acc1": 84.25400001831055, "test_acc5": 97.42200002868653, "epoch": 14, "n_parameters": 85978600}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Epoch: [15]  [  0/625]  eta: 0:25:01  lr: 0.000577  min_lr: 0.000001  loss: 1.9014 (1.9014)  class_acc: 0.7773 (0.7773)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 2.4018  data: 1.7961  max mem: 25582
Epoch: [15]  [100/625]  eta: 0:05:26  lr: 0.000576  min_lr: 0.000001  loss: 1.9355 (1.9142)  class_acc: 0.7461 (0.7580)  loss_scale: 8192.0000 (9489.7426)  weight_decay: 0.0500 (0.0500)  time: 0.6026  data: 0.0002  max mem: 25582
Epoch: [15]  [200/625]  eta: 0:04:20  lr: 0.000574  min_lr: 0.000001  loss: 1.8848 (1.9260)  class_acc: 0.7656 (0.7556)  loss_scale: 8192.0000 (9007.1244)  weight_decay: 0.0500 (0.0500)  time: 0.6045  data: 0.0003  max mem: 25582
Epoch: [15]  [300/625]  eta: 0:03:17  lr: 0.000573  min_lr: 0.000001  loss: 1.9229 (1.9319)  class_acc: 0.7500 (0.7552)  loss_scale: 8192.0000 (10369.2757)  weight_decay: 0.0500 (0.0500)  time: 0.6034  data: 0.0002  max mem: 25582
Epoch: [15]  [400/625]  eta: 0:02:16  lr: 0.000571  min_lr: 0.000001  loss: 1.9297 (1.9325)  class_acc: 0.7617 (0.7551)  loss_scale: 16384.0000 (10051.0324)  weight_decay: 0.0500 (0.0500)  time: 0.6039  data: 0.0003  max mem: 25582
Epoch: [15]  [500/625]  eta: 0:01:15  lr: 0.000569  min_lr: 0.000001  loss: 1.9600 (1.9352)  class_acc: 0.7383 (0.7542)  loss_scale: 8192.0000 (10611.9920)  weight_decay: 0.0500 (0.0500)  time: 0.6048  data: 0.0002  max mem: 25582
Epoch: [15]  [600/625]  eta: 0:00:15  lr: 0.000568  min_lr: 0.000001  loss: 1.8896 (1.9365)  class_acc: 0.7617 (0.7538)  loss_scale: 16384.0000 (10400.1597)  weight_decay: 0.0500 (0.0500)  time: 0.6028  data: 0.0003  max mem: 25582
Epoch: [15]  [624/625]  eta: 0:00:00  lr: 0.000567  min_lr: 0.000001  loss: 1.9531 (1.9363)  class_acc: 0.7617 (0.7541)  loss_scale: 16384.0000 (10629.9392)  weight_decay: 0.0500 (0.0500)  time: 0.5888  data: 0.0008  max mem: 25582
Epoch: [15] Total time: 0:06:18 (0.6062 s / it)
Averaged stats: lr: 0.000567  min_lr: 0.000001  loss: 1.9531 (1.9306)  class_acc: 0.7617 (0.7553)  loss_scale: 16384.0000 (10629.9392)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:08  loss: 0.7161 (0.7161)  acc1: 82.8125 (82.8125)  acc5: 96.8750 (96.8750)  time: 2.7339  data: 2.4807  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.6759 (0.7000)  acc1: 83.9844 (83.5520)  acc5: 96.8750 (96.8320)  time: 0.2111  data: 0.0015  max mem: 25582
Test: Total time: 0:00:07 (0.3171 s / it)
* Acc@1 83.046 Acc@5 96.796 loss 0.715
Accuracy of the network on the 50000 test images: 83.0%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:02  loss: 0.6621 (0.6621)  acc1: 85.5469 (85.5469)  acc5: 96.8750 (96.8750)  time: 2.5166  data: 2.2757  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.6004 (0.6513)  acc1: 84.7656 (84.6240)  acc5: 98.0469 (97.4400)  time: 0.2159  data: 0.0065  max mem: 25582
Test: Total time: 0:00:07 (0.3108 s / it)
* Acc@1 84.552 Acc@5 97.464 loss 0.662
EMA Accuracy of the network on the 50000 test images: 84.6%
Max accuracy: 84.55%
{"train_lr": 0.0005724268551257212, "train_min_lr": 7.476292001338209e-07, "train_loss": 1.9305837890625, "train_class_acc": 0.75534765625, "train_loss_scale": 10629.9392, "train_weight_decay": 0.050000000000000495, "test_loss": 0.6624124604463577, "test_acc1": 84.55200003204345, "test_acc5": 97.4640000314331, "epoch": 15, "n_parameters": 85978600}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Epoch: [16]  [  0/625]  eta: 0:27:29  lr: 0.000567  min_lr: 0.000001  loss: 1.8037 (1.8037)  class_acc: 0.7773 (0.7773)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 2.6388  data: 2.0140  max mem: 25582
Epoch: [16]  [100/625]  eta: 0:05:26  lr: 0.000566  min_lr: 0.000001  loss: 1.8926 (1.9007)  class_acc: 0.7617 (0.7643)  loss_scale: 16384.0000 (15897.3465)  weight_decay: 0.0500 (0.0500)  time: 0.6027  data: 0.0002  max mem: 25582
Epoch: [16]  [200/625]  eta: 0:04:20  lr: 0.000564  min_lr: 0.000001  loss: 1.8994 (1.9038)  class_acc: 0.7578 (0.7640)  loss_scale: 8192.0000 (12063.8408)  weight_decay: 0.0500 (0.0500)  time: 0.5986  data: 0.0003  max mem: 25582
Epoch: [16]  [300/625]  eta: 0:03:17  lr: 0.000562  min_lr: 0.000001  loss: 1.9199 (1.9045)  class_acc: 0.7617 (0.7624)  loss_scale: 16384.0000 (12927.5748)  weight_decay: 0.0500 (0.0500)  time: 0.6009  data: 0.0002  max mem: 25582
Epoch: [16]  [400/625]  eta: 0:02:16  lr: 0.000560  min_lr: 0.000001  loss: 1.8936 (1.9047)  class_acc: 0.7656 (0.7622)  loss_scale: 16384.0000 (13952.9576)  weight_decay: 0.0500 (0.0500)  time: 0.6039  data: 0.0002  max mem: 25582
Epoch: [16]  [500/625]  eta: 0:01:15  lr: 0.000558  min_lr: 0.000001  loss: 1.9043 (1.9064)  class_acc: 0.7500 (0.7612)  loss_scale: 8192.0000 (12901.1737)  weight_decay: 0.0500 (0.0500)  time: 0.6001  data: 0.0002  max mem: 25582
Epoch: [16]  [600/625]  eta: 0:00:15  lr: 0.000556  min_lr: 0.000001  loss: 1.8965 (1.9073)  class_acc: 0.7500 (0.7612)  loss_scale: 8192.0000 (12812.7787)  weight_decay: 0.0500 (0.0500)  time: 0.6021  data: 0.0002  max mem: 25582
Epoch: [16]  [624/625]  eta: 0:00:00  lr: 0.000556  min_lr: 0.000001  loss: 1.9482 (1.9084)  class_acc: 0.7461 (0.7608)  loss_scale: 8192.0000 (12635.3408)  weight_decay: 0.0500 (0.0500)  time: 0.5898  data: 0.0009  max mem: 25582
Epoch: [16] Total time: 0:06:18 (0.6049 s / it)
Averaged stats: lr: 0.000556  min_lr: 0.000001  loss: 1.9482 (1.9101)  class_acc: 0.7461 (0.7605)  loss_scale: 8192.0000 (12635.3408)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:08  loss: 0.6645 (0.6645)  acc1: 84.3750 (84.3750)  acc5: 97.2656 (97.2656)  time: 2.7256  data: 2.4768  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.6384 (0.6899)  acc1: 83.9623 (83.6320)  acc5: 97.6562 (97.0240)  time: 0.2075  data: 0.0002  max mem: 25582
Test: Total time: 0:00:07 (0.3125 s / it)
* Acc@1 83.168 Acc@5 96.820 loss 0.708
Accuracy of the network on the 50000 test images: 83.2%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:03  loss: 0.6321 (0.6321)  acc1: 85.9375 (85.9375)  acc5: 97.2656 (97.2656)  time: 2.5529  data: 2.2988  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.5814 (0.6262)  acc1: 84.7656 (84.9600)  acc5: 97.6562 (97.4240)  time: 0.2146  data: 0.0062  max mem: 25582
Test: Total time: 0:00:07 (0.3124 s / it)
* Acc@1 84.742 Acc@5 97.496 loss 0.638
EMA Accuracy of the network on the 50000 test images: 84.7%
Max accuracy: 84.74%
{"train_lr": 0.0005617545887324481, "train_min_lr": 7.336904795518422e-07, "train_loss": 1.910105859375, "train_class_acc": 0.76052734375, "train_loss_scale": 12635.3408, "train_weight_decay": 0.050000000000000495, "test_loss": 0.6379678902029992, "test_acc1": 84.74200002655029, "test_acc5": 97.49600003143311, "epoch": 16, "n_parameters": 85978600}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Epoch: [17]  [  0/625]  eta: 0:28:00  lr: 0.000556  min_lr: 0.000001  loss: 1.9072 (1.9072)  class_acc: 0.7539 (0.7539)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 2.6880  data: 2.0788  max mem: 25582
Epoch: [17]  [100/625]  eta: 0:05:27  lr: 0.000554  min_lr: 0.000001  loss: 1.8340 (1.8774)  class_acc: 0.7656 (0.7662)  loss_scale: 8192.0000 (9003.0891)  weight_decay: 0.0500 (0.0500)  time: 0.6050  data: 0.0002  max mem: 25582
Epoch: [17]  [200/625]  eta: 0:04:20  lr: 0.000552  min_lr: 0.000001  loss: 1.8691 (1.8828)  class_acc: 0.7578 (0.7664)  loss_scale: 8192.0000 (9170.1493)  weight_decay: 0.0500 (0.0500)  time: 0.6005  data: 0.0002  max mem: 25582
Epoch: [17]  [300/625]  eta: 0:03:18  lr: 0.000550  min_lr: 0.000001  loss: 1.8857 (1.8812)  class_acc: 0.7617 (0.7673)  loss_scale: 16384.0000 (10396.4917)  weight_decay: 0.0500 (0.0500)  time: 0.6018  data: 0.0002  max mem: 25582
Epoch: [17]  [400/625]  eta: 0:02:16  lr: 0.000548  min_lr: 0.000001  loss: 1.9502 (1.8888)  class_acc: 0.7461 (0.7651)  loss_scale: 4096.0000 (9397.3067)  weight_decay: 0.0500 (0.0500)  time: 0.6036  data: 0.0002  max mem: 25582
Epoch: [17]  [500/625]  eta: 0:01:15  lr: 0.000546  min_lr: 0.000001  loss: 1.8721 (1.8920)  class_acc: 0.7539 (0.7651)  loss_scale: 4096.0000 (8429.0938)  weight_decay: 0.0500 (0.0500)  time: 0.6028  data: 0.0002  max mem: 25582
Epoch: [17]  [600/625]  eta: 0:00:15  lr: 0.000543  min_lr: 0.000001  loss: 1.9375 (1.8959)  class_acc: 0.7500 (0.7637)  loss_scale: 4096.0000 (7755.8203)  weight_decay: 0.0500 (0.0500)  time: 0.6043  data: 0.0002  max mem: 25582
Epoch: [17]  [624/625]  eta: 0:00:00  lr: 0.000543  min_lr: 0.000001  loss: 1.8701 (1.8957)  class_acc: 0.7656 (0.7638)  loss_scale: 8192.0000 (7772.5696)  weight_decay: 0.0500 (0.0500)  time: 0.5897  data: 0.0006  max mem: 25582
Epoch: [17] Total time: 0:06:18 (0.6061 s / it)
Averaged stats: lr: 0.000543  min_lr: 0.000001  loss: 1.8701 (1.8908)  class_acc: 0.7656 (0.7659)  loss_scale: 8192.0000 (7772.5696)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:08  loss: 0.7064 (0.7064)  acc1: 84.7656 (84.7656)  acc5: 96.8750 (96.8750)  time: 2.7376  data: 2.4866  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.6556 (0.6888)  acc1: 84.3750 (83.8080)  acc5: 97.2656 (96.8960)  time: 0.2102  data: 0.0002  max mem: 25582
Test: Total time: 0:00:07 (0.3157 s / it)
* Acc@1 83.212 Acc@5 96.758 loss 0.709
Accuracy of the network on the 50000 test images: 83.2%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:14  loss: 0.6082 (0.6082)  acc1: 85.9375 (85.9375)  acc5: 97.2656 (97.2656)  time: 2.9865  data: 2.7411  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.5570 (0.6085)  acc1: 85.9375 (85.2640)  acc5: 97.6562 (97.4400)  time: 0.2089  data: 0.0011  max mem: 25582
Test: Total time: 0:00:08 (0.3251 s / it)
* Acc@1 84.916 Acc@5 97.534 loss 0.621
EMA Accuracy of the network on the 50000 test images: 84.9%
Max accuracy: 84.92%
{"train_lr": 0.0005494716009236739, "train_min_lr": 7.176480450145748e-07, "train_loss": 1.890771484375, "train_class_acc": 0.765903125, "train_loss_scale": 7772.5696, "train_weight_decay": 0.050000000000000495, "test_loss": 0.6209952637553215, "test_acc1": 84.91600002929688, "test_acc5": 97.53400002349854, "epoch": 17, "n_parameters": 85978600}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Epoch: [18]  [  0/625]  eta: 0:25:16  lr: 0.000543  min_lr: 0.000001  loss: 1.8115 (1.8115)  class_acc: 0.8086 (0.8086)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 2.4267  data: 1.7994  max mem: 25582
Epoch: [18]  [100/625]  eta: 0:05:24  lr: 0.000541  min_lr: 0.000001  loss: 1.9160 (1.8607)  class_acc: 0.7617 (0.7746)  loss_scale: 4096.0000 (7097.0297)  weight_decay: 0.0500 (0.0500)  time: 0.6012  data: 0.0002  max mem: 25582
Epoch: [18]  [200/625]  eta: 0:04:19  lr: 0.000538  min_lr: 0.000001  loss: 1.8418 (1.8684)  class_acc: 0.7734 (0.7720)  loss_scale: 4096.0000 (5603.9801)  weight_decay: 0.0500 (0.0500)  time: 0.5973  data: 0.0002  max mem: 25582
Epoch: [18]  [300/625]  eta: 0:03:17  lr: 0.000536  min_lr: 0.000001  loss: 1.8408 (1.8692)  class_acc: 0.7695 (0.7716)  loss_scale: 8192.0000 (6436.5714)  weight_decay: 0.0500 (0.0500)  time: 0.6015  data: 0.0003  max mem: 25582
Epoch: [18]  [400/625]  eta: 0:02:16  lr: 0.000534  min_lr: 0.000001  loss: 1.8594 (1.8679)  class_acc: 0.7578 (0.7711)  loss_scale: 4096.0000 (6966.2643)  weight_decay: 0.0500 (0.0500)  time: 0.6007  data: 0.0002  max mem: 25582
Epoch: [18]  [500/625]  eta: 0:01:15  lr: 0.000531  min_lr: 0.000001  loss: 1.8721 (1.8707)  class_acc: 0.7734 (0.7700)  loss_scale: 4096.0000 (6393.3573)  weight_decay: 0.0500 (0.0500)  time: 0.6031  data: 0.0002  max mem: 25582
Epoch: [18]  [600/625]  eta: 0:00:15  lr: 0.000529  min_lr: 0.000001  loss: 1.8906 (1.8734)  class_acc: 0.7656 (0.7691)  loss_scale: 8192.0000 (6679.0017)  weight_decay: 0.0500 (0.0500)  time: 0.6012  data: 0.0002  max mem: 25582
Epoch: [18]  [624/625]  eta: 0:00:00  lr: 0.000528  min_lr: 0.000001  loss: 1.8496 (1.8732)  class_acc: 0.7695 (0.7691)  loss_scale: 8192.0000 (6737.1008)  weight_decay: 0.0500 (0.0500)  time: 0.5880  data: 0.0006  max mem: 25582
Epoch: [18] Total time: 0:06:17 (0.6033 s / it)
Averaged stats: lr: 0.000528  min_lr: 0.000001  loss: 1.8496 (1.8710)  class_acc: 0.7695 (0.7707)  loss_scale: 8192.0000 (6737.1008)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:10  loss: 0.6748 (0.6748)  acc1: 85.1562 (85.1562)  acc5: 96.8750 (96.8750)  time: 2.8038  data: 2.5492  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.6285 (0.6937)  acc1: 85.1562 (84.2400)  acc5: 97.6562 (96.8640)  time: 0.2089  data: 0.0031  max mem: 25582
Test: Total time: 0:00:07 (0.3182 s / it)
* Acc@1 83.426 Acc@5 96.810 loss 0.715
Accuracy of the network on the 50000 test images: 83.4%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:00  loss: 0.5951 (0.5951)  acc1: 86.7188 (86.7188)  acc5: 97.2656 (97.2656)  time: 2.4333  data: 2.1633  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.5435 (0.5965)  acc1: 86.3281 (85.3760)  acc5: 97.6562 (97.4560)  time: 0.2181  data: 0.0101  max mem: 25582
Test: Total time: 0:00:07 (0.3092 s / it)
* Acc@1 85.000 Acc@5 97.556 loss 0.610
EMA Accuracy of the network on the 50000 test images: 85.0%
Max accuracy: 85.00%
{"train_lr": 0.0005356536204037446, "train_min_lr": 6.996008034655921e-07, "train_loss": 1.8710119140625, "train_class_acc": 0.77066875, "train_loss_scale": 6737.1008, "train_weight_decay": 0.050000000000000495, "test_loss": 0.6095451539754868, "test_acc1": 85.00000002105713, "test_acc5": 97.55600002075195, "epoch": 18, "n_parameters": 85978600}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Epoch: [19]  [  0/625]  eta: 0:27:58  lr: 0.000528  min_lr: 0.000001  loss: 1.7266 (1.7266)  class_acc: 0.8320 (0.8320)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 2.6856  data: 2.0598  max mem: 25582
Epoch: [19]  [100/625]  eta: 0:05:29  lr: 0.000526  min_lr: 0.000001  loss: 1.8604 (1.8498)  class_acc: 0.7617 (0.7761)  loss_scale: 8192.0000 (9084.1980)  weight_decay: 0.0500 (0.0500)  time: 0.6074  data: 0.0002  max mem: 25582
Epoch: [19]  [200/625]  eta: 0:04:22  lr: 0.000523  min_lr: 0.000001  loss: 1.8906 (1.8461)  class_acc: 0.7812 (0.7779)  loss_scale: 4096.0000 (6602.5075)  weight_decay: 0.0500 (0.0500)  time: 0.6025  data: 0.0002  max mem: 25582
Epoch: [19]  [300/625]  eta: 0:03:18  lr: 0.000521  min_lr: 0.000001  loss: 1.8516 (1.8478)  class_acc: 0.7617 (0.7761)  loss_scale: 4096.0000 (6232.4518)  weight_decay: 0.0500 (0.0500)  time: 0.6036  data: 0.0003  max mem: 25582
Epoch: [19]  [400/625]  eta: 0:02:17  lr: 0.000518  min_lr: 0.000001  loss: 1.8184 (1.8478)  class_acc: 0.7734 (0.7758)  loss_scale: 8192.0000 (5822.2444)  weight_decay: 0.0500 (0.0500)  time: 0.6069  data: 0.0003  max mem: 25582
Epoch: [19]  [500/625]  eta: 0:01:16  lr: 0.000516  min_lr: 0.000001  loss: 1.8203 (1.8516)  class_acc: 0.7734 (0.7750)  loss_scale: 8192.0000 (6295.2495)  weight_decay: 0.0500 (0.0500)  time: 0.6063  data: 0.0003  max mem: 25582
Epoch: [19]  [600/625]  eta: 0:00:15  lr: 0.000513  min_lr: 0.000001  loss: 1.7988 (1.8521)  class_acc: 0.7891 (0.7752)  loss_scale: 8192.0000 (6706.2629)  weight_decay: 0.0500 (0.0500)  time: 0.6028  data: 0.0002  max mem: 25582
Epoch: [19]  [624/625]  eta: 0:00:00  lr: 0.000512  min_lr: 0.000001  loss: 1.7949 (1.8510)  class_acc: 0.7852 (0.7754)  loss_scale: 8192.0000 (6704.3328)  weight_decay: 0.0500 (0.0500)  time: 0.5904  data: 0.0007  max mem: 25582
Epoch: [19] Total time: 0:06:20 (0.6080 s / it)
Averaged stats: lr: 0.000512  min_lr: 0.000001  loss: 1.7949 (1.8537)  class_acc: 0.7852 (0.7750)  loss_scale: 8192.0000 (6704.3328)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:00:58  loss: 0.6679 (0.6679)  acc1: 87.1094 (87.1094)  acc5: 97.2656 (97.2656)  time: 2.3367  data: 2.1079  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.6163 (0.6829)  acc1: 84.7656 (83.7280)  acc5: 97.6562 (97.1840)  time: 0.2055  data: 0.0001  max mem: 25582
Test: Total time: 0:00:07 (0.2955 s / it)
* Acc@1 83.528 Acc@5 96.890 loss 0.700
Accuracy of the network on the 50000 test images: 83.5%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:04  loss: 0.5840 (0.5840)  acc1: 86.7188 (86.7188)  acc5: 97.2656 (97.2656)  time: 2.5988  data: 2.3693  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.5372 (0.5881)  acc1: 85.5469 (85.5040)  acc5: 98.0469 (97.5520)  time: 0.2084  data: 0.0002  max mem: 25582
Test: Total time: 0:00:07 (0.3092 s / it)
* Acc@1 85.106 Acc@5 97.580 loss 0.601
EMA Accuracy of the network on the 50000 test images: 85.1%
Max accuracy: 85.11%
{"train_lr": 0.0005203858396175132, "train_min_lr": 6.796600221503591e-07, "train_loss": 1.8536900390625, "train_class_acc": 0.774959375, "train_loss_scale": 6704.3328, "train_weight_decay": 0.050000000000000495, "test_loss": 0.6013468880951405, "test_acc1": 85.10600002929688, "test_acc5": 97.58000002075195, "epoch": 19, "n_parameters": 85978600}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Epoch: [20]  [  0/625]  eta: 0:24:57  lr: 0.000512  min_lr: 0.000001  loss: 1.7793 (1.7793)  class_acc: 0.7852 (0.7852)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  time: 2.3953  data: 1.7588  max mem: 25582
Epoch: [20]  [100/625]  eta: 0:05:24  lr: 0.000510  min_lr: 0.000001  loss: 1.8496 (1.8320)  class_acc: 0.7695 (0.7813)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5996  data: 0.0002  max mem: 25582
Epoch: [20]  [200/625]  eta: 0:04:19  lr: 0.000507  min_lr: 0.000001  loss: 1.8232 (1.8312)  class_acc: 0.7852 (0.7807)  loss_scale: 8192.0000 (5746.6269)  weight_decay: 0.0500 (0.0500)  time: 0.6024  data: 0.0002  max mem: 25582
Epoch: [20]  [300/625]  eta: 0:03:17  lr: 0.000504  min_lr: 0.000001  loss: 1.8135 (1.8318)  class_acc: 0.7891 (0.7808)  loss_scale: 16384.0000 (8001.4884)  weight_decay: 0.0500 (0.0500)  time: 0.6014  data: 0.0002  max mem: 25582
Epoch: [20]  [400/625]  eta: 0:02:16  lr: 0.000501  min_lr: 0.000001  loss: 1.8350 (1.8323)  class_acc: 0.7734 (0.7804)  loss_scale: 8192.0000 (8089.8554)  weight_decay: 0.0500 (0.0500)  time: 0.6022  data: 0.0002  max mem: 25582
Epoch: [20]  [500/625]  eta: 0:01:15  lr: 0.000499  min_lr: 0.000001  loss: 1.8633 (1.8376)  class_acc: 0.7656 (0.7796)  loss_scale: 8192.0000 (8306.4591)  weight_decay: 0.0500 (0.0500)  time: 0.6028  data: 0.0002  max mem: 25582
Epoch: [20]  [600/625]  eta: 0:00:15  lr: 0.000496  min_lr: 0.000001  loss: 1.8115 (1.8404)  class_acc: 0.7734 (0.7791)  loss_scale: 8192.0000 (8437.3511)  weight_decay: 0.0500 (0.0500)  time: 0.6027  data: 0.0003  max mem: 25582
Epoch: [20]  [624/625]  eta: 0:00:00  lr: 0.000495  min_lr: 0.000001  loss: 1.8203 (1.8399)  class_acc: 0.7852 (0.7793)  loss_scale: 8192.0000 (8427.9296)  weight_decay: 0.0500 (0.0500)  time: 0.5897  data: 0.0007  max mem: 25582
Epoch: [20] Total time: 0:06:17 (0.6036 s / it)
Averaged stats: lr: 0.000495  min_lr: 0.000001  loss: 1.8203 (1.8361)  class_acc: 0.7852 (0.7801)  loss_scale: 8192.0000 (8427.9296)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:04  loss: 0.6991 (0.6991)  acc1: 83.9844 (83.9844)  acc5: 96.8750 (96.8750)  time: 2.5645  data: 2.3124  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.6494 (0.6869)  acc1: 84.7656 (83.7600)  acc5: 97.6562 (96.9280)  time: 0.2102  data: 0.0017  max mem: 25582
Test: Total time: 0:00:07 (0.3081 s / it)
* Acc@1 83.386 Acc@5 96.846 loss 0.701
Accuracy of the network on the 50000 test images: 83.4%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:04  loss: 0.5771 (0.5771)  acc1: 87.8906 (87.8906)  acc5: 97.6562 (97.6562)  time: 2.5672  data: 2.3260  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.5326 (0.5820)  acc1: 85.9375 (85.6320)  acc5: 98.4375 (97.7120)  time: 0.2098  data: 0.0015  max mem: 25582
Test: Total time: 0:00:07 (0.3085 s / it)
* Acc@1 85.242 Acc@5 97.640 loss 0.596
EMA Accuracy of the network on the 50000 test images: 85.2%
Max accuracy: 85.24%
{"train_lr": 0.000503762389510578, "train_min_lr": 6.579486426166668e-07, "train_loss": 1.836057421875, "train_class_acc": 0.7801203125, "train_loss_scale": 8427.9296, "train_weight_decay": 0.050000000000000495, "test_loss": 0.5957533775269985, "test_acc1": 85.24200003753663, "test_acc5": 97.64000003692627, "epoch": 20, "n_parameters": 85978600}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Epoch: [21]  [  0/625]  eta: 0:26:38  lr: 0.000495  min_lr: 0.000001  loss: 1.8105 (1.8105)  class_acc: 0.7617 (0.7617)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 2.5573  data: 1.9495  max mem: 25582
Epoch: [21]  [100/625]  eta: 0:05:26  lr: 0.000492  min_lr: 0.000001  loss: 1.7930 (1.7980)  class_acc: 0.7930 (0.7902)  loss_scale: 16384.0000 (9246.4158)  weight_decay: 0.0500 (0.0500)  time: 0.6016  data: 0.0002  max mem: 25582
Epoch: [21]  [200/625]  eta: 0:04:20  lr: 0.000489  min_lr: 0.000001  loss: 1.8301 (1.8077)  class_acc: 0.7812 (0.7867)  loss_scale: 8192.0000 (9618.4677)  weight_decay: 0.0500 (0.0500)  time: 0.6013  data: 0.0002  max mem: 25582
Epoch: [21]  [300/625]  eta: 0:03:18  lr: 0.000486  min_lr: 0.000001  loss: 1.7734 (1.8049)  class_acc: 0.7891 (0.7873)  loss_scale: 16384.0000 (10478.1395)  weight_decay: 0.0500 (0.0500)  time: 0.6060  data: 0.0003  max mem: 25582
Epoch: [21]  [400/625]  eta: 0:02:16  lr: 0.000483  min_lr: 0.000001  loss: 1.8662 (1.8077)  class_acc: 0.7656 (0.7861)  loss_scale: 8192.0000 (10316.6085)  weight_decay: 0.0500 (0.0500)  time: 0.6015  data: 0.0003  max mem: 25582
Epoch: [21]  [500/625]  eta: 0:01:15  lr: 0.000480  min_lr: 0.000001  loss: 1.8105 (1.8092)  class_acc: 0.7891 (0.7859)  loss_scale: 8192.0000 (10497.5329)  weight_decay: 0.0500 (0.0500)  time: 0.6022  data: 0.0003  max mem: 25582
Epoch: [21]  [600/625]  eta: 0:00:15  lr: 0.000477  min_lr: 0.000001  loss: 1.8418 (1.8121)  class_acc: 0.7734 (0.7849)  loss_scale: 8192.0000 (10052.5790)  weight_decay: 0.0500 (0.0500)  time: 0.6005  data: 0.0012  max mem: 25582
Epoch: [21]  [624/625]  eta: 0:00:00  lr: 0.000477  min_lr: 0.000001  loss: 1.8545 (1.8126)  class_acc: 0.7734 (0.7847)  loss_scale: 4096.0000 (9823.8464)  weight_decay: 0.0500 (0.0500)  time: 0.5886  data: 0.0011  max mem: 25582
Epoch: [21] Total time: 0:06:18 (0.6061 s / it)
Averaged stats: lr: 0.000477  min_lr: 0.000001  loss: 1.8545 (1.8179)  class_acc: 0.7734 (0.7846)  loss_scale: 4096.0000 (9823.8464)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:04  loss: 0.7610 (0.7610)  acc1: 85.1562 (85.1562)  acc5: 96.8750 (96.8750)  time: 2.5853  data: 2.3295  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.6246 (0.6936)  acc1: 85.1562 (84.1440)  acc5: 97.2656 (96.8640)  time: 0.2094  data: 0.0003  max mem: 25582
Test: Total time: 0:00:07 (0.3080 s / it)
* Acc@1 83.706 Acc@5 96.892 loss 0.706
Accuracy of the network on the 50000 test images: 83.7%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:02  loss: 0.5730 (0.5730)  acc1: 87.8906 (87.8906)  acc5: 97.6562 (97.6562)  time: 2.4905  data: 2.2499  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.5293 (0.5775)  acc1: 86.3281 (85.8560)  acc5: 98.0469 (97.7440)  time: 0.2085  data: 0.0024  max mem: 25582
Test: Total time: 0:00:07 (0.3038 s / it)
* Acc@1 85.368 Acc@5 97.638 loss 0.592
EMA Accuracy of the network on the 50000 test images: 85.4%
Max accuracy: 85.37%
{"train_lr": 0.00048588575918070775, "train_min_lr": 6.346005227391079e-07, "train_loss": 1.8179365234375, "train_class_acc": 0.78458671875, "train_loss_scale": 9823.8464, "train_weight_decay": 0.050000000000000495, "test_loss": 0.591996478587389, "test_acc1": 85.36800003753662, "test_acc5": 97.63800003967285, "epoch": 21, "n_parameters": 85978600}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Epoch: [22]  [  0/625]  eta: 0:25:29  lr: 0.000477  min_lr: 0.000001  loss: 1.7197 (1.7197)  class_acc: 0.7852 (0.7852)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  time: 2.4471  data: 1.8182  max mem: 25582
Epoch: [22]  [100/625]  eta: 0:05:27  lr: 0.000473  min_lr: 0.000001  loss: 1.7773 (1.7887)  class_acc: 0.7930 (0.7903)  loss_scale: 4096.0000 (4298.7723)  weight_decay: 0.0500 (0.0500)  time: 0.6079  data: 0.0002  max mem: 25582
Epoch: [22]  [200/625]  eta: 0:04:21  lr: 0.000470  min_lr: 0.000001  loss: 1.7695 (1.7917)  class_acc: 0.7891 (0.7912)  loss_scale: 8192.0000 (6235.7015)  weight_decay: 0.0500 (0.0500)  time: 0.6039  data: 0.0002  max mem: 25582
Epoch: [22]  [300/625]  eta: 0:03:19  lr: 0.000467  min_lr: 0.000001  loss: 1.7939 (1.7943)  class_acc: 0.7812 (0.7902)  loss_scale: 16384.0000 (8981.2625)  weight_decay: 0.0500 (0.0500)  time: 0.6086  data: 0.0002  max mem: 25582
Epoch: [22]  [400/625]  eta: 0:02:17  lr: 0.000464  min_lr: 0.000001  loss: 1.7764 (1.7949)  class_acc: 0.8008 (0.7901)  loss_scale: 16384.0000 (11235.9102)  weight_decay: 0.0500 (0.0500)  time: 0.6072  data: 0.0002  max mem: 25582
Epoch: [22]  [500/625]  eta: 0:01:16  lr: 0.000461  min_lr: 0.000001  loss: 1.7744 (1.7941)  class_acc: 0.7812 (0.7903)  loss_scale: 16384.0000 (12230.7705)  weight_decay: 0.0500 (0.0500)  time: 0.6021  data: 0.0002  max mem: 25582
Epoch: [22]  [600/625]  eta: 0:00:15  lr: 0.000458  min_lr: 0.000001  loss: 1.7363 (1.7945)  class_acc: 0.8008 (0.7903)  loss_scale: 8192.0000 (11558.7621)  weight_decay: 0.0500 (0.0500)  time: 0.6069  data: 0.0002  max mem: 25582
Epoch: [22]  [624/625]  eta: 0:00:00  lr: 0.000457  min_lr: 0.000001  loss: 1.7646 (1.7943)  class_acc: 0.7930 (0.7904)  loss_scale: 8192.0000 (11429.4784)  weight_decay: 0.0500 (0.0500)  time: 0.5898  data: 0.0008  max mem: 25582
Epoch: [22] Total time: 0:06:20 (0.6094 s / it)
Averaged stats: lr: 0.000457  min_lr: 0.000001  loss: 1.7646 (1.7987)  class_acc: 0.7930 (0.7896)  loss_scale: 8192.0000 (11429.4784)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:04  loss: 0.7054 (0.7054)  acc1: 84.3750 (84.3750)  acc5: 96.8750 (96.8750)  time: 2.5935  data: 2.3568  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.6388 (0.6896)  acc1: 85.9375 (84.0160)  acc5: 97.6562 (96.8320)  time: 0.2146  data: 0.0085  max mem: 25582
Test: Total time: 0:00:07 (0.3136 s / it)
* Acc@1 83.622 Acc@5 96.834 loss 0.708
Accuracy of the network on the 50000 test images: 83.6%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:04  loss: 0.5755 (0.5755)  acc1: 87.8906 (87.8906)  acc5: 97.6562 (97.6562)  time: 2.5846  data: 2.3349  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.5257 (0.5749)  acc1: 87.1094 (85.8720)  acc5: 98.0469 (97.7440)  time: 0.2098  data: 0.0025  max mem: 25582
Test: Total time: 0:00:07 (0.3082 s / it)
* Acc@1 85.468 Acc@5 97.642 loss 0.590
EMA Accuracy of the network on the 50000 test images: 85.5%
Max accuracy: 85.47%
{"train_lr": 0.0004668661639984669, "train_min_lr": 6.097596114407642e-07, "train_loss": 1.7987138671875, "train_class_acc": 0.78962734375, "train_loss_scale": 11429.4784, "train_weight_decay": 0.050000000000000495, "test_loss": 0.5898453202843666, "test_acc1": 85.4680000402832, "test_acc5": 97.64200003967285, "epoch": 22, "n_parameters": 85978600}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Epoch: [23]  [  0/625]  eta: 0:26:40  lr: 0.000457  min_lr: 0.000001  loss: 1.9043 (1.9043)  class_acc: 0.7617 (0.7617)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 2.5606  data: 1.9453  max mem: 25582
Epoch: [23]  [100/625]  eta: 0:05:27  lr: 0.000454  min_lr: 0.000001  loss: 1.7627 (1.7669)  class_acc: 0.8008 (0.7984)  loss_scale: 16384.0000 (16302.8911)  weight_decay: 0.0500 (0.0500)  time: 0.6030  data: 0.0002  max mem: 25582
Epoch: [23]  [200/625]  eta: 0:04:20  lr: 0.000451  min_lr: 0.000001  loss: 1.7373 (1.7661)  class_acc: 0.8047 (0.7993)  loss_scale: 8192.0000 (12308.3781)  weight_decay: 0.0500 (0.0500)  time: 0.6021  data: 0.0002  max mem: 25582
Epoch: [23]  [300/625]  eta: 0:03:18  lr: 0.000447  min_lr: 0.000001  loss: 1.7686 (1.7699)  class_acc: 0.7930 (0.7979)  loss_scale: 8192.0000 (11593.9934)  weight_decay: 0.0500 (0.0500)  time: 0.6032  data: 0.0002  max mem: 25582
Epoch: [23]  [400/625]  eta: 0:02:16  lr: 0.000444  min_lr: 0.000001  loss: 1.7725 (1.7709)  class_acc: 0.7930 (0.7973)  loss_scale: 16384.0000 (11092.9077)  weight_decay: 0.0500 (0.0500)  time: 0.6005  data: 0.0003  max mem: 25582
Epoch: [23]  [500/625]  eta: 0:01:15  lr: 0.000441  min_lr: 0.000001  loss: 1.8008 (1.7764)  class_acc: 0.7969 (0.7956)  loss_scale: 8192.0000 (10628.3433)  weight_decay: 0.0500 (0.0500)  time: 0.6026  data: 0.0003  max mem: 25582
Epoch: [23]  [600/625]  eta: 0:00:15  lr: 0.000437  min_lr: 0.000001  loss: 1.7529 (1.7783)  class_acc: 0.7930 (0.7953)  loss_scale: 8192.0000 (10836.3394)  weight_decay: 0.0500 (0.0500)  time: 0.6019  data: 0.0011  max mem: 25582
Epoch: [23]  [624/625]  eta: 0:00:00  lr: 0.000437  min_lr: 0.000001  loss: 1.8018 (1.7791)  class_acc: 0.7891 (0.7952)  loss_scale: 8192.0000 (10734.7968)  weight_decay: 0.0500 (0.0500)  time: 0.5881  data: 0.0009  max mem: 25582
Epoch: [23] Total time: 0:06:18 (0.6061 s / it)
Averaged stats: lr: 0.000437  min_lr: 0.000001  loss: 1.8018 (1.7828)  class_acc: 0.7891 (0.7944)  loss_scale: 8192.0000 (10734.7968)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:06  loss: 0.7112 (0.7112)  acc1: 84.3750 (84.3750)  acc5: 97.2656 (97.2656)  time: 2.6605  data: 2.4242  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.6586 (0.6931)  acc1: 84.7656 (84.0320)  acc5: 97.2656 (96.6880)  time: 0.2110  data: 0.0015  max mem: 25582
Test: Total time: 0:00:07 (0.3137 s / it)
* Acc@1 83.618 Acc@5 96.718 loss 0.706
Accuracy of the network on the 50000 test images: 83.6%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:02  loss: 0.5807 (0.5807)  acc1: 86.3281 (86.3281)  acc5: 97.6562 (97.6562)  time: 2.5181  data: 2.2707  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.5251 (0.5739)  acc1: 86.3281 (85.6800)  acc5: 98.1132 (97.7600)  time: 0.2237  data: 0.0162  max mem: 25582
Test: Total time: 0:00:07 (0.3168 s / it)
{"train_lr": 0.0004468208660928128, "train_min_lr": 5.835790612002331e-07, "train_loss": 1.782766796875, "train_class_acc": 0.79436484375, "train_loss_scale": 10734.7968, "train_weight_decay": 0.050000000000000495, "test_loss": 0.5885556507110595, "test_acc1": 85.46400003204346, "test_acc5": 97.62800004241943, "epoch": 23, "n_parameters": 85978600}
* Acc@1 85.464 Acc@5 97.628 loss 0.589
EMA Accuracy of the network on the 50000 test images: 85.5%
Max accuracy: 85.47%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Epoch: [24]  [  0/625]  eta: 0:29:34  lr: 0.000436  min_lr: 0.000001  loss: 1.8662 (1.8662)  class_acc: 0.7695 (0.7695)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 2.8398  data: 2.2269  max mem: 25582
Epoch: [24]  [100/625]  eta: 0:05:28  lr: 0.000433  min_lr: 0.000001  loss: 1.7773 (1.7661)  class_acc: 0.7891 (0.7977)  loss_scale: 16384.0000 (9408.6337)  weight_decay: 0.0500 (0.0500)  time: 0.6014  data: 0.0002  max mem: 25582
Epoch: [24]  [200/625]  eta: 0:04:21  lr: 0.000430  min_lr: 0.000001  loss: 1.7842 (1.7668)  class_acc: 0.7891 (0.7991)  loss_scale: 4096.0000 (9333.1741)  weight_decay: 0.0500 (0.0500)  time: 0.6048  data: 0.0002  max mem: 25582
Epoch: [24]  [300/625]  eta: 0:03:18  lr: 0.000426  min_lr: 0.000001  loss: 1.7217 (1.7624)  class_acc: 0.8008 (0.8001)  loss_scale: 8192.0000 (7851.8007)  weight_decay: 0.0500 (0.0500)  time: 0.6038  data: 0.0003  max mem: 25582
Epoch: [24]  [400/625]  eta: 0:02:17  lr: 0.000423  min_lr: 0.000001  loss: 1.7666 (1.7629)  class_acc: 0.7969 (0.7997)  loss_scale: 4096.0000 (7650.6334)  weight_decay: 0.0500 (0.0500)  time: 0.6053  data: 0.0002  max mem: 25582
Epoch: [24]  [500/625]  eta: 0:01:15  lr: 0.000419  min_lr: 0.000001  loss: 1.7480 (1.7615)  class_acc: 0.8008 (0.8001)  loss_scale: 4096.0000 (6941.1257)  weight_decay: 0.0500 (0.0500)  time: 0.6034  data: 0.0002  max mem: 25582
Epoch: [24]  [600/625]  eta: 0:00:15  lr: 0.000416  min_lr: 0.000001  loss: 1.7607 (1.7651)  class_acc: 0.7969 (0.7992)  loss_scale: 8192.0000 (7142.4426)  weight_decay: 0.0500 (0.0500)  time: 0.6011  data: 0.0002  max mem: 25582
Epoch: [24]  [624/625]  eta: 0:00:00  lr: 0.000415  min_lr: 0.000001  loss: 1.7637 (1.7662)  class_acc: 0.7930 (0.7991)  loss_scale: 8192.0000 (7182.7456)  weight_decay: 0.0500 (0.0500)  time: 0.5919  data: 0.0008  max mem: 25582
Epoch: [24] Total time: 0:06:19 (0.6068 s / it)
Averaged stats: lr: 0.000415  min_lr: 0.000001  loss: 1.7637 (1.7657)  class_acc: 0.7930 (0.7986)  loss_scale: 8192.0000 (7182.7456)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:00:56  loss: 0.6818 (0.6818)  acc1: 83.9844 (83.9844)  acc5: 97.6562 (97.6562)  time: 2.2688  data: 2.0190  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.6528 (0.6959)  acc1: 84.3750 (84.0000)  acc5: 97.2656 (96.8000)  time: 0.2067  data: 0.0001  max mem: 25582
Test: Total time: 0:00:07 (0.2944 s / it)
* Acc@1 83.844 Acc@5 96.808 loss 0.706
Accuracy of the network on the 50000 test images: 83.8%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:04  loss: 0.5826 (0.5826)  acc1: 85.9375 (85.9375)  acc5: 97.6562 (97.6562)  time: 2.5913  data: 2.3443  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.5230 (0.5739)  acc1: 86.7188 (85.7440)  acc5: 98.0469 (97.7120)  time: 0.2098  data: 0.0002  max mem: 25582
Test: Total time: 0:00:07 (0.3084 s / it)
* Acc@1 85.524 Acc@5 97.612 loss 0.588
EMA Accuracy of the network on the 50000 test images: 85.5%
Max accuracy: 85.52%
{"train_lr": 0.00042587345139107036, "train_min_lr": 5.562202838156605e-07, "train_loss": 1.7656541015625, "train_class_acc": 0.798584375, "train_loss_scale": 7182.7456, "train_weight_decay": 0.050000000000000495, "test_loss": 0.5880529914796352, "test_acc1": 85.52400003753662, "test_acc5": 97.61200003967285, "epoch": 24, "n_parameters": 85978600}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Epoch: [25]  [  0/625]  eta: 0:26:27  lr: 0.000415  min_lr: 0.000001  loss: 1.7246 (1.7246)  class_acc: 0.7969 (0.7969)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 2.5403  data: 1.9306  max mem: 25582
Epoch: [25]  [100/625]  eta: 0:05:26  lr: 0.000412  min_lr: 0.000001  loss: 1.7783 (1.7463)  class_acc: 0.7969 (0.8057)  loss_scale: 8192.0000 (10300.8317)  weight_decay: 0.0500 (0.0500)  time: 0.6036  data: 0.0002  max mem: 25582
Epoch: [25]  [200/625]  eta: 0:04:19  lr: 0.000408  min_lr: 0.000001  loss: 1.7383 (1.7487)  class_acc: 0.8008 (0.8037)  loss_scale: 8192.0000 (9822.2488)  weight_decay: 0.0500 (0.0500)  time: 0.5990  data: 0.0002  max mem: 25582
Epoch: [25]  [300/625]  eta: 0:03:17  lr: 0.000405  min_lr: 0.000001  loss: 1.7637 (1.7538)  class_acc: 0.7930 (0.8019)  loss_scale: 8192.0000 (9280.6379)  weight_decay: 0.0500 (0.0500)  time: 0.6062  data: 0.0003  max mem: 25582
Epoch: [25]  [400/625]  eta: 0:02:16  lr: 0.000401  min_lr: 0.000001  loss: 1.7793 (1.7530)  class_acc: 0.7891 (0.8022)  loss_scale: 8192.0000 (10051.0324)  weight_decay: 0.0500 (0.0500)  time: 0.6029  data: 0.0003  max mem: 25582
Epoch: [25]  [500/625]  eta: 0:01:15  lr: 0.000398  min_lr: 0.000001  loss: 1.7549 (1.7544)  class_acc: 0.7969 (0.8018)  loss_scale: 16384.0000 (9974.2914)  weight_decay: 0.0500 (0.0500)  time: 0.6004  data: 0.0003  max mem: 25582
Epoch: [25]  [600/625]  eta: 0:00:15  lr: 0.000394  min_lr: 0.000001  loss: 1.7500 (1.7522)  class_acc: 0.8086 (0.8026)  loss_scale: 8192.0000 (10768.1864)  weight_decay: 0.0500 (0.0500)  time: 0.6021  data: 0.0003  max mem: 25582
Epoch: [25]  [624/625]  eta: 0:00:00  lr: 0.000393  min_lr: 0.000001  loss: 1.7666 (1.7526)  class_acc: 0.7969 (0.8026)  loss_scale: 8192.0000 (10669.2608)  weight_decay: 0.0500 (0.0500)  time: 0.5889  data: 0.0008  max mem: 25582
Epoch: [25] Total time: 0:06:18 (0.6051 s / it)
Averaged stats: lr: 0.000393  min_lr: 0.000001  loss: 1.7666 (1.7495)  class_acc: 0.7969 (0.8031)  loss_scale: 8192.0000 (10669.2608)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:08  loss: 0.6858 (0.6858)  acc1: 83.9844 (83.9844)  acc5: 96.8750 (96.8750)  time: 2.7295  data: 2.4958  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.6510 (0.6939)  acc1: 84.3750 (83.6160)  acc5: 97.2656 (96.7200)  time: 0.2137  data: 0.0018  max mem: 25582
Test: Total time: 0:00:07 (0.3192 s / it)
* Acc@1 83.724 Acc@5 96.844 loss 0.703
Accuracy of the network on the 50000 test images: 83.7%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:06  loss: 0.5846 (0.5846)  acc1: 85.9375 (85.9375)  acc5: 97.6562 (97.6562)  time: 2.6475  data: 2.3928  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.5222 (0.5749)  acc1: 87.1094 (85.6800)  acc5: 98.0469 (97.7120)  time: 0.2126  data: 0.0044  max mem: 25582
Test: Total time: 0:00:07 (0.3144 s / it)
* Acc@1 85.522 Acc@5 97.614 loss 0.588
EMA Accuracy of the network on the 50000 test images: 85.5%
Max accuracy: 85.52%
{"train_lr": 0.00040415306767059887, "train_min_lr": 5.27851955247343e-07, "train_loss": 1.7495068359375, "train_class_acc": 0.8031484375, "train_loss_scale": 10669.2608, "train_weight_decay": 0.050000000000000495, "test_loss": 0.5881277234852313, "test_acc1": 85.5220000402832, "test_acc5": 97.61400003967285, "epoch": 25, "n_parameters": 85978600}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Epoch: [26]  [  0/625]  eta: 0:30:47  lr: 0.000393  min_lr: 0.000001  loss: 1.7129 (1.7129)  class_acc: 0.8164 (0.8164)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 2.9558  data: 2.3271  max mem: 25582
Epoch: [26]  [100/625]  eta: 0:05:28  lr: 0.000389  min_lr: 0.000001  loss: 1.7119 (1.7312)  class_acc: 0.8047 (0.8097)  loss_scale: 8192.0000 (8354.2178)  weight_decay: 0.0500 (0.0500)  time: 0.6006  data: 0.0002  max mem: 25582
Epoch: [26]  [200/625]  eta: 0:04:21  lr: 0.000386  min_lr: 0.000001  loss: 1.7021 (1.7252)  class_acc: 0.8008 (0.8104)  loss_scale: 4096.0000 (6724.7761)  weight_decay: 0.0500 (0.0500)  time: 0.6069  data: 0.0002  max mem: 25582
Epoch: [26]  [300/625]  eta: 0:03:18  lr: 0.000382  min_lr: 0.000000  loss: 1.7021 (1.7249)  class_acc: 0.8086 (0.8097)  loss_scale: 8192.0000 (6491.0033)  weight_decay: 0.0500 (0.0500)  time: 0.6026  data: 0.0002  max mem: 25582
Epoch: [26]  [400/625]  eta: 0:02:17  lr: 0.000379  min_lr: 0.000000  loss: 1.7236 (1.7273)  class_acc: 0.8008 (0.8088)  loss_scale: 16384.0000 (7303.3416)  weight_decay: 0.0500 (0.0500)  time: 0.6068  data: 0.0002  max mem: 25582
Epoch: [26]  [500/625]  eta: 0:01:16  lr: 0.000375  min_lr: 0.000000  loss: 1.7578 (1.7304)  class_acc: 0.8047 (0.8080)  loss_scale: 8192.0000 (7529.7725)  weight_decay: 0.0500 (0.0500)  time: 0.6023  data: 0.0002  max mem: 25582
Epoch: [26]  [600/625]  eta: 0:00:15  lr: 0.000371  min_lr: 0.000000  loss: 1.7158 (1.7311)  class_acc: 0.8008 (0.8080)  loss_scale: 8192.0000 (7885.3111)  weight_decay: 0.0500 (0.0500)  time: 0.6021  data: 0.0002  max mem: 25582
Epoch: [26]  [624/625]  eta: 0:00:00  lr: 0.000370  min_lr: 0.000000  loss: 1.7158 (1.7315)  class_acc: 0.8047 (0.8078)  loss_scale: 8192.0000 (7897.0880)  weight_decay: 0.0500 (0.0500)  time: 0.5885  data: 0.0010  max mem: 25582
Epoch: [26] Total time: 0:06:19 (0.6069 s / it)
Averaged stats: lr: 0.000370  min_lr: 0.000000  loss: 1.7158 (1.7333)  class_acc: 0.8047 (0.8073)  loss_scale: 8192.0000 (7897.0880)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:07  loss: 0.6807 (0.6807)  acc1: 85.1562 (85.1562)  acc5: 97.6562 (97.6562)  time: 2.6973  data: 2.4621  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.6464 (0.6981)  acc1: 83.9844 (83.6640)  acc5: 97.2656 (97.1040)  time: 0.2104  data: 0.0034  max mem: 25582
Test: Total time: 0:00:07 (0.3147 s / it)
* Acc@1 83.682 Acc@5 96.786 loss 0.711
Accuracy of the network on the 50000 test images: 83.7%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:01  loss: 0.5856 (0.5856)  acc1: 86.3281 (86.3281)  acc5: 97.6562 (97.6562)  time: 2.4662  data: 2.2354  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.5220 (0.5760)  acc1: 86.3281 (85.7280)  acc5: 98.0469 (97.6960)  time: 0.2120  data: 0.0002  max mem: 25582
Test: Total time: 0:00:07 (0.3054 s / it)
* Acc@1 85.568 Acc@5 97.632 loss 0.589
EMA Accuracy of the network on the 50000 test images: 85.6%
Max accuracy: 85.57%
{"train_lr": 0.00038179362831979594, "train_min_lr": 4.986489756743277e-07, "train_loss": 1.7333306640625, "train_class_acc": 0.8073234375, "train_loss_scale": 7897.088, "train_weight_decay": 0.050000000000000495, "test_loss": 0.5888955819606781, "test_acc1": 85.56800003479005, "test_acc5": 97.63200002349853, "epoch": 26, "n_parameters": 85978600}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Epoch: [27]  [  0/625]  eta: 0:24:43  lr: 0.000370  min_lr: 0.000000  loss: 1.5850 (1.5850)  class_acc: 0.8125 (0.8125)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 2.3735  data: 1.7499  max mem: 25582
Epoch: [27]  [100/625]  eta: 0:05:25  lr: 0.000367  min_lr: 0.000000  loss: 1.6982 (1.6944)  class_acc: 0.8125 (0.8168)  loss_scale: 8192.0000 (8678.6535)  weight_decay: 0.0500 (0.0500)  time: 0.6055  data: 0.0002  max mem: 25582
Epoch: [27]  [200/625]  eta: 0:04:19  lr: 0.000363  min_lr: 0.000000  loss: 1.6680 (1.6988)  class_acc: 0.8203 (0.8166)  loss_scale: 16384.0000 (8884.8557)  weight_decay: 0.0500 (0.0500)  time: 0.6036  data: 0.0002  max mem: 25582
Epoch: [27]  [300/625]  eta: 0:03:17  lr: 0.000359  min_lr: 0.000000  loss: 1.7002 (1.7055)  class_acc: 0.8086 (0.8153)  loss_scale: 8192.0000 (9716.0930)  weight_decay: 0.0500 (0.0500)  time: 0.6026  data: 0.0002  max mem: 25582
Epoch: [27]  [400/625]  eta: 0:02:16  lr: 0.000356  min_lr: 0.000000  loss: 1.7139 (1.7078)  class_acc: 0.8086 (0.8148)  loss_scale: 4096.0000 (8621.0075)  weight_decay: 0.0500 (0.0500)  time: 0.6022  data: 0.0003  max mem: 25582
Epoch: [27]  [500/625]  eta: 0:01:15  lr: 0.000352  min_lr: 0.000000  loss: 1.7061 (1.7101)  class_acc: 0.8125 (0.8147)  loss_scale: 8192.0000 (8053.0140)  weight_decay: 0.0500 (0.0500)  time: 0.6029  data: 0.0003  max mem: 25582
Epoch: [27]  [600/625]  eta: 0:00:15  lr: 0.000348  min_lr: 0.000000  loss: 1.7021 (1.7099)  class_acc: 0.8164 (0.8149)  loss_scale: 4096.0000 (7476.3927)  weight_decay: 0.0500 (0.0500)  time: 0.6026  data: 0.0002  max mem: 25582
Epoch: [27]  [624/625]  eta: 0:00:00  lr: 0.000347  min_lr: 0.000000  loss: 1.7432 (1.7107)  class_acc: 0.7969 (0.8146)  loss_scale: 4096.0000 (7346.5856)  weight_decay: 0.0500 (0.0500)  time: 0.5890  data: 0.0008  max mem: 25582
Epoch: [27] Total time: 0:06:17 (0.6047 s / it)
Averaged stats: lr: 0.000347  min_lr: 0.000000  loss: 1.7432 (1.7164)  class_acc: 0.7969 (0.8126)  loss_scale: 4096.0000 (7346.5856)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:07  loss: 0.7130 (0.7130)  acc1: 83.9844 (83.9844)  acc5: 96.8750 (96.8750)  time: 2.7133  data: 2.4707  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.6572 (0.6973)  acc1: 83.9844 (83.8560)  acc5: 97.2656 (96.9440)  time: 0.2124  data: 0.0018  max mem: 25582
Test: Total time: 0:00:07 (0.3174 s / it)
* Acc@1 83.876 Acc@5 96.776 loss 0.704
Accuracy of the network on the 50000 test images: 83.9%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:07  loss: 0.5870 (0.5870)  acc1: 86.7188 (86.7188)  acc5: 97.6562 (97.6562)  time: 2.6934  data: 2.4457  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.5223 (0.5775)  acc1: 86.7188 (85.9200)  acc5: 98.0469 (97.7280)  time: 0.2084  data: 0.0005  max mem: 25582
Test: Total time: 0:00:07 (0.3130 s / it)
* Acc@1 85.600 Acc@5 97.618 loss 0.590
EMA Accuracy of the network on the 50000 test images: 85.6%
Max accuracy: 85.60%
{"train_lr": 0.0003589329867175267, "train_min_lr": 4.687913911766606e-07, "train_loss": 1.7164265625, "train_class_acc": 0.81257109375, "train_loss_scale": 7346.5856, "train_weight_decay": 0.050000000000000495, "test_loss": 0.5904486311972141, "test_acc1": 85.60000003479004, "test_acc5": 97.61800002349854, "epoch": 27, "n_parameters": 85978600}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Epoch: [28]  [  0/625]  eta: 0:25:11  lr: 0.000347  min_lr: 0.000000  loss: 1.6572 (1.6572)  class_acc: 0.8438 (0.8438)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  time: 2.4185  data: 1.8109  max mem: 25582
Epoch: [28]  [100/625]  eta: 0:05:24  lr: 0.000344  min_lr: 0.000000  loss: 1.6768 (1.6884)  class_acc: 0.8281 (0.8239)  loss_scale: 8192.0000 (7502.5743)  weight_decay: 0.0500 (0.0500)  time: 0.6023  data: 0.0002  max mem: 25582
Epoch: [28]  [200/625]  eta: 0:04:19  lr: 0.000340  min_lr: 0.000000  loss: 1.6797 (1.6964)  class_acc: 0.8281 (0.8203)  loss_scale: 16384.0000 (10127.9204)  weight_decay: 0.0500 (0.0500)  time: 0.5979  data: 0.0003  max mem: 25582
Epoch: [28]  [300/625]  eta: 0:03:17  lr: 0.000336  min_lr: 0.000000  loss: 1.7432 (1.7068)  class_acc: 0.8008 (0.8164)  loss_scale: 4096.0000 (9593.6213)  weight_decay: 0.0500 (0.0500)  time: 0.5983  data: 0.0002  max mem: 25582
Epoch: [28]  [400/625]  eta: 0:02:16  lr: 0.000332  min_lr: 0.000000  loss: 1.6484 (1.7056)  class_acc: 0.8242 (0.8163)  loss_scale: 8192.0000 (8539.2918)  weight_decay: 0.0500 (0.0500)  time: 0.5993  data: 0.0003  max mem: 25582
Epoch: [28]  [500/625]  eta: 0:01:15  lr: 0.000329  min_lr: 0.000000  loss: 1.7070 (1.7043)  class_acc: 0.8125 (0.8166)  loss_scale: 8192.0000 (8519.0259)  weight_decay: 0.0500 (0.0500)  time: 0.6036  data: 0.0002  max mem: 25582
Epoch: [28]  [600/625]  eta: 0:00:15  lr: 0.000325  min_lr: 0.000000  loss: 1.7090 (1.7046)  class_acc: 0.8164 (0.8163)  loss_scale: 8192.0000 (8682.7022)  weight_decay: 0.0500 (0.0500)  time: 0.6013  data: 0.0002  max mem: 25582
Epoch: [28]  [624/625]  eta: 0:00:00  lr: 0.000324  min_lr: 0.000000  loss: 1.6689 (1.7038)  class_acc: 0.8086 (0.8167)  loss_scale: 8192.0000 (8663.8592)  weight_decay: 0.0500 (0.0500)  time: 0.5904  data: 0.0008  max mem: 25582
Epoch: [28] Total time: 0:06:17 (0.6037 s / it)
Averaged stats: lr: 0.000324  min_lr: 0.000000  loss: 1.6689 (1.7034)  class_acc: 0.8086 (0.8158)  loss_scale: 8192.0000 (8663.8592)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:10  loss: 0.6826 (0.6826)  acc1: 85.5469 (85.5469)  acc5: 97.6562 (97.6562)  time: 2.8207  data: 2.5885  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.6506 (0.6986)  acc1: 85.5469 (83.9840)  acc5: 97.2656 (96.8800)  time: 0.2124  data: 0.0002  max mem: 25582
Test: Total time: 0:00:08 (0.3221 s / it)
* Acc@1 83.870 Acc@5 96.804 loss 0.706
Accuracy of the network on the 50000 test images: 83.9%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:04  loss: 0.5888 (0.5888)  acc1: 86.7188 (86.7188)  acc5: 97.6562 (97.6562)  time: 2.5826  data: 2.3456  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.5219 (0.5799)  acc1: 85.9375 (85.9040)  acc5: 98.4375 (97.7920)  time: 0.2108  data: 0.0017  max mem: 25582
Test: Total time: 0:00:07 (0.3103 s / it)
* Acc@1 85.598 Acc@5 97.612 loss 0.593
EMA Accuracy of the network on the 50000 test images: 85.6%
Max accuracy: 85.60%
{"train_lr": 0.00033571208632120344, "train_min_lr": 4.384632836914219e-07, "train_loss": 1.703367578125, "train_class_acc": 0.8158328125, "train_loss_scale": 8663.8592, "train_weight_decay": 0.050000000000000495, "test_loss": 0.5926311238110066, "test_acc1": 85.59800004821777, "test_acc5": 97.61200001800538, "epoch": 28, "n_parameters": 85978600}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Epoch: [29]  [  0/625]  eta: 0:29:25  lr: 0.000324  min_lr: 0.000000  loss: 1.7363 (1.7363)  class_acc: 0.8047 (0.8047)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 2.8249  data: 1.9627  max mem: 25582
Epoch: [29]  [100/625]  eta: 0:05:27  lr: 0.000320  min_lr: 0.000000  loss: 1.6670 (1.6850)  class_acc: 0.8320 (0.8226)  loss_scale: 4096.0000 (6285.9406)  weight_decay: 0.0500 (0.0500)  time: 0.6023  data: 0.0002  max mem: 25582
Epoch: [29]  [200/625]  eta: 0:04:20  lr: 0.000316  min_lr: 0.000000  loss: 1.6367 (1.6779)  class_acc: 0.8242 (0.8243)  loss_scale: 8192.0000 (5889.2736)  weight_decay: 0.0500 (0.0500)  time: 0.6000  data: 0.0002  max mem: 25582
Epoch: [29]  [300/625]  eta: 0:03:17  lr: 0.000313  min_lr: 0.000000  loss: 1.6973 (1.6833)  class_acc: 0.8164 (0.8224)  loss_scale: 8192.0000 (6817.5947)  weight_decay: 0.0500 (0.0500)  time: 0.6004  data: 0.0002  max mem: 25582
Epoch: [29]  [400/625]  eta: 0:02:16  lr: 0.000309  min_lr: 0.000000  loss: 1.6729 (1.6846)  class_acc: 0.8242 (0.8218)  loss_scale: 8192.0000 (7323.7706)  weight_decay: 0.0500 (0.0500)  time: 0.5999  data: 0.0002  max mem: 25582
Epoch: [29]  [500/625]  eta: 0:01:15  lr: 0.000305  min_lr: 0.000000  loss: 1.7236 (1.6864)  class_acc: 0.8086 (0.8209)  loss_scale: 8192.0000 (7562.4750)  weight_decay: 0.0500 (0.0500)  time: 0.5998  data: 0.0002  max mem: 25582
Epoch: [29]  [600/625]  eta: 0:00:15  lr: 0.000301  min_lr: 0.000000  loss: 1.6621 (1.6868)  class_acc: 0.8164 (0.8208)  loss_scale: 16384.0000 (8076.1398)  weight_decay: 0.0500 (0.0500)  time: 0.6020  data: 0.0002  max mem: 25582
Epoch: [29]  [624/625]  eta: 0:00:00  lr: 0.000301  min_lr: 0.000000  loss: 1.7021 (1.6871)  class_acc: 0.8164 (0.8207)  loss_scale: 8192.0000 (8106.8032)  weight_decay: 0.0500 (0.0500)  time: 0.5903  data: 0.0006  max mem: 25582
Epoch: [29] Total time: 0:06:18 (0.6050 s / it)
Averaged stats: lr: 0.000301  min_lr: 0.000000  loss: 1.7021 (1.6875)  class_acc: 0.8164 (0.8202)  loss_scale: 8192.0000 (8106.8032)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:00:58  loss: 0.7010 (0.7010)  acc1: 85.5469 (85.5469)  acc5: 96.8750 (96.8750)  time: 2.3333  data: 2.1016  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.6363 (0.7076)  acc1: 84.7656 (83.9520)  acc5: 97.2656 (96.7680)  time: 0.2129  data: 0.0079  max mem: 25582
Test: Total time: 0:00:07 (0.3016 s / it)
* Acc@1 83.768 Acc@5 96.704 loss 0.714
Accuracy of the network on the 50000 test images: 83.8%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:08  loss: 0.5951 (0.5951)  acc1: 86.7188 (86.7188)  acc5: 97.6562 (97.6562)  time: 2.7385  data: 2.4829  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.5249 (0.5829)  acc1: 86.7188 (86.0000)  acc5: 98.0469 (97.7440)  time: 0.2075  data: 0.0010  max mem: 25582
Test: Total time: 0:00:07 (0.3137 s / it)
* Acc@1 85.656 Acc@5 97.574 loss 0.595
EMA Accuracy of the network on the 50000 test images: 85.7%
Max accuracy: 85.66%
{"train_lr": 0.00031227409170350387, "train_min_lr": 4.078516360863777e-07, "train_loss": 1.687476953125, "train_class_acc": 0.82015625, "train_loss_scale": 8106.8032, "train_weight_decay": 0.050000000000000495, "test_loss": 0.5953858877718449, "test_acc1": 85.65600004821778, "test_acc5": 97.57400001800536, "epoch": 29, "n_parameters": 85978600}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Epoch: [30]  [  0/625]  eta: 0:27:57  lr: 0.000300  min_lr: 0.000000  loss: 1.6445 (1.6445)  class_acc: 0.8516 (0.8516)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 2.6833  data: 2.0570  max mem: 25582
Epoch: [30]  [100/625]  eta: 0:05:27  lr: 0.000297  min_lr: 0.000000  loss: 1.6729 (1.6637)  class_acc: 0.8086 (0.8276)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5998  data: 0.0002  max mem: 25582
Epoch: [30]  [200/625]  eta: 0:04:20  lr: 0.000293  min_lr: 0.000000  loss: 1.6611 (1.6674)  class_acc: 0.8242 (0.8255)  loss_scale: 8192.0000 (8884.8557)  weight_decay: 0.0500 (0.0500)  time: 0.6045  data: 0.0002  max mem: 25582
Epoch: [30]  [300/625]  eta: 0:03:18  lr: 0.000289  min_lr: 0.000000  loss: 1.6494 (1.6633)  class_acc: 0.8203 (0.8266)  loss_scale: 8192.0000 (9661.6611)  weight_decay: 0.0500 (0.0500)  time: 0.6043  data: 0.0002  max mem: 25582
Epoch: [30]  [400/625]  eta: 0:02:16  lr: 0.000285  min_lr: 0.000000  loss: 1.6426 (1.6646)  class_acc: 0.8242 (0.8264)  loss_scale: 8192.0000 (9295.1621)  weight_decay: 0.0500 (0.0500)  time: 0.6045  data: 0.0002  max mem: 25582
Epoch: [30]  [500/625]  eta: 0:01:15  lr: 0.000282  min_lr: 0.000000  loss: 1.6553 (1.6673)  class_acc: 0.8242 (0.8260)  loss_scale: 8192.0000 (10056.0479)  weight_decay: 0.0500 (0.0500)  time: 0.6012  data: 0.0002  max mem: 25582
Epoch: [30]  [600/625]  eta: 0:00:15  lr: 0.000278  min_lr: 0.000000  loss: 1.6787 (1.6678)  class_acc: 0.8242 (0.8258)  loss_scale: 8192.0000 (9745.8902)  weight_decay: 0.0500 (0.0500)  time: 0.6027  data: 0.0002  max mem: 25582
Epoch: [30]  [624/625]  eta: 0:00:00  lr: 0.000277  min_lr: 0.000000  loss: 1.6426 (1.6682)  class_acc: 0.8242 (0.8255)  loss_scale: 8192.0000 (9764.8640)  weight_decay: 0.0500 (0.0500)  time: 0.5873  data: 0.0007  max mem: 25582
Epoch: [30] Total time: 0:06:18 (0.6055 s / it)
Averaged stats: lr: 0.000277  min_lr: 0.000000  loss: 1.6426 (1.6696)  class_acc: 0.8242 (0.8252)  loss_scale: 8192.0000 (9764.8640)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:05  loss: 0.7257 (0.7257)  acc1: 85.5469 (85.5469)  acc5: 97.2656 (97.2656)  time: 2.6193  data: 2.3804  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.6560 (0.7006)  acc1: 84.7656 (84.2240)  acc5: 97.2656 (96.9920)  time: 0.2110  data: 0.0029  max mem: 25582
Test: Total time: 0:00:07 (0.3125 s / it)
* Acc@1 83.954 Acc@5 96.818 loss 0.713
Accuracy of the network on the 50000 test images: 84.0%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:09  loss: 0.5981 (0.5981)  acc1: 86.7188 (86.7188)  acc5: 97.6562 (97.6562)  time: 2.7842  data: 2.5376  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.5271 (0.5860)  acc1: 86.7188 (85.9680)  acc5: 98.0469 (97.7440)  time: 0.2074  data: 0.0024  max mem: 25582
Test: Total time: 0:00:07 (0.3161 s / it)
* Acc@1 85.598 Acc@5 97.552 loss 0.599
EMA Accuracy of the network on the 50000 test images: 85.6%
Max accuracy: 85.66%
{"train_lr": 0.0002887635058951694, "train_min_lr": 3.7714517934842107e-07, "train_loss": 1.6696306640625, "train_class_acc": 0.825159375, "train_loss_scale": 9764.864, "train_weight_decay": 0.050000000000000495, "test_loss": 0.5986766800284385, "test_acc1": 85.59800004821777, "test_acc5": 97.55200001525878, "epoch": 30, "n_parameters": 85978600}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Epoch: [31]  [  0/625]  eta: 0:31:20  lr: 0.000277  min_lr: 0.000000  loss: 1.6719 (1.6719)  class_acc: 0.8320 (0.8320)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 3.0088  data: 2.3786  max mem: 25582
Epoch: [31]  [100/625]  eta: 0:05:29  lr: 0.000273  min_lr: 0.000000  loss: 1.6221 (1.6519)  class_acc: 0.8359 (0.8313)  loss_scale: 4096.0000 (6326.4950)  weight_decay: 0.0500 (0.0500)  time: 0.6035  data: 0.0003  max mem: 25582
Epoch: [31]  [200/625]  eta: 0:04:21  lr: 0.000270  min_lr: 0.000000  loss: 1.6211 (1.6505)  class_acc: 0.8320 (0.8312)  loss_scale: 2048.0000 (4860.1791)  weight_decay: 0.0500 (0.0500)  time: 0.6027  data: 0.0003  max mem: 25582
Epoch: [31]  [300/625]  eta: 0:03:18  lr: 0.000266  min_lr: 0.000000  loss: 1.6631 (1.6609)  class_acc: 0.8242 (0.8284)  loss_scale: 2048.0000 (3966.7243)  weight_decay: 0.0500 (0.0500)  time: 0.5992  data: 0.0002  max mem: 25582
Epoch: [31]  [400/625]  eta: 0:02:16  lr: 0.000262  min_lr: 0.000000  loss: 1.6738 (1.6640)  class_acc: 0.8203 (0.8280)  loss_scale: 4096.0000 (3998.9626)  weight_decay: 0.0500 (0.0500)  time: 0.5972  data: 0.0002  max mem: 25582
Epoch: [31]  [500/625]  eta: 0:01:15  lr: 0.000258  min_lr: 0.000000  loss: 1.6553 (1.6645)  class_acc: 0.8281 (0.8275)  loss_scale: 8192.0000 (4639.6806)  weight_decay: 0.0500 (0.0500)  time: 0.5996  data: 0.0002  max mem: 25582
Epoch: [31]  [600/625]  eta: 0:00:15  lr: 0.000255  min_lr: 0.000000  loss: 1.6475 (1.6639)  class_acc: 0.8281 (0.8273)  loss_scale: 4096.0000 (4549.2180)  weight_decay: 0.0500 (0.0500)  time: 0.6007  data: 0.0002  max mem: 25582
Epoch: [31]  [624/625]  eta: 0:00:00  lr: 0.000254  min_lr: 0.000000  loss: 1.6650 (1.6645)  class_acc: 0.8242 (0.8271)  loss_scale: 4096.0000 (4531.8144)  weight_decay: 0.0500 (0.0500)  time: 0.5904  data: 0.0010  max mem: 25582
Epoch: [31] Total time: 0:06:18 (0.6063 s / it)
Averaged stats: lr: 0.000254  min_lr: 0.000000  loss: 1.6650 (1.6559)  class_acc: 0.8242 (0.8294)  loss_scale: 4096.0000 (4531.8144)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:06  loss: 0.6402 (0.6402)  acc1: 86.7188 (86.7188)  acc5: 97.2656 (97.2656)  time: 2.6720  data: 2.4201  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.6822 (0.6974)  acc1: 84.3750 (84.2880)  acc5: 97.2656 (97.0400)  time: 0.2101  data: 0.0002  max mem: 25582
Test: Total time: 0:00:07 (0.3135 s / it)
* Acc@1 83.890 Acc@5 96.700 loss 0.719
Accuracy of the network on the 50000 test images: 83.9%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:05  loss: 0.6023 (0.6023)  acc1: 87.1094 (87.1094)  acc5: 97.6562 (97.6562)  time: 2.6016  data: 2.3548  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.5288 (0.5892)  acc1: 86.7188 (86.0000)  acc5: 98.0469 (97.6800)  time: 0.2087  data: 0.0014  max mem: 25582
Test: Total time: 0:00:07 (0.3092 s / it)
* Acc@1 85.558 Acc@5 97.528 loss 0.602
EMA Accuracy of the network on the 50000 test images: 85.6%
Max accuracy: 85.66%
{"train_lr": 0.00026532527947577104, "train_min_lr": 3.465332289942728e-07, "train_loss": 1.6559482421875, "train_class_acc": 0.82938125, "train_loss_scale": 4531.8144, "train_weight_decay": 0.050000000000000495, "test_loss": 0.6023112210631371, "test_acc1": 85.55800003753662, "test_acc5": 97.52800002075195, "epoch": 31, "n_parameters": 85978600}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Epoch: [32]  [  0/625]  eta: 0:27:44  lr: 0.000254  min_lr: 0.000000  loss: 1.6113 (1.6113)  class_acc: 0.8477 (0.8477)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  time: 2.6628  data: 2.0419  max mem: 25582
Epoch: [32]  [100/625]  eta: 0:05:29  lr: 0.000250  min_lr: 0.000000  loss: 1.6104 (1.6369)  class_acc: 0.8398 (0.8350)  loss_scale: 8192.0000 (8070.3366)  weight_decay: 0.0500 (0.0500)  time: 0.6037  data: 0.0002  max mem: 25582
Epoch: [32]  [200/625]  eta: 0:04:22  lr: 0.000246  min_lr: 0.000000  loss: 1.6523 (1.6387)  class_acc: 0.8281 (0.8346)  loss_scale: 16384.0000 (10983.8010)  weight_decay: 0.0500 (0.0500)  time: 0.6073  data: 0.0003  max mem: 25582
Epoch: [32]  [300/625]  eta: 0:03:18  lr: 0.000243  min_lr: 0.000000  loss: 1.6348 (1.6363)  class_acc: 0.8359 (0.8347)  loss_scale: 8192.0000 (10818.3389)  weight_decay: 0.0500 (0.0500)  time: 0.6018  data: 0.0002  max mem: 25582
Epoch: [32]  [400/625]  eta: 0:02:17  lr: 0.000239  min_lr: 0.000000  loss: 1.6553 (1.6438)  class_acc: 0.8281 (0.8327)  loss_scale: 16384.0000 (11041.8354)  weight_decay: 0.0500 (0.0500)  time: 0.6004  data: 0.0003  max mem: 25582
Epoch: [32]  [500/625]  eta: 0:01:16  lr: 0.000235  min_lr: 0.000000  loss: 1.6035 (1.6416)  class_acc: 0.8398 (0.8332)  loss_scale: 8192.0000 (11274.2196)  weight_decay: 0.0500 (0.0500)  time: 0.5999  data: 0.0002  max mem: 25582
Epoch: [32]  [600/625]  eta: 0:00:15  lr: 0.000231  min_lr: 0.000000  loss: 1.6035 (1.6417)  class_acc: 0.8281 (0.8331)  loss_scale: 4096.0000 (10355.8602)  weight_decay: 0.0500 (0.0500)  time: 0.5994  data: 0.0002  max mem: 25582
Epoch: [32]  [624/625]  eta: 0:00:00  lr: 0.000231  min_lr: 0.000000  loss: 1.5938 (1.6411)  class_acc: 0.8398 (0.8333)  loss_scale: 2048.0000 (10036.8384)  weight_decay: 0.0500 (0.0500)  time: 0.5890  data: 0.0006  max mem: 25582
Epoch: [32] Total time: 0:06:19 (0.6072 s / it)
Averaged stats: lr: 0.000231  min_lr: 0.000000  loss: 1.5938 (1.6430)  class_acc: 0.8398 (0.8331)  loss_scale: 2048.0000 (10036.8384)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:02  loss: 0.7548 (0.7548)  acc1: 83.9844 (83.9844)  acc5: 96.4844 (96.4844)  time: 2.5184  data: 2.2748  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.6399 (0.7013)  acc1: 84.3750 (84.2400)  acc5: 97.2656 (96.8960)  time: 0.2096  data: 0.0019  max mem: 25582
Test: Total time: 0:00:07 (0.3054 s / it)
* Acc@1 83.950 Acc@5 96.676 loss 0.717
Accuracy of the network on the 50000 test images: 84.0%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:05  loss: 0.6096 (0.6096)  acc1: 86.7188 (86.7188)  acc5: 97.6562 (97.6562)  time: 2.6052  data: 2.3499  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.5313 (0.5931)  acc1: 86.3281 (85.9520)  acc5: 98.0469 (97.6480)  time: 0.2102  data: 0.0002  max mem: 25582
Test: Total time: 0:00:07 (0.3100 s / it)
* Acc@1 85.512 Acc@5 97.526 loss 0.606
EMA Accuracy of the network on the 50000 test images: 85.5%
Max accuracy: 85.66%
{"train_lr": 0.00024210391690518591, "train_min_lr": 3.162045178773723e-07, "train_loss": 1.64296328125, "train_class_acc": 0.8330671875, "train_loss_scale": 10036.8384, "train_weight_decay": 0.050000000000000495, "test_loss": 0.6063270926475525, "test_acc1": 85.51200002410889, "test_acc5": 97.52600002075195, "epoch": 32, "n_parameters": 85978600}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Epoch: [33]  [  0/625]  eta: 0:29:24  lr: 0.000231  min_lr: 0.000000  loss: 1.6689 (1.6689)  class_acc: 0.8125 (0.8125)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  time: 2.8233  data: 2.2172  max mem: 25582
Epoch: [33]  [100/625]  eta: 0:05:28  lr: 0.000227  min_lr: 0.000000  loss: 1.6035 (1.6124)  class_acc: 0.8398 (0.8417)  loss_scale: 2048.0000 (2149.3861)  weight_decay: 0.0500 (0.0500)  time: 0.6034  data: 0.0002  max mem: 25582
Epoch: [33]  [200/625]  eta: 0:04:20  lr: 0.000223  min_lr: 0.000000  loss: 1.5996 (1.6201)  class_acc: 0.8359 (0.8393)  loss_scale: 4096.0000 (3117.8507)  weight_decay: 0.0500 (0.0500)  time: 0.5967  data: 0.0002  max mem: 25582
Epoch: [33]  [300/625]  eta: 0:03:17  lr: 0.000220  min_lr: 0.000000  loss: 1.6045 (1.6224)  class_acc: 0.8438 (0.8390)  loss_scale: 8192.0000 (4490.6312)  weight_decay: 0.0500 (0.0500)  time: 0.6036  data: 0.0002  max mem: 25582
Epoch: [33]  [400/625]  eta: 0:02:16  lr: 0.000216  min_lr: 0.000000  loss: 1.6250 (1.6247)  class_acc: 0.8281 (0.8381)  loss_scale: 8192.0000 (5699.6708)  weight_decay: 0.0500 (0.0500)  time: 0.5993  data: 0.0002  max mem: 25582
Epoch: [33]  [500/625]  eta: 0:01:15  lr: 0.000212  min_lr: 0.000000  loss: 1.6182 (1.6239)  class_acc: 0.8438 (0.8382)  loss_scale: 8192.0000 (6123.5609)  weight_decay: 0.0500 (0.0500)  time: 0.5956  data: 0.0002  max mem: 25582
Epoch: [33]  [600/625]  eta: 0:00:15  lr: 0.000209  min_lr: 0.000000  loss: 1.6270 (1.6241)  class_acc: 0.8320 (0.8382)  loss_scale: 4096.0000 (5786.1963)  weight_decay: 0.0500 (0.0500)  time: 0.6013  data: 0.0002  max mem: 25582
Epoch: [33]  [624/625]  eta: 0:00:00  lr: 0.000208  min_lr: 0.000000  loss: 1.5977 (1.6248)  class_acc: 0.8359 (0.8379)  loss_scale: 4096.0000 (5747.5072)  weight_decay: 0.0500 (0.0500)  time: 0.5883  data: 0.0009  max mem: 25582
Epoch: [33] Total time: 0:06:17 (0.6041 s / it)
Averaged stats: lr: 0.000208  min_lr: 0.000000  loss: 1.5977 (1.6267)  class_acc: 0.8359 (0.8378)  loss_scale: 4096.0000 (5747.5072)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:07  loss: 0.7115 (0.7115)  acc1: 85.5469 (85.5469)  acc5: 97.2656 (97.2656)  time: 2.7174  data: 2.4683  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.6509 (0.7046)  acc1: 83.9844 (84.3200)  acc5: 97.2656 (97.0240)  time: 0.2095  data: 0.0002  max mem: 25582
Test: Total time: 0:00:07 (0.3143 s / it)
* Acc@1 84.068 Acc@5 96.712 loss 0.721
Accuracy of the network on the 50000 test images: 84.1%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:07  loss: 0.6166 (0.6166)  acc1: 86.3281 (86.3281)  acc5: 97.6562 (97.6562)  time: 2.7155  data: 2.4782  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.5339 (0.5978)  acc1: 86.7188 (85.8720)  acc5: 98.0469 (97.5680)  time: 0.2090  data: 0.0002  max mem: 25582
Test: Total time: 0:00:07 (0.3131 s / it)
* Acc@1 85.464 Acc@5 97.520 loss 0.611
EMA Accuracy of the network on the 50000 test images: 85.5%
Max accuracy: 85.66%
{"train_lr": 0.00021924258560554902, "train_min_lr": 2.8634603258707586e-07, "train_loss": 1.626723046875, "train_class_acc": 0.83784140625, "train_loss_scale": 5747.5072, "train_weight_decay": 0.050000000000000495, "test_loss": 0.6108713507652282, "test_acc1": 85.46400004028321, "test_acc5": 97.52000002075195, "epoch": 33, "n_parameters": 85978600}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Epoch: [34]  [  0/625]  eta: 0:28:32  lr: 0.000208  min_lr: 0.000000  loss: 1.5693 (1.5693)  class_acc: 0.8398 (0.8398)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 2.7397  data: 2.0305  max mem: 25582
Epoch: [34]  [100/625]  eta: 0:05:27  lr: 0.000204  min_lr: 0.000000  loss: 1.6006 (1.6157)  class_acc: 0.8438 (0.8402)  loss_scale: 4096.0000 (4988.1980)  weight_decay: 0.0500 (0.0500)  time: 0.6021  data: 0.0002  max mem: 25582
Epoch: [34]  [200/625]  eta: 0:04:20  lr: 0.000201  min_lr: 0.000000  loss: 1.6143 (1.6188)  class_acc: 0.8398 (0.8402)  loss_scale: 8192.0000 (5563.2239)  weight_decay: 0.0500 (0.0500)  time: 0.5989  data: 0.0003  max mem: 25582
Epoch: [34]  [300/625]  eta: 0:03:17  lr: 0.000197  min_lr: 0.000000  loss: 1.6221 (1.6208)  class_acc: 0.8320 (0.8393)  loss_scale: 16384.0000 (7035.3223)  weight_decay: 0.0500 (0.0500)  time: 0.5976  data: 0.0002  max mem: 25582
Epoch: [34]  [400/625]  eta: 0:02:16  lr: 0.000194  min_lr: 0.000000  loss: 1.5947 (1.6203)  class_acc: 0.8359 (0.8391)  loss_scale: 4096.0000 (7476.9875)  weight_decay: 0.0500 (0.0500)  time: 0.6016  data: 0.0002  max mem: 25582
Epoch: [34]  [500/625]  eta: 0:01:15  lr: 0.000190  min_lr: 0.000000  loss: 1.6328 (1.6213)  class_acc: 0.8398 (0.8389)  loss_scale: 8192.0000 (6932.9501)  weight_decay: 0.0500 (0.0500)  time: 0.6029  data: 0.0002  max mem: 25582
Epoch: [34]  [600/625]  eta: 0:00:15  lr: 0.000187  min_lr: 0.000000  loss: 1.5801 (1.6195)  class_acc: 0.8438 (0.8391)  loss_scale: 4096.0000 (6508.6190)  weight_decay: 0.0500 (0.0500)  time: 0.5999  data: 0.0002  max mem: 25582
Epoch: [34]  [624/625]  eta: 0:00:00  lr: 0.000186  min_lr: 0.000000  loss: 1.6162 (1.6202)  class_acc: 0.8359 (0.8388)  loss_scale: 4096.0000 (6415.9744)  weight_decay: 0.0500 (0.0500)  time: 0.5891  data: 0.0008  max mem: 25582
Epoch: [34] Total time: 0:06:17 (0.6045 s / it)
Averaged stats: lr: 0.000186  min_lr: 0.000000  loss: 1.6162 (1.6143)  class_acc: 0.8359 (0.8411)  loss_scale: 4096.0000 (6415.9744)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:00:58  loss: 0.7666 (0.7666)  acc1: 84.7656 (84.7656)  acc5: 96.4844 (96.4844)  time: 2.3512  data: 2.0963  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.6551 (0.7114)  acc1: 84.7656 (83.9040)  acc5: 97.2656 (96.9600)  time: 0.2067  data: 0.0014  max mem: 25582
Test: Total time: 0:00:07 (0.2975 s / it)
* Acc@1 84.092 Acc@5 96.686 loss 0.721
Accuracy of the network on the 50000 test images: 84.1%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:06  loss: 0.6229 (0.6229)  acc1: 86.3281 (86.3281)  acc5: 97.6562 (97.6562)  time: 2.6623  data: 2.4264  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.5375 (0.6032)  acc1: 86.7925 (85.8080)  acc5: 98.0469 (97.5520)  time: 0.2087  data: 0.0028  max mem: 25582
Test: Total time: 0:00:07 (0.3114 s / it)
* Acc@1 85.426 Acc@5 97.482 loss 0.616
EMA Accuracy of the network on the 50000 test images: 85.4%
Max accuracy: 85.66%
{"train_lr": 0.00019688223328648584, "train_min_lr": 2.571418606141521e-07, "train_loss": 1.6143119140625, "train_class_acc": 0.84111484375, "train_loss_scale": 6415.9744, "train_weight_decay": 0.050000000000000495, "test_loss": 0.6158089743554592, "test_acc1": 85.42600004852295, "test_acc5": 97.48200002075195, "epoch": 34, "n_parameters": 85978600}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Epoch: [35]  [  0/625]  eta: 0:29:30  lr: 0.000186  min_lr: 0.000000  loss: 1.7002 (1.7002)  class_acc: 0.8242 (0.8242)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  time: 2.8335  data: 2.2133  max mem: 25582
Epoch: [35]  [100/625]  eta: 0:05:27  lr: 0.000182  min_lr: 0.000000  loss: 1.6025 (1.6058)  class_acc: 0.8398 (0.8432)  loss_scale: 4096.0000 (6367.0495)  weight_decay: 0.0500 (0.0500)  time: 0.6032  data: 0.0002  max mem: 25582
Epoch: [35]  [200/625]  eta: 0:04:20  lr: 0.000179  min_lr: 0.000000  loss: 1.5918 (1.5966)  class_acc: 0.8477 (0.8466)  loss_scale: 4096.0000 (5318.6866)  weight_decay: 0.0500 (0.0500)  time: 0.5980  data: 0.0002  max mem: 25582
Epoch: [35]  [300/625]  eta: 0:03:17  lr: 0.000176  min_lr: 0.000000  loss: 1.6162 (1.5961)  class_acc: 0.8281 (0.8463)  loss_scale: 4096.0000 (5198.2458)  weight_decay: 0.0500 (0.0500)  time: 0.6030  data: 0.0002  max mem: 25582
Epoch: [35]  [400/625]  eta: 0:02:16  lr: 0.000172  min_lr: 0.000000  loss: 1.6172 (1.5989)  class_acc: 0.8438 (0.8456)  loss_scale: 8192.0000 (5434.0948)  weight_decay: 0.0500 (0.0500)  time: 0.5996  data: 0.0002  max mem: 25582
Epoch: [35]  [500/625]  eta: 0:01:15  lr: 0.000169  min_lr: 0.000000  loss: 1.6025 (1.5998)  class_acc: 0.8398 (0.8454)  loss_scale: 16384.0000 (6344.3034)  weight_decay: 0.0500 (0.0500)  time: 0.5996  data: 0.0002  max mem: 25582
Epoch: [35]  [600/625]  eta: 0:00:15  lr: 0.000165  min_lr: 0.000000  loss: 1.6143 (1.6015)  class_acc: 0.8438 (0.8447)  loss_scale: 8192.0000 (7156.0732)  weight_decay: 0.0500 (0.0500)  time: 0.6041  data: 0.0002  max mem: 25582
Epoch: [35]  [624/625]  eta: 0:00:00  lr: 0.000165  min_lr: 0.000000  loss: 1.5967 (1.6022)  class_acc: 0.8438 (0.8445)  loss_scale: 8192.0000 (7195.8528)  weight_decay: 0.0500 (0.0500)  time: 0.5890  data: 0.0007  max mem: 25582
Epoch: [35] Total time: 0:06:17 (0.6043 s / it)
Averaged stats: lr: 0.000165  min_lr: 0.000000  loss: 1.5967 (1.6018)  class_acc: 0.8438 (0.8450)  loss_scale: 8192.0000 (7195.8528)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:05  loss: 0.7345 (0.7345)  acc1: 85.1562 (85.1562)  acc5: 97.6562 (97.6562)  time: 2.6041  data: 2.3709  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.6682 (0.7146)  acc1: 84.7656 (84.1440)  acc5: 96.8750 (96.9440)  time: 0.2130  data: 0.0048  max mem: 25582
Test: Total time: 0:00:07 (0.3116 s / it)
* Acc@1 84.076 Acc@5 96.684 loss 0.725
Accuracy of the network on the 50000 test images: 84.1%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:03  loss: 0.6309 (0.6309)  acc1: 86.7188 (86.7188)  acc5: 97.6562 (97.6562)  time: 2.5461  data: 2.2896  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.5448 (0.6087)  acc1: 86.7188 (85.7120)  acc5: 98.0469 (97.5200)  time: 0.2134  data: 0.0040  max mem: 25582
Test: Total time: 0:00:07 (0.3099 s / it)
* Acc@1 85.368 Acc@5 97.466 loss 0.621
EMA Accuracy of the network on the 50000 test images: 85.4%
Max accuracy: 85.66%
{"train_lr": 0.00017516071895561552, "train_min_lr": 2.287720553901865e-07, "train_loss": 1.6017541015625, "train_class_acc": 0.8450484375, "train_loss_scale": 7195.8528, "train_weight_decay": 0.050000000000000495, "test_loss": 0.6211030814051628, "test_acc1": 85.3680000402832, "test_acc5": 97.46600002075195, "epoch": 35, "n_parameters": 85978600}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Epoch: [36]  [  0/625]  eta: 0:29:30  lr: 0.000165  min_lr: 0.000000  loss: 1.6094 (1.6094)  class_acc: 0.8555 (0.8555)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 2.8324  data: 2.2038  max mem: 25582
Epoch: [36]  [100/625]  eta: 0:05:27  lr: 0.000161  min_lr: 0.000000  loss: 1.5430 (1.5718)  class_acc: 0.8555 (0.8512)  loss_scale: 8192.0000 (11111.9208)  weight_decay: 0.0500 (0.0500)  time: 0.6020  data: 0.0002  max mem: 25582
Epoch: [36]  [200/625]  eta: 0:04:20  lr: 0.000158  min_lr: 0.000000  loss: 1.5693 (1.5812)  class_acc: 0.8477 (0.8491)  loss_scale: 8192.0000 (9659.2239)  weight_decay: 0.0500 (0.0500)  time: 0.6005  data: 0.0002  max mem: 25582
Epoch: [36]  [300/625]  eta: 0:03:18  lr: 0.000155  min_lr: 0.000000  loss: 1.5850 (1.5840)  class_acc: 0.8555 (0.8491)  loss_scale: 16384.0000 (11730.0731)  weight_decay: 0.0500 (0.0500)  time: 0.5991  data: 0.0002  max mem: 25582
Epoch: [36]  [400/625]  eta: 0:02:16  lr: 0.000151  min_lr: 0.000000  loss: 1.5703 (1.5879)  class_acc: 0.8438 (0.8478)  loss_scale: 4096.0000 (11583.2020)  weight_decay: 0.0500 (0.0500)  time: 0.6030  data: 0.0010  max mem: 25582
Epoch: [36]  [500/625]  eta: 0:01:15  lr: 0.000148  min_lr: 0.000000  loss: 1.5293 (1.5883)  class_acc: 0.8594 (0.8480)  loss_scale: 8192.0000 (10211.3852)  weight_decay: 0.0500 (0.0500)  time: 0.5998  data: 0.0002  max mem: 25582
Epoch: [36]  [600/625]  eta: 0:00:15  lr: 0.000145  min_lr: 0.000000  loss: 1.6318 (1.5901)  class_acc: 0.8359 (0.8476)  loss_scale: 8192.0000 (9875.3810)  weight_decay: 0.0500 (0.0500)  time: 0.6067  data: 0.0002  max mem: 25582
Epoch: [36]  [624/625]  eta: 0:00:00  lr: 0.000144  min_lr: 0.000000  loss: 1.5596 (1.5900)  class_acc: 0.8438 (0.8476)  loss_scale: 16384.0000 (9954.9184)  weight_decay: 0.0500 (0.0500)  time: 0.5887  data: 0.0007  max mem: 25582
Epoch: [36] Total time: 0:06:18 (0.6061 s / it)
Averaged stats: lr: 0.000144  min_lr: 0.000000  loss: 1.5596 (1.5887)  class_acc: 0.8438 (0.8485)  loss_scale: 16384.0000 (9954.9184)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:10  loss: 0.7651 (0.7651)  acc1: 84.7656 (84.7656)  acc5: 96.4844 (96.4844)  time: 2.8030  data: 2.5518  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.6590 (0.7222)  acc1: 85.1562 (84.2240)  acc5: 97.2656 (96.9280)  time: 0.2167  data: 0.0104  max mem: 25582
Test: Total time: 0:00:08 (0.3239 s / it)
* Acc@1 84.108 Acc@5 96.726 loss 0.729
Accuracy of the network on the 50000 test images: 84.1%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:07  loss: 0.6371 (0.6371)  acc1: 86.7188 (86.7188)  acc5: 97.6562 (97.6562)  time: 2.7104  data: 2.4540  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.5523 (0.6146)  acc1: 86.7188 (85.7120)  acc5: 97.6562 (97.4880)  time: 0.2088  data: 0.0025  max mem: 25582
Test: Total time: 0:00:07 (0.3134 s / it)
* Acc@1 85.360 Acc@5 97.442 loss 0.627
EMA Accuracy of the network on the 50000 test images: 85.4%
Max accuracy: 85.66%
{"train_lr": 0.00015421196297192452, "train_min_lr": 2.014115261983026e-07, "train_loss": 1.5886513671875, "train_class_acc": 0.848503125, "train_loss_scale": 9954.9184, "train_weight_decay": 0.050000000000000495, "test_loss": 0.6268524549901485, "test_acc1": 85.36000002685547, "test_acc5": 97.44200002075195, "epoch": 36, "n_parameters": 85978600}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Epoch: [37]  [  0/625]  eta: 0:30:44  lr: 0.000144  min_lr: 0.000000  loss: 1.7256 (1.7256)  class_acc: 0.8203 (0.8203)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 2.9520  data: 2.3255  max mem: 25582
Epoch: [37]  [100/625]  eta: 0:05:29  lr: 0.000141  min_lr: 0.000000  loss: 1.5723 (1.5795)  class_acc: 0.8477 (0.8523)  loss_scale: 8192.0000 (8759.7624)  weight_decay: 0.0500 (0.0500)  time: 0.6065  data: 0.0002  max mem: 25582
Epoch: [37]  [200/625]  eta: 0:04:21  lr: 0.000138  min_lr: 0.000000  loss: 1.5811 (1.5816)  class_acc: 0.8438 (0.8512)  loss_scale: 16384.0000 (11126.4478)  weight_decay: 0.0500 (0.0500)  time: 0.5983  data: 0.0002  max mem: 25582
Epoch: [37]  [300/625]  eta: 0:03:18  lr: 0.000134  min_lr: 0.000000  loss: 1.5264 (1.5723)  class_acc: 0.8555 (0.8529)  loss_scale: 8192.0000 (11593.9934)  weight_decay: 0.0500 (0.0500)  time: 0.6041  data: 0.0003  max mem: 25582
Epoch: [37]  [400/625]  eta: 0:02:17  lr: 0.000131  min_lr: 0.000000  loss: 1.5547 (1.5730)  class_acc: 0.8516 (0.8527)  loss_scale: 16384.0000 (11113.3367)  weight_decay: 0.0500 (0.0500)  time: 0.6043  data: 0.0002  max mem: 25582
Epoch: [37]  [500/625]  eta: 0:01:16  lr: 0.000128  min_lr: 0.000000  loss: 1.5430 (1.5756)  class_acc: 0.8516 (0.8524)  loss_scale: 8192.0000 (11936.4471)  weight_decay: 0.0500 (0.0500)  time: 0.6055  data: 0.0002  max mem: 25582
Epoch: [37]  [600/625]  eta: 0:00:15  lr: 0.000125  min_lr: 0.000000  loss: 1.5537 (1.5765)  class_acc: 0.8516 (0.8523)  loss_scale: 8192.0000 (11313.4110)  weight_decay: 0.0500 (0.0500)  time: 0.5987  data: 0.0002  max mem: 25582
Epoch: [37]  [624/625]  eta: 0:00:00  lr: 0.000124  min_lr: 0.000000  loss: 1.5283 (1.5764)  class_acc: 0.8594 (0.8523)  loss_scale: 8192.0000 (11311.5136)  weight_decay: 0.0500 (0.0500)  time: 0.5898  data: 0.0007  max mem: 25582
Epoch: [37] Total time: 0:06:19 (0.6072 s / it)
Averaged stats: lr: 0.000124  min_lr: 0.000000  loss: 1.5283 (1.5789)  class_acc: 0.8594 (0.8516)  loss_scale: 8192.0000 (11311.5136)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:05  loss: 0.7063 (0.7063)  acc1: 87.1094 (87.1094)  acc5: 96.8750 (96.8750)  time: 2.6106  data: 2.3560  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.6646 (0.7191)  acc1: 84.3750 (84.4320)  acc5: 97.2656 (96.9600)  time: 0.2091  data: 0.0002  max mem: 25582
Test: Total time: 0:00:07 (0.3102 s / it)
* Acc@1 84.200 Acc@5 96.672 loss 0.729
Accuracy of the network on the 50000 test images: 84.2%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:00  loss: 0.6420 (0.6420)  acc1: 86.7188 (86.7188)  acc5: 97.6562 (97.6562)  time: 2.4143  data: 2.1845  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.5623 (0.6209)  acc1: 86.3281 (85.5040)  acc5: 98.0469 (97.5840)  time: 0.2199  data: 0.0104  max mem: 25582
Test: Total time: 0:00:07 (0.3110 s / it)
* Acc@1 85.302 Acc@5 97.436 loss 0.633
EMA Accuracy of the network on the 50000 test images: 85.3%
Max accuracy: 85.66%
{"train_lr": 0.00013416512138221995, "train_min_lr": 1.752289597992672e-07, "train_loss": 1.5789095703125, "train_class_acc": 0.851578125, "train_loss_scale": 11311.5136, "train_weight_decay": 0.050000000000000495, "test_loss": 0.6326230205595493, "test_acc1": 85.30200002410889, "test_acc5": 97.43600002075195, "epoch": 37, "n_parameters": 85978600}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Epoch: [38]  [  0/625]  eta: 0:29:37  lr: 0.000124  min_lr: 0.000000  loss: 1.6201 (1.6201)  class_acc: 0.8164 (0.8164)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 2.8438  data: 1.9462  max mem: 25582
Epoch: [38]  [100/625]  eta: 0:05:27  lr: 0.000121  min_lr: 0.000000  loss: 1.5771 (1.5675)  class_acc: 0.8516 (0.8542)  loss_scale: 8192.0000 (13707.4059)  weight_decay: 0.0500 (0.0500)  time: 0.6020  data: 0.0002  max mem: 25582
Epoch: [38]  [200/625]  eta: 0:04:20  lr: 0.000118  min_lr: 0.000000  loss: 1.5537 (1.5671)  class_acc: 0.8594 (0.8554)  loss_scale: 4096.0000 (9577.7114)  weight_decay: 0.0500 (0.0500)  time: 0.6003  data: 0.0002  max mem: 25582
Epoch: [38]  [300/625]  eta: 0:03:17  lr: 0.000115  min_lr: 0.000000  loss: 1.5449 (1.5665)  class_acc: 0.8633 (0.8556)  loss_scale: 8192.0000 (8287.2558)  weight_decay: 0.0500 (0.0500)  time: 0.5994  data: 0.0002  max mem: 25582
Epoch: [38]  [400/625]  eta: 0:02:16  lr: 0.000113  min_lr: 0.000000  loss: 1.5703 (1.5685)  class_acc: 0.8477 (0.8555)  loss_scale: 4096.0000 (8048.9975)  weight_decay: 0.0500 (0.0500)  time: 0.5997  data: 0.0002  max mem: 25582
Epoch: [38]  [500/625]  eta: 0:01:15  lr: 0.000110  min_lr: 0.000000  loss: 1.5459 (1.5665)  class_acc: 0.8555 (0.8558)  loss_scale: 4096.0000 (7259.9760)  weight_decay: 0.0500 (0.0500)  time: 0.6014  data: 0.0002  max mem: 25582
Epoch: [38]  [600/625]  eta: 0:00:15  lr: 0.000107  min_lr: 0.000000  loss: 1.5322 (1.5671)  class_acc: 0.8594 (0.8557)  loss_scale: 8192.0000 (7360.5324)  weight_decay: 0.0500 (0.0500)  time: 0.5996  data: 0.0002  max mem: 25582
Epoch: [38]  [624/625]  eta: 0:00:00  lr: 0.000106  min_lr: 0.000000  loss: 1.5713 (1.5676)  class_acc: 0.8633 (0.8556)  loss_scale: 4096.0000 (7254.8352)  weight_decay: 0.0500 (0.0500)  time: 0.5884  data: 0.0007  max mem: 25582
Epoch: [38] Total time: 0:06:17 (0.6042 s / it)
Averaged stats: lr: 0.000106  min_lr: 0.000000  loss: 1.5713 (1.5695)  class_acc: 0.8633 (0.8544)  loss_scale: 4096.0000 (7254.8352)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:07  loss: 0.7499 (0.7499)  acc1: 85.9375 (85.9375)  acc5: 96.8750 (96.8750)  time: 2.7053  data: 2.4607  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.6649 (0.7241)  acc1: 84.9057 (84.0640)  acc5: 96.8750 (97.0720)  time: 0.2099  data: 0.0012  max mem: 25582
Test: Total time: 0:00:07 (0.3142 s / it)
* Acc@1 84.120 Acc@5 96.730 loss 0.732
Accuracy of the network on the 50000 test images: 84.1%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:03  loss: 0.6477 (0.6477)  acc1: 87.1094 (87.1094)  acc5: 97.6562 (97.6562)  time: 2.5380  data: 2.2834  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.5689 (0.6271)  acc1: 85.9375 (85.4720)  acc5: 98.0469 (97.5680)  time: 0.2141  data: 0.0002  max mem: 25582
Test: Total time: 0:00:07 (0.3105 s / it)
* Acc@1 85.296 Acc@5 97.388 loss 0.639
EMA Accuracy of the network on the 50000 test images: 85.3%
Max accuracy: 85.66%
{"train_lr": 0.00011514378963115368, "train_min_lr": 1.503857804215173e-07, "train_loss": 1.5695361328125, "train_class_acc": 0.8543734375, "train_loss_scale": 7254.8352, "train_weight_decay": 0.050000000000000495, "test_loss": 0.6386086350679397, "test_acc1": 85.29600003479004, "test_acc5": 97.38800002075196, "epoch": 38, "n_parameters": 85978600}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Epoch: [39]  [  0/625]  eta: 0:29:52  lr: 0.000106  min_lr: 0.000000  loss: 1.5303 (1.5303)  class_acc: 0.8633 (0.8633)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  time: 2.8683  data: 2.2670  max mem: 25582
Epoch: [39]  [100/625]  eta: 0:05:28  lr: 0.000103  min_lr: 0.000000  loss: 1.5537 (1.5448)  class_acc: 0.8594 (0.8632)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6039  data: 0.0002  max mem: 25582
Epoch: [39]  [200/625]  eta: 0:04:20  lr: 0.000100  min_lr: 0.000000  loss: 1.5225 (1.5537)  class_acc: 0.8594 (0.8587)  loss_scale: 8192.0000 (5991.1642)  weight_decay: 0.0500 (0.0500)  time: 0.6013  data: 0.0003  max mem: 25582
Epoch: [39]  [300/625]  eta: 0:03:17  lr: 0.000098  min_lr: 0.000000  loss: 1.5293 (1.5567)  class_acc: 0.8555 (0.8578)  loss_scale: 16384.0000 (8491.3754)  weight_decay: 0.0500 (0.0500)  time: 0.5978  data: 0.0002  max mem: 25582
Epoch: [39]  [400/625]  eta: 0:02:16  lr: 0.000095  min_lr: 0.000000  loss: 1.5742 (1.5568)  class_acc: 0.8438 (0.8579)  loss_scale: 16384.0000 (10541.3267)  weight_decay: 0.0500 (0.0500)  time: 0.6026  data: 0.0002  max mem: 25582
Epoch: [39]  [500/625]  eta: 0:01:15  lr: 0.000092  min_lr: 0.000000  loss: 1.5176 (1.5580)  class_acc: 0.8555 (0.8575)  loss_scale: 8192.0000 (10922.6667)  weight_decay: 0.0500 (0.0500)  time: 0.6039  data: 0.0002  max mem: 25582
Epoch: [39]  [600/625]  eta: 0:00:15  lr: 0.000089  min_lr: 0.000000  loss: 1.5488 (1.5572)  class_acc: 0.8594 (0.8580)  loss_scale: 16384.0000 (10727.2945)  weight_decay: 0.0500 (0.0500)  time: 0.6026  data: 0.0002  max mem: 25582
Epoch: [39]  [624/625]  eta: 0:00:00  lr: 0.000089  min_lr: 0.000000  loss: 1.5508 (1.5571)  class_acc: 0.8555 (0.8580)  loss_scale: 16384.0000 (10944.5120)  weight_decay: 0.0500 (0.0500)  time: 0.5871  data: 0.0007  max mem: 25582
Epoch: [39] Total time: 0:06:17 (0.6046 s / it)
Averaged stats: lr: 0.000089  min_lr: 0.000000  loss: 1.5508 (1.5589)  class_acc: 0.8555 (0.8572)  loss_scale: 16384.0000 (10944.5120)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:00:59  loss: 0.7688 (0.7688)  acc1: 84.3750 (84.3750)  acc5: 96.4844 (96.4844)  time: 2.3822  data: 2.1278  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.6513 (0.7259)  acc1: 84.9057 (84.2560)  acc5: 96.8750 (97.0080)  time: 0.2057  data: 0.0001  max mem: 25582
Test: Total time: 0:00:07 (0.2979 s / it)
* Acc@1 84.064 Acc@5 96.638 loss 0.737
Accuracy of the network on the 50000 test images: 84.1%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:05  loss: 0.6537 (0.6537)  acc1: 86.7188 (86.7188)  acc5: 97.6562 (97.6562)  time: 2.6354  data: 2.3892  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.5765 (0.6338)  acc1: 85.9375 (85.4720)  acc5: 98.0469 (97.5680)  time: 0.2093  data: 0.0018  max mem: 25582
Test: Total time: 0:00:07 (0.3103 s / it)
* Acc@1 85.268 Acc@5 97.358 loss 0.645
EMA Accuracy of the network on the 50000 test images: 85.3%
Max accuracy: 85.66%
{"train_lr": 9.726524055420646e-05, "train_min_lr": 1.270351545271124e-07, "train_loss": 1.558899609375, "train_class_acc": 0.857240625, "train_loss_scale": 10944.512, "train_weight_decay": 0.050000000000000495, "test_loss": 0.6448228944838047, "test_acc1": 85.26800002929687, "test_acc5": 97.3580000152588, "epoch": 39, "n_parameters": 85978600}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Epoch: [40]  [  0/625]  eta: 0:30:53  lr: 0.000089  min_lr: 0.000000  loss: 1.5078 (1.5078)  class_acc: 0.8477 (0.8477)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 2.9658  data: 2.2542  max mem: 25582
Epoch: [40]  [100/625]  eta: 0:05:30  lr: 0.000086  min_lr: 0.000000  loss: 1.5674 (1.5495)  class_acc: 0.8477 (0.8591)  loss_scale: 8192.0000 (11111.9208)  weight_decay: 0.0500 (0.0500)  time: 0.6044  data: 0.0002  max mem: 25582
Epoch: [40]  [200/625]  eta: 0:04:21  lr: 0.000083  min_lr: 0.000000  loss: 1.6094 (1.5494)  class_acc: 0.8438 (0.8597)  loss_scale: 8192.0000 (9781.4925)  weight_decay: 0.0500 (0.0500)  time: 0.6028  data: 0.0003  max mem: 25582
Epoch: [40]  [300/625]  eta: 0:03:18  lr: 0.000081  min_lr: 0.000000  loss: 1.5459 (1.5498)  class_acc: 0.8555 (0.8594)  loss_scale: 4096.0000 (7892.6246)  weight_decay: 0.0500 (0.0500)  time: 0.6045  data: 0.0002  max mem: 25582
Epoch: [40]  [400/625]  eta: 0:02:17  lr: 0.000078  min_lr: 0.000000  loss: 1.5420 (1.5508)  class_acc: 0.8555 (0.8593)  loss_scale: 8192.0000 (7711.9202)  weight_decay: 0.0500 (0.0500)  time: 0.6011  data: 0.0002  max mem: 25582
Epoch: [40]  [500/625]  eta: 0:01:15  lr: 0.000076  min_lr: 0.000000  loss: 1.5068 (1.5497)  class_acc: 0.8711 (0.8594)  loss_scale: 8192.0000 (8216.5269)  weight_decay: 0.0500 (0.0500)  time: 0.6004  data: 0.0002  max mem: 25582
Epoch: [40]  [600/625]  eta: 0:00:15  lr: 0.000073  min_lr: 0.000000  loss: 1.4893 (1.5491)  class_acc: 0.8633 (0.8595)  loss_scale: 8192.0000 (8212.4459)  weight_decay: 0.0500 (0.0500)  time: 0.6066  data: 0.0002  max mem: 25582
Epoch: [40]  [624/625]  eta: 0:00:00  lr: 0.000073  min_lr: 0.000000  loss: 1.5498 (1.5497)  class_acc: 0.8594 (0.8595)  loss_scale: 16384.0000 (8434.4832)  weight_decay: 0.0500 (0.0500)  time: 0.5903  data: 0.0009  max mem: 25582
Epoch: [40] Total time: 0:06:19 (0.6065 s / it)
Averaged stats: lr: 0.000073  min_lr: 0.000000  loss: 1.5498 (1.5510)  class_acc: 0.8594 (0.8598)  loss_scale: 16384.0000 (8434.4832)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:06  loss: 0.7509 (0.7509)  acc1: 85.5469 (85.5469)  acc5: 96.8750 (96.8750)  time: 2.6764  data: 2.4289  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.6674 (0.7307)  acc1: 84.9057 (84.4160)  acc5: 96.8750 (96.8640)  time: 0.2094  data: 0.0027  max mem: 25582
Test: Total time: 0:00:07 (0.3141 s / it)
* Acc@1 84.140 Acc@5 96.602 loss 0.739
Accuracy of the network on the 50000 test images: 84.1%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:09  loss: 0.6603 (0.6603)  acc1: 87.5000 (87.5000)  acc5: 97.6562 (97.6562)  time: 2.7801  data: 2.5364  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.5858 (0.6406)  acc1: 86.3281 (85.4720)  acc5: 98.0469 (97.5840)  time: 0.2089  data: 0.0026  max mem: 25582
Test: Total time: 0:00:07 (0.3159 s / it)
* Acc@1 85.206 Acc@5 97.330 loss 0.651
EMA Accuracy of the network on the 50000 test images: 85.2%
Max accuracy: 85.66%
{"train_lr": 8.063970135165775e-05, "train_min_lr": 1.0532104648956232e-07, "train_loss": 1.5509810546875, "train_class_acc": 0.85984921875, "train_loss_scale": 8434.4832, "train_weight_decay": 0.050000000000000495, "test_loss": 0.6511312368512153, "test_acc1": 85.20600002929687, "test_acc5": 97.33000001525879, "epoch": 40, "n_parameters": 85978600}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Epoch: [41]  [  0/625]  eta: 0:28:19  lr: 0.000073  min_lr: 0.000000  loss: 1.4678 (1.4678)  class_acc: 0.8672 (0.8672)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 2.7191  data: 1.9162  max mem: 25582
Epoch: [41]  [100/625]  eta: 0:05:27  lr: 0.000070  min_lr: 0.000000  loss: 1.5352 (1.5418)  class_acc: 0.8555 (0.8625)  loss_scale: 8192.0000 (9895.2871)  weight_decay: 0.0500 (0.0500)  time: 0.6005  data: 0.0002  max mem: 25582
Epoch: [41]  [200/625]  eta: 0:04:20  lr: 0.000068  min_lr: 0.000000  loss: 1.4961 (1.5406)  class_acc: 0.8633 (0.8631)  loss_scale: 16384.0000 (11126.4478)  weight_decay: 0.0500 (0.0500)  time: 0.6013  data: 0.0002  max mem: 25582
Epoch: [41]  [300/625]  eta: 0:03:17  lr: 0.000066  min_lr: 0.000000  loss: 1.5479 (1.5423)  class_acc: 0.8594 (0.8629)  loss_scale: 8192.0000 (10709.4751)  weight_decay: 0.0500 (0.0500)  time: 0.6028  data: 0.0002  max mem: 25582
Epoch: [41]  [400/625]  eta: 0:02:16  lr: 0.000063  min_lr: 0.000000  loss: 1.5518 (1.5433)  class_acc: 0.8633 (0.8626)  loss_scale: 4096.0000 (9060.2294)  weight_decay: 0.0500 (0.0500)  time: 0.6062  data: 0.0002  max mem: 25582
Epoch: [41]  [500/625]  eta: 0:01:15  lr: 0.000061  min_lr: 0.000000  loss: 1.5254 (1.5427)  class_acc: 0.8594 (0.8624)  loss_scale: 8192.0000 (8658.0120)  weight_decay: 0.0500 (0.0500)  time: 0.5998  data: 0.0012  max mem: 25582
Epoch: [41]  [600/625]  eta: 0:00:15  lr: 0.000059  min_lr: 0.000000  loss: 1.5225 (1.5429)  class_acc: 0.8555 (0.8625)  loss_scale: 8192.0000 (8662.2562)  weight_decay: 0.0500 (0.0500)  time: 0.6041  data: 0.0002  max mem: 25582
Epoch: [41]  [624/625]  eta: 0:00:00  lr: 0.000058  min_lr: 0.000000  loss: 1.5781 (1.5436)  class_acc: 0.8516 (0.8622)  loss_scale: 8192.0000 (8644.1984)  weight_decay: 0.0500 (0.0500)  time: 0.5881  data: 0.0007  max mem: 25582
Epoch: [41] Total time: 0:06:18 (0.6053 s / it)
Averaged stats: lr: 0.000058  min_lr: 0.000000  loss: 1.5781 (1.5430)  class_acc: 0.8516 (0.8621)  loss_scale: 8192.0000 (8644.1984)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:08  loss: 0.7476 (0.7476)  acc1: 85.5469 (85.5469)  acc5: 96.4844 (96.4844)  time: 2.7209  data: 2.4695  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.6772 (0.7317)  acc1: 85.9375 (84.6560)  acc5: 97.2656 (96.9280)  time: 0.2095  data: 0.0015  max mem: 25582
Test: Total time: 0:00:07 (0.3138 s / it)
* Acc@1 84.190 Acc@5 96.652 loss 0.740
Accuracy of the network on the 50000 test images: 84.2%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:07  loss: 0.6663 (0.6663)  acc1: 86.7188 (86.7188)  acc5: 97.6562 (97.6562)  time: 2.6868  data: 2.4393  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.5942 (0.6473)  acc1: 85.9375 (85.3760)  acc5: 97.6562 (97.5200)  time: 0.2093  data: 0.0002  max mem: 25582
Test: Total time: 0:00:07 (0.3125 s / it)
* Acc@1 85.122 Acc@5 97.288 loss 0.657
EMA Accuracy of the network on the 50000 test images: 85.1%
Max accuracy: 85.66%
{"train_lr": 6.536967400124447e-05, "train_min_lr": 8.537733100559234e-08, "train_loss": 1.542972265625, "train_class_acc": 0.862134375, "train_loss_scale": 8644.1984, "train_weight_decay": 0.050000000000000495, "test_loss": 0.6573970021307468, "test_acc1": 85.1220000265503, "test_acc5": 97.28800001251221, "epoch": 41, "n_parameters": 85978600}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Epoch: [42]  [  0/625]  eta: 0:29:37  lr: 0.000058  min_lr: 0.000000  loss: 1.5830 (1.5830)  class_acc: 0.8633 (0.8633)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 2.8447  data: 2.1233  max mem: 25582
Epoch: [42]  [100/625]  eta: 0:05:29  lr: 0.000056  min_lr: 0.000000  loss: 1.4990 (1.5277)  class_acc: 0.8711 (0.8649)  loss_scale: 8192.0000 (9246.4158)  weight_decay: 0.0500 (0.0500)  time: 0.6045  data: 0.0002  max mem: 25582
Epoch: [42]  [200/625]  eta: 0:04:21  lr: 0.000054  min_lr: 0.000000  loss: 1.5420 (1.5291)  class_acc: 0.8672 (0.8654)  loss_scale: 8192.0000 (8721.8308)  weight_decay: 0.0500 (0.0500)  time: 0.6020  data: 0.0002  max mem: 25582
Epoch: [42]  [300/625]  eta: 0:03:18  lr: 0.000052  min_lr: 0.000000  loss: 1.5391 (1.5280)  class_acc: 0.8633 (0.8656)  loss_scale: 8192.0000 (8654.6711)  weight_decay: 0.0500 (0.0500)  time: 0.5990  data: 0.0002  max mem: 25582
Epoch: [42]  [400/625]  eta: 0:02:16  lr: 0.000050  min_lr: 0.000000  loss: 1.5479 (1.5276)  class_acc: 0.8672 (0.8661)  loss_scale: 16384.0000 (9744.5985)  weight_decay: 0.0500 (0.0500)  time: 0.6025  data: 0.0002  max mem: 25582
Epoch: [42]  [500/625]  eta: 0:01:15  lr: 0.000048  min_lr: 0.000000  loss: 1.5273 (1.5314)  class_acc: 0.8633 (0.8654)  loss_scale: 8192.0000 (11020.7745)  weight_decay: 0.0500 (0.0500)  time: 0.6019  data: 0.0002  max mem: 25582
Epoch: [42]  [600/625]  eta: 0:00:15  lr: 0.000046  min_lr: 0.000000  loss: 1.5264 (1.5318)  class_acc: 0.8633 (0.8651)  loss_scale: 4096.0000 (10481.9434)  weight_decay: 0.0500 (0.0500)  time: 0.5991  data: 0.0002  max mem: 25582
Epoch: [42]  [624/625]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000000  loss: 1.5029 (1.5311)  class_acc: 0.8633 (0.8653)  loss_scale: 4096.0000 (10236.7232)  weight_decay: 0.0500 (0.0500)  time: 0.5892  data: 0.0010  max mem: 25582
Epoch: [42] Total time: 0:06:18 (0.6052 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000000  loss: 1.5029 (1.5358)  class_acc: 0.8633 (0.8641)  loss_scale: 4096.0000 (10236.7232)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:07  loss: 0.7451 (0.7451)  acc1: 85.5469 (85.5469)  acc5: 96.8750 (96.8750)  time: 2.6832  data: 2.4276  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.6565 (0.7365)  acc1: 85.5469 (84.3360)  acc5: 97.2656 (96.8640)  time: 0.2101  data: 0.0002  max mem: 25582
Test: Total time: 0:00:07 (0.3137 s / it)
* Acc@1 84.198 Acc@5 96.598 loss 0.744
Accuracy of the network on the 50000 test images: 84.2%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:05  loss: 0.6721 (0.6721)  acc1: 87.1094 (87.1094)  acc5: 97.6562 (97.6562)  time: 2.6370  data: 2.3799  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.6021 (0.6540)  acc1: 85.8491 (85.3280)  acc5: 97.6562 (97.4880)  time: 0.2138  data: 0.0002  max mem: 25582
Test: Total time: 0:00:07 (0.3162 s / it)
* Acc@1 85.058 Acc@5 97.248 loss 0.664
EMA Accuracy of the network on the 50000 test images: 85.1%
Max accuracy: 85.66%
{"train_lr": 5.1549303299381e-05, "train_min_lr": 6.732696771311946e-08, "train_loss": 1.5358416015625, "train_class_acc": 0.8641359375, "train_loss_scale": 10236.7232, "train_weight_decay": 0.050000000000000495, "test_loss": 0.6636608131229877, "test_acc1": 85.05800002655029, "test_acc5": 97.24800001251221, "epoch": 42, "n_parameters": 85978600}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Epoch: [43]  [  0/625]  eta: 0:31:33  lr: 0.000045  min_lr: 0.000000  loss: 1.5908 (1.5908)  class_acc: 0.8672 (0.8672)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  time: 3.0295  data: 2.3978  max mem: 25582
Epoch: [43]  [100/625]  eta: 0:05:29  lr: 0.000043  min_lr: 0.000000  loss: 1.5166 (1.5367)  class_acc: 0.8672 (0.8636)  loss_scale: 4096.0000 (4339.3267)  weight_decay: 0.0500 (0.0500)  time: 0.6033  data: 0.0002  max mem: 25582
Epoch: [43]  [200/625]  eta: 0:04:21  lr: 0.000041  min_lr: 0.000000  loss: 1.5176 (1.5334)  class_acc: 0.8711 (0.8656)  loss_scale: 4096.0000 (4503.5622)  weight_decay: 0.0500 (0.0500)  time: 0.6012  data: 0.0002  max mem: 25582
Epoch: [43]  [300/625]  eta: 0:03:18  lr: 0.000039  min_lr: 0.000000  loss: 1.5430 (1.5334)  class_acc: 0.8594 (0.8656)  loss_scale: 4096.0000 (4626.7110)  weight_decay: 0.0500 (0.0500)  time: 0.6007  data: 0.0002  max mem: 25582
Epoch: [43]  [400/625]  eta: 0:02:17  lr: 0.000038  min_lr: 0.000000  loss: 1.5137 (1.5299)  class_acc: 0.8789 (0.8668)  loss_scale: 4096.0000 (4586.2943)  weight_decay: 0.0500 (0.0500)  time: 0.6029  data: 0.0002  max mem: 25582
Epoch: [43]  [500/625]  eta: 0:01:15  lr: 0.000036  min_lr: 0.000000  loss: 1.5469 (1.5287)  class_acc: 0.8633 (0.8667)  loss_scale: 8192.0000 (5305.9960)  weight_decay: 0.0500 (0.0500)  time: 0.6034  data: 0.0002  max mem: 25582
Epoch: [43]  [600/625]  eta: 0:00:15  lr: 0.000034  min_lr: 0.000000  loss: 1.5127 (1.5295)  class_acc: 0.8672 (0.8666)  loss_scale: 8192.0000 (6454.0965)  weight_decay: 0.0500 (0.0500)  time: 0.6088  data: 0.0002  max mem: 25582
Epoch: [43]  [624/625]  eta: 0:00:00  lr: 0.000034  min_lr: 0.000000  loss: 1.5312 (1.5290)  class_acc: 0.8672 (0.8668)  loss_scale: 8192.0000 (6520.8320)  weight_decay: 0.0500 (0.0500)  time: 0.5905  data: 0.0009  max mem: 25582
Epoch: [43] Total time: 0:06:19 (0.6067 s / it)
Averaged stats: lr: 0.000034  min_lr: 0.000000  loss: 1.5312 (1.5296)  class_acc: 0.8672 (0.8659)  loss_scale: 8192.0000 (6520.8320)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:04  loss: 0.7540 (0.7540)  acc1: 85.9375 (85.9375)  acc5: 96.4844 (96.4844)  time: 2.5686  data: 2.3324  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.6681 (0.7361)  acc1: 85.9375 (84.5120)  acc5: 96.8750 (96.9600)  time: 0.2209  data: 0.0128  max mem: 25582
Test: Total time: 0:00:07 (0.3167 s / it)
* Acc@1 84.150 Acc@5 96.640 loss 0.746
Accuracy of the network on the 50000 test images: 84.2%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:06  loss: 0.6783 (0.6783)  acc1: 87.1094 (87.1094)  acc5: 97.2656 (97.2656)  time: 2.6487  data: 2.4068  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.6097 (0.6605)  acc1: 85.8491 (85.3120)  acc5: 98.0469 (97.4240)  time: 0.2089  data: 0.0002  max mem: 25582
Test: Total time: 0:00:07 (0.3104 s / it)
* Acc@1 85.054 Acc@5 97.200 loss 0.670
EMA Accuracy of the network on the 50000 test images: 85.1%
Max accuracy: 85.66%
{"train_lr": 3.926379642718431e-05, "train_min_lr": 5.1281243104196814e-08, "train_loss": 1.529640625, "train_class_acc": 0.8659328125, "train_loss_scale": 6520.832, "train_weight_decay": 0.050000000000000495, "test_loss": 0.6700101236999035, "test_acc1": 85.0540000265503, "test_acc5": 97.2000000125122, "epoch": 43, "n_parameters": 85978600}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Epoch: [44]  [  0/625]  eta: 0:28:04  lr: 0.000034  min_lr: 0.000000  loss: 1.5586 (1.5586)  class_acc: 0.8828 (0.8828)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 2.6950  data: 2.0698  max mem: 25582
Epoch: [44]  [100/625]  eta: 0:05:29  lr: 0.000032  min_lr: 0.000000  loss: 1.5088 (1.5239)  class_acc: 0.8633 (0.8673)  loss_scale: 16384.0000 (10463.0495)  weight_decay: 0.0500 (0.0500)  time: 0.6079  data: 0.0003  max mem: 25582
Epoch: [44]  [200/625]  eta: 0:04:22  lr: 0.000030  min_lr: 0.000000  loss: 1.4844 (1.5178)  class_acc: 0.8711 (0.8685)  loss_scale: 8192.0000 (11085.6915)  weight_decay: 0.0500 (0.0500)  time: 0.6076  data: 0.0002  max mem: 25582
Epoch: [44]  [300/625]  eta: 0:03:18  lr: 0.000029  min_lr: 0.000000  loss: 1.5303 (1.5216)  class_acc: 0.8633 (0.8677)  loss_scale: 4096.0000 (9022.0864)  weight_decay: 0.0500 (0.0500)  time: 0.6013  data: 0.0003  max mem: 25582
Epoch: [44]  [400/625]  eta: 0:02:17  lr: 0.000027  min_lr: 0.000000  loss: 1.4971 (1.5249)  class_acc: 0.8750 (0.8670)  loss_scale: 8192.0000 (8324.7880)  weight_decay: 0.0500 (0.0500)  time: 0.6017  data: 0.0003  max mem: 25582
Epoch: [44]  [500/625]  eta: 0:01:16  lr: 0.000026  min_lr: 0.000000  loss: 1.5137 (1.5252)  class_acc: 0.8672 (0.8666)  loss_scale: 16384.0000 (8690.7146)  weight_decay: 0.0500 (0.0500)  time: 0.6035  data: 0.0002  max mem: 25582
Epoch: [44]  [600/625]  eta: 0:00:15  lr: 0.000024  min_lr: 0.000000  loss: 1.5264 (1.5270)  class_acc: 0.8711 (0.8660)  loss_scale: 8192.0000 (9343.7870)  weight_decay: 0.0500 (0.0500)  time: 0.6002  data: 0.0002  max mem: 25582
Epoch: [44]  [624/625]  eta: 0:00:00  lr: 0.000024  min_lr: 0.000000  loss: 1.5303 (1.5277)  class_acc: 0.8594 (0.8659)  loss_scale: 8192.0000 (9299.5584)  weight_decay: 0.0500 (0.0500)  time: 0.5877  data: 0.0007  max mem: 25582
Epoch: [44] Total time: 0:06:19 (0.6069 s / it)
Averaged stats: lr: 0.000024  min_lr: 0.000000  loss: 1.5303 (1.5252)  class_acc: 0.8594 (0.8672)  loss_scale: 8192.0000 (9299.5584)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:00:57  loss: 0.7479 (0.7479)  acc1: 85.5469 (85.5469)  acc5: 96.8750 (96.8750)  time: 2.2858  data: 2.0482  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.6628 (0.7345)  acc1: 85.1562 (84.6240)  acc5: 96.8750 (96.8160)  time: 0.2069  data: 0.0022  max mem: 25582
Test: Total time: 0:00:07 (0.2948 s / it)
* Acc@1 84.200 Acc@5 96.590 loss 0.745
Accuracy of the network on the 50000 test images: 84.2%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:11  loss: 0.6847 (0.6847)  acc1: 87.1094 (87.1094)  acc5: 97.2656 (97.2656)  time: 2.8729  data: 2.6170  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.6164 (0.6669)  acc1: 85.8491 (85.1840)  acc5: 97.6562 (97.4080)  time: 0.2104  data: 0.0002  max mem: 25582
Test: Total time: 0:00:08 (0.3211 s / it)
* Acc@1 84.976 Acc@5 97.158 loss 0.676
EMA Accuracy of the network on the 50000 test images: 85.0%
Max accuracy: 85.66%
{"train_lr": 2.8588897619866827e-05, "train_min_lr": 3.7339084406783094e-08, "train_loss": 1.5252228515625, "train_class_acc": 0.8671625, "train_loss_scale": 9299.5584, "train_weight_decay": 0.050000000000000495, "test_loss": 0.6761927770078182, "test_acc1": 84.97600002929687, "test_acc5": 97.15800001800537, "epoch": 44, "n_parameters": 85978600}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Epoch: [45]  [  0/625]  eta: 0:31:34  lr: 0.000024  min_lr: 0.000000  loss: 1.4834 (1.4834)  class_acc: 0.8750 (0.8750)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 3.0312  data: 2.4131  max mem: 25582
Epoch: [45]  [100/625]  eta: 0:05:30  lr: 0.000022  min_lr: 0.000000  loss: 1.5303 (1.5237)  class_acc: 0.8711 (0.8678)  loss_scale: 16384.0000 (11598.5743)  weight_decay: 0.0500 (0.0500)  time: 0.6050  data: 0.0002  max mem: 25582
Epoch: [45]  [200/625]  eta: 0:04:21  lr: 0.000021  min_lr: 0.000000  loss: 1.5264 (1.5170)  class_acc: 0.8633 (0.8691)  loss_scale: 8192.0000 (11982.3284)  weight_decay: 0.0500 (0.0500)  time: 0.6022  data: 0.0003  max mem: 25582
Epoch: [45]  [300/625]  eta: 0:03:18  lr: 0.000020  min_lr: 0.000000  loss: 1.5352 (1.5189)  class_acc: 0.8633 (0.8680)  loss_scale: 4096.0000 (9961.0365)  weight_decay: 0.0500 (0.0500)  time: 0.6010  data: 0.0002  max mem: 25582
Epoch: [45]  [400/625]  eta: 0:02:16  lr: 0.000018  min_lr: 0.000000  loss: 1.4854 (1.5193)  class_acc: 0.8750 (0.8681)  loss_scale: 8192.0000 (8774.2244)  weight_decay: 0.0500 (0.0500)  time: 0.5994  data: 0.0002  max mem: 25582
Epoch: [45]  [500/625]  eta: 0:01:15  lr: 0.000017  min_lr: 0.000000  loss: 1.5635 (1.5208)  class_acc: 0.8633 (0.8679)  loss_scale: 4096.0000 (7930.3792)  weight_decay: 0.0500 (0.0500)  time: 0.5982  data: 0.0002  max mem: 25582
Epoch: [45]  [600/625]  eta: 0:00:15  lr: 0.000016  min_lr: 0.000000  loss: 1.4580 (1.5193)  class_acc: 0.8750 (0.8684)  loss_scale: 4096.0000 (7626.3295)  weight_decay: 0.0500 (0.0500)  time: 0.6005  data: 0.0002  max mem: 25582
Epoch: [45]  [624/625]  eta: 0:00:00  lr: 0.000016  min_lr: 0.000000  loss: 1.5332 (1.5198)  class_acc: 0.8594 (0.8682)  loss_scale: 4096.0000 (7490.7648)  weight_decay: 0.0500 (0.0500)  time: 0.5916  data: 0.0006  max mem: 25582
Epoch: [45] Total time: 0:06:18 (0.6061 s / it)
Averaged stats: lr: 0.000016  min_lr: 0.000000  loss: 1.5332 (1.5217)  class_acc: 0.8594 (0.8682)  loss_scale: 4096.0000 (7490.7648)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:08  loss: 0.7612 (0.7612)  acc1: 85.9375 (85.9375)  acc5: 96.4844 (96.4844)  time: 2.7328  data: 2.4769  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.6611 (0.7355)  acc1: 85.5469 (84.6080)  acc5: 96.8750 (96.8160)  time: 0.2135  data: 0.0068  max mem: 25582
Test: Total time: 0:00:07 (0.3183 s / it)
* Acc@1 84.206 Acc@5 96.574 loss 0.745
Accuracy of the network on the 50000 test images: 84.2%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:05  loss: 0.6905 (0.6905)  acc1: 87.1094 (87.1094)  acc5: 97.2656 (97.2656)  time: 2.6348  data: 2.3786  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.6211 (0.6728)  acc1: 85.5469 (85.0880)  acc5: 97.6562 (97.3440)  time: 0.2142  data: 0.0065  max mem: 25582
Test: Total time: 0:00:07 (0.3147 s / it)
* Acc@1 84.938 Acc@5 97.088 loss 0.682
EMA Accuracy of the network on the 50000 test images: 84.9%
Max accuracy: 85.66%
{"train_lr": 1.959042117833973e-05, "train_min_lr": 2.5586449665486115e-08, "train_loss": 1.521716796875, "train_class_acc": 0.86820390625, "train_loss_scale": 7490.7648, "train_weight_decay": 0.050000000000000495, "test_loss": 0.6820592892169952, "test_acc1": 84.93800002929687, "test_acc5": 97.08800001525879, "epoch": 45, "n_parameters": 85978600}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Epoch: [46]  [  0/625]  eta: 0:28:29  lr: 0.000016  min_lr: 0.000000  loss: 1.6162 (1.6162)  class_acc: 0.8516 (0.8516)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  time: 2.7354  data: 2.0119  max mem: 25582
Epoch: [46]  [100/625]  eta: 0:05:29  lr: 0.000015  min_lr: 0.000000  loss: 1.4912 (1.5193)  class_acc: 0.8828 (0.8704)  loss_scale: 4096.0000 (4379.8812)  weight_decay: 0.0500 (0.0500)  time: 0.6045  data: 0.0002  max mem: 25582
Epoch: [46]  [200/625]  eta: 0:04:21  lr: 0.000013  min_lr: 0.000000  loss: 1.5264 (1.5246)  class_acc: 0.8633 (0.8686)  loss_scale: 8192.0000 (6276.4577)  weight_decay: 0.0500 (0.0500)  time: 0.6025  data: 0.0002  max mem: 25582
Epoch: [46]  [300/625]  eta: 0:03:18  lr: 0.000012  min_lr: 0.000000  loss: 1.5137 (1.5215)  class_acc: 0.8633 (0.8692)  loss_scale: 16384.0000 (8845.1827)  weight_decay: 0.0500 (0.0500)  time: 0.6019  data: 0.0002  max mem: 25582
Epoch: [46]  [400/625]  eta: 0:02:17  lr: 0.000011  min_lr: 0.000000  loss: 1.5078 (1.5197)  class_acc: 0.8672 (0.8696)  loss_scale: 4096.0000 (7895.7805)  weight_decay: 0.0500 (0.0500)  time: 0.5972  data: 0.0002  max mem: 25582
Epoch: [46]  [500/625]  eta: 0:01:16  lr: 0.000010  min_lr: 0.000000  loss: 1.5078 (1.5180)  class_acc: 0.8711 (0.8697)  loss_scale: 8192.0000 (7529.7725)  weight_decay: 0.0500 (0.0500)  time: 0.6042  data: 0.0002  max mem: 25582
Epoch: [46]  [600/625]  eta: 0:00:15  lr: 0.000009  min_lr: 0.000000  loss: 1.5303 (1.5186)  class_acc: 0.8672 (0.8694)  loss_scale: 4096.0000 (7449.1314)  weight_decay: 0.0500 (0.0500)  time: 0.6011  data: 0.0002  max mem: 25582
Epoch: [46]  [624/625]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000000  loss: 1.5371 (1.5191)  class_acc: 0.8594 (0.8692)  loss_scale: 4096.0000 (7320.3712)  weight_decay: 0.0500 (0.0500)  time: 0.5873  data: 0.0007  max mem: 25582
Epoch: [46] Total time: 0:06:19 (0.6069 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000000  loss: 1.5371 (1.5189)  class_acc: 0.8594 (0.8691)  loss_scale: 4096.0000 (7320.3712)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:05  loss: 0.7566 (0.7566)  acc1: 85.9375 (85.9375)  acc5: 96.4844 (96.4844)  time: 2.6058  data: 2.3744  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.6676 (0.7366)  acc1: 85.5469 (84.6880)  acc5: 97.2656 (96.7680)  time: 0.2074  data: 0.0002  max mem: 25582
Test: Total time: 0:00:07 (0.3086 s / it)
* Acc@1 84.266 Acc@5 96.592 loss 0.745
Accuracy of the network on the 50000 test images: 84.3%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:09  loss: 0.6969 (0.6969)  acc1: 87.1094 (87.1094)  acc5: 97.2656 (97.2656)  time: 2.7632  data: 2.5247  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.6252 (0.6783)  acc1: 85.5469 (85.0720)  acc5: 97.6562 (97.3280)  time: 0.2081  data: 0.0023  max mem: 25582
Test: Total time: 0:00:07 (0.3143 s / it)
* Acc@1 84.878 Acc@5 97.046 loss 0.688
EMA Accuracy of the network on the 50000 test images: 84.9%
Max accuracy: 85.66%
{"train_lr": 1.2323845702162158e-05, "train_min_lr": 1.609579778163369e-08, "train_loss": 1.5188779296875, "train_class_acc": 0.86908359375, "train_loss_scale": 7320.3712, "train_weight_decay": 0.050000000000000495, "test_loss": 0.6875619734823704, "test_acc1": 84.87800002929687, "test_acc5": 97.04600001800537, "epoch": 46, "n_parameters": 85978600}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Epoch: [47]  [  0/625]  eta: 0:30:35  lr: 0.000009  min_lr: 0.000000  loss: 1.5156 (1.5156)  class_acc: 0.8594 (0.8594)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  time: 2.9366  data: 2.2567  max mem: 25582
Epoch: [47]  [100/625]  eta: 0:05:31  lr: 0.000008  min_lr: 0.000000  loss: 1.5049 (1.5212)  class_acc: 0.8672 (0.8694)  loss_scale: 8192.0000 (5069.3069)  weight_decay: 0.0500 (0.0500)  time: 0.6088  data: 0.0002  max mem: 25582
Epoch: [47]  [200/625]  eta: 0:04:22  lr: 0.000008  min_lr: 0.000000  loss: 1.5127 (1.5184)  class_acc: 0.8672 (0.8698)  loss_scale: 8192.0000 (6622.8856)  weight_decay: 0.0500 (0.0500)  time: 0.6041  data: 0.0002  max mem: 25582
Epoch: [47]  [300/625]  eta: 0:03:19  lr: 0.000007  min_lr: 0.000000  loss: 1.5430 (1.5210)  class_acc: 0.8555 (0.8692)  loss_scale: 8192.0000 (7579.6412)  weight_decay: 0.0500 (0.0500)  time: 0.6047  data: 0.0002  max mem: 25582
Epoch: [47]  [400/625]  eta: 0:02:17  lr: 0.000006  min_lr: 0.000000  loss: 1.5674 (1.5234)  class_acc: 0.8555 (0.8684)  loss_scale: 8192.0000 (8508.6484)  weight_decay: 0.0500 (0.0500)  time: 0.6000  data: 0.0002  max mem: 25582
Epoch: [47]  [500/625]  eta: 0:01:16  lr: 0.000005  min_lr: 0.000000  loss: 1.4756 (1.5191)  class_acc: 0.8633 (0.8693)  loss_scale: 4096.0000 (8085.7166)  weight_decay: 0.0500 (0.0500)  time: 0.6062  data: 0.0002  max mem: 25582
Epoch: [47]  [600/625]  eta: 0:00:15  lr: 0.000005  min_lr: 0.000000  loss: 1.5039 (1.5194)  class_acc: 0.8672 (0.8691)  loss_scale: 8192.0000 (7524.0998)  weight_decay: 0.0500 (0.0500)  time: 0.6009  data: 0.0002  max mem: 25582
Epoch: [47]  [624/625]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000000  loss: 1.5078 (1.5186)  class_acc: 0.8828 (0.8694)  loss_scale: 8192.0000 (7549.7472)  weight_decay: 0.0500 (0.0500)  time: 0.5880  data: 0.0007  max mem: 25582
Epoch: [47] Total time: 0:06:20 (0.6084 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000000  loss: 1.5078 (1.5160)  class_acc: 0.8828 (0.8702)  loss_scale: 8192.0000 (7549.7472)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:08  loss: 0.7622 (0.7622)  acc1: 85.9375 (85.9375)  acc5: 96.4844 (96.4844)  time: 2.7320  data: 2.4918  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.6685 (0.7390)  acc1: 85.9375 (84.5920)  acc5: 97.2656 (96.7360)  time: 0.2088  data: 0.0002  max mem: 25582
Test: Total time: 0:00:07 (0.3150 s / it)
* Acc@1 84.224 Acc@5 96.568 loss 0.747
Accuracy of the network on the 50000 test images: 84.2%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:12  loss: 0.7029 (0.7029)  acc1: 87.1094 (87.1094)  acc5: 97.2656 (97.2656)  time: 2.8906  data: 2.6596  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.6286 (0.6839)  acc1: 85.5469 (84.9920)  acc5: 97.6562 (97.2960)  time: 0.2092  data: 0.0022  max mem: 25582
Test: Total time: 0:00:08 (0.3207 s / it)
* Acc@1 84.784 Acc@5 97.004 loss 0.693
EMA Accuracy of the network on the 50000 test images: 84.8%
Max accuracy: 85.66%
{"train_lr": 6.833972045526321e-06, "train_min_lr": 8.925641780051694e-09, "train_loss": 1.5159798828125, "train_class_acc": 0.87015546875, "train_loss_scale": 7549.7472, "train_weight_decay": 0.050000000000000495, "test_loss": 0.6929548986256122, "test_acc1": 84.78400002929688, "test_acc5": 97.00400001800537, "epoch": 47, "n_parameters": 85978600}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Epoch: [48]  [  0/625]  eta: 0:29:33  lr: 0.000005  min_lr: 0.000000  loss: 1.5771 (1.5771)  class_acc: 0.8555 (0.8555)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 2.8384  data: 2.2173  max mem: 25582
Epoch: [48]  [100/625]  eta: 0:05:29  lr: 0.000004  min_lr: 0.000000  loss: 1.4814 (1.5042)  class_acc: 0.8789 (0.8721)  loss_scale: 16384.0000 (9165.3069)  weight_decay: 0.0500 (0.0500)  time: 0.6074  data: 0.0002  max mem: 25582
Epoch: [48]  [200/625]  eta: 0:04:21  lr: 0.000004  min_lr: 0.000000  loss: 1.5186 (1.5162)  class_acc: 0.8594 (0.8692)  loss_scale: 16384.0000 (12756.6965)  weight_decay: 0.0500 (0.0500)  time: 0.6049  data: 0.0003  max mem: 25582
Epoch: [48]  [300/625]  eta: 0:03:18  lr: 0.000003  min_lr: 0.000000  loss: 1.5244 (1.5164)  class_acc: 0.8672 (0.8700)  loss_scale: 8192.0000 (12301.6080)  weight_decay: 0.0500 (0.0500)  time: 0.6034  data: 0.0002  max mem: 25582
Epoch: [48]  [400/625]  eta: 0:02:17  lr: 0.000003  min_lr: 0.000000  loss: 1.5068 (1.5168)  class_acc: 0.8711 (0.8698)  loss_scale: 8192.0000 (11419.7706)  weight_decay: 0.0500 (0.0500)  time: 0.5998  data: 0.0002  max mem: 25582
Epoch: [48]  [500/625]  eta: 0:01:15  lr: 0.000002  min_lr: 0.000000  loss: 1.4697 (1.5150)  class_acc: 0.8789 (0.8704)  loss_scale: 4096.0000 (10113.2774)  weight_decay: 0.0500 (0.0500)  time: 0.6027  data: 0.0003  max mem: 25582
Epoch: [48]  [600/625]  eta: 0:00:15  lr: 0.000002  min_lr: 0.000000  loss: 1.5000 (1.5141)  class_acc: 0.8711 (0.8707)  loss_scale: 8192.0000 (9466.4626)  weight_decay: 0.0500 (0.0500)  time: 0.6042  data: 0.0003  max mem: 25582
Epoch: [48]  [624/625]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.5010 (1.5141)  class_acc: 0.8750 (0.8708)  loss_scale: 8192.0000 (9417.5232)  weight_decay: 0.0500 (0.0500)  time: 0.5909  data: 0.0014  max mem: 25582
Epoch: [48] Total time: 0:06:19 (0.6066 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.5010 (1.5148)  class_acc: 0.8750 (0.8705)  loss_scale: 8192.0000 (9417.5232)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:03  loss: 0.7574 (0.7574)  acc1: 86.3281 (86.3281)  acc5: 96.4844 (96.4844)  time: 2.5551  data: 2.3102  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.6652 (0.7381)  acc1: 85.9375 (84.5760)  acc5: 97.2656 (96.8160)  time: 0.2073  data: 0.0001  max mem: 25582
Test: Total time: 0:00:07 (0.3055 s / it)
* Acc@1 84.272 Acc@5 96.596 loss 0.746
Accuracy of the network on the 50000 test images: 84.3%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:00:59  loss: 0.7090 (0.7090)  acc1: 87.1094 (87.1094)  acc5: 97.2656 (97.2656)  time: 2.3830  data: 2.1348  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.6319 (0.6890)  acc1: 85.8491 (85.0080)  acc5: 97.6562 (97.2960)  time: 0.2173  data: 0.0112  max mem: 25582
Test: Total time: 0:00:07 (0.3063 s / it)
* Acc@1 84.762 Acc@5 96.966 loss 0.698
EMA Accuracy of the network on the 50000 test images: 84.8%
Max accuracy: 85.66%
{"train_lr": 3.154647105093584e-06, "train_min_lr": 4.120188056808745e-09, "train_loss": 1.51481875, "train_class_acc": 0.87047734375, "train_loss_scale": 9417.5232, "train_weight_decay": 0.050000000000000495, "test_loss": 0.697995484918356, "test_acc1": 84.76200002929687, "test_acc5": 96.96600001800537, "epoch": 48, "n_parameters": 85978600}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Epoch: [49]  [  0/625]  eta: 0:30:26  lr: 0.000002  min_lr: 0.000000  loss: 1.4932 (1.4932)  class_acc: 0.8789 (0.8789)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 2.9218  data: 2.2950  max mem: 25582
Epoch: [49]  [100/625]  eta: 0:05:30  lr: 0.000002  min_lr: 0.000000  loss: 1.5186 (1.5182)  class_acc: 0.8672 (0.8685)  loss_scale: 16384.0000 (12166.3366)  weight_decay: 0.0500 (0.0500)  time: 0.6035  data: 0.0002  max mem: 25582
Epoch: [49]  [200/625]  eta: 0:04:22  lr: 0.000001  min_lr: 0.000000  loss: 1.4990 (1.5189)  class_acc: 0.8633 (0.8680)  loss_scale: 8192.0000 (12797.4527)  weight_decay: 0.0500 (0.0500)  time: 0.6036  data: 0.0002  max mem: 25582
Epoch: [49]  [300/625]  eta: 0:03:18  lr: 0.000001  min_lr: 0.000000  loss: 1.5225 (1.5159)  class_acc: 0.8672 (0.8691)  loss_scale: 4096.0000 (11022.4585)  weight_decay: 0.0500 (0.0500)  time: 0.5999  data: 0.0002  max mem: 25582
Epoch: [49]  [400/625]  eta: 0:02:17  lr: 0.000001  min_lr: 0.000000  loss: 1.5049 (1.5145)  class_acc: 0.8672 (0.8699)  loss_scale: 4096.0000 (9295.1621)  weight_decay: 0.0500 (0.0500)  time: 0.6051  data: 0.0002  max mem: 25582
Epoch: [49]  [500/625]  eta: 0:01:16  lr: 0.000001  min_lr: 0.000000  loss: 1.5107 (1.5154)  class_acc: 0.8711 (0.8699)  loss_scale: 8192.0000 (8985.0379)  weight_decay: 0.0500 (0.0500)  time: 0.6029  data: 0.0002  max mem: 25582
Epoch: [49]  [600/625]  eta: 0:00:15  lr: 0.000001  min_lr: 0.000000  loss: 1.5342 (1.5172)  class_acc: 0.8672 (0.8695)  loss_scale: 16384.0000 (9684.5524)  weight_decay: 0.0500 (0.0500)  time: 0.6062  data: 0.0002  max mem: 25582
Epoch: [49]  [624/625]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.4941 (1.5171)  class_acc: 0.8711 (0.8697)  loss_scale: 16384.0000 (9941.8112)  weight_decay: 0.0500 (0.0500)  time: 0.5884  data: 0.0007  max mem: 25582
Epoch: [49] Total time: 0:06:19 (0.6075 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.4941 (1.5139)  class_acc: 0.8711 (0.8705)  loss_scale: 16384.0000 (9941.8112)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:00:57  loss: 0.7564 (0.7564)  acc1: 86.3281 (86.3281)  acc5: 96.4844 (96.4844)  time: 2.2988  data: 2.0538  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.6644 (0.7379)  acc1: 85.9375 (84.5920)  acc5: 97.2656 (96.8000)  time: 0.2050  data: 0.0001  max mem: 25582
Test: Total time: 0:00:07 (0.2937 s / it)
* Acc@1 84.262 Acc@5 96.580 loss 0.746
Accuracy of the network on the 50000 test images: 84.3%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
).requires_grad to False
set Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False).requires_grad to False
Test:  [ 0/25]  eta: 0:01:04  loss: 0.7140 (0.7140)  acc1: 86.7188 (86.7188)  acc5: 97.2656 (97.2656)  time: 2.5765  data: 2.3202  max mem: 25582
Test:  [24/25]  eta: 0:00:00  loss: 0.6349 (0.6938)  acc1: 85.8491 (84.9760)  acc5: 97.6562 (97.2480)  time: 0.2116  data: 0.0053  max mem: 25582
Test: Total time: 0:00:07 (0.3098 s / it)
* Acc@1 84.728 Acc@5 96.934 loss 0.703
EMA Accuracy of the network on the 50000 test images: 84.7%
Max accuracy: 85.66%
{"train_lr": 1.3085551426214094e-06, "train_min_lr": 1.7090638320841446e-09, "train_loss": 1.5138634765625, "train_class_acc": 0.870484375, "train_loss_scale": 9941.8112, "train_weight_decay": 0.050000000000000495, "test_loss": 0.7027080807089806, "test_acc1": 84.72800002929688, "test_acc5": 96.93400001525879, "epoch": 49, "n_parameters": 85978600}
Training time 6:01:54
