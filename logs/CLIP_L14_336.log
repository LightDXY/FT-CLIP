/home/v-hezhenhu/.local/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
sh: 1: azcopy: not found
sh: 1: azcopy: not found
sh: 1: azcopy: not found
sh: 1: azcopy: not found
sh: 1: azcopy: not found
sh: 1: azcopy: not found
sh: 1: azcopy: not found
sh: 1: azcopy: not found
**********find WORLD_SIZE 24 in env**********
**********find WORLD_SIZE 24 in env**********
**********find WORLD_SIZE 24 in env**********
**********find WORLD_SIZE 24 in env**********
**********find WORLD_SIZE 24 in env**********
**********find WORLD_SIZE 24 in env**********
**********find WORLD_SIZE 24 in env**********
**********find WORLD_SIZE 24 in env**********
************World_size is 24, current rank 0 ***********, local rank 0
************World_size is 24, current rank 3 ***********, local rank 3
************World_size is 24, current rank 5 ***********, local rank 5
************World_size is 24, current rank 1 ***********, local rank 1
************World_size is 24, current rank 7 ***********, local rank 7
************World_size is 24, current rank 6 ***********, local rank 6
************World_size is 24, current rank 2 ***********, local rank 2
************World_size is 24, current rank 4 ***********, local rank 4
Namespace(aa='rand-m9-mstd0.5-inc1', abs_pos_emb=True, attn_drop_rate=0.0, auto_resume=True, backbone_decay=1.0, batch_size=20, bce_loss=False, clip_grad=None, clip_mean_and_std=True, color_jitter=0.4, crop_pct=None, crop_scale=0.08, cutmix=0.0, cutmix_minmax=None, data='imagenet', data_path='/tmp/DATASET/imagenet', data_set='IMNET', deepscale=False, deepscale_config=None, deepspeed=False, deepspeed_config=None, deepspeed_mpi=False, device='cuda:0', disable_eval_during_finetuning=False, disable_weight_decay_on_rel_pos_bias=False, dist_eval=True, dist_on_itp=False, dist_url='env://', distributed=True, drop=0.0, drop_path=0.0, enable_deepspeed=True, epochs=30, eval=False, eval_all=True, eval_data_path=None, finetune='OUTPUT/SLIP/declip_model/clip_vitL14.pth', freeze_layers=0, imagenet_default_mean_and_std=False, init_scale=0.001, input_size=336, layer_decay=0.65, layer_scale_init_value=0.0, local_rank=0, log_dir=None, lr=0.0006, min_lr=1e-06, mixup=0.0, mixup_mode='batch', mixup_prob=1.0, mixup_switch_prob=0.5, model='declip_L14_336', model_ema=True, model_ema_decay=0.9998, model_ema_force_cpu=False, model_key='state', model_prefix='visual.', momentum=0.9, nb_classes=1000, num_gpu=1, num_workers=8, opt='adamw', opt_betas=None, opt_eps=1e-08, output_dir='OUTPUT/SLIP_/maskclip_ft/CLIP_openai_Large_P14_336/FT30_2048_6E4_D065_EMA98_DPR00', pin_mem=True, rank=0, recount=1, rel_pos_bias=False, remode='pixel', reprob=0.25, resplit=False, resume='', resume_tag='best', save_ckpt=False, save_ckpt_freq=5, seed=0, smoothing=0.1, src=False, start_epoch=0, three_aug=False, train_interpolation='bicubic', train_set='zip', update_freq=4, use_mean_pooling=True, warmup_epochs=5, warmup_lr=1e-06, warmup_steps=-1, weight_decay=0.05, weight_decay_end=None, world_size=24)
use crop scale (0.08, 1)
Transform = 
RandomResizedCropAndInterpolation(size=(336, 336), scale=(0.08, 1), ratio=(0.75, 1.3333), interpolation=PIL.Image.BICUBIC)
RandomHorizontalFlip(p=0.5)
<timm.data.auto_augment.RandAugment object at 0x7f2589cc89d0>
ToTensor()
Normalize(mean=tensor([0.4815, 0.4578, 0.4082]), std=tensor([0.2686, 0.2613, 0.2758]))
<timm.data.random_erasing.RandomErasing object at 0x7f2589cc8d30>
---------------------------
Only ImageNet-1K, length 1281167
USE ZIP DATALOADER
Number of the class = 1000
Set crop pct to 1 for clip task
Transform = 
Resize(size=336, interpolation=bicubic, max_size=None, antialias=None)
CenterCrop(size=(336, 336))
ToTensor()
Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------------------------
Only ImageNet-1K, length 1281167
Only ImageNet-1K, length 1281167
Only ImageNet-1K, length 1281167
Only ImageNet-1K, length 1281167
Only ImageNet-1K, length 1281167
Only ImageNet-1K, length 1281167
Only ImageNet-1K, length 1281167
Only ImageNet-1K, length 50000
USE ZIP DATALOADER
Number of the class = 1000
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f258e8c33d0>
Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
No Layer Scale
{'num_classes': 1000, 'drop_rate': 0.0, 'drop_path_rate': 0.0, 'attn_drop_rate': 0.0, 'use_mean_pooling': True, 'init_scale': 0.001, 'use_rel_pos_bias': False, 'use_abs_pos_emb': True}
drop_path_rate: 0.0
layer_scale: False
Using DPR [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Only ImageNet-1K, length 50000
Only ImageNet-1K, length 50000
Only ImageNet-1K, length 50000
Only ImageNet-1K, length 50000
Only ImageNet-1K, length 50000
Only ImageNet-1K, length 50000
Only ImageNet-1K, length 50000
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Patch size = (14, 14)
Load ckpt from OUTPUT/SLIP/declip_model/clip_vitL14.pth
torch.Size([257, 1024])
Position interpolate from 16x16 to 24x24
Weights of VisualTransformer not initialized from pretrained model: ['visual.patch_embed.proj.weight', 'visual.patch_embed.proj.bias', 'visual.fc_norm.weight', 'visual.fc_norm.bias', 'visual.head.weight', 'visual.head.bias']
Weights from pretrained model not used in VisualTransformer: ['visual.proj', 'visual.ln_post.weight', 'visual.ln_post.bias']
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Using EMA with decay = 0.99980000
Model = VisualTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
  )
  (conv1): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
  (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (transformer): Transformer(
    (dropout): Dropout(p=0, inplace=False)
    (resblocks): Sequential(
      (0): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (2): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (3): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (4): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (5): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (6): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (7): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (8): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (9): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (10): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (11): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (12): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (13): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (14): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (15): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (16): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (17): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (18): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (19): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (20): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (21): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (22): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (23): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
    )
  )
  (fc_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (head): Linear(in_features=1024, out_features=1000, bias=True)
)
number of params: 304532456
LR = 0.00060000
Batch size = 1920
Update frequent = 4
Number of training examples = 1281167
Number of training training per epoch = 667
Assigned values = [2.1029740616282293e-05, 3.2353447101972754e-05, 4.977453400303501e-05, 7.65762061585154e-05, 0.00011780954793617752, 0.00018124545836335003, 0.0002788391667128462, 0.0004289833334043787, 0.0006599743590836596, 0.0010153451678210146, 0.0015620694889554071, 0.002403183829162165, 0.003697205891018715, 0.005688009063105715, 0.008750783174008792, 0.013462743344628911, 0.02071191283789063, 0.03186448128906251, 0.049022278906250015, 0.07541889062500001, 0.11602906250000002, 0.17850625000000003, 0.274625, 0.42250000000000004, 0.65, 1.0]
Param groups = {
  "layer_0_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "class_embedding",
      "positional_embedding",
      "ln_pre.weight",
      "ln_pre.bias"
    ],
    "lr_scale": 2.1029740616282293e-05
  },
  "layer_0_decay": {
    "weight_decay": 0.05,
    "params": [
      "conv1.weight"
    ],
    "lr_scale": 2.1029740616282293e-05
  },
  "layer_1_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.0.attn.in_proj_weight",
      "transformer.resblocks.0.attn.out_proj.weight",
      "transformer.resblocks.0.mlp.c_fc.weight",
      "transformer.resblocks.0.mlp.c_proj.weight"
    ],
    "lr_scale": 3.2353447101972754e-05
  },
  "layer_1_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.0.attn.in_proj_bias",
      "transformer.resblocks.0.attn.out_proj.bias",
      "transformer.resblocks.0.ln_1.weight",
      "transformer.resblocks.0.ln_1.bias",
      "transformer.resblocks.0.mlp.c_fc.bias",
      "transformer.resblocks.0.mlp.c_proj.bias",
      "transformer.resblocks.0.ln_2.weight",
      "transformer.resblocks.0.ln_2.bias"
    ],
    "lr_scale": 3.2353447101972754e-05
  },
  "layer_2_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.1.attn.in_proj_weight",
      "transformer.resblocks.1.attn.out_proj.weight",
      "transformer.resblocks.1.mlp.c_fc.weight",
      "transformer.resblocks.1.mlp.c_proj.weight"
    ],
    "lr_scale": 4.977453400303501e-05
  },
  "layer_2_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.1.attn.in_proj_bias",
      "transformer.resblocks.1.attn.out_proj.bias",
      "transformer.resblocks.1.ln_1.weight",
      "transformer.resblocks.1.ln_1.bias",
      "transformer.resblocks.1.mlp.c_fc.bias",
      "transformer.resblocks.1.mlp.c_proj.bias",
      "transformer.resblocks.1.ln_2.weight",
      "transformer.resblocks.1.ln_2.bias"
    ],
    "lr_scale": 4.977453400303501e-05
  },
  "layer_3_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.2.attn.in_proj_weight",
      "transformer.resblocks.2.attn.out_proj.weight",
      "transformer.resblocks.2.mlp.c_fc.weight",
      "transformer.resblocks.2.mlp.c_proj.weight"
    ],
    "lr_scale": 7.65762061585154e-05
  },
  "layer_3_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.2.attn.in_proj_bias",
      "transformer.resblocks.2.attn.out_proj.bias",
      "transformer.resblocks.2.ln_1.weight",
      "transformer.resblocks.2.ln_1.bias",
      "transformer.resblocks.2.mlp.c_fc.bias",
      "transformer.resblocks.2.mlp.c_proj.bias",
      "transformer.resblocks.2.ln_2.weight",
      "transformer.resblocks.2.ln_2.bias"
    ],
    "lr_scale": 7.65762061585154e-05
  },
  "layer_4_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.3.attn.in_proj_weight",
      "transformer.resblocks.3.attn.out_proj.weight",
      "transformer.resblocks.3.mlp.c_fc.weight",
      "transformer.resblocks.3.mlp.c_proj.weight"
    ],
    "lr_scale": 0.00011780954793617752
  },
  "layer_4_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.3.attn.in_proj_bias",
      "transformer.resblocks.3.attn.out_proj.bias",
      "transformer.resblocks.3.ln_1.weight",
      "transformer.resblocks.3.ln_1.bias",
      "transformer.resblocks.3.mlp.c_fc.bias",
      "transformer.resblocks.3.mlp.c_proj.bias",
      "transformer.resblocks.3.ln_2.weight",
      "transformer.resblocks.3.ln_2.bias"
    ],
    "lr_scale": 0.00011780954793617752
  },
  "layer_5_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.4.attn.in_proj_weight",
      "transformer.resblocks.4.attn.out_proj.weight",
      "transformer.resblocks.4.mlp.c_fc.weight",
      "transformer.resblocks.4.mlp.c_proj.weight"
    ],
    "lr_scale": 0.00018124545836335003
  },
  "layer_5_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.4.attn.in_proj_bias",
      "transformer.resblocks.4.attn.out_proj.bias",
      "transformer.resblocks.4.ln_1.weight",
      "transformer.resblocks.4.ln_1.bias",
      "transformer.resblocks.4.mlp.c_fc.bias",
      "transformer.resblocks.4.mlp.c_proj.bias",
      "transformer.resblocks.4.ln_2.weight",
      "transformer.resblocks.4.ln_2.bias"
    ],
    "lr_scale": 0.00018124545836335003
  },
  "layer_6_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.5.attn.in_proj_weight",
      "transformer.resblocks.5.attn.out_proj.weight",
      "transformer.resblocks.5.mlp.c_fc.weight",
      "transformer.resblocks.5.mlp.c_proj.weight"
    ],
    "lr_scale": 0.0002788391667128462
  },
  "layer_6_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.5.attn.in_proj_bias",
      "transformer.resblocks.5.attn.out_proj.bias",
      "transformer.resblocks.5.ln_1.weight",
      "transformer.resblocks.5.ln_1.bias",
      "transformer.resblocks.5.mlp.c_fc.bias",
      "transformer.resblocks.5.mlp.c_proj.bias",
      "transformer.resblocks.5.ln_2.weight",
      "transformer.resblocks.5.ln_2.bias"
    ],
    "lr_scale": 0.0002788391667128462
  },
  "layer_7_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.6.attn.in_proj_weight",
      "transformer.resblocks.6.attn.out_proj.weight",
      "transformer.resblocks.6.mlp.c_fc.weight",
      "transformer.resblocks.6.mlp.c_proj.weight"
    ],
    "lr_scale": 0.0004289833334043787
  },
  "layer_7_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.6.attn.in_proj_bias",
      "transformer.resblocks.6.attn.out_proj.bias",
      "transformer.resblocks.6.ln_1.weight",
      "transformer.resblocks.6.ln_1.bias",
      "transformer.resblocks.6.mlp.c_fc.bias",
      "transformer.resblocks.6.mlp.c_proj.bias",
      "transformer.resblocks.6.ln_2.weight",
      "transformer.resblocks.6.ln_2.bias"
    ],
    "lr_scale": 0.0004289833334043787
  },
  "layer_8_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.7.attn.in_proj_weight",
      "transformer.resblocks.7.attn.out_proj.weight",
      "transformer.resblocks.7.mlp.c_fc.weight",
      "transformer.resblocks.7.mlp.c_proj.weight"
    ],
    "lr_scale": 0.0006599743590836596
  },
  "layer_8_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.7.attn.in_proj_bias",
      "transformer.resblocks.7.attn.out_proj.bias",
      "transformer.resblocks.7.ln_1.weight",
      "transformer.resblocks.7.ln_1.bias",
      "transformer.resblocks.7.mlp.c_fc.bias",
      "transformer.resblocks.7.mlp.c_proj.bias",
      "transformer.resblocks.7.ln_2.weight",
      "transformer.resblocks.7.ln_2.bias"
    ],
    "lr_scale": 0.0006599743590836596
  },
  "layer_9_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.8.attn.in_proj_weight",
      "transformer.resblocks.8.attn.out_proj.weight",
      "transformer.resblocks.8.mlp.c_fc.weight",
      "transformer.resblocks.8.mlp.c_proj.weight"
    ],
    "lr_scale": 0.0010153451678210146
  },
  "layer_9_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.8.attn.in_proj_bias",
      "transformer.resblocks.8.attn.out_proj.bias",
      "transformer.resblocks.8.ln_1.weight",
      "transformer.resblocks.8.ln_1.bias",
      "transformer.resblocks.8.mlp.c_fc.bias",
      "transformer.resblocks.8.mlp.c_proj.bias",
      "transformer.resblocks.8.ln_2.weight",
      "transformer.resblocks.8.ln_2.bias"
    ],
    "lr_scale": 0.0010153451678210146
  },
  "layer_10_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.9.attn.in_proj_weight",
      "transformer.resblocks.9.attn.out_proj.weight",
      "transformer.resblocks.9.mlp.c_fc.weight",
      "transformer.resblocks.9.mlp.c_proj.weight"
    ],
    "lr_scale": 0.0015620694889554071
  },
  "layer_10_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.9.attn.in_proj_bias",
      "transformer.resblocks.9.attn.out_proj.bias",
      "transformer.resblocks.9.ln_1.weight",
      "transformer.resblocks.9.ln_1.bias",
      "transformer.resblocks.9.mlp.c_fc.bias",
      "transformer.resblocks.9.mlp.c_proj.bias",
      "transformer.resblocks.9.ln_2.weight",
      "transformer.resblocks.9.ln_2.bias"
    ],
    "lr_scale": 0.0015620694889554071
  },
  "layer_11_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.10.attn.in_proj_weight",
      "transformer.resblocks.10.attn.out_proj.weight",
      "transformer.resblocks.10.mlp.c_fc.weight",
      "transformer.resblocks.10.mlp.c_proj.weight"
    ],
    "lr_scale": 0.002403183829162165
  },
  "layer_11_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.10.attn.in_proj_bias",
      "transformer.resblocks.10.attn.out_proj.bias",
      "transformer.resblocks.10.ln_1.weight",
      "transformer.resblocks.10.ln_1.bias",
      "transformer.resblocks.10.mlp.c_fc.bias",
      "transformer.resblocks.10.mlp.c_proj.bias",
      "transformer.resblocks.10.ln_2.weight",
      "transformer.resblocks.10.ln_2.bias"
    ],
    "lr_scale": 0.002403183829162165
  },
  "layer_12_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.11.attn.in_proj_weight",
      "transformer.resblocks.11.attn.out_proj.weight",
      "transformer.resblocks.11.mlp.c_fc.weight",
      "transformer.resblocks.11.mlp.c_proj.weight"
    ],
    "lr_scale": 0.003697205891018715
  },
  "layer_12_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.11.attn.in_proj_bias",
      "transformer.resblocks.11.attn.out_proj.bias",
      "transformer.resblocks.11.ln_1.weight",
      "transformer.resblocks.11.ln_1.bias",
      "transformer.resblocks.11.mlp.c_fc.bias",
      "transformer.resblocks.11.mlp.c_proj.bias",
      "transformer.resblocks.11.ln_2.weight",
      "transformer.resblocks.11.ln_2.bias"
    ],
    "lr_scale": 0.003697205891018715
  },
  "layer_13_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.12.attn.in_proj_weight",
      "transformer.resblocks.12.attn.out_proj.weight",
      "transformer.resblocks.12.mlp.c_fc.weight",
      "transformer.resblocks.12.mlp.c_proj.weight"
    ],
    "lr_scale": 0.005688009063105715
  },
  "layer_13_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.12.attn.in_proj_bias",
      "transformer.resblocks.12.attn.out_proj.bias",
      "transformer.resblocks.12.ln_1.weight",
      "transformer.resblocks.12.ln_1.bias",
      "transformer.resblocks.12.mlp.c_fc.bias",
      "transformer.resblocks.12.mlp.c_proj.bias",
      "transformer.resblocks.12.ln_2.weight",
      "transformer.resblocks.12.ln_2.bias"
    ],
    "lr_scale": 0.005688009063105715
  },
  "layer_14_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.13.attn.in_proj_weight",
      "transformer.resblocks.13.attn.out_proj.weight",
      "transformer.resblocks.13.mlp.c_fc.weight",
      "transformer.resblocks.13.mlp.c_proj.weight"
    ],
    "lr_scale": 0.008750783174008792
  },
  "layer_14_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.13.attn.in_proj_bias",
      "transformer.resblocks.13.attn.out_proj.bias",
      "transformer.resblocks.13.ln_1.weight",
      "transformer.resblocks.13.ln_1.bias",
      "transformer.resblocks.13.mlp.c_fc.bias",
      "transformer.resblocks.13.mlp.c_proj.bias",
      "transformer.resblocks.13.ln_2.weight",
      "transformer.resblocks.13.ln_2.bias"
    ],
    "lr_scale": 0.008750783174008792
  },
  "layer_15_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.14.attn.in_proj_weight",
      "transformer.resblocks.14.attn.out_proj.weight",
      "transformer.resblocks.14.mlp.c_fc.weight",
      "transformer.resblocks.14.mlp.c_proj.weight"
    ],
    "lr_scale": 0.013462743344628911
  },
  "layer_15_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.14.attn.in_proj_bias",
      "transformer.resblocks.14.attn.out_proj.bias",
      "transformer.resblocks.14.ln_1.weight",
      "transformer.resblocks.14.ln_1.bias",
      "transformer.resblocks.14.mlp.c_fc.bias",
      "transformer.resblocks.14.mlp.c_proj.bias",
      "transformer.resblocks.14.ln_2.weight",
      "transformer.resblocks.14.ln_2.bias"
    ],
    "lr_scale": 0.013462743344628911
  },
  "layer_16_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.15.attn.in_proj_weight",
      "transformer.resblocks.15.attn.out_proj.weight",
      "transformer.resblocks.15.mlp.c_fc.weight",
      "transformer.resblocks.15.mlp.c_proj.weight"
    ],
    "lr_scale": 0.02071191283789063
  },
  "layer_16_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.15.attn.in_proj_bias",
      "transformer.resblocks.15.attn.out_proj.bias",
      "transformer.resblocks.15.ln_1.weight",
      "transformer.resblocks.15.ln_1.bias",
      "transformer.resblocks.15.mlp.c_fc.bias",
      "transformer.resblocks.15.mlp.c_proj.bias",
      "transformer.resblocks.15.ln_2.weight",
      "transformer.resblocks.15.ln_2.bias"
    ],
    "lr_scale": 0.02071191283789063
  },
  "layer_17_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.16.attn.in_proj_weight",
      "transformer.resblocks.16.attn.out_proj.weight",
      "transformer.resblocks.16.mlp.c_fc.weight",
      "transformer.resblocks.16.mlp.c_proj.weight"
    ],
    "lr_scale": 0.03186448128906251
  },
  "layer_17_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.16.attn.in_proj_bias",
      "transformer.resblocks.16.attn.out_proj.bias",
      "transformer.resblocks.16.ln_1.weight",
      "transformer.resblocks.16.ln_1.bias",
      "transformer.resblocks.16.mlp.c_fc.bias",
      "transformer.resblocks.16.mlp.c_proj.bias",
      "transformer.resblocks.16.ln_2.weight",
      "transformer.resblocks.16.ln_2.bias"
    ],
    "lr_scale": 0.03186448128906251
  },
  "layer_18_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.17.attn.in_proj_weight",
      "transformer.resblocks.17.attn.out_proj.weight",
      "transformer.resblocks.17.mlp.c_fc.weight",
      "transformer.resblocks.17.mlp.c_proj.weight"
    ],
    "lr_scale": 0.049022278906250015
  },
  "layer_18_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.17.attn.in_proj_bias",
      "transformer.resblocks.17.attn.out_proj.bias",
      "transformer.resblocks.17.ln_1.weight",
      "transformer.resblocks.17.ln_1.bias",
      "transformer.resblocks.17.mlp.c_fc.bias",
      "transformer.resblocks.17.mlp.c_proj.bias",
      "transformer.resblocks.17.ln_2.weight",
      "transformer.resblocks.17.ln_2.bias"
    ],
    "lr_scale": 0.049022278906250015
  },
  "layer_19_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.18.attn.in_proj_weight",
      "transformer.resblocks.18.attn.out_proj.weight",
      "transformer.resblocks.18.mlp.c_fc.weight",
      "transformer.resblocks.18.mlp.c_proj.weight"
    ],
    "lr_scale": 0.07541889062500001
  },
  "layer_19_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.18.attn.in_proj_bias",
      "transformer.resblocks.18.attn.out_proj.bias",
      "transformer.resblocks.18.ln_1.weight",
      "transformer.resblocks.18.ln_1.bias",
      "transformer.resblocks.18.mlp.c_fc.bias",
      "transformer.resblocks.18.mlp.c_proj.bias",
      "transformer.resblocks.18.ln_2.weight",
      "transformer.resblocks.18.ln_2.bias"
    ],
    "lr_scale": 0.07541889062500001
  },
  "layer_20_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.19.attn.in_proj_weight",
      "transformer.resblocks.19.attn.out_proj.weight",
      "transformer.resblocks.19.mlp.c_fc.weight",
      "transformer.resblocks.19.mlp.c_proj.weight"
    ],
    "lr_scale": 0.11602906250000002
  },
  "layer_20_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.19.attn.in_proj_bias",
      "transformer.resblocks.19.attn.out_proj.bias",
      "transformer.resblocks.19.ln_1.weight",
      "transformer.resblocks.19.ln_1.bias",
      "transformer.resblocks.19.mlp.c_fc.bias",
      "transformer.resblocks.19.mlp.c_proj.bias",
      "transformer.resblocks.19.ln_2.weight",
      "transformer.resblocks.19.ln_2.bias"
    ],
    "lr_scale": 0.11602906250000002
  },
  "layer_21_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.20.attn.in_proj_weight",
      "transformer.resblocks.20.attn.out_proj.weight",
      "transformer.resblocks.20.mlp.c_fc.weight",
      "transformer.resblocks.20.mlp.c_proj.weight"
    ],
    "lr_scale": 0.17850625000000003
  },
  "layer_21_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.20.attn.in_proj_bias",
      "transformer.resblocks.20.attn.out_proj.bias",
      "transformer.resblocks.20.ln_1.weight",
      "transformer.resblocks.20.ln_1.bias",
      "transformer.resblocks.20.mlp.c_fc.bias",
      "transformer.resblocks.20.mlp.c_proj.bias",
      "transformer.resblocks.20.ln_2.weight",
      "transformer.resblocks.20.ln_2.bias"
    ],
    "lr_scale": 0.17850625000000003
  },
  "layer_22_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.21.attn.in_proj_weight",
      "transformer.resblocks.21.attn.out_proj.weight",
      "transformer.resblocks.21.mlp.c_fc.weight",
      "transformer.resblocks.21.mlp.c_proj.weight"
    ],
    "lr_scale": 0.274625
  },
  "layer_22_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.21.attn.in_proj_bias",
      "transformer.resblocks.21.attn.out_proj.bias",
      "transformer.resblocks.21.ln_1.weight",
      "transformer.resblocks.21.ln_1.bias",
      "transformer.resblocks.21.mlp.c_fc.bias",
      "transformer.resblocks.21.mlp.c_proj.bias",
      "transformer.resblocks.21.ln_2.weight",
      "transformer.resblocks.21.ln_2.bias"
    ],
    "lr_scale": 0.274625
  },
  "layer_23_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.22.attn.in_proj_weight",
      "transformer.resblocks.22.attn.out_proj.weight",
      "transformer.resblocks.22.mlp.c_fc.weight",
      "transformer.resblocks.22.mlp.c_proj.weight"
    ],
    "lr_scale": 0.42250000000000004
  },
  "layer_23_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.22.attn.in_proj_bias",
      "transformer.resblocks.22.attn.out_proj.bias",
      "transformer.resblocks.22.ln_1.weight",
      "transformer.resblocks.22.ln_1.bias",
      "transformer.resblocks.22.mlp.c_fc.bias",
      "transformer.resblocks.22.mlp.c_proj.bias",
      "transformer.resblocks.22.ln_2.weight",
      "transformer.resblocks.22.ln_2.bias"
    ],
    "lr_scale": 0.42250000000000004
  },
  "layer_24_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.23.attn.in_proj_weight",
      "transformer.resblocks.23.attn.out_proj.weight",
      "transformer.resblocks.23.mlp.c_fc.weight",
      "transformer.resblocks.23.mlp.c_proj.weight"
    ],
    "lr_scale": 0.65
  },
  "layer_24_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.23.attn.in_proj_bias",
      "transformer.resblocks.23.attn.out_proj.bias",
      "transformer.resblocks.23.ln_1.weight",
      "transformer.resblocks.23.ln_1.bias",
      "transformer.resblocks.23.mlp.c_fc.bias",
      "transformer.resblocks.23.mlp.c_proj.bias",
      "transformer.resblocks.23.ln_2.weight",
      "transformer.resblocks.23.ln_2.bias"
    ],
    "lr_scale": 0.65
  },
  "layer_25_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "fc_norm.weight",
      "fc_norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  },
  "layer_25_decay": {
    "weight_decay": 0.05,
    "params": [
      "head.weight"
    ],
    "lr_scale": 1.0
  }
}
Installed CUDA version 11.0 does not match the version torch was compiled with 11.3 but since the APIs are compatible, accepting this combination
Using /home/v-hezhenhu/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 1.1082875728607178 seconds
Using /home/v-hezhenhu/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 1.1060898303985596 seconds
model.gradient_accumulation_steps() = 4
Use step level LR scheduler!
Set warmup steps = 3335
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
criterion = LabelSmoothingCrossEntropy()
latest_ckpt: -1
Start training for 30 epochs
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Epoch: [0]  [   0/2669]  eta: 1 day, 7:13:41  lr: 0.000000  min_lr: 0.000000  loss: 6.9062 (6.9062)  class_acc: 0.0000 (0.0000)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 42.1211  data: 33.8722  max mem: 26206
Epoch: [0]  [ 100/2669]  eta: 0:54:29  lr: 0.000004  min_lr: 0.000000  loss: 6.9062 (6.9062)  class_acc: 0.0000 (0.0079)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8400  data: 0.0002  max mem: 26773
Epoch: [0]  [ 200/2669]  eta: 0:43:36  lr: 0.000009  min_lr: 0.000000  loss: 6.8984 (6.9043)  class_acc: 0.0500 (0.0294)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8496  data: 0.0002  max mem: 26773
Epoch: [0]  [ 300/2669]  eta: 0:39:02  lr: 0.000013  min_lr: 0.000000  loss: 6.8789 (6.8987)  class_acc: 0.0000 (0.0395)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8460  data: 0.0002  max mem: 26773
Epoch: [0]  [ 400/2669]  eta: 0:36:04  lr: 0.000018  min_lr: 0.000000  loss: 6.8516 (6.8907)  class_acc: 0.0000 (0.0324)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8466  data: 0.0002  max mem: 26773
Epoch: [0]  [ 500/2669]  eta: 0:33:44  lr: 0.000022  min_lr: 0.000000  loss: 6.6797 (6.8709)  class_acc: 0.0000 (0.0269)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8512  data: 0.0002  max mem: 26773
Epoch: [0]  [ 600/2669]  eta: 0:31:42  lr: 0.000027  min_lr: 0.000000  loss: 6.5117 (6.8222)  class_acc: 0.0000 (0.0230)  loss_scale: 256.0000 (146.3161)  weight_decay: 0.0500 (0.0500)  time: 0.8506  data: 0.0002  max mem: 26773
Epoch: [0]  [ 700/2669]  eta: 0:29:51  lr: 0.000031  min_lr: 0.000000  loss: 6.3477 (6.7647)  class_acc: 0.0000 (0.0211)  loss_scale: 256.0000 (161.9629)  weight_decay: 0.0500 (0.0500)  time: 0.8653  data: 0.0002  max mem: 26773
Epoch: [0]  [ 800/2669]  eta: 0:28:07  lr: 0.000036  min_lr: 0.000000  loss: 6.1055 (6.6940)  class_acc: 0.0000 (0.0199)  loss_scale: 256.0000 (173.7029)  weight_decay: 0.0500 (0.0500)  time: 0.8483  data: 0.0002  max mem: 26773
Epoch: [0]  [ 900/2669]  eta: 0:26:26  lr: 0.000040  min_lr: 0.000000  loss: 6.0508 (6.6235)  class_acc: 0.0000 (0.0196)  loss_scale: 256.0000 (182.8368)  weight_decay: 0.0500 (0.0500)  time: 0.8540  data: 0.0002  max mem: 26773
Epoch: [0]  [1000/2669]  eta: 0:24:49  lr: 0.000045  min_lr: 0.000000  loss: 5.9258 (6.5655)  class_acc: 0.0000 (0.0186)  loss_scale: 256.0000 (190.1459)  weight_decay: 0.0500 (0.0500)  time: 0.8520  data: 0.0002  max mem: 26773
Epoch: [0]  [1100/2669]  eta: 0:23:14  lr: 0.000049  min_lr: 0.000000  loss: 5.8125 (6.5051)  class_acc: 0.0000 (0.0193)  loss_scale: 512.0000 (213.3333)  weight_decay: 0.0500 (0.0500)  time: 0.8495  data: 0.0002  max mem: 26773
Epoch: [0]  [1200/2669]  eta: 0:21:40  lr: 0.000054  min_lr: 0.000000  loss: 5.7227 (6.4408)  class_acc: 0.0000 (0.0202)  loss_scale: 512.0000 (238.2015)  weight_decay: 0.0500 (0.0500)  time: 0.8493  data: 0.0002  max mem: 26773
Epoch: [0]  [1300/2669]  eta: 0:20:08  lr: 0.000058  min_lr: 0.000000  loss: 5.5117 (6.3755)  class_acc: 0.0500 (0.0226)  loss_scale: 512.0000 (259.2467)  weight_decay: 0.0500 (0.0500)  time: 0.8515  data: 0.0002  max mem: 26773
Epoch: [0]  [1400/2669]  eta: 0:18:37  lr: 0.000063  min_lr: 0.000000  loss: 5.2734 (6.3041)  class_acc: 0.0500 (0.0256)  loss_scale: 512.0000 (277.2877)  weight_decay: 0.0500 (0.0500)  time: 0.8520  data: 0.0002  max mem: 26773
Epoch: [0]  [1500/2669]  eta: 0:17:07  lr: 0.000067  min_lr: 0.000000  loss: 5.0586 (6.2304)  class_acc: 0.1000 (0.0296)  loss_scale: 512.0000 (292.9247)  weight_decay: 0.0500 (0.0500)  time: 0.8526  data: 0.0002  max mem: 26773
Epoch: [0]  [1600/2669]  eta: 0:15:37  lr: 0.000072  min_lr: 0.000000  loss: 4.9141 (6.1531)  class_acc: 0.1500 (0.0362)  loss_scale: 1024.0000 (326.4360)  weight_decay: 0.0500 (0.0500)  time: 0.8531  data: 0.0002  max mem: 26773
Epoch: [0]  [1700/2669]  eta: 0:14:08  lr: 0.000076  min_lr: 0.000000  loss: 4.4688 (6.0671)  class_acc: 0.2500 (0.0461)  loss_scale: 1024.0000 (367.4450)  weight_decay: 0.0500 (0.0500)  time: 0.8513  data: 0.0002  max mem: 26773
Epoch: [0]  [1800/2669]  eta: 0:12:39  lr: 0.000081  min_lr: 0.000000  loss: 4.3320 (5.9789)  class_acc: 0.3000 (0.0588)  loss_scale: 1024.0000 (403.9001)  weight_decay: 0.0500 (0.0500)  time: 0.8517  data: 0.0002  max mem: 26773
Epoch: [0]  [1900/2669]  eta: 0:11:11  lr: 0.000085  min_lr: 0.000000  loss: 4.0508 (5.8842)  class_acc: 0.3500 (0.0727)  loss_scale: 1024.0000 (436.5197)  weight_decay: 0.0500 (0.0500)  time: 0.8507  data: 0.0002  max mem: 26773
Epoch: [0]  [2000/2669]  eta: 0:09:43  lr: 0.000090  min_lr: 0.000000  loss: 3.7148 (5.7828)  class_acc: 0.4000 (0.0892)  loss_scale: 1024.0000 (465.8791)  weight_decay: 0.0500 (0.0500)  time: 0.8521  data: 0.0002  max mem: 26773
Epoch: [0]  [2100/2669]  eta: 0:08:15  lr: 0.000094  min_lr: 0.000000  loss: 3.4531 (5.6773)  class_acc: 0.4500 (0.1063)  loss_scale: 2048.0000 (516.8129)  weight_decay: 0.0500 (0.0500)  time: 0.8529  data: 0.0002  max mem: 26773
Epoch: [0]  [2200/2669]  eta: 0:06:48  lr: 0.000099  min_lr: 0.000000  loss: 3.0508 (5.5693)  class_acc: 0.5500 (0.1237)  loss_scale: 2048.0000 (586.3807)  weight_decay: 0.0500 (0.0500)  time: 0.8517  data: 0.0002  max mem: 26773
Epoch: [0]  [2300/2669]  eta: 0:05:20  lr: 0.000103  min_lr: 0.000000  loss: 3.1270 (5.4648)  class_acc: 0.5500 (0.1407)  loss_scale: 2048.0000 (649.9018)  weight_decay: 0.0500 (0.0500)  time: 0.8497  data: 0.0002  max mem: 26773
Epoch: [0]  [2400/2669]  eta: 0:03:53  lr: 0.000108  min_lr: 0.000000  loss: 2.7363 (5.3595)  class_acc: 0.5500 (0.1577)  loss_scale: 2048.0000 (708.1316)  weight_decay: 0.0500 (0.0500)  time: 0.8492  data: 0.0002  max mem: 26773
Epoch: [0]  [2500/2669]  eta: 0:02:26  lr: 0.000112  min_lr: 0.000000  loss: 2.9062 (5.2601)  class_acc: 0.5500 (0.1743)  loss_scale: 2048.0000 (761.7049)  weight_decay: 0.0500 (0.0500)  time: 0.8519  data: 0.0002  max mem: 26773
Epoch: [0]  [2600/2669]  eta: 0:00:59  lr: 0.000117  min_lr: 0.000000  loss: 2.7754 (5.1638)  class_acc: 0.6000 (0.1900)  loss_scale: 4096.0000 (841.0796)  weight_decay: 0.0500 (0.0500)  time: 0.8540  data: 0.0002  max mem: 26773
Epoch: [0]  [2668/2669]  eta: 0:00:00  lr: 0.000120  min_lr: 0.000000  loss: 2.5801 (5.1009)  class_acc: 0.6000 (0.2006)  loss_scale: 4096.0000 (922.8186)  weight_decay: 0.0500 (0.0500)  time: 0.8193  data: 0.0007  max mem: 26773
Epoch: [0] Total time: 0:38:34 (0.8672 s / it)
Averaged stats: lr: 0.000120  min_lr: 0.000000  loss: 2.5801 (5.0976)  class_acc: 0.6000 (0.2019)  loss_scale: 4096.0000 (922.8186)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Test:  [  0/105]  eta: 0:07:28  loss: 1.1756 (1.1756)  acc1: 80.0000 (80.0000)  acc5: 95.0000 (95.0000)  time: 4.2721  data: 3.9756  max mem: 26773
Test:  [104/105]  eta: 0:00:00  loss: 0.9826 (1.1347)  acc1: 80.0000 (76.5835)  acc5: 95.0000 (95.1056)  time: 0.3893  data: 0.0002  max mem: 26773
Test: Total time: 0:00:37 (0.3561 s / it)
* Acc@1 77.131 Acc@5 95.467 loss 1.116
Accuracy of the network on the 50000 test images: 77.1%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Test:  [  0/105]  eta: 0:05:37  loss: 6.8422 (6.8422)  acc1: 30.0000 (30.0000)  acc5: 75.0000 (75.0000)  time: 3.2135  data: 2.9088  max mem: 26773
Test:  [104/105]  eta: 0:00:00  loss: 6.8441 (6.8463)  acc1: 25.0000 (23.3205)  acc5: 50.0000 (44.4338)  time: 0.2896  data: 0.0002  max mem: 26773
Test: Total time: 0:00:34 (0.3320 s / it)
* Acc@1 24.094 Acc@5 45.056 loss 6.846
EMA Accuracy of the network on the 50000 test images: 24.1%
Max accuracy: 24.09%
{"train_lr": 5.992801439712062e-05, "train_min_lr": 1.2602705984202765e-09, "train_loss": 5.097641547878404, "train_class_acc": 0.20191545253500867, "train_loss_scale": 922.8185907046477, "train_weight_decay": 0.04999999999999853, "test_loss": 6.845828604130518, "test_acc1": 24.094289827255277, "test_acc5": 45.05558221369162, "epoch": 0, "n_parameters": 304532456}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Epoch: [1]  [   0/2669]  eta: 1 day, 5:22:13  lr: 0.000120  min_lr: 0.000000  loss: 2.8066 (2.8066)  class_acc: 0.7000 (0.7000)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  time: 39.6156  data: 25.7414  max mem: 26773
Epoch: [1]  [ 100/2669]  eta: 0:52:42  lr: 0.000125  min_lr: 0.000000  loss: 2.7461 (2.6418)  class_acc: 0.6000 (0.6005)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8473  data: 0.0002  max mem: 26773
Epoch: [1]  [ 200/2669]  eta: 0:42:48  lr: 0.000129  min_lr: 0.000000  loss: 2.2871 (2.6014)  class_acc: 0.7000 (0.6087)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8432  data: 0.0002  max mem: 26773
Epoch: [1]  [ 300/2669]  eta: 0:38:35  lr: 0.000134  min_lr: 0.000000  loss: 2.4219 (2.5933)  class_acc: 0.6000 (0.6115)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8493  data: 0.0002  max mem: 26773
Epoch: [1]  [ 400/2669]  eta: 0:35:45  lr: 0.000138  min_lr: 0.000000  loss: 2.5039 (2.5775)  class_acc: 0.6500 (0.6162)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8507  data: 0.0002  max mem: 26773
Epoch: [1]  [ 500/2669]  eta: 0:33:29  lr: 0.000143  min_lr: 0.000000  loss: 2.5254 (2.5548)  class_acc: 0.6000 (0.6214)  loss_scale: 8192.0000 (4864.5110)  weight_decay: 0.0500 (0.0500)  time: 0.8479  data: 0.0002  max mem: 26773
Epoch: [1]  [ 600/2669]  eta: 0:31:30  lr: 0.000147  min_lr: 0.000000  loss: 2.5059 (2.5400)  class_acc: 0.6000 (0.6250)  loss_scale: 8192.0000 (5418.1697)  weight_decay: 0.0500 (0.0500)  time: 0.8501  data: 0.0002  max mem: 26773
Epoch: [1]  [ 700/2669]  eta: 0:29:41  lr: 0.000152  min_lr: 0.000000  loss: 2.4336 (2.5144)  class_acc: 0.6000 (0.6300)  loss_scale: 8192.0000 (5813.8659)  weight_decay: 0.0500 (0.0500)  time: 0.8491  data: 0.0002  max mem: 26773
Epoch: [1]  [ 800/2669]  eta: 0:27:58  lr: 0.000156  min_lr: 0.000000  loss: 2.2891 (2.5044)  class_acc: 0.6500 (0.6322)  loss_scale: 8192.0000 (6110.7615)  weight_decay: 0.0500 (0.0500)  time: 0.8515  data: 0.0002  max mem: 26773
Epoch: [1]  [ 900/2669]  eta: 0:26:19  lr: 0.000161  min_lr: 0.000000  loss: 2.2148 (2.4917)  class_acc: 0.6500 (0.6346)  loss_scale: 4096.0000 (6150.8191)  weight_decay: 0.0500 (0.0500)  time: 0.8534  data: 0.0002  max mem: 26773
Epoch: [1]  [1000/2669]  eta: 0:24:43  lr: 0.000165  min_lr: 0.000000  loss: 2.3867 (2.4819)  class_acc: 0.6500 (0.6354)  loss_scale: 4096.0000 (5945.5425)  weight_decay: 0.0500 (0.0500)  time: 0.8498  data: 0.0002  max mem: 26773
Epoch: [1]  [1100/2669]  eta: 0:23:08  lr: 0.000170  min_lr: 0.000000  loss: 2.3770 (2.4696)  class_acc: 0.6500 (0.6390)  loss_scale: 4096.0000 (5777.5550)  weight_decay: 0.0500 (0.0500)  time: 0.8491  data: 0.0002  max mem: 26773
Epoch: [1]  [1200/2669]  eta: 0:21:36  lr: 0.000174  min_lr: 0.000000  loss: 2.2305 (2.4610)  class_acc: 0.6500 (0.6403)  loss_scale: 4096.0000 (5637.5420)  weight_decay: 0.0500 (0.0500)  time: 0.8534  data: 0.0002  max mem: 26773
Epoch: [1]  [1300/2669]  eta: 0:20:04  lr: 0.000179  min_lr: 0.000000  loss: 2.2656 (2.4517)  class_acc: 0.7000 (0.6413)  loss_scale: 4096.0000 (5519.0530)  weight_decay: 0.0500 (0.0500)  time: 0.8520  data: 0.0002  max mem: 26773
Epoch: [1]  [1400/2669]  eta: 0:18:34  lr: 0.000183  min_lr: 0.000000  loss: 2.3711 (2.4417)  class_acc: 0.6500 (0.6433)  loss_scale: 8192.0000 (5493.4932)  weight_decay: 0.0500 (0.0500)  time: 0.8519  data: 0.0002  max mem: 26773
Epoch: [1]  [1500/2669]  eta: 0:17:04  lr: 0.000188  min_lr: 0.000000  loss: 2.1113 (2.4305)  class_acc: 0.7000 (0.6465)  loss_scale: 8192.0000 (5673.2738)  weight_decay: 0.0500 (0.0500)  time: 0.8496  data: 0.0002  max mem: 26773
Epoch: [1]  [1600/2669]  eta: 0:15:35  lr: 0.000192  min_lr: 0.000000  loss: 2.2559 (2.4206)  class_acc: 0.7000 (0.6478)  loss_scale: 8192.0000 (5830.5959)  weight_decay: 0.0500 (0.0500)  time: 0.8506  data: 0.0002  max mem: 26773
Epoch: [1]  [1700/2669]  eta: 0:14:06  lr: 0.000197  min_lr: 0.000000  loss: 2.1758 (2.4134)  class_acc: 0.6500 (0.6491)  loss_scale: 8192.0000 (5969.4203)  weight_decay: 0.0500 (0.0500)  time: 0.8531  data: 0.0002  max mem: 26773
Epoch: [1]  [1800/2669]  eta: 0:12:38  lr: 0.000201  min_lr: 0.000000  loss: 2.1152 (2.4020)  class_acc: 0.6500 (0.6508)  loss_scale: 8192.0000 (6092.8284)  weight_decay: 0.0500 (0.0500)  time: 0.8519  data: 0.0002  max mem: 26773
Epoch: [1]  [1900/2669]  eta: 0:11:10  lr: 0.000206  min_lr: 0.000000  loss: 2.3730 (2.3966)  class_acc: 0.6500 (0.6522)  loss_scale: 16384.0000 (6263.5834)  weight_decay: 0.0500 (0.0500)  time: 0.8542  data: 0.0002  max mem: 26773
Epoch: [1]  [2000/2669]  eta: 0:09:42  lr: 0.000210  min_lr: 0.000000  loss: 2.2988 (2.3890)  class_acc: 0.6500 (0.6535)  loss_scale: 16384.0000 (6769.3513)  weight_decay: 0.0500 (0.0500)  time: 0.8524  data: 0.0002  max mem: 26773
Epoch: [1]  [2100/2669]  eta: 0:08:14  lr: 0.000215  min_lr: 0.000000  loss: 2.1270 (2.3837)  class_acc: 0.7500 (0.6551)  loss_scale: 16384.0000 (7226.9738)  weight_decay: 0.0500 (0.0500)  time: 0.8522  data: 0.0002  max mem: 26773
Epoch: [1]  [2200/2669]  eta: 0:06:47  lr: 0.000219  min_lr: 0.000000  loss: 2.1758 (2.3782)  class_acc: 0.7000 (0.6558)  loss_scale: 16384.0000 (7643.0132)  weight_decay: 0.0500 (0.0500)  time: 0.8496  data: 0.0002  max mem: 26773
Epoch: [1]  [2300/2669]  eta: 0:05:20  lr: 0.000224  min_lr: 0.000000  loss: 2.1641 (2.3716)  class_acc: 0.6500 (0.6570)  loss_scale: 16384.0000 (8022.8909)  weight_decay: 0.0500 (0.0500)  time: 0.8515  data: 0.0002  max mem: 26773
Epoch: [1]  [2400/2669]  eta: 0:03:53  lr: 0.000228  min_lr: 0.000000  loss: 2.1855 (2.3668)  class_acc: 0.6500 (0.6579)  loss_scale: 8192.0000 (8309.7110)  weight_decay: 0.0500 (0.0500)  time: 0.8517  data: 0.0002  max mem: 26773
Epoch: [1]  [2500/2669]  eta: 0:02:26  lr: 0.000233  min_lr: 0.000000  loss: 2.2070 (2.3608)  class_acc: 0.6500 (0.6589)  loss_scale: 8192.0000 (8305.0044)  weight_decay: 0.0500 (0.0500)  time: 0.8504  data: 0.0002  max mem: 26773
Epoch: [1]  [2600/2669]  eta: 0:00:59  lr: 0.000237  min_lr: 0.000000  loss: 2.1309 (2.3551)  class_acc: 0.7000 (0.6606)  loss_scale: 8192.0000 (8300.6597)  weight_decay: 0.0500 (0.0500)  time: 0.8560  data: 0.0002  max mem: 26773
Epoch: [1]  [2668/2669]  eta: 0:00:00  lr: 0.000240  min_lr: 0.000000  loss: 2.2656 (2.3528)  class_acc: 0.6500 (0.6613)  loss_scale: 8192.0000 (8297.9310)  weight_decay: 0.0500 (0.0500)  time: 0.8194  data: 0.0008  max mem: 26773
Epoch: [1] Total time: 0:38:32 (0.8664 s / it)
Averaged stats: lr: 0.000240  min_lr: 0.000000  loss: 2.2656 (2.3446)  class_acc: 0.6500 (0.6616)  loss_scale: 8192.0000 (8297.9310)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Test:  [  0/105]  eta: 0:05:54  loss: 0.5125 (0.5125)  acc1: 85.0000 (85.0000)  acc5: 100.0000 (100.0000)  time: 3.3725  data: 3.0715  max mem: 26773
Test:  [104/105]  eta: 0:00:00  loss: 0.5423 (0.6967)  acc1: 85.0000 (83.5893)  acc5: 100.0000 (97.5048)  time: 0.2870  data: 0.0001  max mem: 26773
Test: Total time: 0:00:34 (0.3304 s / it)
* Acc@1 83.727 Acc@5 97.427 loss 0.691
Accuracy of the network on the 50000 test images: 83.7%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Test:  [  0/105]  eta: 0:06:44  loss: 6.5473 (6.5473)  acc1: 65.0000 (65.0000)  acc5: 85.0000 (85.0000)  time: 3.8521  data: 3.5545  max mem: 26773
Test:  [104/105]  eta: 0:00:00  loss: 6.5945 (6.5998)  acc1: 50.0000 (41.8906)  acc5: 80.0000 (67.5144)  time: 0.2893  data: 0.0002  max mem: 26773
Test: Total time: 0:00:35 (0.3346 s / it)
* Acc@1 41.983 Acc@5 68.342 loss 6.598
EMA Accuracy of the network on the 50000 test images: 42.0%
Max accuracy: 41.98%
{"train_lr": 0.00017996400719856034, "train_min_lr": 3.784596391652482e-09, "train_loss": 2.3445790990003044, "train_class_acc": 0.661637943148425, "train_loss_scale": 8297.931034482759, "train_weight_decay": 0.04999999999999853, "test_loss": 6.597813192435673, "test_acc1": 41.98256557901472, "test_acc5": 68.34213051823417, "epoch": 1, "n_parameters": 304532456}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Epoch: [2]  [   0/2669]  eta: 1 day, 3:48:57  lr: 0.000240  min_lr: 0.000000  loss: 2.1270 (2.1270)  class_acc: 0.7000 (0.7000)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 37.5189  data: 36.8675  max mem: 26773
Epoch: [2]  [ 100/2669]  eta: 0:51:53  lr: 0.000245  min_lr: 0.000000  loss: 2.1055 (2.1531)  class_acc: 0.7500 (0.7030)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8485  data: 0.0002  max mem: 26773
Epoch: [2]  [ 200/2669]  eta: 0:42:28  lr: 0.000249  min_lr: 0.000000  loss: 2.3223 (2.1878)  class_acc: 0.6500 (0.6973)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8513  data: 0.0002  max mem: 26773
Epoch: [2]  [ 300/2669]  eta: 0:38:22  lr: 0.000254  min_lr: 0.000000  loss: 2.0977 (2.1816)  class_acc: 0.7000 (0.6995)  loss_scale: 8192.0000 (9389.5017)  weight_decay: 0.0500 (0.0500)  time: 0.8544  data: 0.0002  max mem: 26773
Epoch: [2]  [ 400/2669]  eta: 0:35:37  lr: 0.000258  min_lr: 0.000000  loss: 2.0781 (2.1849)  class_acc: 0.7000 (0.6993)  loss_scale: 8192.0000 (9090.8728)  weight_decay: 0.0500 (0.0500)  time: 0.8519  data: 0.0002  max mem: 26773
Epoch: [2]  [ 500/2669]  eta: 0:33:23  lr: 0.000263  min_lr: 0.000000  loss: 2.1289 (2.2014)  class_acc: 0.7500 (0.6959)  loss_scale: 8192.0000 (8911.4571)  weight_decay: 0.0500 (0.0500)  time: 0.8515  data: 0.0003  max mem: 26773
Epoch: [2]  [ 600/2669]  eta: 0:31:25  lr: 0.000267  min_lr: 0.000000  loss: 2.1094 (2.1900)  class_acc: 0.7000 (0.6968)  loss_scale: 8192.0000 (8791.7471)  weight_decay: 0.0500 (0.0500)  time: 0.8515  data: 0.0002  max mem: 26773
Epoch: [2]  [ 700/2669]  eta: 0:29:38  lr: 0.000272  min_lr: 0.000000  loss: 2.0840 (2.1796)  class_acc: 0.7000 (0.6984)  loss_scale: 8192.0000 (8706.1912)  weight_decay: 0.0500 (0.0500)  time: 0.8537  data: 0.0002  max mem: 26773
Epoch: [2]  [ 800/2669]  eta: 0:27:56  lr: 0.000276  min_lr: 0.000000  loss: 2.1172 (2.1752)  class_acc: 0.7000 (0.6994)  loss_scale: 8192.0000 (8744.2697)  weight_decay: 0.0500 (0.0500)  time: 0.8539  data: 0.0002  max mem: 26773
Epoch: [2]  [ 900/2669]  eta: 0:26:17  lr: 0.000281  min_lr: 0.000000  loss: 2.1836 (2.1770)  class_acc: 0.7000 (0.6992)  loss_scale: 16384.0000 (9592.1865)  weight_decay: 0.0500 (0.0500)  time: 0.8525  data: 0.0002  max mem: 26773
Epoch: [2]  [1000/2669]  eta: 0:24:41  lr: 0.000285  min_lr: 0.000000  loss: 2.2148 (2.1713)  class_acc: 0.7000 (0.7001)  loss_scale: 8192.0000 (9796.0280)  weight_decay: 0.0500 (0.0500)  time: 0.8526  data: 0.0002  max mem: 26773
Epoch: [2]  [1100/2669]  eta: 0:23:07  lr: 0.000290  min_lr: 0.000000  loss: 2.0664 (2.1609)  class_acc: 0.7000 (0.7022)  loss_scale: 8192.0000 (9650.3397)  weight_decay: 0.0500 (0.0500)  time: 0.8512  data: 0.0003  max mem: 26773
Epoch: [2]  [1200/2669]  eta: 0:21:35  lr: 0.000294  min_lr: 0.000000  loss: 1.9854 (2.1602)  class_acc: 0.7000 (0.7019)  loss_scale: 8192.0000 (9528.9126)  weight_decay: 0.0500 (0.0500)  time: 0.8636  data: 0.0002  max mem: 26773
Epoch: [2]  [1300/2669]  eta: 0:20:04  lr: 0.000299  min_lr: 0.000000  loss: 2.0469 (2.1559)  class_acc: 0.7000 (0.7020)  loss_scale: 8192.0000 (9426.1522)  weight_decay: 0.0500 (0.0500)  time: 0.8532  data: 0.0002  max mem: 26773
Epoch: [2]  [1400/2669]  eta: 0:18:33  lr: 0.000303  min_lr: 0.000000  loss: 2.2246 (2.1575)  class_acc: 0.6500 (0.7007)  loss_scale: 8192.0000 (9338.0614)  weight_decay: 0.0500 (0.0500)  time: 0.8485  data: 0.0002  max mem: 26773
Epoch: [2]  [1500/2669]  eta: 0:17:03  lr: 0.000308  min_lr: 0.000000  loss: 2.1367 (2.1595)  class_acc: 0.7000 (0.7004)  loss_scale: 16384.0000 (9490.9314)  weight_decay: 0.0500 (0.0500)  time: 0.8485  data: 0.0002  max mem: 26773
Epoch: [2]  [1600/2669]  eta: 0:15:34  lr: 0.000312  min_lr: 0.000000  loss: 1.9600 (2.1593)  class_acc: 0.7000 (0.7007)  loss_scale: 16384.0000 (9921.4791)  weight_decay: 0.0500 (0.0500)  time: 0.8541  data: 0.0002  max mem: 26773
Epoch: [2]  [1700/2669]  eta: 0:14:06  lr: 0.000317  min_lr: 0.000000  loss: 2.0215 (2.1579)  class_acc: 0.7500 (0.7011)  loss_scale: 8192.0000 (9848.7008)  weight_decay: 0.0500 (0.0500)  time: 0.8526  data: 0.0002  max mem: 26773
Epoch: [2]  [1800/2669]  eta: 0:12:37  lr: 0.000321  min_lr: 0.000000  loss: 2.1172 (2.1564)  class_acc: 0.7000 (0.7021)  loss_scale: 8192.0000 (9756.7129)  weight_decay: 0.0500 (0.0500)  time: 0.8549  data: 0.0002  max mem: 26773
Epoch: [2]  [1900/2669]  eta: 0:11:09  lr: 0.000326  min_lr: 0.000000  loss: 2.1074 (2.1558)  class_acc: 0.7500 (0.7026)  loss_scale: 8192.0000 (9674.4029)  weight_decay: 0.0500 (0.0500)  time: 0.8507  data: 0.0002  max mem: 26773
Epoch: [2]  [2000/2669]  eta: 0:09:41  lr: 0.000330  min_lr: 0.000000  loss: 1.9893 (2.1539)  class_acc: 0.7000 (0.7030)  loss_scale: 8192.0000 (9600.3198)  weight_decay: 0.0500 (0.0500)  time: 0.8501  data: 0.0002  max mem: 26773
Epoch: [2]  [2100/2669]  eta: 0:08:14  lr: 0.000335  min_lr: 0.000000  loss: 2.0625 (2.1483)  class_acc: 0.7500 (0.7044)  loss_scale: 8192.0000 (9533.2889)  weight_decay: 0.0500 (0.0500)  time: 0.8491  data: 0.0002  max mem: 26773
Epoch: [2]  [2200/2669]  eta: 0:06:47  lr: 0.000339  min_lr: 0.000000  loss: 1.9111 (2.1484)  class_acc: 0.7000 (0.7046)  loss_scale: 16384.0000 (9762.6606)  weight_decay: 0.0500 (0.0500)  time: 0.8507  data: 0.0002  max mem: 26773
Epoch: [2]  [2300/2669]  eta: 0:05:20  lr: 0.000344  min_lr: 0.000000  loss: 1.8359 (2.1462)  class_acc: 0.8000 (0.7054)  loss_scale: 16384.0000 (10050.4198)  weight_decay: 0.0500 (0.0500)  time: 0.8502  data: 0.0002  max mem: 26773
Epoch: [2]  [2400/2669]  eta: 0:03:53  lr: 0.000348  min_lr: 0.000000  loss: 2.0684 (2.1432)  class_acc: 0.7500 (0.7063)  loss_scale: 16384.0000 (10314.2091)  weight_decay: 0.0500 (0.0500)  time: 0.8552  data: 0.0002  max mem: 26773
Epoch: [2]  [2500/2669]  eta: 0:02:26  lr: 0.000353  min_lr: 0.000000  loss: 2.0508 (2.1400)  class_acc: 0.7500 (0.7073)  loss_scale: 16384.0000 (10556.9036)  weight_decay: 0.0500 (0.0500)  time: 0.8495  data: 0.0002  max mem: 26773
Epoch: [2]  [2600/2669]  eta: 0:00:59  lr: 0.000357  min_lr: 0.000000  loss: 2.2207 (2.1407)  class_acc: 0.7000 (0.7073)  loss_scale: 16384.0000 (10780.9366)  weight_decay: 0.0500 (0.0500)  time: 0.8520  data: 0.0009  max mem: 26773
Epoch: [2]  [2668/2669]  eta: 0:00:00  lr: 0.000360  min_lr: 0.000000  loss: 2.0957 (2.1398)  class_acc: 0.6500 (0.7076)  loss_scale: 32768.0000 (11093.5892)  weight_decay: 0.0500 (0.0500)  time: 0.8112  data: 0.0008  max mem: 26773
Epoch: [2] Total time: 0:38:29 (0.8652 s / it)
Averaged stats: lr: 0.000360  min_lr: 0.000000  loss: 2.0957 (2.1329)  class_acc: 0.6500 (0.7080)  loss_scale: 32768.0000 (11093.5892)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Test:  [  0/105]  eta: 0:04:23  loss: 0.4757 (0.4757)  acc1: 85.0000 (85.0000)  acc5: 100.0000 (100.0000)  time: 2.5060  data: 2.2095  max mem: 26773
Test:  [104/105]  eta: 0:00:00  loss: 0.5394 (0.6469)  acc1: 85.0000 (85.1248)  acc5: 100.0000 (97.8407)  time: 0.2868  data: 0.0001  max mem: 26773
Test: Total time: 0:00:34 (0.3302 s / it)
* Acc@1 84.937 Acc@5 97.747 loss 0.636
Accuracy of the network on the 50000 test images: 84.9%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Test:  [  0/105]  eta: 0:06:36  loss: 6.0545 (6.0545)  acc1: 85.0000 (85.0000)  acc5: 85.0000 (85.0000)  time: 3.7804  data: 3.4741  max mem: 26773
Test:  [104/105]  eta: 0:00:00  loss: 6.1871 (6.1765)  acc1: 65.0000 (57.9175)  acc5: 85.0000 (83.2534)  time: 0.2894  data: 0.0002  max mem: 26773
Test: Total time: 0:00:35 (0.3348 s / it)
* Acc@1 58.369 Acc@5 83.701 loss 6.172
EMA Accuracy of the network on the 50000 test images: 58.4%
Max accuracy: 58.37%
{"train_lr": 0.0003000000000000003, "train_min_lr": 6.308922184884684e-09, "train_loss": 2.132884119463706, "train_class_acc": 0.7080116305692956, "train_loss_scale": 11093.589205397302, "train_weight_decay": 0.04999999999999853, "test_loss": 6.172017918881916, "test_acc1": 58.36932181701855, "test_acc5": 83.70121561100449, "epoch": 2, "n_parameters": 304532456}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Epoch: [3]  [   0/2669]  eta: 1 day, 5:40:40  lr: 0.000360  min_lr: 0.000000  loss: 1.6504 (1.6504)  class_acc: 0.9000 (0.9000)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 40.0300  data: 34.3243  max mem: 26773
Epoch: [3]  [ 100/2669]  eta: 0:52:46  lr: 0.000365  min_lr: 0.000000  loss: 1.8877 (2.0322)  class_acc: 0.7500 (0.7327)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8473  data: 0.0002  max mem: 26773
Epoch: [3]  [ 200/2669]  eta: 0:42:49  lr: 0.000369  min_lr: 0.000000  loss: 2.1641 (2.0648)  class_acc: 0.7000 (0.7264)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8474  data: 0.0002  max mem: 26773
Epoch: [3]  [ 300/2669]  eta: 0:38:33  lr: 0.000374  min_lr: 0.000000  loss: 1.9834 (2.0679)  class_acc: 0.7000 (0.7254)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8506  data: 0.0002  max mem: 26773
Epoch: [3]  [ 400/2669]  eta: 0:35:43  lr: 0.000378  min_lr: 0.000000  loss: 2.0020 (2.0735)  class_acc: 0.7500 (0.7242)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8497  data: 0.0002  max mem: 26773
Epoch: [3]  [ 500/2669]  eta: 0:33:28  lr: 0.000383  min_lr: 0.000000  loss: 1.8740 (2.0631)  class_acc: 0.7500 (0.7256)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8609  data: 0.0002  max mem: 26773
Epoch: [3]  [ 600/2669]  eta: 0:31:30  lr: 0.000387  min_lr: 0.000000  loss: 2.0332 (2.0600)  class_acc: 0.7500 (0.7252)  loss_scale: 16384.0000 (16820.1797)  weight_decay: 0.0500 (0.0500)  time: 0.8610  data: 0.0002  max mem: 26773
Epoch: [3]  [ 700/2669]  eta: 0:29:42  lr: 0.000392  min_lr: 0.000000  loss: 2.1250 (2.0704)  class_acc: 0.7000 (0.7228)  loss_scale: 16384.0000 (16757.9572)  weight_decay: 0.0500 (0.0500)  time: 0.8541  data: 0.0002  max mem: 26773
Epoch: [3]  [ 800/2669]  eta: 0:27:59  lr: 0.000396  min_lr: 0.000000  loss: 1.9414 (2.0665)  class_acc: 0.7500 (0.7232)  loss_scale: 8192.0000 (15831.7303)  weight_decay: 0.0500 (0.0500)  time: 0.8508  data: 0.0002  max mem: 26773
Epoch: [3]  [ 900/2669]  eta: 0:26:20  lr: 0.000401  min_lr: 0.000000  loss: 2.0410 (2.0662)  class_acc: 0.7500 (0.7233)  loss_scale: 8192.0000 (14983.8135)  weight_decay: 0.0500 (0.0500)  time: 0.8485  data: 0.0002  max mem: 26773
Epoch: [3]  [1000/2669]  eta: 0:24:43  lr: 0.000405  min_lr: 0.000000  loss: 2.0137 (2.0668)  class_acc: 0.7000 (0.7233)  loss_scale: 8192.0000 (14305.3107)  weight_decay: 0.0500 (0.0500)  time: 0.8479  data: 0.0002  max mem: 26773
Epoch: [3]  [1100/2669]  eta: 0:23:09  lr: 0.000410  min_lr: 0.000000  loss: 2.1309 (2.0712)  class_acc: 0.7500 (0.7223)  loss_scale: 8192.0000 (13750.0599)  weight_decay: 0.0500 (0.0500)  time: 0.8497  data: 0.0002  max mem: 26773
Epoch: [3]  [1200/2669]  eta: 0:21:36  lr: 0.000414  min_lr: 0.000000  loss: 2.0371 (2.0621)  class_acc: 0.7000 (0.7233)  loss_scale: 8192.0000 (13287.2739)  weight_decay: 0.0500 (0.0500)  time: 0.8489  data: 0.0002  max mem: 26773
Epoch: [3]  [1300/2669]  eta: 0:20:04  lr: 0.000419  min_lr: 0.000000  loss: 1.9111 (2.0629)  class_acc: 0.7500 (0.7232)  loss_scale: 8192.0000 (12996.3782)  weight_decay: 0.0500 (0.0500)  time: 0.8503  data: 0.0002  max mem: 26773
Epoch: [3]  [1400/2669]  eta: 0:18:34  lr: 0.000423  min_lr: 0.000000  loss: 2.0332 (2.0643)  class_acc: 0.7500 (0.7231)  loss_scale: 8192.0000 (12653.4532)  weight_decay: 0.0500 (0.0500)  time: 0.8532  data: 0.0002  max mem: 26773
Epoch: [3]  [1500/2669]  eta: 0:17:04  lr: 0.000428  min_lr: 0.000000  loss: 2.3457 (2.0665)  class_acc: 0.6500 (0.7225)  loss_scale: 8192.0000 (12356.2212)  weight_decay: 0.0500 (0.0500)  time: 0.8493  data: 0.0002  max mem: 26773
Epoch: [3]  [1600/2669]  eta: 0:15:34  lr: 0.000432  min_lr: 0.000000  loss: 1.9102 (2.0625)  class_acc: 0.7500 (0.7235)  loss_scale: 8192.0000 (12096.1199)  weight_decay: 0.0500 (0.0500)  time: 0.8476  data: 0.0003  max mem: 26773
Epoch: [3]  [1700/2669]  eta: 0:14:05  lr: 0.000437  min_lr: 0.000000  loss: 1.9062 (2.0563)  class_acc: 0.7500 (0.7250)  loss_scale: 8192.0000 (11866.6008)  weight_decay: 0.0500 (0.0500)  time: 0.8496  data: 0.0002  max mem: 26773
Epoch: [3]  [1800/2669]  eta: 0:12:37  lr: 0.000441  min_lr: 0.000000  loss: 1.8428 (2.0555)  class_acc: 0.7500 (0.7261)  loss_scale: 16384.0000 (11835.4159)  weight_decay: 0.0500 (0.0500)  time: 0.8520  data: 0.0002  max mem: 26773
Epoch: [3]  [1900/2669]  eta: 0:11:09  lr: 0.000446  min_lr: 0.000000  loss: 2.0293 (2.0534)  class_acc: 0.7000 (0.7266)  loss_scale: 16384.0000 (12074.6891)  weight_decay: 0.0500 (0.0500)  time: 0.8502  data: 0.0002  max mem: 26773
Epoch: [3]  [2000/2669]  eta: 0:09:41  lr: 0.000450  min_lr: 0.000000  loss: 2.0664 (2.0550)  class_acc: 0.7000 (0.7259)  loss_scale: 16384.0000 (12290.0470)  weight_decay: 0.0500 (0.0500)  time: 0.8521  data: 0.0002  max mem: 26773
Epoch: [3]  [2100/2669]  eta: 0:08:14  lr: 0.000455  min_lr: 0.000000  loss: 2.1523 (2.0542)  class_acc: 0.7000 (0.7259)  loss_scale: 16384.0000 (12484.9043)  weight_decay: 0.0500 (0.0500)  time: 0.8490  data: 0.0002  max mem: 26773
Epoch: [3]  [2200/2669]  eta: 0:06:47  lr: 0.000459  min_lr: 0.000000  loss: 2.0273 (2.0536)  class_acc: 0.7000 (0.7259)  loss_scale: 16384.0000 (12662.0554)  weight_decay: 0.0500 (0.0500)  time: 0.8667  data: 0.0002  max mem: 26773
Epoch: [3]  [2300/2669]  eta: 0:05:20  lr: 0.000464  min_lr: 0.000000  loss: 2.0312 (2.0510)  class_acc: 0.7000 (0.7265)  loss_scale: 32768.0000 (12966.2164)  weight_decay: 0.0500 (0.0500)  time: 0.8481  data: 0.0002  max mem: 26773
Epoch: [3]  [2400/2669]  eta: 0:03:53  lr: 0.000468  min_lr: 0.000000  loss: 1.8135 (2.0496)  class_acc: 0.7000 (0.7268)  loss_scale: 16384.0000 (13108.5648)  weight_decay: 0.0500 (0.0500)  time: 0.8669  data: 0.0002  max mem: 26773
Epoch: [3]  [2500/2669]  eta: 0:02:26  lr: 0.000473  min_lr: 0.000000  loss: 1.9883 (2.0499)  class_acc: 0.7000 (0.7272)  loss_scale: 16384.0000 (13239.5298)  weight_decay: 0.0500 (0.0500)  time: 0.8512  data: 0.0002  max mem: 26773
Epoch: [3]  [2600/2669]  eta: 0:00:59  lr: 0.000477  min_lr: 0.000000  loss: 2.0625 (2.0514)  class_acc: 0.7000 (0.7267)  loss_scale: 16384.0000 (13360.4245)  weight_decay: 0.0500 (0.0500)  time: 0.8537  data: 0.0002  max mem: 26773
Epoch: [3]  [2668/2669]  eta: 0:00:00  lr: 0.000480  min_lr: 0.000000  loss: 2.1973 (2.0527)  class_acc: 0.7000 (0.7263)  loss_scale: 16384.0000 (13436.3538)  weight_decay: 0.0500 (0.0500)  time: 0.8175  data: 0.0015  max mem: 26773
Epoch: [3] Total time: 0:38:30 (0.8656 s / it)
Averaged stats: lr: 0.000480  min_lr: 0.000000  loss: 2.1973 (2.0448)  class_acc: 0.7000 (0.7285)  loss_scale: 16384.0000 (13436.3538)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Test:  [  0/105]  eta: 0:07:55  loss: 0.5302 (0.5302)  acc1: 85.0000 (85.0000)  acc5: 100.0000 (100.0000)  time: 4.5332  data: 4.2368  max mem: 26773
Test:  [104/105]  eta: 0:00:00  loss: 0.4998 (0.6213)  acc1: 85.0000 (85.3167)  acc5: 100.0000 (97.6008)  time: 0.2871  data: 0.0002  max mem: 26773
Test: Total time: 0:00:35 (0.3389 s / it)
* Acc@1 85.449 Acc@5 97.907 loss 0.611
Accuracy of the network on the 50000 test images: 85.4%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Test:  [  0/105]  eta: 0:04:40  loss: 5.3559 (5.3559)  acc1: 85.0000 (85.0000)  acc5: 100.0000 (100.0000)  time: 2.6677  data: 2.3617  max mem: 26773
Test:  [104/105]  eta: 0:00:00  loss: 5.5480 (5.5167)  acc1: 75.0000 (71.0653)  acc5: 95.0000 (91.3628)  time: 0.2895  data: 0.0002  max mem: 26773
Test: Total time: 0:00:35 (0.3395 s / it)
* Acc@1 71.329 Acc@5 92.610 loss 5.509
EMA Accuracy of the network on the 50000 test images: 71.3%
Max accuracy: 71.33%
{"train_lr": 0.00042003599280143966, "train_min_lr": 8.83324797811688e-09, "train_loss": 2.0448239167769633, "train_class_acc": 0.7284803023140157, "train_loss_scale": 13436.353823088455, "train_weight_decay": 0.04999999999999853, "test_loss": 5.508752324846056, "test_acc1": 71.32917466410748, "test_acc5": 92.61036468330134, "epoch": 3, "n_parameters": 304532456}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Epoch: [4]  [   0/2669]  eta: 1 day, 4:06:58  lr: 0.000480  min_lr: 0.000000  loss: 1.6387 (1.6387)  class_acc: 0.8000 (0.8000)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 37.9238  data: 35.6053  max mem: 26773
Epoch: [4]  [ 100/2669]  eta: 0:52:28  lr: 0.000485  min_lr: 0.000000  loss: 1.9980 (2.0355)  class_acc: 0.7500 (0.7292)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8593  data: 0.0002  max mem: 26773
Epoch: [4]  [ 200/2669]  eta: 0:42:40  lr: 0.000489  min_lr: 0.000000  loss: 2.0469 (2.0237)  class_acc: 0.7000 (0.7323)  loss_scale: 16384.0000 (18666.3483)  weight_decay: 0.0500 (0.0500)  time: 0.8471  data: 0.0002  max mem: 26773
Epoch: [4]  [ 300/2669]  eta: 0:38:28  lr: 0.000494  min_lr: 0.000000  loss: 1.9668 (2.0275)  class_acc: 0.7000 (0.7331)  loss_scale: 16384.0000 (17908.0930)  weight_decay: 0.0500 (0.0500)  time: 0.8504  data: 0.0002  max mem: 26773
Epoch: [4]  [ 400/2669]  eta: 0:35:40  lr: 0.000498  min_lr: 0.000000  loss: 2.1426 (2.0069)  class_acc: 0.7000 (0.7385)  loss_scale: 16384.0000 (17528.0200)  weight_decay: 0.0500 (0.0500)  time: 0.8507  data: 0.0002  max mem: 26773
Epoch: [4]  [ 500/2669]  eta: 0:33:25  lr: 0.000503  min_lr: 0.000000  loss: 1.9756 (1.9989)  class_acc: 0.7000 (0.7403)  loss_scale: 16384.0000 (17299.6727)  weight_decay: 0.0500 (0.0500)  time: 0.8513  data: 0.0002  max mem: 26773
Epoch: [4]  [ 600/2669]  eta: 0:31:26  lr: 0.000507  min_lr: 0.000000  loss: 1.9570 (1.9916)  class_acc: 0.7500 (0.7413)  loss_scale: 16384.0000 (17147.3145)  weight_decay: 0.0500 (0.0500)  time: 0.8478  data: 0.0002  max mem: 26773
Epoch: [4]  [ 700/2669]  eta: 0:29:38  lr: 0.000512  min_lr: 0.000000  loss: 1.9951 (1.9984)  class_acc: 0.7500 (0.7396)  loss_scale: 32768.0000 (17365.6377)  weight_decay: 0.0500 (0.0500)  time: 0.8501  data: 0.0002  max mem: 26773
Epoch: [4]  [ 800/2669]  eta: 0:27:55  lr: 0.000516  min_lr: 0.000000  loss: 1.9150 (1.9928)  class_acc: 0.7500 (0.7413)  loss_scale: 16384.0000 (17774.9014)  weight_decay: 0.0500 (0.0500)  time: 0.8487  data: 0.0002  max mem: 26773
Epoch: [4]  [ 900/2669]  eta: 0:26:17  lr: 0.000521  min_lr: 0.000000  loss: 2.1191 (1.9926)  class_acc: 0.7000 (0.7412)  loss_scale: 16384.0000 (17620.5283)  weight_decay: 0.0500 (0.0500)  time: 0.8512  data: 0.0002  max mem: 26773
Epoch: [4]  [1000/2669]  eta: 0:24:41  lr: 0.000525  min_lr: 0.000000  loss: 1.9629 (1.9932)  class_acc: 0.7500 (0.7425)  loss_scale: 16384.0000 (17496.9990)  weight_decay: 0.0500 (0.0500)  time: 0.8491  data: 0.0002  max mem: 26773
Epoch: [4]  [1100/2669]  eta: 0:23:07  lr: 0.000530  min_lr: 0.000000  loss: 1.9717 (1.9876)  class_acc: 0.7500 (0.7439)  loss_scale: 16384.0000 (17395.9092)  weight_decay: 0.0500 (0.0500)  time: 0.8478  data: 0.0002  max mem: 26773
Epoch: [4]  [1200/2669]  eta: 0:21:34  lr: 0.000534  min_lr: 0.000000  loss: 1.8867 (1.9906)  class_acc: 0.7500 (0.7431)  loss_scale: 16384.0000 (17311.6536)  weight_decay: 0.0500 (0.0500)  time: 0.8515  data: 0.0002  max mem: 26773
Epoch: [4]  [1300/2669]  eta: 0:20:03  lr: 0.000539  min_lr: 0.000000  loss: 1.8525 (1.9888)  class_acc: 0.7500 (0.7434)  loss_scale: 32768.0000 (17970.7671)  weight_decay: 0.0500 (0.0500)  time: 0.8493  data: 0.0002  max mem: 26773
Epoch: [4]  [1400/2669]  eta: 0:18:32  lr: 0.000543  min_lr: 0.000000  loss: 2.0273 (1.9865)  class_acc: 0.7500 (0.7436)  loss_scale: 16384.0000 (18535.7887)  weight_decay: 0.0500 (0.0500)  time: 0.8492  data: 0.0002  max mem: 26773
Epoch: [4]  [1500/2669]  eta: 0:17:02  lr: 0.000548  min_lr: 0.000000  loss: 2.0195 (1.9885)  class_acc: 0.7500 (0.7435)  loss_scale: 16384.0000 (18392.4317)  weight_decay: 0.0500 (0.0500)  time: 0.8478  data: 0.0002  max mem: 26773
Epoch: [4]  [1600/2669]  eta: 0:15:34  lr: 0.000552  min_lr: 0.000000  loss: 1.8965 (1.9863)  class_acc: 0.7000 (0.7440)  loss_scale: 16384.0000 (18266.9831)  weight_decay: 0.0500 (0.0500)  time: 0.8623  data: 0.0002  max mem: 26773
Epoch: [4]  [1700/2669]  eta: 0:14:05  lr: 0.000557  min_lr: 0.000000  loss: 1.8125 (1.9832)  class_acc: 0.7500 (0.7443)  loss_scale: 16384.0000 (18156.2845)  weight_decay: 0.0500 (0.0500)  time: 0.8490  data: 0.0002  max mem: 26773
Epoch: [4]  [1800/2669]  eta: 0:12:37  lr: 0.000561  min_lr: 0.000000  loss: 2.0879 (1.9826)  class_acc: 0.7000 (0.7448)  loss_scale: 16384.0000 (18057.8790)  weight_decay: 0.0500 (0.0500)  time: 0.8519  data: 0.0002  max mem: 26773
Epoch: [4]  [1900/2669]  eta: 0:11:09  lr: 0.000566  min_lr: 0.000000  loss: 2.0645 (1.9831)  class_acc: 0.7000 (0.7445)  loss_scale: 32768.0000 (18193.9106)  weight_decay: 0.0500 (0.0500)  time: 0.8505  data: 0.0002  max mem: 26773
Epoch: [4]  [2000/2669]  eta: 0:09:41  lr: 0.000570  min_lr: 0.000000  loss: 1.9365 (1.9830)  class_acc: 0.7000 (0.7441)  loss_scale: 8192.0000 (17866.0110)  weight_decay: 0.0500 (0.0500)  time: 0.8510  data: 0.0002  max mem: 26773
Epoch: [4]  [2100/2669]  eta: 0:08:14  lr: 0.000575  min_lr: 0.000000  loss: 1.8428 (1.9831)  class_acc: 0.8000 (0.7439)  loss_scale: 4096.0000 (17276.8929)  weight_decay: 0.0500 (0.0500)  time: 0.8471  data: 0.0002  max mem: 26773
Epoch: [4]  [2200/2669]  eta: 0:06:46  lr: 0.000579  min_lr: 0.000000  loss: 1.9990 (1.9843)  class_acc: 0.7500 (0.7434)  loss_scale: 4096.0000 (16678.0336)  weight_decay: 0.0500 (0.0500)  time: 0.8494  data: 0.0002  max mem: 26773
Epoch: [4]  [2300/2669]  eta: 0:05:19  lr: 0.000584  min_lr: 0.000000  loss: 1.8555 (1.9862)  class_acc: 0.8000 (0.7434)  loss_scale: 4096.0000 (16131.2264)  weight_decay: 0.0500 (0.0500)  time: 0.8521  data: 0.0002  max mem: 26773
Epoch: [4]  [2400/2669]  eta: 0:03:52  lr: 0.000588  min_lr: 0.000000  loss: 1.9180 (1.9859)  class_acc: 0.7500 (0.7431)  loss_scale: 4096.0000 (15629.9675)  weight_decay: 0.0500 (0.0500)  time: 0.8461  data: 0.0002  max mem: 26773
Epoch: [4]  [2500/2669]  eta: 0:02:26  lr: 0.000593  min_lr: 0.000000  loss: 1.9512 (1.9875)  class_acc: 0.7500 (0.7425)  loss_scale: 4096.0000 (15168.7933)  weight_decay: 0.0500 (0.0500)  time: 0.8479  data: 0.0002  max mem: 26773
Epoch: [4]  [2600/2669]  eta: 0:00:59  lr: 0.000597  min_lr: 0.000000  loss: 1.7891 (1.9891)  class_acc: 0.7000 (0.7419)  loss_scale: 8192.0000 (14821.8193)  weight_decay: 0.0500 (0.0500)  time: 0.8498  data: 0.0002  max mem: 26773
Epoch: [4]  [2668/2669]  eta: 0:00:00  lr: 0.000600  min_lr: 0.000000  loss: 1.8799 (1.9886)  class_acc: 0.7500 (0.7421)  loss_scale: 8192.0000 (14655.3283)  weight_decay: 0.0500 (0.0500)  time: 0.8143  data: 0.0008  max mem: 26773
Epoch: [4] Total time: 0:38:27 (0.8646 s / it)
Averaged stats: lr: 0.000600  min_lr: 0.000000  loss: 1.8799 (1.9827)  class_acc: 0.7500 (0.7438)  loss_scale: 8192.0000 (14655.3283)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Test:  [  0/105]  eta: 0:07:21  loss: 0.3024 (0.3024)  acc1: 95.0000 (95.0000)  acc5: 100.0000 (100.0000)  time: 4.2014  data: 3.8947  max mem: 26773
Test:  [104/105]  eta: 0:00:00  loss: 0.4543 (0.6153)  acc1: 85.0000 (86.0365)  acc5: 100.0000 (98.1286)  time: 0.2869  data: 0.0001  max mem: 26773
Test: Total time: 0:00:35 (0.3356 s / it)
* Acc@1 85.946 Acc@5 98.003 loss 0.600
Accuracy of the network on the 50000 test images: 85.9%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Test:  [  0/105]  eta: 0:07:24  loss: 4.4442 (4.4442)  acc1: 85.0000 (85.0000)  acc5: 100.0000 (100.0000)  time: 4.2340  data: 3.9211  max mem: 26773
Test:  [104/105]  eta: 0:00:00  loss: 4.6657 (4.6003)  acc1: 80.0000 (79.7025)  acc5: 95.0000 (95.7774)  time: 0.2895  data: 0.0001  max mem: 26773
Test: Total time: 0:00:35 (0.3408 s / it)
* Acc@1 79.754 Acc@5 96.133 loss 4.589
EMA Accuracy of the network on the 50000 test images: 79.8%
Max accuracy: 79.75%
{"train_lr": 0.0005400719856028794, "train_min_lr": 1.1357573771349102e-08, "train_loss": 1.9827228414601292, "train_class_acc": 0.743818726956096, "train_loss_scale": 14655.328335832084, "train_weight_decay": 0.04999999999999853, "test_loss": 4.58923816472765, "test_acc1": 79.75447856685861, "test_acc5": 96.13323736404351, "epoch": 4, "n_parameters": 304532456}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Epoch: [5]  [   0/2669]  eta: 1 day, 6:14:05  lr: 0.000600  min_lr: 0.000000  loss: 2.3223 (2.3223)  class_acc: 0.7500 (0.7500)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 40.7815  data: 32.8134  max mem: 26773
Epoch: [5]  [ 100/2669]  eta: 0:53:05  lr: 0.000600  min_lr: 0.000000  loss: 1.8936 (2.0037)  class_acc: 0.7500 (0.7485)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8453  data: 0.0002  max mem: 26773
Epoch: [5]  [ 200/2669]  eta: 0:43:01  lr: 0.000600  min_lr: 0.000000  loss: 1.9375 (1.9810)  class_acc: 0.7500 (0.7502)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8494  data: 0.0002  max mem: 26773
Epoch: [5]  [ 300/2669]  eta: 0:38:40  lr: 0.000600  min_lr: 0.000000  loss: 1.8633 (1.9547)  class_acc: 0.7500 (0.7542)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8477  data: 0.0002  max mem: 26773
Epoch: [5]  [ 400/2669]  eta: 0:35:48  lr: 0.000600  min_lr: 0.000000  loss: 1.7031 (1.9550)  class_acc: 0.7500 (0.7511)  loss_scale: 8192.0000 (8314.5736)  weight_decay: 0.0500 (0.0500)  time: 0.8465  data: 0.0002  max mem: 26773
Epoch: [5]  [ 500/2669]  eta: 0:33:32  lr: 0.000600  min_lr: 0.000000  loss: 1.9287 (1.9525)  class_acc: 0.7500 (0.7504)  loss_scale: 16384.0000 (9925.2375)  weight_decay: 0.0500 (0.0500)  time: 0.8502  data: 0.0002  max mem: 26773
Epoch: [5]  [ 600/2669]  eta: 0:31:34  lr: 0.000600  min_lr: 0.000000  loss: 1.7812 (1.9520)  class_acc: 0.8000 (0.7496)  loss_scale: 16384.0000 (10999.9068)  weight_decay: 0.0500 (0.0500)  time: 0.8472  data: 0.0002  max mem: 26773
Epoch: [5]  [ 700/2669]  eta: 0:29:44  lr: 0.000600  min_lr: 0.000000  loss: 2.0000 (1.9509)  class_acc: 0.7000 (0.7494)  loss_scale: 16384.0000 (11767.9658)  weight_decay: 0.0500 (0.0500)  time: 0.8594  data: 0.0002  max mem: 26773
Epoch: [5]  [ 800/2669]  eta: 0:28:01  lr: 0.000600  min_lr: 0.000000  loss: 1.8926 (1.9499)  class_acc: 0.7500 (0.7498)  loss_scale: 16384.0000 (12344.2497)  weight_decay: 0.0500 (0.0500)  time: 0.8483  data: 0.0002  max mem: 26773
Epoch: [5]  [ 900/2669]  eta: 0:26:21  lr: 0.000600  min_lr: 0.000000  loss: 1.8477 (1.9516)  class_acc: 0.7500 (0.7505)  loss_scale: 16384.0000 (12792.6127)  weight_decay: 0.0500 (0.0500)  time: 0.8483  data: 0.0002  max mem: 26773
Epoch: [5]  [1000/2669]  eta: 0:24:45  lr: 0.000600  min_lr: 0.000000  loss: 1.9424 (1.9487)  class_acc: 0.7500 (0.7514)  loss_scale: 32768.0000 (14657.2148)  weight_decay: 0.0500 (0.0500)  time: 0.8528  data: 0.0002  max mem: 26773
Epoch: [5]  [1100/2669]  eta: 0:23:10  lr: 0.000600  min_lr: 0.000000  loss: 1.9199 (1.9466)  class_acc: 0.7500 (0.7510)  loss_scale: 16384.0000 (14814.0527)  weight_decay: 0.0500 (0.0500)  time: 0.8485  data: 0.0002  max mem: 26773
Epoch: [5]  [1200/2669]  eta: 0:21:37  lr: 0.000600  min_lr: 0.000000  loss: 1.9453 (1.9514)  class_acc: 0.7000 (0.7499)  loss_scale: 16384.0000 (14944.7727)  weight_decay: 0.0500 (0.0500)  time: 0.8498  data: 0.0002  max mem: 26773
Epoch: [5]  [1300/2669]  eta: 0:20:05  lr: 0.000599  min_lr: 0.000000  loss: 1.9082 (1.9524)  class_acc: 0.7000 (0.7483)  loss_scale: 16384.0000 (15055.3974)  weight_decay: 0.0500 (0.0500)  time: 0.8498  data: 0.0002  max mem: 26773
Epoch: [5]  [1400/2669]  eta: 0:18:34  lr: 0.000599  min_lr: 0.000000  loss: 1.7959 (1.9483)  class_acc: 0.8000 (0.7500)  loss_scale: 16384.0000 (15150.2298)  weight_decay: 0.0500 (0.0500)  time: 0.8477  data: 0.0002  max mem: 26773
Epoch: [5]  [1500/2669]  eta: 0:17:04  lr: 0.000599  min_lr: 0.000000  loss: 1.8760 (1.9465)  class_acc: 0.7500 (0.7509)  loss_scale: 16384.0000 (15232.4264)  weight_decay: 0.0500 (0.0500)  time: 0.8481  data: 0.0002  max mem: 26773
Epoch: [5]  [1600/2669]  eta: 0:15:34  lr: 0.000599  min_lr: 0.000000  loss: 1.8018 (1.9446)  class_acc: 0.7500 (0.7516)  loss_scale: 16384.0000 (15959.3054)  weight_decay: 0.0500 (0.0500)  time: 0.8497  data: 0.0002  max mem: 26773
Epoch: [5]  [1700/2669]  eta: 0:14:05  lr: 0.000599  min_lr: 0.000000  loss: 1.7734 (1.9431)  class_acc: 0.7500 (0.7519)  loss_scale: 16384.0000 (15984.2728)  weight_decay: 0.0500 (0.0500)  time: 0.8488  data: 0.0002  max mem: 26773
Epoch: [5]  [1800/2669]  eta: 0:12:37  lr: 0.000599  min_lr: 0.000000  loss: 2.0391 (1.9460)  class_acc: 0.7000 (0.7516)  loss_scale: 16384.0000 (16006.4675)  weight_decay: 0.0500 (0.0500)  time: 0.8488  data: 0.0002  max mem: 26773
Epoch: [5]  [1900/2669]  eta: 0:11:09  lr: 0.000599  min_lr: 0.000000  loss: 1.8145 (1.9433)  class_acc: 0.7500 (0.7520)  loss_scale: 16384.0000 (16026.3272)  weight_decay: 0.0500 (0.0500)  time: 0.8480  data: 0.0003  max mem: 26773
Epoch: [5]  [2000/2669]  eta: 0:09:41  lr: 0.000599  min_lr: 0.000000  loss: 1.8594 (1.9437)  class_acc: 0.7500 (0.7521)  loss_scale: 16384.0000 (16044.2019)  weight_decay: 0.0500 (0.0500)  time: 0.8491  data: 0.0002  max mem: 26773
Epoch: [5]  [2100/2669]  eta: 0:08:14  lr: 0.000599  min_lr: 0.000000  loss: 1.7646 (1.9427)  class_acc: 0.8000 (0.7521)  loss_scale: 16384.0000 (16107.1642)  weight_decay: 0.0500 (0.0500)  time: 0.8476  data: 0.0002  max mem: 26773
Epoch: [5]  [2200/2669]  eta: 0:06:47  lr: 0.000598  min_lr: 0.000000  loss: 1.7568 (1.9424)  class_acc: 0.8000 (0.7523)  loss_scale: 16384.0000 (16164.4053)  weight_decay: 0.0500 (0.0500)  time: 0.8498  data: 0.0002  max mem: 26773
Epoch: [5]  [2300/2669]  eta: 0:05:19  lr: 0.000598  min_lr: 0.000000  loss: 1.6592 (1.9385)  class_acc: 0.8000 (0.7532)  loss_scale: 16384.0000 (16173.9487)  weight_decay: 0.0500 (0.0500)  time: 0.8500  data: 0.0002  max mem: 26773
Epoch: [5]  [2400/2669]  eta: 0:03:53  lr: 0.000598  min_lr: 0.000000  loss: 1.7539 (1.9389)  class_acc: 0.8000 (0.7532)  loss_scale: 16384.0000 (16182.6972)  weight_decay: 0.0500 (0.0500)  time: 0.8492  data: 0.0002  max mem: 26773
Epoch: [5]  [2500/2669]  eta: 0:02:26  lr: 0.000598  min_lr: 0.000000  loss: 1.7432 (1.9361)  class_acc: 0.7500 (0.7536)  loss_scale: 16384.0000 (16190.7461)  weight_decay: 0.0500 (0.0500)  time: 0.8492  data: 0.0002  max mem: 26773
Epoch: [5]  [2600/2669]  eta: 0:00:59  lr: 0.000598  min_lr: 0.000000  loss: 1.6924 (1.9335)  class_acc: 0.7500 (0.7543)  loss_scale: 16384.0000 (16198.1761)  weight_decay: 0.0500 (0.0500)  time: 0.8699  data: 0.0002  max mem: 26773
Epoch: [5]  [2668/2669]  eta: 0:00:00  lr: 0.000598  min_lr: 0.000000  loss: 1.7773 (1.9329)  class_acc: 0.8000 (0.7546)  loss_scale: 16384.0000 (16227.4063)  weight_decay: 0.0500 (0.0500)  time: 0.8243  data: 0.0009  max mem: 26773
Epoch: [5] Total time: 0:38:29 (0.8652 s / it)
Averaged stats: lr: 0.000598  min_lr: 0.000000  loss: 1.7773 (1.9339)  class_acc: 0.8000 (0.7555)  loss_scale: 16384.0000 (16227.4063)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Test:  [  0/105]  eta: 0:05:46  loss: 0.3794 (0.3794)  acc1: 95.0000 (95.0000)  acc5: 100.0000 (100.0000)  time: 3.3045  data: 3.0068  max mem: 26773
Test:  [104/105]  eta: 0:00:00  loss: 0.5284 (0.6081)  acc1: 90.0000 (86.7083)  acc5: 100.0000 (98.1286)  time: 0.2869  data: 0.0001  max mem: 26773
Test: Total time: 0:00:34 (0.3329 s / it)
* Acc@1 86.178 Acc@5 98.123 loss 0.594
Accuracy of the network on the 50000 test images: 86.2%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Test:  [  0/105]  eta: 0:07:28  loss: 3.3627 (3.3627)  acc1: 85.0000 (85.0000)  acc5: 100.0000 (100.0000)  time: 4.2747  data: 3.9645  max mem: 26773
Test:  [104/105]  eta: 0:00:00  loss: 3.5913 (3.5095)  acc1: 85.0000 (83.6372)  acc5: 100.0000 (97.0250)  time: 0.2897  data: 0.0001  max mem: 26773
Test: Total time: 0:00:35 (0.3390 s / it)
* Acc@1 83.275 Acc@5 97.265 loss 3.497
EMA Accuracy of the network on the 50000 test images: 83.3%
Max accuracy: 83.28%
{"train_lr": 0.0005992141392068403, "train_min_lr": 1.2601317921128706e-08, "train_loss": 1.9339296111221733, "train_class_acc": 0.7555222503681747, "train_loss_scale": 16227.406296851574, "train_weight_decay": 0.04999999999999853, "test_loss": 3.4973033333581593, "test_acc1": 83.27535188739603, "test_acc5": 97.26487523992323, "epoch": 5, "n_parameters": 304532456}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Epoch: [6]  [   0/2669]  eta: 1 day, 4:55:50  lr: 0.000598  min_lr: 0.000000  loss: 1.6104 (1.6104)  class_acc: 0.9000 (0.9000)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 39.0224  data: 30.6278  max mem: 26773
Epoch: [6]  [ 100/2669]  eta: 0:52:47  lr: 0.000597  min_lr: 0.000000  loss: 2.0391 (1.8640)  class_acc: 0.7500 (0.7738)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8487  data: 0.0002  max mem: 26773
Epoch: [6]  [ 200/2669]  eta: 0:42:55  lr: 0.000597  min_lr: 0.000000  loss: 1.7861 (1.8938)  class_acc: 0.8000 (0.7667)  loss_scale: 8192.0000 (13857.1144)  weight_decay: 0.0500 (0.0500)  time: 0.8459  data: 0.0002  max mem: 26773
Epoch: [6]  [ 300/2669]  eta: 0:38:38  lr: 0.000597  min_lr: 0.000000  loss: 1.7891 (1.8868)  class_acc: 0.8000 (0.7650)  loss_scale: 8192.0000 (11975.0166)  weight_decay: 0.0500 (0.0500)  time: 0.8485  data: 0.0002  max mem: 26773
Epoch: [6]  [ 400/2669]  eta: 0:35:46  lr: 0.000597  min_lr: 0.000000  loss: 1.9619 (1.8908)  class_acc: 0.7500 (0.7657)  loss_scale: 8192.0000 (11031.6209)  weight_decay: 0.0500 (0.0500)  time: 0.8483  data: 0.0002  max mem: 26773
Epoch: [6]  [ 500/2669]  eta: 0:33:30  lr: 0.000597  min_lr: 0.000000  loss: 1.8359 (1.8887)  class_acc: 0.7500 (0.7686)  loss_scale: 8192.0000 (10464.8303)  weight_decay: 0.0500 (0.0500)  time: 0.8484  data: 0.0002  max mem: 26773
Epoch: [6]  [ 600/2669]  eta: 0:31:31  lr: 0.000596  min_lr: 0.000000  loss: 1.9746 (1.8864)  class_acc: 0.7000 (0.7688)  loss_scale: 8192.0000 (10086.6556)  weight_decay: 0.0500 (0.0500)  time: 0.8469  data: 0.0002  max mem: 26773
Epoch: [6]  [ 700/2669]  eta: 0:29:41  lr: 0.000596  min_lr: 0.000000  loss: 2.0254 (1.8921)  class_acc: 0.7000 (0.7667)  loss_scale: 16384.0000 (10353.9401)  weight_decay: 0.0500 (0.0500)  time: 0.8482  data: 0.0002  max mem: 26773
Epoch: [6]  [ 800/2669]  eta: 0:27:58  lr: 0.000596  min_lr: 0.000000  loss: 1.6963 (1.8786)  class_acc: 0.8000 (0.7687)  loss_scale: 16384.0000 (11106.7566)  weight_decay: 0.0500 (0.0500)  time: 0.8490  data: 0.0002  max mem: 26773
Epoch: [6]  [ 900/2669]  eta: 0:26:18  lr: 0.000596  min_lr: 0.000000  loss: 1.8047 (1.8777)  class_acc: 0.8000 (0.7686)  loss_scale: 16384.0000 (11692.4661)  weight_decay: 0.0500 (0.0500)  time: 0.8491  data: 0.0002  max mem: 26773
Epoch: [6]  [1000/2669]  eta: 0:24:42  lr: 0.000596  min_lr: 0.000000  loss: 1.6973 (1.8771)  class_acc: 0.8000 (0.7688)  loss_scale: 16384.0000 (12161.1508)  weight_decay: 0.0500 (0.0500)  time: 0.8533  data: 0.0002  max mem: 26773
Epoch: [6]  [1100/2669]  eta: 0:23:09  lr: 0.000595  min_lr: 0.000000  loss: 1.9131 (1.8787)  class_acc: 0.7500 (0.7687)  loss_scale: 16384.0000 (12544.6975)  weight_decay: 0.0500 (0.0500)  time: 0.8501  data: 0.0002  max mem: 26773
Epoch: [6]  [1200/2669]  eta: 0:21:36  lr: 0.000595  min_lr: 0.000000  loss: 1.8369 (1.8768)  class_acc: 0.7500 (0.7687)  loss_scale: 32768.0000 (13328.1998)  weight_decay: 0.0500 (0.0500)  time: 0.8600  data: 0.0002  max mem: 26773
Epoch: [6]  [1300/2669]  eta: 0:20:04  lr: 0.000595  min_lr: 0.000000  loss: 1.9600 (1.8777)  class_acc: 0.7500 (0.7684)  loss_scale: 32768.0000 (14822.4197)  weight_decay: 0.0500 (0.0500)  time: 0.8487  data: 0.0002  max mem: 26773
Epoch: [6]  [1400/2669]  eta: 0:18:33  lr: 0.000595  min_lr: 0.000000  loss: 1.6973 (1.8782)  class_acc: 0.8000 (0.7686)  loss_scale: 32768.0000 (16103.3319)  weight_decay: 0.0500 (0.0500)  time: 0.8576  data: 0.0002  max mem: 26773
Epoch: [6]  [1500/2669]  eta: 0:17:04  lr: 0.000594  min_lr: 0.000000  loss: 1.8613 (1.8766)  class_acc: 0.8000 (0.7693)  loss_scale: 16384.0000 (16798.7848)  weight_decay: 0.0500 (0.0500)  time: 0.8606  data: 0.0002  max mem: 26773
Epoch: [6]  [1600/2669]  eta: 0:15:35  lr: 0.000594  min_lr: 0.000000  loss: 1.8965 (1.8767)  class_acc: 0.7500 (0.7697)  loss_scale: 16384.0000 (16772.8770)  weight_decay: 0.0500 (0.0500)  time: 0.8473  data: 0.0002  max mem: 26773
Epoch: [6]  [1700/2669]  eta: 0:14:06  lr: 0.000594  min_lr: 0.000000  loss: 1.8457 (1.8798)  class_acc: 0.7500 (0.7690)  loss_scale: 16384.0000 (16750.0153)  weight_decay: 0.0500 (0.0500)  time: 0.8474  data: 0.0002  max mem: 26773
Epoch: [6]  [1800/2669]  eta: 0:12:37  lr: 0.000593  min_lr: 0.000000  loss: 1.8330 (1.8800)  class_acc: 0.7500 (0.7690)  loss_scale: 16384.0000 (16729.6924)  weight_decay: 0.0500 (0.0500)  time: 0.8525  data: 0.0002  max mem: 26773
Epoch: [6]  [1900/2669]  eta: 0:11:09  lr: 0.000593  min_lr: 0.000000  loss: 1.8398 (1.8780)  class_acc: 0.7500 (0.7692)  loss_scale: 16384.0000 (16711.5076)  weight_decay: 0.0500 (0.0500)  time: 0.8479  data: 0.0002  max mem: 26773
Epoch: [6]  [2000/2669]  eta: 0:09:41  lr: 0.000593  min_lr: 0.000000  loss: 1.8809 (1.8766)  class_acc: 0.7000 (0.7691)  loss_scale: 32768.0000 (16875.2744)  weight_decay: 0.0500 (0.0500)  time: 0.8522  data: 0.0002  max mem: 26773
Epoch: [6]  [2100/2669]  eta: 0:08:14  lr: 0.000592  min_lr: 0.000000  loss: 1.6465 (1.8796)  class_acc: 0.8000 (0.7684)  loss_scale: 16384.0000 (17117.0300)  weight_decay: 0.0500 (0.0500)  time: 0.8474  data: 0.0002  max mem: 26773
Epoch: [6]  [2200/2669]  eta: 0:06:47  lr: 0.000592  min_lr: 0.000000  loss: 1.6602 (1.8777)  class_acc: 0.8000 (0.7694)  loss_scale: 16384.0000 (17083.7256)  weight_decay: 0.0500 (0.0500)  time: 0.8494  data: 0.0002  max mem: 26773
Epoch: [6]  [2300/2669]  eta: 0:05:19  lr: 0.000592  min_lr: 0.000000  loss: 1.8740 (1.8791)  class_acc: 0.7500 (0.7692)  loss_scale: 16384.0000 (17053.3159)  weight_decay: 0.0500 (0.0500)  time: 0.8497  data: 0.0002  max mem: 26773
Epoch: [6]  [2400/2669]  eta: 0:03:53  lr: 0.000592  min_lr: 0.000000  loss: 1.8340 (1.8791)  class_acc: 0.8000 (0.7690)  loss_scale: 16384.0000 (17025.4394)  weight_decay: 0.0500 (0.0500)  time: 0.8473  data: 0.0002  max mem: 26773
Epoch: [6]  [2500/2669]  eta: 0:02:26  lr: 0.000591  min_lr: 0.000000  loss: 1.7393 (1.8774)  class_acc: 0.7500 (0.7692)  loss_scale: 16384.0000 (16999.7921)  weight_decay: 0.0500 (0.0500)  time: 0.8497  data: 0.0002  max mem: 26773
Epoch: [6]  [2600/2669]  eta: 0:00:59  lr: 0.000591  min_lr: 0.000000  loss: 1.9785 (1.8796)  class_acc: 0.7000 (0.7688)  loss_scale: 32768.0000 (17253.2780)  weight_decay: 0.0500 (0.0500)  time: 0.8443  data: 0.0004  max mem: 26773
Epoch: [6]  [2668/2669]  eta: 0:00:00  lr: 0.000591  min_lr: 0.000000  loss: 1.8418 (1.8788)  class_acc: 0.8000 (0.7691)  loss_scale: 16384.0000 (17231.4483)  weight_decay: 0.0500 (0.0500)  time: 0.8185  data: 0.0008  max mem: 26773
Epoch: [6] Total time: 0:38:27 (0.8647 s / it)
Averaged stats: lr: 0.000591  min_lr: 0.000000  loss: 1.8418 (1.8819)  class_acc: 0.8000 (0.7685)  loss_scale: 16384.0000 (17231.4483)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Test:  [  0/105]  eta: 0:06:18  loss: 0.3505 (0.3505)  acc1: 90.0000 (90.0000)  acc5: 100.0000 (100.0000)  time: 3.6034  data: 3.2938  max mem: 26773
Test:  [104/105]  eta: 0:00:00  loss: 0.5072 (0.6068)  acc1: 85.0000 (86.7562)  acc5: 100.0000 (97.6488)  time: 0.2871  data: 0.0002  max mem: 26773
Test: Total time: 0:00:35 (0.3336 s / it)
* Acc@1 86.662 Acc@5 98.165 loss 0.581
Accuracy of the network on the 50000 test images: 86.7%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Test:  [  0/105]  eta: 0:06:10  loss: 2.3135 (2.3135)  acc1: 85.0000 (85.0000)  acc5: 100.0000 (100.0000)  time: 3.5250  data: 3.2116  max mem: 26773
Test:  [104/105]  eta: 0:00:00  loss: 2.5993 (2.4632)  acc1: 90.0000 (85.4127)  acc5: 100.0000 (97.6967)  time: 0.2897  data: 0.0002  max mem: 26773
Test: Total time: 0:00:35 (0.3368 s / it)
* Acc@1 85.119 Acc@5 97.809 loss 2.450
EMA Accuracy of the network on the 50000 test images: 85.1%
Max accuracy: 85.12%
{"train_lr": 0.000594506779488454, "train_min_lr": 1.2502323367263585e-08, "train_loss": 1.8818846497161577, "train_class_acc": 0.7684579703762293, "train_loss_scale": 17231.44827586207, "train_weight_decay": 0.04999999999999853, "test_loss": 2.449886204231353, "test_acc1": 85.11876199616123, "test_acc5": 97.80870121561101, "epoch": 6, "n_parameters": 304532456}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Epoch: [7]  [   0/2669]  eta: 1 day, 5:13:24  lr: 0.000591  min_lr: 0.000000  loss: 1.8779 (1.8779)  class_acc: 0.7000 (0.7000)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 39.4173  data: 33.7176  max mem: 26773
Epoch: [7]  [ 100/2669]  eta: 0:52:38  lr: 0.000590  min_lr: 0.000000  loss: 1.8857 (1.8513)  class_acc: 0.8000 (0.7797)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8455  data: 0.0002  max mem: 26773
Epoch: [7]  [ 200/2669]  eta: 0:42:45  lr: 0.000590  min_lr: 0.000000  loss: 1.8398 (1.8315)  class_acc: 0.7500 (0.7813)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8487  data: 0.0002  max mem: 26773
Epoch: [7]  [ 300/2669]  eta: 0:38:30  lr: 0.000590  min_lr: 0.000000  loss: 1.7148 (1.8242)  class_acc: 0.8000 (0.7872)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8476  data: 0.0002  max mem: 26773
Epoch: [7]  [ 400/2669]  eta: 0:35:41  lr: 0.000589  min_lr: 0.000000  loss: 1.8115 (1.8265)  class_acc: 0.8000 (0.7864)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8509  data: 0.0002  max mem: 26773
Epoch: [7]  [ 500/2669]  eta: 0:33:26  lr: 0.000589  min_lr: 0.000000  loss: 1.8311 (1.8380)  class_acc: 0.8000 (0.7831)  loss_scale: 16384.0000 (17038.0519)  weight_decay: 0.0500 (0.0500)  time: 0.8454  data: 0.0002  max mem: 26773
Epoch: [7]  [ 600/2669]  eta: 0:31:28  lr: 0.000588  min_lr: 0.000000  loss: 1.6768 (1.8400)  class_acc: 0.8000 (0.7829)  loss_scale: 16384.0000 (16929.2246)  weight_decay: 0.0500 (0.0500)  time: 0.8499  data: 0.0003  max mem: 26773
Epoch: [7]  [ 700/2669]  eta: 0:29:40  lr: 0.000588  min_lr: 0.000000  loss: 1.6182 (1.8330)  class_acc: 0.8500 (0.7839)  loss_scale: 16384.0000 (16851.4465)  weight_decay: 0.0500 (0.0500)  time: 0.8502  data: 0.0002  max mem: 26773
Epoch: [7]  [ 800/2669]  eta: 0:27:58  lr: 0.000588  min_lr: 0.000000  loss: 1.8105 (1.8318)  class_acc: 0.7500 (0.7829)  loss_scale: 16384.0000 (16793.0886)  weight_decay: 0.0500 (0.0500)  time: 0.8509  data: 0.0002  max mem: 26773
Epoch: [7]  [ 900/2669]  eta: 0:26:19  lr: 0.000587  min_lr: 0.000000  loss: 1.7432 (1.8365)  class_acc: 0.8500 (0.7820)  loss_scale: 16384.0000 (16747.6848)  weight_decay: 0.0500 (0.0500)  time: 0.8608  data: 0.0002  max mem: 26773
Epoch: [7]  [1000/2669]  eta: 0:24:43  lr: 0.000587  min_lr: 0.000000  loss: 1.7646 (1.8369)  class_acc: 0.8000 (0.7819)  loss_scale: 32768.0000 (17071.4406)  weight_decay: 0.0500 (0.0500)  time: 0.8490  data: 0.0002  max mem: 26773
Epoch: [7]  [1100/2669]  eta: 0:23:09  lr: 0.000586  min_lr: 0.000000  loss: 1.6934 (1.8361)  class_acc: 0.8000 (0.7817)  loss_scale: 16384.0000 (17217.3370)  weight_decay: 0.0500 (0.0500)  time: 0.8625  data: 0.0002  max mem: 26773
Epoch: [7]  [1200/2669]  eta: 0:21:36  lr: 0.000586  min_lr: 0.000000  loss: 1.9424 (1.8432)  class_acc: 0.7500 (0.7794)  loss_scale: 16384.0000 (17147.9500)  weight_decay: 0.0500 (0.0500)  time: 0.8614  data: 0.0002  max mem: 26773
Epoch: [7]  [1300/2669]  eta: 0:20:04  lr: 0.000585  min_lr: 0.000000  loss: 1.7822 (1.8424)  class_acc: 0.8000 (0.7791)  loss_scale: 16384.0000 (17089.2298)  weight_decay: 0.0500 (0.0500)  time: 0.8504  data: 0.0002  max mem: 26773
Epoch: [7]  [1400/2669]  eta: 0:18:33  lr: 0.000585  min_lr: 0.000000  loss: 1.9463 (1.8427)  class_acc: 0.7500 (0.7787)  loss_scale: 16384.0000 (17038.8922)  weight_decay: 0.0500 (0.0500)  time: 0.8488  data: 0.0002  max mem: 26773
Epoch: [7]  [1500/2669]  eta: 0:17:04  lr: 0.000585  min_lr: 0.000000  loss: 1.7139 (1.8392)  class_acc: 0.8000 (0.7791)  loss_scale: 16384.0000 (16995.2618)  weight_decay: 0.0500 (0.0500)  time: 0.8499  data: 0.0002  max mem: 26773
Epoch: [7]  [1600/2669]  eta: 0:15:34  lr: 0.000584  min_lr: 0.000000  loss: 1.7119 (1.8377)  class_acc: 0.8000 (0.7789)  loss_scale: 16384.0000 (16998.0162)  weight_decay: 0.0500 (0.0500)  time: 0.8487  data: 0.0002  max mem: 26773
Epoch: [7]  [1700/2669]  eta: 0:14:05  lr: 0.000584  min_lr: 0.000000  loss: 1.6602 (1.8367)  class_acc: 0.7500 (0.7793)  loss_scale: 16384.0000 (16961.9189)  weight_decay: 0.0500 (0.0500)  time: 0.8506  data: 0.0003  max mem: 26773
Epoch: [7]  [1800/2669]  eta: 0:12:37  lr: 0.000583  min_lr: 0.000000  loss: 1.8809 (1.8345)  class_acc: 0.7500 (0.7803)  loss_scale: 8192.0000 (16556.8462)  weight_decay: 0.0500 (0.0500)  time: 0.8504  data: 0.0002  max mem: 26773
Epoch: [7]  [1900/2669]  eta: 0:11:09  lr: 0.000583  min_lr: 0.000000  loss: 1.7637 (1.8345)  class_acc: 0.8000 (0.7800)  loss_scale: 8192.0000 (16116.8227)  weight_decay: 0.0500 (0.0500)  time: 0.8473  data: 0.0003  max mem: 26773
Epoch: [7]  [2000/2669]  eta: 0:09:41  lr: 0.000582  min_lr: 0.000000  loss: 1.9141 (1.8357)  class_acc: 0.7500 (0.7790)  loss_scale: 8192.0000 (15720.7796)  weight_decay: 0.0500 (0.0500)  time: 0.8481  data: 0.0002  max mem: 26773
Epoch: [7]  [2100/2669]  eta: 0:08:14  lr: 0.000582  min_lr: 0.000000  loss: 1.8428 (1.8350)  class_acc: 0.8000 (0.7789)  loss_scale: 8192.0000 (15362.4369)  weight_decay: 0.0500 (0.0500)  time: 0.8477  data: 0.0002  max mem: 26773
Epoch: [7]  [2200/2669]  eta: 0:06:46  lr: 0.000581  min_lr: 0.000000  loss: 1.9365 (1.8347)  class_acc: 0.7500 (0.7789)  loss_scale: 8192.0000 (15036.6561)  weight_decay: 0.0500 (0.0500)  time: 0.8493  data: 0.0002  max mem: 26773
Epoch: [7]  [2300/2669]  eta: 0:05:19  lr: 0.000581  min_lr: 0.000000  loss: 1.8613 (1.8349)  class_acc: 0.7500 (0.7790)  loss_scale: 16384.0000 (14974.1643)  weight_decay: 0.0500 (0.0500)  time: 0.8485  data: 0.0002  max mem: 26773
Epoch: [7]  [2400/2669]  eta: 0:03:52  lr: 0.000580  min_lr: 0.000000  loss: 1.8789 (1.8383)  class_acc: 0.7000 (0.7782)  loss_scale: 16384.0000 (15032.8830)  weight_decay: 0.0500 (0.0500)  time: 0.8491  data: 0.0002  max mem: 26773
Epoch: [7]  [2500/2669]  eta: 0:02:26  lr: 0.000580  min_lr: 0.000000  loss: 1.7510 (1.8367)  class_acc: 0.8000 (0.7783)  loss_scale: 16384.0000 (15086.9060)  weight_decay: 0.0500 (0.0500)  time: 0.8485  data: 0.0003  max mem: 26773
Epoch: [7]  [2600/2669]  eta: 0:00:59  lr: 0.000579  min_lr: 0.000000  loss: 1.7832 (1.8393)  class_acc: 0.7500 (0.7780)  loss_scale: 16384.0000 (15136.7751)  weight_decay: 0.0500 (0.0500)  time: 0.8481  data: 0.0003  max mem: 26773
Epoch: [7]  [2668/2669]  eta: 0:00:00  lr: 0.000579  min_lr: 0.000000  loss: 1.7383 (1.8380)  class_acc: 0.8000 (0.7779)  loss_scale: 16384.0000 (15168.0960)  weight_decay: 0.0500 (0.0500)  time: 0.8161  data: 0.0011  max mem: 26773
Epoch: [7] Total time: 0:38:28 (0.8650 s / it)
Averaged stats: lr: 0.000579  min_lr: 0.000000  loss: 1.7383 (1.8402)  class_acc: 0.8000 (0.7791)  loss_scale: 16384.0000 (15168.0960)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Test:  [  0/105]  eta: 0:05:07  loss: 0.3093 (0.3093)  acc1: 90.0000 (90.0000)  acc5: 100.0000 (100.0000)  time: 2.9251  data: 2.6258  max mem: 26773
Test:  [104/105]  eta: 0:00:00  loss: 0.5112 (0.5867)  acc1: 90.0000 (86.9962)  acc5: 100.0000 (97.8407)  time: 0.2870  data: 0.0002  max mem: 26773
Test: Total time: 0:00:34 (0.3298 s / it)
* Acc@1 86.922 Acc@5 98.221 loss 0.564
Accuracy of the network on the 50000 test images: 86.9%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Test:  [  0/105]  eta: 0:07:37  loss: 1.5020 (1.5020)  acc1: 85.0000 (85.0000)  acc5: 100.0000 (100.0000)  time: 4.3563  data: 4.0529  max mem: 26773
Test:  [104/105]  eta: 0:00:00  loss: 1.7528 (1.6569)  acc1: 85.0000 (86.2284)  acc5: 100.0000 (97.9367)  time: 0.2895  data: 0.0002  max mem: 26773
Test: Total time: 0:00:35 (0.3403 s / it)
* Acc@1 86.062 Acc@5 98.083 loss 1.645
EMA Accuracy of the network on the 50000 test images: 86.1%
Max accuracy: 86.06%
{"train_lr": 0.0005851627572264001, "train_min_lr": 1.2305821002779736e-08, "train_loss": 1.8401521285255809, "train_class_acc": 0.7791088953086439, "train_loss_scale": 15168.095952023989, "train_weight_decay": 0.04999999999999853, "test_loss": 1.64474756045947, "test_acc1": 86.06246001279591, "test_acc5": 98.08261356365963, "epoch": 7, "n_parameters": 304532456}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Epoch: [8]  [   0/2669]  eta: 1 day, 6:18:38  lr: 0.000579  min_lr: 0.000000  loss: 2.3848 (2.3848)  class_acc: 0.6500 (0.6500)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 40.8836  data: 25.8137  max mem: 26773
Epoch: [8]  [ 100/2669]  eta: 0:53:10  lr: 0.000578  min_lr: 0.000000  loss: 1.7891 (1.8235)  class_acc: 0.7500 (0.7797)  loss_scale: 32768.0000 (19952.7921)  weight_decay: 0.0500 (0.0500)  time: 0.8466  data: 0.0003  max mem: 26773
Epoch: [8]  [ 200/2669]  eta: 0:43:03  lr: 0.000578  min_lr: 0.000000  loss: 1.8770 (1.8198)  class_acc: 0.8000 (0.7831)  loss_scale: 32768.0000 (26328.5174)  weight_decay: 0.0500 (0.0500)  time: 0.8488  data: 0.0003  max mem: 26773
Epoch: [8]  [ 300/2669]  eta: 0:38:41  lr: 0.000577  min_lr: 0.000000  loss: 1.7646 (1.8144)  class_acc: 0.8000 (0.7874)  loss_scale: 16384.0000 (24004.4651)  weight_decay: 0.0500 (0.0500)  time: 0.8475  data: 0.0002  max mem: 26773
Epoch: [8]  [ 400/2669]  eta: 0:35:48  lr: 0.000577  min_lr: 0.000000  loss: 1.7178 (1.8099)  class_acc: 0.8000 (0.7870)  loss_scale: 16384.0000 (22104.0998)  weight_decay: 0.0500 (0.0500)  time: 0.8462  data: 0.0003  max mem: 26773
Epoch: [8]  [ 500/2669]  eta: 0:33:31  lr: 0.000576  min_lr: 0.000000  loss: 1.6367 (1.8016)  class_acc: 0.8500 (0.7908)  loss_scale: 16384.0000 (20962.3633)  weight_decay: 0.0500 (0.0500)  time: 0.8482  data: 0.0002  max mem: 26773
Epoch: [8]  [ 600/2669]  eta: 0:31:32  lr: 0.000576  min_lr: 0.000000  loss: 1.6650 (1.8005)  class_acc: 0.8000 (0.7898)  loss_scale: 16384.0000 (20200.5724)  weight_decay: 0.0500 (0.0500)  time: 0.8472  data: 0.0003  max mem: 26773
Epoch: [8]  [ 700/2669]  eta: 0:29:43  lr: 0.000575  min_lr: 0.000000  loss: 1.6982 (1.7995)  class_acc: 0.8000 (0.7895)  loss_scale: 16384.0000 (19656.1255)  weight_decay: 0.0500 (0.0500)  time: 0.8496  data: 0.0003  max mem: 26773
Epoch: [8]  [ 800/2669]  eta: 0:27:59  lr: 0.000575  min_lr: 0.000000  loss: 1.8838 (1.8060)  class_acc: 0.7500 (0.7879)  loss_scale: 16384.0000 (20065.7978)  weight_decay: 0.0500 (0.0500)  time: 0.8504  data: 0.0002  max mem: 26773
Epoch: [8]  [ 900/2669]  eta: 0:26:19  lr: 0.000574  min_lr: 0.000000  loss: 1.7773 (1.8059)  class_acc: 0.8000 (0.7883)  loss_scale: 16384.0000 (19657.1632)  weight_decay: 0.0500 (0.0500)  time: 0.8493  data: 0.0003  max mem: 26773
Epoch: [8]  [1000/2669]  eta: 0:24:43  lr: 0.000573  min_lr: 0.000000  loss: 1.8115 (1.8029)  class_acc: 0.7500 (0.7883)  loss_scale: 16384.0000 (19330.1738)  weight_decay: 0.0500 (0.0500)  time: 0.8459  data: 0.0003  max mem: 26773
Epoch: [8]  [1100/2669]  eta: 0:23:09  lr: 0.000573  min_lr: 0.000000  loss: 1.6729 (1.8013)  class_acc: 0.8000 (0.7884)  loss_scale: 16384.0000 (19062.5831)  weight_decay: 0.0500 (0.0500)  time: 0.8488  data: 0.0003  max mem: 26773
Epoch: [8]  [1200/2669]  eta: 0:21:36  lr: 0.000572  min_lr: 0.000000  loss: 1.7061 (1.7999)  class_acc: 0.8000 (0.7887)  loss_scale: 16384.0000 (18839.5537)  weight_decay: 0.0500 (0.0500)  time: 0.8494  data: 0.0003  max mem: 26773
Epoch: [8]  [1300/2669]  eta: 0:20:04  lr: 0.000572  min_lr: 0.000000  loss: 1.7607 (1.8017)  class_acc: 0.7500 (0.7872)  loss_scale: 16384.0000 (18776.7440)  weight_decay: 0.0500 (0.0500)  time: 0.8490  data: 0.0003  max mem: 26773
Epoch: [8]  [1400/2669]  eta: 0:18:34  lr: 0.000571  min_lr: 0.000000  loss: 1.7344 (1.8041)  class_acc: 0.8000 (0.7868)  loss_scale: 16384.0000 (19050.3469)  weight_decay: 0.0500 (0.0500)  time: 0.8471  data: 0.0003  max mem: 26773
Epoch: [8]  [1500/2669]  eta: 0:17:04  lr: 0.000570  min_lr: 0.000000  loss: 1.7754 (1.8043)  class_acc: 0.8000 (0.7867)  loss_scale: 16384.0000 (18872.7089)  weight_decay: 0.0500 (0.0500)  time: 0.8593  data: 0.0003  max mem: 26773
Epoch: [8]  [1600/2669]  eta: 0:15:34  lr: 0.000570  min_lr: 0.000000  loss: 1.7305 (1.8052)  class_acc: 0.8000 (0.7872)  loss_scale: 16384.0000 (18717.2617)  weight_decay: 0.0500 (0.0500)  time: 0.8499  data: 0.0002  max mem: 26773
Epoch: [8]  [1700/2669]  eta: 0:14:05  lr: 0.000569  min_lr: 0.000000  loss: 1.8535 (1.8077)  class_acc: 0.8000 (0.7868)  loss_scale: 16384.0000 (18580.0917)  weight_decay: 0.0500 (0.0500)  time: 0.8493  data: 0.0002  max mem: 26773
Epoch: [8]  [1800/2669]  eta: 0:12:37  lr: 0.000569  min_lr: 0.000000  loss: 1.8447 (1.8066)  class_acc: 0.7500 (0.7866)  loss_scale: 16384.0000 (18458.1544)  weight_decay: 0.0500 (0.0500)  time: 0.8511  data: 0.0003  max mem: 26773
Epoch: [8]  [1900/2669]  eta: 0:11:09  lr: 0.000568  min_lr: 0.000000  loss: 1.7969 (1.8094)  class_acc: 0.8000 (0.7858)  loss_scale: 32768.0000 (18745.5024)  weight_decay: 0.0500 (0.0500)  time: 0.8478  data: 0.0002  max mem: 26773
Epoch: [8]  [2000/2669]  eta: 0:09:41  lr: 0.000567  min_lr: 0.000000  loss: 1.9209 (1.8090)  class_acc: 0.7500 (0.7862)  loss_scale: 16384.0000 (18873.1234)  weight_decay: 0.0500 (0.0500)  time: 0.8521  data: 0.0002  max mem: 26773
Epoch: [8]  [2100/2669]  eta: 0:08:14  lr: 0.000567  min_lr: 0.000000  loss: 1.6465 (1.8100)  class_acc: 0.8000 (0.7860)  loss_scale: 16384.0000 (18754.6502)  weight_decay: 0.0500 (0.0500)  time: 0.8470  data: 0.0002  max mem: 26773
Epoch: [8]  [2200/2669]  eta: 0:06:46  lr: 0.000566  min_lr: 0.000000  loss: 1.7324 (1.8095)  class_acc: 0.8000 (0.7860)  loss_scale: 16384.0000 (18646.9423)  weight_decay: 0.0500 (0.0500)  time: 0.8456  data: 0.0002  max mem: 26773
Epoch: [8]  [2300/2669]  eta: 0:05:19  lr: 0.000565  min_lr: 0.000000  loss: 1.7227 (1.8103)  class_acc: 0.8000 (0.7861)  loss_scale: 16384.0000 (18548.5963)  weight_decay: 0.0500 (0.0500)  time: 0.8472  data: 0.0002  max mem: 26773
Epoch: [8]  [2400/2669]  eta: 0:03:53  lr: 0.000565  min_lr: 0.000000  loss: 1.7695 (1.8104)  class_acc: 0.8000 (0.7865)  loss_scale: 16384.0000 (18458.4423)  weight_decay: 0.0500 (0.0500)  time: 0.8532  data: 0.0002  max mem: 26773
Epoch: [8]  [2500/2669]  eta: 0:02:26  lr: 0.000564  min_lr: 0.000000  loss: 1.7246 (1.8102)  class_acc: 0.8000 (0.7868)  loss_scale: 32768.0000 (18729.2507)  weight_decay: 0.0500 (0.0500)  time: 0.8481  data: 0.0002  max mem: 26773
Epoch: [8]  [2600/2669]  eta: 0:00:59  lr: 0.000563  min_lr: 0.000000  loss: 1.7744 (1.8113)  class_acc: 0.8000 (0.7865)  loss_scale: 32768.0000 (19268.9950)  weight_decay: 0.0500 (0.0500)  time: 0.8481  data: 0.0002  max mem: 26773
Epoch: [8]  [2668/2669]  eta: 0:00:00  lr: 0.000563  min_lr: 0.000000  loss: 1.8867 (1.8102)  class_acc: 0.8000 (0.7869)  loss_scale: 16384.0000 (19503.5922)  weight_decay: 0.0500 (0.0500)  time: 0.8109  data: 0.0011  max mem: 26773
Epoch: [8] Total time: 0:38:27 (0.8647 s / it)
Averaged stats: lr: 0.000563  min_lr: 0.000000  loss: 1.8867 (1.8074)  class_acc: 0.8000 (0.7874)  loss_scale: 16384.0000 (19503.5922)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Test:  [  0/105]  eta: 0:06:18  loss: 0.2934 (0.2934)  acc1: 90.0000 (90.0000)  acc5: 100.0000 (100.0000)  time: 3.6075  data: 3.3110  max mem: 26773
Test:  [104/105]  eta: 0:00:00  loss: 0.4105 (0.5989)  acc1: 90.0000 (87.1401)  acc5: 100.0000 (97.5528)  time: 0.2867  data: 0.0001  max mem: 26773
Test: Total time: 0:00:35 (0.3366 s / it)
* Acc@1 87.022 Acc@5 98.203 loss 0.558
Accuracy of the network on the 50000 test images: 87.0%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Test:  [  0/105]  eta: 0:06:53  loss: 0.9789 (0.9789)  acc1: 90.0000 (90.0000)  acc5: 100.0000 (100.0000)  time: 3.9384  data: 3.6402  max mem: 26773
Test:  [104/105]  eta: 0:00:00  loss: 1.1773 (1.1379)  acc1: 85.0000 (86.7562)  acc5: 100.0000 (98.0326)  time: 0.2894  data: 0.0002  max mem: 26773
Test: Total time: 0:00:35 (0.3362 s / it)
* Acc@1 86.690 Acc@5 98.251 loss 1.127
EMA Accuracy of the network on the 50000 test images: 86.7%
Max accuracy: 86.69%
{"train_lr": 0.0005713294332335974, "train_min_lr": 1.2014909787350082e-08, "train_loss": 1.8074441935526377, "train_class_acc": 0.787395377278045, "train_loss_scale": 19503.59220389805, "train_weight_decay": 0.04999999999999853, "test_loss": 1.1270872355808341, "test_acc1": 86.69025911708253, "test_acc5": 98.25055982085732, "epoch": 8, "n_parameters": 304532456}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Epoch: [9]  [   0/2669]  eta: 1 day, 5:26:48  lr: 0.000563  min_lr: 0.000000  loss: 2.0625 (2.0625)  class_acc: 0.6500 (0.6500)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 39.7186  data: 37.8589  max mem: 26773
Epoch: [9]  [ 100/2669]  eta: 0:52:38  lr: 0.000562  min_lr: 0.000000  loss: 1.7090 (1.7388)  class_acc: 0.8000 (0.8030)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8456  data: 0.0002  max mem: 26773
Epoch: [9]  [ 200/2669]  eta: 0:42:46  lr: 0.000562  min_lr: 0.000000  loss: 1.7656 (1.7521)  class_acc: 0.8000 (0.7983)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8468  data: 0.0003  max mem: 26773
Epoch: [9]  [ 300/2669]  eta: 0:38:31  lr: 0.000561  min_lr: 0.000000  loss: 1.7705 (1.7496)  class_acc: 0.8000 (0.8032)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8482  data: 0.0002  max mem: 26773
Epoch: [9]  [ 400/2669]  eta: 0:35:40  lr: 0.000560  min_lr: 0.000000  loss: 1.6523 (1.7442)  class_acc: 0.8500 (0.8020)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8464  data: 0.0003  max mem: 26773
Epoch: [9]  [ 500/2669]  eta: 0:33:25  lr: 0.000559  min_lr: 0.000000  loss: 1.6504 (1.7486)  class_acc: 0.8000 (0.8001)  loss_scale: 16384.0000 (16449.4052)  weight_decay: 0.0500 (0.0500)  time: 0.8491  data: 0.0003  max mem: 26773
Epoch: [9]  [ 600/2669]  eta: 0:31:27  lr: 0.000559  min_lr: 0.000000  loss: 1.6357 (1.7438)  class_acc: 0.8000 (0.8012)  loss_scale: 16384.0000 (17256.3594)  weight_decay: 0.0500 (0.0500)  time: 0.8477  data: 0.0002  max mem: 26773
Epoch: [9]  [ 700/2669]  eta: 0:29:37  lr: 0.000558  min_lr: 0.000000  loss: 1.6934 (1.7591)  class_acc: 0.7500 (0.7980)  loss_scale: 16384.0000 (17131.9144)  weight_decay: 0.0500 (0.0500)  time: 0.8476  data: 0.0002  max mem: 26773
Epoch: [9]  [ 800/2669]  eta: 0:27:55  lr: 0.000557  min_lr: 0.000000  loss: 1.7646 (1.7612)  class_acc: 0.8000 (0.7966)  loss_scale: 16384.0000 (17038.5418)  weight_decay: 0.0500 (0.0500)  time: 0.8449  data: 0.0003  max mem: 26773
Epoch: [9]  [ 900/2669]  eta: 0:26:16  lr: 0.000557  min_lr: 0.000000  loss: 1.8262 (1.7723)  class_acc: 0.7500 (0.7946)  loss_scale: 16384.0000 (16965.8957)  weight_decay: 0.0500 (0.0500)  time: 0.8458  data: 0.0002  max mem: 26773
Epoch: [9]  [1000/2669]  eta: 0:24:41  lr: 0.000556  min_lr: 0.000000  loss: 1.7598 (1.7702)  class_acc: 0.8000 (0.7953)  loss_scale: 16384.0000 (16907.7642)  weight_decay: 0.0500 (0.0500)  time: 0.8485  data: 0.0002  max mem: 26773
Epoch: [9]  [1100/2669]  eta: 0:23:07  lr: 0.000555  min_lr: 0.000000  loss: 1.6416 (1.7707)  class_acc: 0.8000 (0.7953)  loss_scale: 32768.0000 (17634.0054)  weight_decay: 0.0500 (0.0500)  time: 0.8478  data: 0.0003  max mem: 26773
Epoch: [9]  [1200/2669]  eta: 0:21:35  lr: 0.000554  min_lr: 0.000000  loss: 1.7559 (1.7684)  class_acc: 0.8000 (0.7961)  loss_scale: 16384.0000 (17529.9251)  weight_decay: 0.0500 (0.0500)  time: 0.8494  data: 0.0003  max mem: 26773
Epoch: [9]  [1300/2669]  eta: 0:20:03  lr: 0.000554  min_lr: 0.000000  loss: 1.6982 (1.7669)  class_acc: 0.8000 (0.7964)  loss_scale: 16384.0000 (17441.8447)  weight_decay: 0.0500 (0.0500)  time: 0.8501  data: 0.0002  max mem: 26773
Epoch: [9]  [1400/2669]  eta: 0:18:33  lr: 0.000553  min_lr: 0.000000  loss: 1.6914 (1.7677)  class_acc: 0.8000 (0.7960)  loss_scale: 16384.0000 (17366.3383)  weight_decay: 0.0500 (0.0500)  time: 0.8508  data: 0.0002  max mem: 26773
Epoch: [9]  [1500/2669]  eta: 0:17:03  lr: 0.000552  min_lr: 0.000000  loss: 1.8232 (1.7682)  class_acc: 0.7500 (0.7959)  loss_scale: 16384.0000 (17300.8927)  weight_decay: 0.0500 (0.0500)  time: 0.8495  data: 0.0003  max mem: 26773
Epoch: [9]  [1600/2669]  eta: 0:15:34  lr: 0.000551  min_lr: 0.000000  loss: 1.7412 (1.7700)  class_acc: 0.7500 (0.7953)  loss_scale: 16384.0000 (17243.6227)  weight_decay: 0.0500 (0.0500)  time: 0.8513  data: 0.0003  max mem: 26773
Epoch: [9]  [1700/2669]  eta: 0:14:05  lr: 0.000551  min_lr: 0.000000  loss: 1.8281 (1.7687)  class_acc: 0.7500 (0.7956)  loss_scale: 32768.0000 (18021.4368)  weight_decay: 0.0500 (0.0500)  time: 0.8485  data: 0.0002  max mem: 26773
Epoch: [9]  [1800/2669]  eta: 0:12:37  lr: 0.000550  min_lr: 0.000000  loss: 1.8027 (1.7685)  class_acc: 0.8000 (0.7956)  loss_scale: 32768.0000 (18840.2354)  weight_decay: 0.0500 (0.0500)  time: 0.8509  data: 0.0003  max mem: 26773
Epoch: [9]  [1900/2669]  eta: 0:11:09  lr: 0.000549  min_lr: 0.000000  loss: 1.6963 (1.7667)  class_acc: 0.8000 (0.7959)  loss_scale: 16384.0000 (19176.4335)  weight_decay: 0.0500 (0.0500)  time: 0.8466  data: 0.0003  max mem: 26773
Epoch: [9]  [2000/2669]  eta: 0:09:41  lr: 0.000548  min_lr: 0.000000  loss: 1.6719 (1.7668)  class_acc: 0.7500 (0.7959)  loss_scale: 16384.0000 (19036.8816)  weight_decay: 0.0500 (0.0500)  time: 0.8517  data: 0.0003  max mem: 26773
Epoch: [9]  [2100/2669]  eta: 0:08:14  lr: 0.000547  min_lr: 0.000000  loss: 1.7842 (1.7683)  class_acc: 0.7500 (0.7950)  loss_scale: 16384.0000 (18910.6140)  weight_decay: 0.0500 (0.0500)  time: 0.8500  data: 0.0003  max mem: 26773
Epoch: [9]  [2200/2669]  eta: 0:06:46  lr: 0.000547  min_lr: 0.000000  loss: 1.6934 (1.7711)  class_acc: 0.8000 (0.7945)  loss_scale: 16384.0000 (18795.8201)  weight_decay: 0.0500 (0.0500)  time: 0.8476  data: 0.0002  max mem: 26773
Epoch: [9]  [2300/2669]  eta: 0:05:19  lr: 0.000546  min_lr: 0.000000  loss: 1.7236 (1.7696)  class_acc: 0.7500 (0.7952)  loss_scale: 16384.0000 (18691.0039)  weight_decay: 0.0500 (0.0500)  time: 0.8482  data: 0.0002  max mem: 26773
Epoch: [9]  [2400/2669]  eta: 0:03:52  lr: 0.000545  min_lr: 0.000000  loss: 1.7227 (1.7667)  class_acc: 0.8000 (0.7964)  loss_scale: 8192.0000 (18560.7997)  weight_decay: 0.0500 (0.0500)  time: 0.8454  data: 0.0002  max mem: 26773
Epoch: [9]  [2500/2669]  eta: 0:02:26  lr: 0.000544  min_lr: 0.000000  loss: 1.8672 (1.7670)  class_acc: 0.7500 (0.7966)  loss_scale: 8192.0000 (18146.2135)  weight_decay: 0.0500 (0.0500)  time: 0.8466  data: 0.0002  max mem: 26773
Epoch: [9]  [2600/2669]  eta: 0:00:59  lr: 0.000543  min_lr: 0.000000  loss: 1.6475 (1.7684)  class_acc: 0.8000 (0.7967)  loss_scale: 8192.0000 (17763.5063)  weight_decay: 0.0500 (0.0500)  time: 0.8490  data: 0.0003  max mem: 26773
Epoch: [9]  [2668/2669]  eta: 0:00:00  lr: 0.000543  min_lr: 0.000000  loss: 1.6660 (1.7685)  class_acc: 0.8000 (0.7967)  loss_scale: 8192.0000 (17523.1424)  weight_decay: 0.0500 (0.0500)  time: 0.8150  data: 0.0009  max mem: 26773
Epoch: [9] Total time: 0:38:27 (0.8645 s / it)
Averaged stats: lr: 0.000543  min_lr: 0.000000  loss: 1.6660 (1.7752)  class_acc: 0.8000 (0.7957)  loss_scale: 8192.0000 (17523.1424)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Test:  [  0/105]  eta: 0:06:27  loss: 0.3341 (0.3341)  acc1: 95.0000 (95.0000)  acc5: 100.0000 (100.0000)  time: 3.6866  data: 3.3911  max mem: 26773
Test:  [104/105]  eta: 0:00:00  loss: 0.5053 (0.5809)  acc1: 85.0000 (86.7083)  acc5: 100.0000 (98.3685)  time: 0.2867  data: 0.0002  max mem: 26773
Test: Total time: 0:00:35 (0.3373 s / it)
* Acc@1 87.024 Acc@5 98.227 loss 0.565
Accuracy of the network on the 50000 test images: 87.0%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Test:  [  0/105]  eta: 0:08:13  loss: 0.6696 (0.6696)  acc1: 90.0000 (90.0000)  acc5: 100.0000 (100.0000)  time: 4.6966  data: 4.3983  max mem: 26773
Test:  [104/105]  eta: 0:00:00  loss: 0.7939 (0.8428)  acc1: 85.0000 (86.8042)  acc5: 100.0000 (98.1286)  time: 0.2894  data: 0.0002  max mem: 26773
Test: Total time: 0:00:36 (0.3429 s / it)
* Acc@1 87.156 Acc@5 98.329 loss 0.834
EMA Accuracy of the network on the 50000 test images: 87.2%
Max accuracy: 87.16%
{"train_lr": 0.0005532249672930374, "train_min_lr": 1.1634177564623746e-08, "train_loss": 1.7752169905037716, "train_class_acc": 0.7956771741615146, "train_loss_scale": 17523.142428785606, "train_weight_decay": 0.04999999999999853, "test_loss": 0.8343138896638439, "test_acc1": 87.15611004478566, "test_acc5": 98.32853486884197, "epoch": 9, "n_parameters": 304532456}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Epoch: [10]  [   0/2669]  eta: 1 day, 4:50:52  lr: 0.000543  min_lr: 0.000000  loss: 1.8633 (1.8633)  class_acc: 0.8000 (0.8000)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 38.9108  data: 35.9395  max mem: 26773
Epoch: [10]  [ 100/2669]  eta: 0:52:13  lr: 0.000542  min_lr: 0.000000  loss: 1.7549 (1.7590)  class_acc: 0.8000 (0.7990)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8446  data: 0.0003  max mem: 26773
Epoch: [10]  [ 200/2669]  eta: 0:42:33  lr: 0.000541  min_lr: 0.000000  loss: 1.7021 (1.7569)  class_acc: 0.8000 (0.8030)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8499  data: 0.0003  max mem: 26773
Epoch: [10]  [ 300/2669]  eta: 0:38:26  lr: 0.000540  min_lr: 0.000000  loss: 1.5596 (1.7316)  class_acc: 0.8000 (0.8081)  loss_scale: 16384.0000 (10097.1163)  weight_decay: 0.0500 (0.0500)  time: 0.8459  data: 0.0003  max mem: 26773
Epoch: [10]  [ 400/2669]  eta: 0:35:39  lr: 0.000539  min_lr: 0.000000  loss: 1.5361 (1.7194)  class_acc: 0.8500 (0.8133)  loss_scale: 16384.0000 (11664.9177)  weight_decay: 0.0500 (0.0500)  time: 0.8494  data: 0.0003  max mem: 26773
Epoch: [10]  [ 500/2669]  eta: 0:33:26  lr: 0.000539  min_lr: 0.000000  loss: 1.8242 (1.7331)  class_acc: 0.8000 (0.8099)  loss_scale: 16384.0000 (12606.8503)  weight_decay: 0.0500 (0.0500)  time: 0.8478  data: 0.0003  max mem: 26773
Epoch: [10]  [ 600/2669]  eta: 0:31:27  lr: 0.000538  min_lr: 0.000000  loss: 1.6836 (1.7365)  class_acc: 0.7500 (0.8090)  loss_scale: 16384.0000 (13235.3278)  weight_decay: 0.0500 (0.0500)  time: 0.8481  data: 0.0002  max mem: 26773
Epoch: [10]  [ 700/2669]  eta: 0:29:38  lr: 0.000537  min_lr: 0.000000  loss: 1.7412 (1.7363)  class_acc: 0.8000 (0.8089)  loss_scale: 16384.0000 (13684.4964)  weight_decay: 0.0500 (0.0500)  time: 0.8482  data: 0.0003  max mem: 26773
Epoch: [10]  [ 800/2669]  eta: 0:27:55  lr: 0.000536  min_lr: 0.000000  loss: 1.8799 (1.7404)  class_acc: 0.8000 (0.8071)  loss_scale: 16384.0000 (14348.7840)  weight_decay: 0.0500 (0.0500)  time: 0.8482  data: 0.0002  max mem: 26773
Epoch: [10]  [ 900/2669]  eta: 0:26:16  lr: 0.000535  min_lr: 0.000000  loss: 1.5791 (1.7438)  class_acc: 0.8000 (0.8068)  loss_scale: 16384.0000 (14574.6681)  weight_decay: 0.0500 (0.0500)  time: 0.8474  data: 0.0003  max mem: 26773
Epoch: [10]  [1000/2669]  eta: 0:24:40  lr: 0.000534  min_lr: 0.000000  loss: 1.6191 (1.7472)  class_acc: 0.8500 (0.8056)  loss_scale: 16384.0000 (14755.4206)  weight_decay: 0.0500 (0.0500)  time: 0.8597  data: 0.0002  max mem: 26773
Epoch: [10]  [1100/2669]  eta: 0:23:06  lr: 0.000533  min_lr: 0.000000  loss: 1.6445 (1.7507)  class_acc: 0.8000 (0.8054)  loss_scale: 16384.0000 (14903.3388)  weight_decay: 0.0500 (0.0500)  time: 0.8497  data: 0.0003  max mem: 26773
Epoch: [10]  [1200/2669]  eta: 0:21:34  lr: 0.000532  min_lr: 0.000000  loss: 1.7217 (1.7510)  class_acc: 0.8000 (0.8046)  loss_scale: 16384.0000 (15026.6245)  weight_decay: 0.0500 (0.0500)  time: 0.8474  data: 0.0003  max mem: 26773
Epoch: [10]  [1300/2669]  eta: 0:20:02  lr: 0.000532  min_lr: 0.000000  loss: 1.6230 (1.7479)  class_acc: 0.8000 (0.8052)  loss_scale: 16384.0000 (15332.4520)  weight_decay: 0.0500 (0.0500)  time: 0.8454  data: 0.0003  max mem: 26773
Epoch: [10]  [1400/2669]  eta: 0:18:32  lr: 0.000531  min_lr: 0.000000  loss: 1.6162 (1.7465)  class_acc: 0.8500 (0.8059)  loss_scale: 8192.0000 (15232.0914)  weight_decay: 0.0500 (0.0500)  time: 0.8519  data: 0.0002  max mem: 26773
Epoch: [10]  [1500/2669]  eta: 0:17:02  lr: 0.000530  min_lr: 0.000000  loss: 1.7324 (1.7464)  class_acc: 0.8000 (0.8052)  loss_scale: 8192.0000 (14763.0646)  weight_decay: 0.0500 (0.0500)  time: 0.8467  data: 0.0003  max mem: 26773
Epoch: [10]  [1600/2669]  eta: 0:15:33  lr: 0.000529  min_lr: 0.000000  loss: 1.6973 (1.7467)  class_acc: 0.8000 (0.8052)  loss_scale: 8192.0000 (14352.6296)  weight_decay: 0.0500 (0.0500)  time: 0.8470  data: 0.0002  max mem: 26773
Epoch: [10]  [1700/2669]  eta: 0:14:05  lr: 0.000528  min_lr: 0.000000  loss: 1.6621 (1.7448)  class_acc: 0.8500 (0.8061)  loss_scale: 8192.0000 (13990.4527)  weight_decay: 0.0500 (0.0500)  time: 0.8474  data: 0.0002  max mem: 26773
Epoch: [10]  [1800/2669]  eta: 0:12:36  lr: 0.000527  min_lr: 0.000000  loss: 1.8311 (1.7458)  class_acc: 0.7500 (0.8058)  loss_scale: 8192.0000 (13668.4953)  weight_decay: 0.0500 (0.0500)  time: 0.8495  data: 0.0002  max mem: 26773
Epoch: [10]  [1900/2669]  eta: 0:11:08  lr: 0.000526  min_lr: 0.000000  loss: 1.7725 (1.7450)  class_acc: 0.8000 (0.8059)  loss_scale: 16384.0000 (13440.7407)  weight_decay: 0.0500 (0.0500)  time: 0.8594  data: 0.0003  max mem: 26773
Epoch: [10]  [2000/2669]  eta: 0:09:41  lr: 0.000525  min_lr: 0.000000  loss: 1.7744 (1.7442)  class_acc: 0.7500 (0.8058)  loss_scale: 16384.0000 (13587.8301)  weight_decay: 0.0500 (0.0500)  time: 0.8488  data: 0.0002  max mem: 26773
Epoch: [10]  [2100/2669]  eta: 0:08:14  lr: 0.000524  min_lr: 0.000000  loss: 1.6426 (1.7437)  class_acc: 0.8000 (0.8063)  loss_scale: 16384.0000 (13720.9177)  weight_decay: 0.0500 (0.0500)  time: 0.8640  data: 0.0002  max mem: 26773
Epoch: [10]  [2200/2669]  eta: 0:06:46  lr: 0.000523  min_lr: 0.000000  loss: 1.6494 (1.7445)  class_acc: 0.8500 (0.8060)  loss_scale: 16384.0000 (13841.9119)  weight_decay: 0.0500 (0.0500)  time: 0.8476  data: 0.0002  max mem: 26773
Epoch: [10]  [2300/2669]  eta: 0:05:19  lr: 0.000522  min_lr: 0.000000  loss: 1.7744 (1.7455)  class_acc: 0.7500 (0.8056)  loss_scale: 16384.0000 (13952.3894)  weight_decay: 0.0500 (0.0500)  time: 0.8510  data: 0.0002  max mem: 26773
Epoch: [10]  [2400/2669]  eta: 0:03:52  lr: 0.000521  min_lr: 0.000000  loss: 1.5996 (1.7468)  class_acc: 0.8000 (0.8054)  loss_scale: 16384.0000 (14067.3120)  weight_decay: 0.0500 (0.0500)  time: 0.8509  data: 0.0002  max mem: 26773
Epoch: [10]  [2500/2669]  eta: 0:02:26  lr: 0.000520  min_lr: 0.000000  loss: 1.5479 (1.7474)  class_acc: 0.8500 (0.8051)  loss_scale: 16384.0000 (14251.6561)  weight_decay: 0.0500 (0.0500)  time: 0.8529  data: 0.0002  max mem: 26773
Epoch: [10]  [2600/2669]  eta: 0:00:59  lr: 0.000519  min_lr: 0.000000  loss: 1.6025 (1.7471)  class_acc: 0.8000 (0.8052)  loss_scale: 16384.0000 (14333.6378)  weight_decay: 0.0500 (0.0500)  time: 0.8645  data: 0.0002  max mem: 26773
Epoch: [10]  [2668/2669]  eta: 0:00:00  lr: 0.000519  min_lr: 0.000000  loss: 1.6826 (1.7473)  class_acc: 0.8000 (0.8052)  loss_scale: 16384.0000 (14385.1274)  weight_decay: 0.0500 (0.0500)  time: 0.8203  data: 0.0009  max mem: 26773
Epoch: [10] Total time: 0:38:28 (0.8648 s / it)
Averaged stats: lr: 0.000519  min_lr: 0.000000  loss: 1.6826 (1.7481)  class_acc: 0.8000 (0.8032)  loss_scale: 16384.0000 (14385.1274)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Test:  [  0/105]  eta: 0:07:27  loss: 0.4906 (0.4906)  acc1: 85.0000 (85.0000)  acc5: 100.0000 (100.0000)  time: 4.2645  data: 3.9615  max mem: 26773
Test:  [104/105]  eta: 0:00:00  loss: 0.5167 (0.5794)  acc1: 90.0000 (87.3321)  acc5: 100.0000 (98.0806)  time: 0.2870  data: 0.0002  max mem: 26773
Test: Total time: 0:00:35 (0.3364 s / it)
* Acc@1 87.172 Acc@5 98.283 loss 0.559
Accuracy of the network on the 50000 test images: 87.2%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Test:  [  0/105]  eta: 0:08:36  loss: 0.4971 (0.4971)  acc1: 90.0000 (90.0000)  acc5: 100.0000 (100.0000)  time: 4.9202  data: 4.6226  max mem: 26773
Test:  [104/105]  eta: 0:00:00  loss: 0.6023 (0.6832)  acc1: 85.0000 (87.2361)  acc5: 100.0000 (98.2726)  time: 0.2896  data: 0.0002  max mem: 26773
Test: Total time: 0:00:36 (0.3451 s / it)
* Acc@1 87.526 Acc@5 98.425 loss 0.676
EMA Accuracy of the network on the 50000 test images: 87.5%
Max accuracy: 87.53%
{"train_lr": 0.0005311348776476884, "train_min_lr": 1.1169628709191758e-08, "train_loss": 1.7481439334043916, "train_class_acc": 0.8031757689169099, "train_loss_scale": 14385.127436281859, "train_weight_decay": 0.04999999999999853, "test_loss": 0.6760214890456862, "test_acc1": 87.52599168266156, "test_acc5": 98.42450415866922, "epoch": 10, "n_parameters": 304532456}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Epoch: [11]  [   0/2669]  eta: 1 day, 7:11:23  lr: 0.000519  min_lr: 0.000000  loss: 1.9912 (1.9912)  class_acc: 0.7000 (0.7000)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 42.0696  data: 25.0587  max mem: 26773
Epoch: [11]  [ 100/2669]  eta: 0:53:38  lr: 0.000518  min_lr: 0.000000  loss: 1.6592 (1.6832)  class_acc: 0.8000 (0.8158)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8445  data: 0.0003  max mem: 26773
Epoch: [11]  [ 200/2669]  eta: 0:43:12  lr: 0.000517  min_lr: 0.000000  loss: 1.5137 (1.6901)  class_acc: 0.8000 (0.8152)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8454  data: 0.0003  max mem: 26773
Epoch: [11]  [ 300/2669]  eta: 0:38:46  lr: 0.000516  min_lr: 0.000000  loss: 1.7246 (1.7207)  class_acc: 0.8000 (0.8101)  loss_scale: 16384.0000 (17690.3654)  weight_decay: 0.0500 (0.0500)  time: 0.8478  data: 0.0003  max mem: 26773
Epoch: [11]  [ 400/2669]  eta: 0:35:53  lr: 0.000515  min_lr: 0.000000  loss: 1.6309 (1.7278)  class_acc: 0.8000 (0.8096)  loss_scale: 16384.0000 (17364.5885)  weight_decay: 0.0500 (0.0500)  time: 0.8469  data: 0.0003  max mem: 26773
Epoch: [11]  [ 500/2669]  eta: 0:33:34  lr: 0.000514  min_lr: 0.000000  loss: 1.5908 (1.7157)  class_acc: 0.8500 (0.8142)  loss_scale: 16384.0000 (17168.8623)  weight_decay: 0.0500 (0.0500)  time: 0.8479  data: 0.0005  max mem: 26773
Epoch: [11]  [ 600/2669]  eta: 0:31:34  lr: 0.000513  min_lr: 0.000000  loss: 1.7549 (1.7096)  class_acc: 0.8000 (0.8146)  loss_scale: 16384.0000 (17038.2696)  weight_decay: 0.0500 (0.0500)  time: 0.8608  data: 0.0003  max mem: 26773
Epoch: [11]  [ 700/2669]  eta: 0:29:45  lr: 0.000512  min_lr: 0.000000  loss: 1.5684 (1.7065)  class_acc: 0.8500 (0.8150)  loss_scale: 16384.0000 (16944.9358)  weight_decay: 0.0500 (0.0500)  time: 0.8655  data: 0.0003  max mem: 26773
Epoch: [11]  [ 800/2669]  eta: 0:28:01  lr: 0.000511  min_lr: 0.000000  loss: 1.8438 (1.7132)  class_acc: 0.8000 (0.8128)  loss_scale: 16384.0000 (16874.9064)  weight_decay: 0.0500 (0.0500)  time: 0.8491  data: 0.0003  max mem: 26773
Epoch: [11]  [ 900/2669]  eta: 0:26:21  lr: 0.000510  min_lr: 0.000000  loss: 1.7197 (1.7198)  class_acc: 0.8000 (0.8102)  loss_scale: 16384.0000 (17620.5283)  weight_decay: 0.0500 (0.0500)  time: 0.8470  data: 0.0003  max mem: 26773
Epoch: [11]  [1000/2669]  eta: 0:24:44  lr: 0.000509  min_lr: 0.000000  loss: 1.8965 (1.7201)  class_acc: 0.7500 (0.8109)  loss_scale: 16384.0000 (17496.9990)  weight_decay: 0.0500 (0.0500)  time: 0.8459  data: 0.0002  max mem: 26773
Epoch: [11]  [1100/2669]  eta: 0:23:10  lr: 0.000508  min_lr: 0.000000  loss: 1.8828 (1.7216)  class_acc: 0.7500 (0.8104)  loss_scale: 16384.0000 (17395.9092)  weight_decay: 0.0500 (0.0500)  time: 0.8487  data: 0.0002  max mem: 26773
Epoch: [11]  [1200/2669]  eta: 0:21:36  lr: 0.000507  min_lr: 0.000000  loss: 1.6162 (1.7233)  class_acc: 0.8000 (0.8097)  loss_scale: 16384.0000 (17311.6536)  weight_decay: 0.0500 (0.0500)  time: 0.8490  data: 0.0003  max mem: 26773
Epoch: [11]  [1300/2669]  eta: 0:20:05  lr: 0.000506  min_lr: 0.000000  loss: 1.5781 (1.7234)  class_acc: 0.8500 (0.8100)  loss_scale: 16384.0000 (17240.3505)  weight_decay: 0.0500 (0.0500)  time: 0.8463  data: 0.0002  max mem: 26773
Epoch: [11]  [1400/2669]  eta: 0:18:34  lr: 0.000505  min_lr: 0.000000  loss: 1.5957 (1.7220)  class_acc: 0.8000 (0.8097)  loss_scale: 32768.0000 (17623.6174)  weight_decay: 0.0500 (0.0500)  time: 0.8535  data: 0.0003  max mem: 26773
Epoch: [11]  [1500/2669]  eta: 0:17:04  lr: 0.000504  min_lr: 0.000000  loss: 1.6738 (1.7230)  class_acc: 0.8000 (0.8093)  loss_scale: 16384.0000 (17999.4777)  weight_decay: 0.0500 (0.0500)  time: 0.8485  data: 0.0003  max mem: 26773
Epoch: [11]  [1600/2669]  eta: 0:15:35  lr: 0.000503  min_lr: 0.000000  loss: 1.6709 (1.7214)  class_acc: 0.8000 (0.8099)  loss_scale: 16384.0000 (17898.5734)  weight_decay: 0.0500 (0.0500)  time: 0.8479  data: 0.0003  max mem: 26773
Epoch: [11]  [1700/2669]  eta: 0:14:06  lr: 0.000502  min_lr: 0.000000  loss: 1.6230 (1.7231)  class_acc: 0.8000 (0.8093)  loss_scale: 16384.0000 (17809.5332)  weight_decay: 0.0500 (0.0500)  time: 0.8505  data: 0.0002  max mem: 26773
Epoch: [11]  [1800/2669]  eta: 0:12:37  lr: 0.000501  min_lr: 0.000000  loss: 1.6836 (1.7235)  class_acc: 0.8000 (0.8091)  loss_scale: 16384.0000 (17730.3809)  weight_decay: 0.0500 (0.0500)  time: 0.8502  data: 0.0002  max mem: 26773
Epoch: [11]  [1900/2669]  eta: 0:11:09  lr: 0.000500  min_lr: 0.000000  loss: 1.5225 (1.7219)  class_acc: 0.8000 (0.8096)  loss_scale: 16384.0000 (17659.5560)  weight_decay: 0.0500 (0.0500)  time: 0.8495  data: 0.0002  max mem: 26773
Epoch: [11]  [2000/2669]  eta: 0:09:41  lr: 0.000499  min_lr: 0.000000  loss: 1.6211 (1.7211)  class_acc: 0.8000 (0.8099)  loss_scale: 32768.0000 (17939.7021)  weight_decay: 0.0500 (0.0500)  time: 0.8519  data: 0.0002  max mem: 26773
Epoch: [11]  [2100/2669]  eta: 0:08:14  lr: 0.000498  min_lr: 0.000000  loss: 1.7148 (1.7200)  class_acc: 0.8000 (0.8104)  loss_scale: 32768.0000 (18645.4755)  weight_decay: 0.0500 (0.0500)  time: 0.8510  data: 0.0003  max mem: 26773
Epoch: [11]  [2200/2669]  eta: 0:06:47  lr: 0.000496  min_lr: 0.000000  loss: 1.6152 (1.7197)  class_acc: 0.8500 (0.8107)  loss_scale: 32768.0000 (19287.1168)  weight_decay: 0.0500 (0.0500)  time: 0.8521  data: 0.0002  max mem: 26773
Epoch: [11]  [2300/2669]  eta: 0:05:20  lr: 0.000495  min_lr: 0.000000  loss: 1.7178 (1.7207)  class_acc: 0.7500 (0.8105)  loss_scale: 32768.0000 (19872.9874)  weight_decay: 0.0500 (0.0500)  time: 0.8514  data: 0.0002  max mem: 26773
Epoch: [11]  [2400/2669]  eta: 0:03:53  lr: 0.000494  min_lr: 0.000000  loss: 1.6514 (1.7209)  class_acc: 0.8000 (0.8106)  loss_scale: 32768.0000 (20410.0558)  weight_decay: 0.0500 (0.0500)  time: 0.8467  data: 0.0002  max mem: 26773
Epoch: [11]  [2500/2669]  eta: 0:02:26  lr: 0.000493  min_lr: 0.000000  loss: 1.5850 (1.7201)  class_acc: 0.8500 (0.8107)  loss_scale: 16384.0000 (20498.0152)  weight_decay: 0.0500 (0.0500)  time: 0.8483  data: 0.0002  max mem: 26773
Epoch: [11]  [2600/2669]  eta: 0:00:59  lr: 0.000492  min_lr: 0.000000  loss: 1.6270 (1.7198)  class_acc: 0.8500 (0.8103)  loss_scale: 16384.0000 (20339.8447)  weight_decay: 0.0500 (0.0500)  time: 0.8489  data: 0.0003  max mem: 26773
Epoch: [11]  [2668/2669]  eta: 0:00:00  lr: 0.000491  min_lr: 0.000000  loss: 1.7441 (1.7187)  class_acc: 0.8000 (0.8107)  loss_scale: 16384.0000 (20240.5037)  weight_decay: 0.0500 (0.0500)  time: 0.8148  data: 0.0020  max mem: 26773
Epoch: [11] Total time: 0:38:29 (0.8655 s / it)
Averaged stats: lr: 0.000491  min_lr: 0.000000  loss: 1.7441 (1.7206)  class_acc: 0.8000 (0.8104)  loss_scale: 16384.0000 (20240.5037)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Test:  [  0/105]  eta: 0:05:20  loss: 0.2930 (0.2930)  acc1: 90.0000 (90.0000)  acc5: 100.0000 (100.0000)  time: 3.0479  data: 2.7479  max mem: 26773
Test:  [104/105]  eta: 0:00:00  loss: 0.4987 (0.5830)  acc1: 85.0000 (87.0441)  acc5: 100.0000 (97.9846)  time: 0.2867  data: 0.0001  max mem: 26773
Test: Total time: 0:00:34 (0.3320 s / it)
* Acc@1 87.090 Acc@5 98.149 loss 0.570
Accuracy of the network on the 50000 test images: 87.1%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Test:  [  0/105]  eta: 0:07:12  loss: 0.4044 (0.4044)  acc1: 90.0000 (90.0000)  acc5: 100.0000 (100.0000)  time: 4.1213  data: 3.8236  max mem: 26773
Test:  [104/105]  eta: 0:00:00  loss: 0.4897 (0.5968)  acc1: 85.0000 (87.7639)  acc5: 100.0000 (98.3205)  time: 0.2895  data: 0.0002  max mem: 26773
Test: Total time: 0:00:35 (0.3378 s / it)
* Acc@1 87.762 Acc@5 98.472 loss 0.590
EMA Accuracy of the network on the 50000 test images: 87.8%
Max accuracy: 87.76%
{"train_lr": 0.0005054075382072422, "train_min_lr": 1.0628589434012097e-08, "train_loss": 1.7206083237678036, "train_class_acc": 0.8104416676431612, "train_loss_scale": 20240.50374812594, "train_weight_decay": 0.04999999999999853, "test_loss": 0.5895346585897699, "test_acc1": 87.76191618682022, "test_acc5": 98.47248880358285, "epoch": 11, "n_parameters": 304532456}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Epoch: [12]  [   0/2669]  eta: 1 day, 6:08:15  lr: 0.000491  min_lr: 0.000000  loss: 1.8037 (1.8037)  class_acc: 0.8000 (0.8000)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 40.6503  data: 34.0948  max mem: 26773
Epoch: [12]  [ 100/2669]  eta: 0:53:00  lr: 0.000490  min_lr: 0.000000  loss: 1.6162 (1.6786)  class_acc: 0.8500 (0.8243)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8456  data: 0.0002  max mem: 26773
Epoch: [12]  [ 200/2669]  eta: 0:42:54  lr: 0.000489  min_lr: 0.000000  loss: 1.6992 (1.6778)  class_acc: 0.8000 (0.8219)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8471  data: 0.0003  max mem: 26773
Epoch: [12]  [ 300/2669]  eta: 0:38:37  lr: 0.000488  min_lr: 0.000000  loss: 1.6680 (1.6713)  class_acc: 0.8000 (0.8221)  loss_scale: 32768.0000 (17037.1827)  weight_decay: 0.0500 (0.0500)  time: 0.8445  data: 0.0002  max mem: 26773
Epoch: [12]  [ 400/2669]  eta: 0:35:48  lr: 0.000487  min_lr: 0.000000  loss: 1.5469 (1.6748)  class_acc: 0.8500 (0.8223)  loss_scale: 16384.0000 (16874.2943)  weight_decay: 0.0500 (0.0500)  time: 0.8464  data: 0.0002  max mem: 26773
Epoch: [12]  [ 500/2669]  eta: 0:33:31  lr: 0.000486  min_lr: 0.000000  loss: 1.5645 (1.6627)  class_acc: 0.8500 (0.8256)  loss_scale: 8192.0000 (16416.7026)  weight_decay: 0.0500 (0.0500)  time: 0.8474  data: 0.0002  max mem: 26773
Epoch: [12]  [ 600/2669]  eta: 0:31:31  lr: 0.000485  min_lr: 0.000000  loss: 1.5039 (1.6622)  class_acc: 0.8500 (0.8265)  loss_scale: 8192.0000 (15048.1997)  weight_decay: 0.0500 (0.0500)  time: 0.8475  data: 0.0002  max mem: 26773
Epoch: [12]  [ 700/2669]  eta: 0:29:42  lr: 0.000484  min_lr: 0.000000  loss: 1.6152 (1.6662)  class_acc: 0.8000 (0.8249)  loss_scale: 8192.0000 (14070.1398)  weight_decay: 0.0500 (0.0500)  time: 0.8473  data: 0.0003  max mem: 26773
Epoch: [12]  [ 800/2669]  eta: 0:27:58  lr: 0.000483  min_lr: 0.000000  loss: 1.7744 (1.6731)  class_acc: 0.8000 (0.8230)  loss_scale: 8192.0000 (13336.2896)  weight_decay: 0.0500 (0.0500)  time: 0.8463  data: 0.0003  max mem: 26773
Epoch: [12]  [ 900/2669]  eta: 0:26:19  lr: 0.000481  min_lr: 0.000000  loss: 1.8311 (1.6768)  class_acc: 0.8000 (0.8229)  loss_scale: 8192.0000 (12765.3363)  weight_decay: 0.0500 (0.0500)  time: 0.8479  data: 0.0002  max mem: 26773
Epoch: [12]  [1000/2669]  eta: 0:24:43  lr: 0.000480  min_lr: 0.000000  loss: 1.6240 (1.6775)  class_acc: 0.8000 (0.8228)  loss_scale: 8192.0000 (12357.5624)  weight_decay: 0.0500 (0.0500)  time: 0.8605  data: 0.0002  max mem: 26773
Epoch: [12]  [1100/2669]  eta: 0:23:09  lr: 0.000479  min_lr: 0.000000  loss: 1.6768 (1.6814)  class_acc: 0.8000 (0.8209)  loss_scale: 16384.0000 (12723.2698)  weight_decay: 0.0500 (0.0500)  time: 0.8622  data: 0.0002  max mem: 26773
Epoch: [12]  [1200/2669]  eta: 0:21:36  lr: 0.000478  min_lr: 0.000000  loss: 1.7031 (1.6845)  class_acc: 0.8000 (0.8201)  loss_scale: 16384.0000 (13028.0766)  weight_decay: 0.0500 (0.0500)  time: 0.8502  data: 0.0003  max mem: 26773
Epoch: [12]  [1300/2669]  eta: 0:20:04  lr: 0.000477  min_lr: 0.000000  loss: 1.6338 (1.6848)  class_acc: 0.8000 (0.8198)  loss_scale: 16384.0000 (13286.0261)  weight_decay: 0.0500 (0.0500)  time: 0.8497  data: 0.0003  max mem: 26773
Epoch: [12]  [1400/2669]  eta: 0:18:33  lr: 0.000476  min_lr: 0.000000  loss: 1.6484 (1.6827)  class_acc: 0.8500 (0.8207)  loss_scale: 16384.0000 (13507.1520)  weight_decay: 0.0500 (0.0500)  time: 0.8487  data: 0.0003  max mem: 26773
Epoch: [12]  [1500/2669]  eta: 0:17:03  lr: 0.000475  min_lr: 0.000000  loss: 1.6260 (1.6836)  class_acc: 0.8000 (0.8204)  loss_scale: 16384.0000 (13698.8141)  weight_decay: 0.0500 (0.0500)  time: 0.8472  data: 0.0003  max mem: 26773
Epoch: [12]  [1600/2669]  eta: 0:15:34  lr: 0.000473  min_lr: 0.000000  loss: 1.7031 (1.6844)  class_acc: 0.8000 (0.8203)  loss_scale: 16384.0000 (14071.2055)  weight_decay: 0.0500 (0.0500)  time: 0.8667  data: 0.0003  max mem: 26773
Epoch: [12]  [1700/2669]  eta: 0:14:06  lr: 0.000472  min_lr: 0.000000  loss: 1.6768 (1.6859)  class_acc: 0.8000 (0.8201)  loss_scale: 16384.0000 (14207.1723)  weight_decay: 0.0500 (0.0500)  time: 0.8625  data: 0.0002  max mem: 26773
Epoch: [12]  [1800/2669]  eta: 0:12:38  lr: 0.000471  min_lr: 0.000000  loss: 1.7646 (1.6867)  class_acc: 0.7500 (0.8196)  loss_scale: 16384.0000 (14328.0400)  weight_decay: 0.0500 (0.0500)  time: 0.8618  data: 0.0003  max mem: 26773
Epoch: [12]  [1900/2669]  eta: 0:11:10  lr: 0.000470  min_lr: 0.000000  loss: 1.8320 (1.6889)  class_acc: 0.8000 (0.8188)  loss_scale: 16384.0000 (14436.1915)  weight_decay: 0.0500 (0.0500)  time: 0.8500  data: 0.0003  max mem: 26773
Epoch: [12]  [2000/2669]  eta: 0:09:42  lr: 0.000469  min_lr: 0.000000  loss: 1.8047 (1.6911)  class_acc: 0.8000 (0.8183)  loss_scale: 16384.0000 (14533.5332)  weight_decay: 0.0500 (0.0500)  time: 0.8487  data: 0.0003  max mem: 26773
Epoch: [12]  [2100/2669]  eta: 0:08:14  lr: 0.000468  min_lr: 0.000000  loss: 1.6846 (1.6899)  class_acc: 0.8000 (0.8184)  loss_scale: 32768.0000 (15073.9039)  weight_decay: 0.0500 (0.0500)  time: 0.8507  data: 0.0002  max mem: 26773
Epoch: [12]  [2200/2669]  eta: 0:06:47  lr: 0.000467  min_lr: 0.000000  loss: 1.6631 (1.6905)  class_acc: 0.8000 (0.8179)  loss_scale: 32768.0000 (15877.8155)  weight_decay: 0.0500 (0.0500)  time: 0.8497  data: 0.0003  max mem: 26773
Epoch: [12]  [2300/2669]  eta: 0:05:20  lr: 0.000465  min_lr: 0.000000  loss: 1.7012 (1.6917)  class_acc: 0.8000 (0.8175)  loss_scale: 16384.0000 (16512.1669)  weight_decay: 0.0500 (0.0500)  time: 0.8475  data: 0.0003  max mem: 26773
Epoch: [12]  [2400/2669]  eta: 0:03:53  lr: 0.000464  min_lr: 0.000000  loss: 1.4580 (1.6906)  class_acc: 0.8500 (0.8176)  loss_scale: 16384.0000 (16506.8288)  weight_decay: 0.0500 (0.0500)  time: 0.8499  data: 0.0003  max mem: 26773
Epoch: [12]  [2500/2669]  eta: 0:02:26  lr: 0.000463  min_lr: 0.000000  loss: 1.5986 (1.6896)  class_acc: 0.8500 (0.8178)  loss_scale: 8192.0000 (16351.2451)  weight_decay: 0.0500 (0.0500)  time: 0.8600  data: 0.0003  max mem: 26773
Epoch: [12]  [2600/2669]  eta: 0:00:59  lr: 0.000462  min_lr: 0.000000  loss: 1.6182 (1.6870)  class_acc: 0.8500 (0.8181)  loss_scale: 8192.0000 (16037.5486)  weight_decay: 0.0500 (0.0500)  time: 0.8493  data: 0.0003  max mem: 26773
Epoch: [12]  [2668/2669]  eta: 0:00:00  lr: 0.000461  min_lr: 0.000000  loss: 1.5693 (1.6868)  class_acc: 0.8500 (0.8183)  loss_scale: 8192.0000 (15840.5277)  weight_decay: 0.0500 (0.0500)  time: 0.8175  data: 0.0013  max mem: 26773
Epoch: [12] Total time: 0:38:30 (0.8657 s / it)
Averaged stats: lr: 0.000461  min_lr: 0.000000  loss: 1.5693 (1.6944)  class_acc: 0.8500 (0.8172)  loss_scale: 8192.0000 (15840.5277)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Test:  [  0/105]  eta: 0:06:49  loss: 0.1953 (0.1953)  acc1: 95.0000 (95.0000)  acc5: 100.0000 (100.0000)  time: 3.8962  data: 3.5828  max mem: 26773
Test:  [104/105]  eta: 0:00:00  loss: 0.4738 (0.5849)  acc1: 85.0000 (86.8042)  acc5: 100.0000 (98.1286)  time: 0.2872  data: 0.0002  max mem: 26773
Test: Total time: 0:00:34 (0.3328 s / it)
* Acc@1 87.052 Acc@5 98.177 loss 0.567
Accuracy of the network on the 50000 test images: 87.1%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Test:  [  0/105]  eta: 0:07:27  loss: 0.3445 (0.3445)  acc1: 90.0000 (90.0000)  acc5: 100.0000 (100.0000)  time: 4.2632  data: 3.9645  max mem: 26773
Test:  [104/105]  eta: 0:00:00  loss: 0.4307 (0.5497)  acc1: 85.0000 (87.7639)  acc5: 100.0000 (98.3205)  time: 0.2902  data: 0.0001  max mem: 26773
Test: Total time: 0:00:35 (0.3409 s / it)
* Acc@1 87.932 Acc@5 98.516 loss 0.542
EMA Accuracy of the network on the 50000 test images: 87.9%
Max accuracy: 87.93%
{"train_lr": 0.0004764486844834333, "train_min_lr": 1.0019592251655534e-08, "train_loss": 1.6943801811252577, "train_class_acc": 0.8172218715125087, "train_loss_scale": 15840.527736131933, "train_weight_decay": 0.04999999999999853, "test_loss": 0.5418974146690397, "test_acc1": 87.93186180422265, "test_acc5": 98.51647472808702, "epoch": 12, "n_parameters": 304532456}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Epoch: [13]  [   0/2669]  eta: 1 day, 5:57:29  lr: 0.000461  min_lr: 0.000000  loss: 1.4746 (1.4746)  class_acc: 0.8500 (0.8500)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 40.4082  data: 38.9811  max mem: 26773
Epoch: [13]  [ 100/2669]  eta: 0:52:55  lr: 0.000460  min_lr: 0.000000  loss: 1.6973 (1.6814)  class_acc: 0.8500 (0.8267)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8447  data: 0.0002  max mem: 26773
Epoch: [13]  [ 200/2669]  eta: 0:42:52  lr: 0.000459  min_lr: 0.000000  loss: 1.6396 (1.6464)  class_acc: 0.8000 (0.8328)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8476  data: 0.0003  max mem: 26773
Epoch: [13]  [ 300/2669]  eta: 0:38:36  lr: 0.000457  min_lr: 0.000000  loss: 1.5850 (1.6487)  class_acc: 0.8000 (0.8306)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8534  data: 0.0008  max mem: 26773
Epoch: [13]  [ 400/2669]  eta: 0:35:45  lr: 0.000456  min_lr: 0.000000  loss: 1.6064 (1.6481)  class_acc: 0.8500 (0.8287)  loss_scale: 16384.0000 (10194.0349)  weight_decay: 0.0500 (0.0500)  time: 0.8513  data: 0.0003  max mem: 26773
Epoch: [13]  [ 500/2669]  eta: 0:33:29  lr: 0.000455  min_lr: 0.000000  loss: 1.4951 (1.6529)  class_acc: 0.8500 (0.8283)  loss_scale: 16384.0000 (11429.5569)  weight_decay: 0.0500 (0.0500)  time: 0.8495  data: 0.0002  max mem: 26773
Epoch: [13]  [ 600/2669]  eta: 0:31:31  lr: 0.000454  min_lr: 0.000000  loss: 1.5771 (1.6488)  class_acc: 0.8000 (0.8283)  loss_scale: 16384.0000 (12253.9235)  weight_decay: 0.0500 (0.0500)  time: 0.8505  data: 0.0003  max mem: 26773
Epoch: [13]  [ 700/2669]  eta: 0:29:43  lr: 0.000453  min_lr: 0.000000  loss: 1.7100 (1.6452)  class_acc: 0.8000 (0.8291)  loss_scale: 16384.0000 (12843.0927)  weight_decay: 0.0500 (0.0500)  time: 0.8512  data: 0.0002  max mem: 26773
Epoch: [13]  [ 800/2669]  eta: 0:28:00  lr: 0.000451  min_lr: 0.000000  loss: 1.5527 (1.6414)  class_acc: 0.8500 (0.8304)  loss_scale: 16384.0000 (13285.1536)  weight_decay: 0.0500 (0.0500)  time: 0.8497  data: 0.0002  max mem: 26773
Epoch: [13]  [ 900/2669]  eta: 0:26:21  lr: 0.000450  min_lr: 0.000000  loss: 1.6367 (1.6407)  class_acc: 0.8000 (0.8300)  loss_scale: 16384.0000 (13847.2986)  weight_decay: 0.0500 (0.0500)  time: 0.8521  data: 0.0002  max mem: 26773
Epoch: [13]  [1000/2669]  eta: 0:24:45  lr: 0.000449  min_lr: 0.000000  loss: 1.6826 (1.6454)  class_acc: 0.8000 (0.8290)  loss_scale: 16384.0000 (14100.7153)  weight_decay: 0.0500 (0.0500)  time: 0.8553  data: 0.0003  max mem: 26773
Epoch: [13]  [1100/2669]  eta: 0:23:10  lr: 0.000448  min_lr: 0.000000  loss: 1.5576 (1.6496)  class_acc: 0.8500 (0.8287)  loss_scale: 16384.0000 (14308.0981)  weight_decay: 0.0500 (0.0500)  time: 0.8501  data: 0.0003  max mem: 26773
Epoch: [13]  [1200/2669]  eta: 0:21:37  lr: 0.000446  min_lr: 0.000000  loss: 1.6855 (1.6563)  class_acc: 0.8000 (0.8276)  loss_scale: 16384.0000 (14480.9459)  weight_decay: 0.0500 (0.0500)  time: 0.8502  data: 0.0003  max mem: 26773
Epoch: [13]  [1300/2669]  eta: 0:20:05  lr: 0.000445  min_lr: 0.000000  loss: 1.5205 (1.6573)  class_acc: 0.8500 (0.8278)  loss_scale: 16384.0000 (14627.2221)  weight_decay: 0.0500 (0.0500)  time: 0.8492  data: 0.0003  max mem: 26773
Epoch: [13]  [1400/2669]  eta: 0:18:35  lr: 0.000444  min_lr: 0.000000  loss: 1.6191 (1.6571)  class_acc: 0.8000 (0.8277)  loss_scale: 32768.0000 (15430.8979)  weight_decay: 0.0500 (0.0500)  time: 0.8493  data: 0.0003  max mem: 26773
Epoch: [13]  [1500/2669]  eta: 0:17:05  lr: 0.000443  min_lr: 0.000000  loss: 1.6279 (1.6584)  class_acc: 0.8500 (0.8268)  loss_scale: 16384.0000 (15909.1805)  weight_decay: 0.0500 (0.0500)  time: 0.8487  data: 0.0002  max mem: 26773
Epoch: [13]  [1600/2669]  eta: 0:15:35  lr: 0.000441  min_lr: 0.000000  loss: 1.5996 (1.6571)  class_acc: 0.8000 (0.8269)  loss_scale: 16384.0000 (15938.8382)  weight_decay: 0.0500 (0.0500)  time: 0.8505  data: 0.0002  max mem: 26773
Epoch: [13]  [1700/2669]  eta: 0:14:07  lr: 0.000440  min_lr: 0.000000  loss: 1.6299 (1.6575)  class_acc: 0.8500 (0.8268)  loss_scale: 16384.0000 (15965.0088)  weight_decay: 0.0500 (0.0500)  time: 0.8528  data: 0.0002  max mem: 26773
Epoch: [13]  [1800/2669]  eta: 0:12:38  lr: 0.000439  min_lr: 0.000000  loss: 1.6641 (1.6576)  class_acc: 0.8000 (0.8266)  loss_scale: 16384.0000 (15988.2732)  weight_decay: 0.0500 (0.0500)  time: 0.8498  data: 0.0003  max mem: 26773
Epoch: [13]  [1900/2669]  eta: 0:11:10  lr: 0.000438  min_lr: 0.000000  loss: 1.6172 (1.6586)  class_acc: 0.8500 (0.8265)  loss_scale: 16384.0000 (16009.0900)  weight_decay: 0.0500 (0.0500)  time: 0.8499  data: 0.0003  max mem: 26773
Epoch: [13]  [2000/2669]  eta: 0:09:42  lr: 0.000436  min_lr: 0.000000  loss: 1.7471 (1.6599)  class_acc: 0.8000 (0.8264)  loss_scale: 32768.0000 (16404.4698)  weight_decay: 0.0500 (0.0500)  time: 0.8649  data: 0.0003  max mem: 26773
Epoch: [13]  [2100/2669]  eta: 0:08:15  lr: 0.000435  min_lr: 0.000000  loss: 1.6992 (1.6612)  class_acc: 0.8000 (0.8257)  loss_scale: 16384.0000 (16855.7906)  weight_decay: 0.0500 (0.0500)  time: 0.8649  data: 0.0002  max mem: 26773
Epoch: [13]  [2200/2669]  eta: 0:06:47  lr: 0.000434  min_lr: 0.000000  loss: 1.6660 (1.6634)  class_acc: 0.8000 (0.8251)  loss_scale: 16384.0000 (16834.3553)  weight_decay: 0.0500 (0.0500)  time: 0.8471  data: 0.0002  max mem: 26773
Epoch: [13]  [2300/2669]  eta: 0:05:20  lr: 0.000433  min_lr: 0.000000  loss: 1.6152 (1.6635)  class_acc: 0.8500 (0.8249)  loss_scale: 16384.0000 (16814.7831)  weight_decay: 0.0500 (0.0500)  time: 0.8503  data: 0.0002  max mem: 26773
Epoch: [13]  [2400/2669]  eta: 0:03:53  lr: 0.000431  min_lr: 0.000000  loss: 1.8066 (1.6649)  class_acc: 0.8000 (0.8244)  loss_scale: 16384.0000 (16796.8413)  weight_decay: 0.0500 (0.0500)  time: 0.8526  data: 0.0002  max mem: 26773
Epoch: [13]  [2500/2669]  eta: 0:02:26  lr: 0.000430  min_lr: 0.000000  loss: 1.6152 (1.6642)  class_acc: 0.8000 (0.8245)  loss_scale: 16384.0000 (16780.3343)  weight_decay: 0.0500 (0.0500)  time: 0.8498  data: 0.0002  max mem: 26773
Epoch: [13]  [2600/2669]  eta: 0:00:59  lr: 0.000429  min_lr: 0.000000  loss: 1.5986 (1.6638)  class_acc: 0.8500 (0.8243)  loss_scale: 32768.0000 (16928.8735)  weight_decay: 0.0500 (0.0500)  time: 0.8485  data: 0.0002  max mem: 26773
Epoch: [13]  [2668/2669]  eta: 0:00:00  lr: 0.000428  min_lr: 0.000000  loss: 1.4980 (1.6634)  class_acc: 0.8500 (0.8245)  loss_scale: 16384.0000 (17099.4183)  weight_decay: 0.0500 (0.0500)  time: 0.8161  data: 0.0009  max mem: 26773
Epoch: [13] Total time: 0:38:32 (0.8665 s / it)
Averaged stats: lr: 0.000428  min_lr: 0.000000  loss: 1.4980 (1.6690)  class_acc: 0.8500 (0.8243)  loss_scale: 16384.0000 (17099.4183)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Test:  [  0/105]  eta: 0:07:23  loss: 0.2265 (0.2265)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 4.2243  data: 3.9268  max mem: 26773
Test:  [104/105]  eta: 0:00:00  loss: 0.4540 (0.5739)  acc1: 90.0000 (87.3321)  acc5: 100.0000 (98.2246)  time: 0.2868  data: 0.0002  max mem: 26773
Test: Total time: 0:00:35 (0.3359 s / it)
* Acc@1 87.342 Acc@5 98.179 loss 0.570
Accuracy of the network on the 50000 test images: 87.3%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Test:  [  0/105]  eta: 0:05:50  loss: 0.3027 (0.3027)  acc1: 90.0000 (90.0000)  acc5: 100.0000 (100.0000)  time: 3.3405  data: 3.0428  max mem: 26773
Test:  [104/105]  eta: 0:00:00  loss: 0.4114 (0.5240)  acc1: 90.0000 (88.1478)  acc5: 100.0000 (98.3205)  time: 0.2895  data: 0.0002  max mem: 26773
Test: Total time: 0:00:35 (0.3382 s / it)
* Acc@1 88.070 Acc@5 98.556 loss 0.515
EMA Accuracy of the network on the 50000 test images: 88.1%
Max accuracy: 88.07%
{"train_lr": 0.0004447150148986703, "train_min_lr": 9.35224141148522e-09, "train_loss": 1.669003266921227, "train_class_acc": 0.8242589782724257, "train_loss_scale": 17099.418290854574, "train_weight_decay": 0.04999999999999853, "test_loss": 0.5149416178937942, "test_acc1": 88.06981765834932, "test_acc5": 98.5564619321817, "epoch": 13, "n_parameters": 304532456}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Epoch: [14]  [   0/2669]  eta: 1 day, 5:47:30  lr: 0.000428  min_lr: 0.000000  loss: 1.6240 (1.6240)  class_acc: 0.9000 (0.9000)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 40.1838  data: 38.8461  max mem: 26773
Epoch: [14]  [ 100/2669]  eta: 0:53:04  lr: 0.000427  min_lr: 0.000000  loss: 1.5928 (1.6328)  class_acc: 0.8500 (0.8307)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8681  data: 0.0003  max mem: 26773
Epoch: [14]  [ 200/2669]  eta: 0:43:02  lr: 0.000425  min_lr: 0.000000  loss: 1.5791 (1.6263)  class_acc: 0.8500 (0.8328)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8485  data: 0.0003  max mem: 26773
Epoch: [14]  [ 300/2669]  eta: 0:38:41  lr: 0.000424  min_lr: 0.000000  loss: 1.4678 (1.6266)  class_acc: 0.8500 (0.8339)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8494  data: 0.0002  max mem: 26773
Epoch: [14]  [ 400/2669]  eta: 0:35:49  lr: 0.000423  min_lr: 0.000000  loss: 1.4268 (1.6290)  class_acc: 0.8000 (0.8325)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8485  data: 0.0003  max mem: 26773
Epoch: [14]  [ 500/2669]  eta: 0:33:33  lr: 0.000422  min_lr: 0.000000  loss: 1.5635 (1.6365)  class_acc: 0.8500 (0.8323)  loss_scale: 32768.0000 (17103.4571)  weight_decay: 0.0500 (0.0500)  time: 0.8493  data: 0.0003  max mem: 26773
Epoch: [14]  [ 600/2669]  eta: 0:31:34  lr: 0.000420  min_lr: 0.000000  loss: 1.5537 (1.6329)  class_acc: 0.8000 (0.8330)  loss_scale: 32768.0000 (19709.8702)  weight_decay: 0.0500 (0.0500)  time: 0.8511  data: 0.0003  max mem: 26773
Epoch: [14]  [ 700/2669]  eta: 0:29:44  lr: 0.000419  min_lr: 0.000000  loss: 1.6045 (1.6338)  class_acc: 0.8000 (0.8327)  loss_scale: 16384.0000 (20123.5720)  weight_decay: 0.0500 (0.0500)  time: 0.8617  data: 0.0003  max mem: 26773
Epoch: [14]  [ 800/2669]  eta: 0:28:01  lr: 0.000418  min_lr: 0.000000  loss: 1.6279 (1.6366)  class_acc: 0.8000 (0.8320)  loss_scale: 16384.0000 (19656.7091)  weight_decay: 0.0500 (0.0500)  time: 0.8641  data: 0.0003  max mem: 26773
Epoch: [14]  [ 900/2669]  eta: 0:26:22  lr: 0.000416  min_lr: 0.000000  loss: 1.6699 (1.6379)  class_acc: 0.8500 (0.8322)  loss_scale: 16384.0000 (19293.4784)  weight_decay: 0.0500 (0.0500)  time: 0.8502  data: 0.0002  max mem: 26773
Epoch: [14]  [1000/2669]  eta: 0:24:45  lr: 0.000415  min_lr: 0.000000  loss: 1.5791 (1.6400)  class_acc: 0.8500 (0.8313)  loss_scale: 16384.0000 (19002.8212)  weight_decay: 0.0500 (0.0500)  time: 0.8491  data: 0.0003  max mem: 26773
Epoch: [14]  [1100/2669]  eta: 0:23:11  lr: 0.000414  min_lr: 0.000000  loss: 1.5049 (1.6352)  class_acc: 0.8500 (0.8321)  loss_scale: 16384.0000 (18764.9628)  weight_decay: 0.0500 (0.0500)  time: 0.8503  data: 0.0002  max mem: 26773
Epoch: [14]  [1200/2669]  eta: 0:21:38  lr: 0.000413  min_lr: 0.000000  loss: 1.7891 (1.6339)  class_acc: 0.8000 (0.8327)  loss_scale: 32768.0000 (19194.2448)  weight_decay: 0.0500 (0.0500)  time: 0.8482  data: 0.0003  max mem: 26773
Epoch: [14]  [1300/2669]  eta: 0:20:06  lr: 0.000411  min_lr: 0.000000  loss: 1.5459 (1.6319)  class_acc: 0.8500 (0.8340)  loss_scale: 16384.0000 (19003.4251)  weight_decay: 0.0500 (0.0500)  time: 0.8506  data: 0.0002  max mem: 26773
Epoch: [14]  [1400/2669]  eta: 0:18:35  lr: 0.000410  min_lr: 0.000000  loss: 1.5010 (1.6309)  class_acc: 0.8000 (0.8336)  loss_scale: 16384.0000 (18816.4568)  weight_decay: 0.0500 (0.0500)  time: 0.8518  data: 0.0003  max mem: 26773
Epoch: [14]  [1500/2669]  eta: 0:17:05  lr: 0.000409  min_lr: 0.000000  loss: 1.5947 (1.6337)  class_acc: 0.8500 (0.8329)  loss_scale: 16384.0000 (18654.4011)  weight_decay: 0.0500 (0.0500)  time: 0.8510  data: 0.0002  max mem: 26773
Epoch: [14]  [1600/2669]  eta: 0:15:36  lr: 0.000407  min_lr: 0.000000  loss: 1.7021 (1.6334)  class_acc: 0.8500 (0.8332)  loss_scale: 16384.0000 (18512.5896)  weight_decay: 0.0500 (0.0500)  time: 0.8537  data: 0.0003  max mem: 26773
Epoch: [14]  [1700/2669]  eta: 0:14:07  lr: 0.000406  min_lr: 0.000000  loss: 1.5791 (1.6325)  class_acc: 0.8500 (0.8332)  loss_scale: 16384.0000 (18387.4521)  weight_decay: 0.0500 (0.0500)  time: 0.8509  data: 0.0003  max mem: 26773
Epoch: [14]  [1800/2669]  eta: 0:12:39  lr: 0.000405  min_lr: 0.000000  loss: 1.5996 (1.6335)  class_acc: 0.8500 (0.8328)  loss_scale: 32768.0000 (19022.1788)  weight_decay: 0.0500 (0.0500)  time: 0.8532  data: 0.0002  max mem: 26773
Epoch: [14]  [1900/2669]  eta: 0:11:11  lr: 0.000403  min_lr: 0.000000  loss: 1.5205 (1.6346)  class_acc: 0.8500 (0.8324)  loss_scale: 32768.0000 (19745.2625)  weight_decay: 0.0500 (0.0500)  time: 0.8618  data: 0.0002  max mem: 26773
Epoch: [14]  [2000/2669]  eta: 0:09:43  lr: 0.000402  min_lr: 0.000000  loss: 1.4404 (1.6353)  class_acc: 0.8500 (0.8321)  loss_scale: 16384.0000 (20281.4433)  weight_decay: 0.0500 (0.0500)  time: 0.8468  data: 0.0002  max mem: 26773
Epoch: [14]  [2100/2669]  eta: 0:08:15  lr: 0.000401  min_lr: 0.000000  loss: 1.6338 (1.6383)  class_acc: 0.8000 (0.8316)  loss_scale: 16384.0000 (20095.9391)  weight_decay: 0.0500 (0.0500)  time: 0.8524  data: 0.0003  max mem: 26773
Epoch: [14]  [2200/2669]  eta: 0:06:47  lr: 0.000399  min_lr: 0.000000  loss: 1.6758 (1.6391)  class_acc: 0.8000 (0.8311)  loss_scale: 16384.0000 (19927.2912)  weight_decay: 0.0500 (0.0500)  time: 0.8512  data: 0.0003  max mem: 26773
Epoch: [14]  [2300/2669]  eta: 0:05:20  lr: 0.000398  min_lr: 0.000000  loss: 1.7520 (1.6402)  class_acc: 0.8000 (0.8307)  loss_scale: 16384.0000 (19773.3020)  weight_decay: 0.0500 (0.0500)  time: 0.8512  data: 0.0003  max mem: 26773
Epoch: [14]  [2400/2669]  eta: 0:03:53  lr: 0.000397  min_lr: 0.000000  loss: 1.6504 (1.6432)  class_acc: 0.8000 (0.8297)  loss_scale: 16384.0000 (19632.1399)  weight_decay: 0.0500 (0.0500)  time: 0.8489  data: 0.0002  max mem: 26773
Epoch: [14]  [2500/2669]  eta: 0:02:26  lr: 0.000395  min_lr: 0.000000  loss: 1.6445 (1.6450)  class_acc: 0.8500 (0.8290)  loss_scale: 16384.0000 (19502.2663)  weight_decay: 0.0500 (0.0500)  time: 0.8498  data: 0.0003  max mem: 26773
Epoch: [14]  [2600/2669]  eta: 0:00:59  lr: 0.000394  min_lr: 0.000000  loss: 1.6133 (1.6459)  class_acc: 0.8500 (0.8289)  loss_scale: 32768.0000 (19999.6924)  weight_decay: 0.0500 (0.0500)  time: 0.8507  data: 0.0003  max mem: 26773
Epoch: [14]  [2668/2669]  eta: 0:00:00  lr: 0.000393  min_lr: 0.000000  loss: 1.5703 (1.6464)  class_acc: 0.8500 (0.8287)  loss_scale: 32768.0000 (20320.3358)  weight_decay: 0.0500 (0.0500)  time: 0.8292  data: 0.0009  max mem: 26773
Epoch: [14] Total time: 0:38:33 (0.8669 s / it)
Averaged stats: lr: 0.000393  min_lr: 0.000000  loss: 1.5703 (1.6480)  class_acc: 0.8500 (0.8300)  loss_scale: 32768.0000 (20320.3358)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Test:  [  0/105]  eta: 0:04:44  loss: 0.2512 (0.2512)  acc1: 90.0000 (90.0000)  acc5: 100.0000 (100.0000)  time: 2.7055  data: 2.4059  max mem: 26773
Test:  [104/105]  eta: 0:00:00  loss: 0.4956 (0.5830)  acc1: 85.0000 (87.7639)  acc5: 100.0000 (98.0326)  time: 0.2867  data: 0.0002  max mem: 26773
Test: Total time: 0:00:35 (0.3357 s / it)
* Acc@1 86.946 Acc@5 98.037 loss 0.587
Accuracy of the network on the 50000 test images: 86.9%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Test:  [  0/105]  eta: 0:06:05  loss: 0.2780 (0.2780)  acc1: 90.0000 (90.0000)  acc5: 100.0000 (100.0000)  time: 3.4823  data: 3.1844  max mem: 26773
Test:  [104/105]  eta: 0:00:00  loss: 0.4056 (0.5096)  acc1: 90.0000 (88.2917)  acc5: 100.0000 (98.4165)  time: 0.2895  data: 0.0002  max mem: 26773
Test: Total time: 0:00:35 (0.3375 s / it)
* Acc@1 88.140 Acc@5 98.582 loss 0.500
EMA Accuracy of the network on the 50000 test images: 88.1%
Max accuracy: 88.14%
{"train_lr": 0.00041070698837908663, "train_min_lr": 8.637061434906624e-09, "train_loss": 1.648026145618597, "train_class_acc": 0.8299592533514186, "train_loss_scale": 20320.335832083958, "train_weight_decay": 0.04999999999999853, "test_loss": 0.4997927087166953, "test_acc1": 88.13979526551503, "test_acc5": 98.58245361484325, "epoch": 14, "n_parameters": 304532456}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Epoch: [15]  [   0/2669]  eta: 1 day, 6:18:17  lr: 0.000393  min_lr: 0.000000  loss: 1.1543 (1.1543)  class_acc: 0.9500 (0.9500)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 40.8759  data: 30.3503  max mem: 26773
Epoch: [15]  [ 100/2669]  eta: 0:53:08  lr: 0.000392  min_lr: 0.000000  loss: 1.6484 (1.6251)  class_acc: 0.8500 (0.8381)  loss_scale: 16384.0000 (22710.4950)  weight_decay: 0.0500 (0.0500)  time: 0.8468  data: 0.0003  max mem: 26773
Epoch: [15]  [ 200/2669]  eta: 0:43:04  lr: 0.000390  min_lr: 0.000000  loss: 1.4277 (1.6165)  class_acc: 0.8500 (0.8423)  loss_scale: 16384.0000 (19562.9851)  weight_decay: 0.0500 (0.0500)  time: 0.8505  data: 0.0002  max mem: 26773
Epoch: [15]  [ 300/2669]  eta: 0:38:45  lr: 0.000389  min_lr: 0.000000  loss: 1.6035 (1.6105)  class_acc: 0.8000 (0.8414)  loss_scale: 16384.0000 (18506.8439)  weight_decay: 0.0500 (0.0500)  time: 0.8598  data: 0.0003  max mem: 26773
Epoch: [15]  [ 400/2669]  eta: 0:35:54  lr: 0.000388  min_lr: 0.000000  loss: 1.7344 (1.6262)  class_acc: 0.8000 (0.8368)  loss_scale: 16384.0000 (17977.4564)  weight_decay: 0.0500 (0.0500)  time: 0.8646  data: 0.0003  max mem: 26773
Epoch: [15]  [ 500/2669]  eta: 0:33:35  lr: 0.000386  min_lr: 0.000000  loss: 1.6602 (1.6302)  class_acc: 0.8500 (0.8363)  loss_scale: 16384.0000 (17659.4012)  weight_decay: 0.0500 (0.0500)  time: 0.8478  data: 0.0003  max mem: 26773
Epoch: [15]  [ 600/2669]  eta: 0:31:35  lr: 0.000385  min_lr: 0.000000  loss: 1.6074 (1.6264)  class_acc: 0.8500 (0.8374)  loss_scale: 32768.0000 (18701.2047)  weight_decay: 0.0500 (0.0500)  time: 0.8504  data: 0.0003  max mem: 26773
Epoch: [15]  [ 700/2669]  eta: 0:29:46  lr: 0.000384  min_lr: 0.000000  loss: 1.5000 (1.6219)  class_acc: 0.8500 (0.8382)  loss_scale: 16384.0000 (19258.7960)  weight_decay: 0.0500 (0.0500)  time: 0.8483  data: 0.0002  max mem: 26773
Epoch: [15]  [ 800/2669]  eta: 0:28:03  lr: 0.000382  min_lr: 0.000000  loss: 1.5996 (1.6222)  class_acc: 0.8500 (0.8376)  loss_scale: 16384.0000 (18899.8951)  weight_decay: 0.0500 (0.0500)  time: 0.8618  data: 0.0002  max mem: 26773
Epoch: [15]  [ 900/2669]  eta: 0:26:23  lr: 0.000381  min_lr: 0.000000  loss: 1.4746 (1.6125)  class_acc: 0.8500 (0.8403)  loss_scale: 16384.0000 (18620.6615)  weight_decay: 0.0500 (0.0500)  time: 0.8485  data: 0.0003  max mem: 26773
Epoch: [15]  [1000/2669]  eta: 0:24:46  lr: 0.000380  min_lr: 0.000000  loss: 1.6768 (1.6217)  class_acc: 0.8500 (0.8384)  loss_scale: 16384.0000 (18397.2188)  weight_decay: 0.0500 (0.0500)  time: 0.8475  data: 0.0003  max mem: 26773
Epoch: [15]  [1100/2669]  eta: 0:23:11  lr: 0.000378  min_lr: 0.000000  loss: 1.6260 (1.6221)  class_acc: 0.8000 (0.8381)  loss_scale: 16384.0000 (18214.3651)  weight_decay: 0.0500 (0.0500)  time: 0.8491  data: 0.0003  max mem: 26773
Epoch: [15]  [1200/2669]  eta: 0:21:38  lr: 0.000377  min_lr: 0.000000  loss: 1.6387 (1.6222)  class_acc: 0.8000 (0.8378)  loss_scale: 32768.0000 (18689.4921)  weight_decay: 0.0500 (0.0500)  time: 0.8611  data: 0.0002  max mem: 26773
Epoch: [15]  [1300/2669]  eta: 0:20:06  lr: 0.000375  min_lr: 0.000000  loss: 1.5137 (1.6229)  class_acc: 0.8500 (0.8376)  loss_scale: 16384.0000 (18638.2168)  weight_decay: 0.0500 (0.0500)  time: 0.8503  data: 0.0002  max mem: 26773
Epoch: [15]  [1400/2669]  eta: 0:18:35  lr: 0.000374  min_lr: 0.000000  loss: 1.5127 (1.6230)  class_acc: 0.8500 (0.8378)  loss_scale: 16384.0000 (18477.3162)  weight_decay: 0.0500 (0.0500)  time: 0.8592  data: 0.0003  max mem: 26773
Epoch: [15]  [1500/2669]  eta: 0:17:05  lr: 0.000373  min_lr: 0.000000  loss: 1.5195 (1.6248)  class_acc: 0.8500 (0.8372)  loss_scale: 16384.0000 (18337.8548)  weight_decay: 0.0500 (0.0500)  time: 0.8497  data: 0.0003  max mem: 26773
Epoch: [15]  [1600/2669]  eta: 0:15:36  lr: 0.000371  min_lr: 0.000000  loss: 1.6367 (1.6255)  class_acc: 0.8500 (0.8369)  loss_scale: 16384.0000 (18215.8151)  weight_decay: 0.0500 (0.0500)  time: 0.8485  data: 0.0003  max mem: 26773
Epoch: [15]  [1700/2669]  eta: 0:14:07  lr: 0.000370  min_lr: 0.000000  loss: 1.5400 (1.6249)  class_acc: 0.8500 (0.8371)  loss_scale: 16384.0000 (18108.1246)  weight_decay: 0.0500 (0.0500)  time: 0.8513  data: 0.0002  max mem: 26773
Epoch: [15]  [1800/2669]  eta: 0:12:38  lr: 0.000369  min_lr: 0.000000  loss: 1.5273 (1.6288)  class_acc: 0.8500 (0.8357)  loss_scale: 32768.0000 (18685.5836)  weight_decay: 0.0500 (0.0500)  time: 0.8655  data: 0.0003  max mem: 26773
Epoch: [15]  [1900/2669]  eta: 0:11:10  lr: 0.000367  min_lr: 0.000000  loss: 1.5635 (1.6274)  class_acc: 0.8500 (0.8356)  loss_scale: 32768.0000 (19426.3735)  weight_decay: 0.0500 (0.0500)  time: 0.8506  data: 0.0003  max mem: 26773
Epoch: [15]  [2000/2669]  eta: 0:09:42  lr: 0.000366  min_lr: 0.000000  loss: 1.6553 (1.6262)  class_acc: 0.8500 (0.8356)  loss_scale: 16384.0000 (19552.7196)  weight_decay: 0.0500 (0.0500)  time: 0.8488  data: 0.0003  max mem: 26773
Epoch: [15]  [2100/2669]  eta: 0:08:15  lr: 0.000364  min_lr: 0.000000  loss: 1.5518 (1.6282)  class_acc: 0.8500 (0.8351)  loss_scale: 16384.0000 (19401.9000)  weight_decay: 0.0500 (0.0500)  time: 0.8503  data: 0.0003  max mem: 26773
Epoch: [15]  [2200/2669]  eta: 0:06:47  lr: 0.000363  min_lr: 0.000000  loss: 1.5693 (1.6304)  class_acc: 0.8500 (0.8350)  loss_scale: 16384.0000 (19264.7851)  weight_decay: 0.0500 (0.0500)  time: 0.8496  data: 0.0002  max mem: 26773
Epoch: [15]  [2300/2669]  eta: 0:05:20  lr: 0.000362  min_lr: 0.000000  loss: 1.6260 (1.6308)  class_acc: 0.8500 (0.8349)  loss_scale: 16384.0000 (19139.5880)  weight_decay: 0.0500 (0.0500)  time: 0.8496  data: 0.0002  max mem: 26773
Epoch: [15]  [2400/2669]  eta: 0:03:53  lr: 0.000360  min_lr: 0.000000  loss: 1.7021 (1.6304)  class_acc: 0.8500 (0.8352)  loss_scale: 16384.0000 (19024.8197)  weight_decay: 0.0500 (0.0500)  time: 0.8496  data: 0.0003  max mem: 26773
Epoch: [15]  [2500/2669]  eta: 0:02:26  lr: 0.000359  min_lr: 0.000000  loss: 1.5742 (1.6305)  class_acc: 0.8500 (0.8353)  loss_scale: 16384.0000 (19024.0448)  weight_decay: 0.0500 (0.0500)  time: 0.8481  data: 0.0002  max mem: 26773
Epoch: [15]  [2600/2669]  eta: 0:00:59  lr: 0.000358  min_lr: 0.000000  loss: 1.6748 (1.6294)  class_acc: 0.8500 (0.8358)  loss_scale: 16384.0000 (18922.5436)  weight_decay: 0.0500 (0.0500)  time: 0.8464  data: 0.0003  max mem: 26773
Epoch: [15]  [2668/2669]  eta: 0:00:00  lr: 0.000357  min_lr: 0.000000  loss: 1.5459 (1.6298)  class_acc: 0.8500 (0.8356)  loss_scale: 16384.0000 (18858.7946)  weight_decay: 0.0500 (0.0500)  time: 0.8152  data: 0.0009  max mem: 26773
Epoch: [15] Total time: 0:38:31 (0.8661 s / it)
Averaged stats: lr: 0.000357  min_lr: 0.000000  loss: 1.5459 (1.6246)  class_acc: 0.8500 (0.8368)  loss_scale: 16384.0000 (18858.7946)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Test:  [  0/105]  eta: 0:07:11  loss: 0.3572 (0.3572)  acc1: 85.0000 (85.0000)  acc5: 100.0000 (100.0000)  time: 4.1104  data: 3.8117  max mem: 26773
Test:  [104/105]  eta: 0:00:00  loss: 0.4607 (0.5914)  acc1: 90.0000 (87.8599)  acc5: 100.0000 (98.1766)  time: 0.2869  data: 0.0002  max mem: 26773
Test: Total time: 0:00:35 (0.3349 s / it)
* Acc@1 87.200 Acc@5 98.105 loss 0.580
Accuracy of the network on the 50000 test images: 87.2%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Test:  [  0/105]  eta: 0:07:31  loss: 0.2616 (0.2616)  acc1: 90.0000 (90.0000)  acc5: 100.0000 (100.0000)  time: 4.2993  data: 4.0015  max mem: 26773
Test:  [104/105]  eta: 0:00:00  loss: 0.4032 (0.5028)  acc1: 90.0000 (88.5317)  acc5: 100.0000 (98.4165)  time: 0.2896  data: 0.0002  max mem: 26773
Test: Total time: 0:00:35 (0.3394 s / it)
* Acc@1 88.206 Acc@5 98.580 loss 0.492
EMA Accuracy of the network on the 50000 test images: 88.2%
Max accuracy: 88.21%
{"train_lr": 0.000374960931818298, "train_min_lr": 7.885331137378312e-09, "train_loss": 1.6246447241467157, "train_class_acc": 0.836769129800285, "train_loss_scale": 18858.79460269865, "train_weight_decay": 0.04999999999999853, "test_loss": 0.4920577290305306, "test_acc1": 88.20577415227127, "test_acc5": 98.58045425463851, "epoch": 15, "n_parameters": 304532456}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Epoch: [16]  [   0/2669]  eta: 1 day, 5:29:28  lr: 0.000357  min_lr: 0.000000  loss: 1.6074 (1.6074)  class_acc: 0.8500 (0.8500)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 39.7783  data: 37.1650  max mem: 26773
Epoch: [16]  [ 100/2669]  eta: 0:52:50  lr: 0.000355  min_lr: 0.000000  loss: 1.4668 (1.6270)  class_acc: 0.8500 (0.8381)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8479  data: 0.0003  max mem: 26773
Epoch: [16]  [ 200/2669]  eta: 0:42:51  lr: 0.000354  min_lr: 0.000000  loss: 1.5684 (1.6294)  class_acc: 0.8500 (0.8338)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8476  data: 0.0002  max mem: 26773
Epoch: [16]  [ 300/2669]  eta: 0:38:37  lr: 0.000352  min_lr: 0.000000  loss: 1.4951 (1.6114)  class_acc: 0.8000 (0.8382)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8514  data: 0.0003  max mem: 26773
Epoch: [16]  [ 400/2669]  eta: 0:35:50  lr: 0.000351  min_lr: 0.000000  loss: 1.5742 (1.6068)  class_acc: 0.8500 (0.8410)  loss_scale: 32768.0000 (19897.7756)  weight_decay: 0.0500 (0.0500)  time: 0.8499  data: 0.0003  max mem: 26773
Epoch: [16]  [ 500/2669]  eta: 0:33:35  lr: 0.000350  min_lr: 0.000000  loss: 1.6191 (1.6025)  class_acc: 0.8500 (0.8417)  loss_scale: 32768.0000 (22466.6826)  weight_decay: 0.0500 (0.0500)  time: 0.8627  data: 0.0002  max mem: 26773
Epoch: [16]  [ 600/2669]  eta: 0:31:35  lr: 0.000348  min_lr: 0.000000  loss: 1.4775 (1.6023)  class_acc: 0.9000 (0.8420)  loss_scale: 16384.0000 (23690.0100)  weight_decay: 0.0500 (0.0500)  time: 0.8479  data: 0.0003  max mem: 26773
Epoch: [16]  [ 700/2669]  eta: 0:29:45  lr: 0.000347  min_lr: 0.000000  loss: 1.5928 (1.5969)  class_acc: 0.8500 (0.8437)  loss_scale: 16384.0000 (22647.7832)  weight_decay: 0.0500 (0.0500)  time: 0.8542  data: 0.0002  max mem: 26773
Epoch: [16]  [ 800/2669]  eta: 0:28:02  lr: 0.000345  min_lr: 0.000000  loss: 1.5186 (1.5907)  class_acc: 0.8500 (0.8444)  loss_scale: 16384.0000 (21865.7878)  weight_decay: 0.0500 (0.0500)  time: 0.8520  data: 0.0002  max mem: 26773
Epoch: [16]  [ 900/2669]  eta: 0:26:22  lr: 0.000344  min_lr: 0.000000  loss: 1.4502 (1.5945)  class_acc: 0.8500 (0.8445)  loss_scale: 16384.0000 (21257.3762)  weight_decay: 0.0500 (0.0500)  time: 0.8509  data: 0.0003  max mem: 26773
Epoch: [16]  [1000/2669]  eta: 0:24:46  lr: 0.000343  min_lr: 0.000000  loss: 1.5488 (1.5941)  class_acc: 0.8500 (0.8441)  loss_scale: 16384.0000 (20770.5255)  weight_decay: 0.0500 (0.0500)  time: 0.8532  data: 0.0003  max mem: 26773
Epoch: [16]  [1100/2669]  eta: 0:23:11  lr: 0.000341  min_lr: 0.000000  loss: 1.5947 (1.5943)  class_acc: 0.8500 (0.8440)  loss_scale: 16384.0000 (20401.8747)  weight_decay: 0.0500 (0.0500)  time: 0.8490  data: 0.0002  max mem: 26773
Epoch: [16]  [1200/2669]  eta: 0:21:39  lr: 0.000340  min_lr: 0.000000  loss: 1.6729 (1.5943)  class_acc: 0.8500 (0.8438)  loss_scale: 32768.0000 (21349.6753)  weight_decay: 0.0500 (0.0500)  time: 0.8493  data: 0.0002  max mem: 26773
Epoch: [16]  [1300/2669]  eta: 0:20:07  lr: 0.000339  min_lr: 0.000000  loss: 1.6172 (1.5937)  class_acc: 0.8000 (0.8444)  loss_scale: 16384.0000 (20967.9939)  weight_decay: 0.0500 (0.0500)  time: 0.8511  data: 0.0002  max mem: 26773
Epoch: [16]  [1400/2669]  eta: 0:18:36  lr: 0.000337  min_lr: 0.000000  loss: 1.4688 (1.5910)  class_acc: 0.8500 (0.8450)  loss_scale: 16384.0000 (20640.7994)  weight_decay: 0.0500 (0.0500)  time: 0.8528  data: 0.0003  max mem: 26773
Epoch: [16]  [1500/2669]  eta: 0:17:06  lr: 0.000336  min_lr: 0.000000  loss: 1.4395 (1.5892)  class_acc: 0.9000 (0.8457)  loss_scale: 16384.0000 (20357.2019)  weight_decay: 0.0500 (0.0500)  time: 0.8519  data: 0.0002  max mem: 26773
Epoch: [16]  [1600/2669]  eta: 0:15:36  lr: 0.000334  min_lr: 0.000000  loss: 1.6064 (1.5903)  class_acc: 0.8500 (0.8450)  loss_scale: 16384.0000 (20109.0319)  weight_decay: 0.0500 (0.0500)  time: 0.8625  data: 0.0003  max mem: 26773
Epoch: [16]  [1700/2669]  eta: 0:14:07  lr: 0.000333  min_lr: 0.000000  loss: 1.6348 (1.5887)  class_acc: 0.8500 (0.8453)  loss_scale: 16384.0000 (19890.0412)  weight_decay: 0.0500 (0.0500)  time: 0.8492  data: 0.0003  max mem: 26773
Epoch: [16]  [1800/2669]  eta: 0:12:39  lr: 0.000332  min_lr: 0.000000  loss: 1.5176 (1.5855)  class_acc: 0.8500 (0.8456)  loss_scale: 32768.0000 (20514.1144)  weight_decay: 0.0500 (0.0500)  time: 0.8619  data: 0.0002  max mem: 26773
Epoch: [16]  [1900/2669]  eta: 0:11:11  lr: 0.000330  min_lr: 0.000000  loss: 1.4199 (1.5846)  class_acc: 0.8500 (0.8458)  loss_scale: 16384.0000 (20383.0405)  weight_decay: 0.0500 (0.0500)  time: 0.8495  data: 0.0003  max mem: 26773
Epoch: [16]  [2000/2669]  eta: 0:09:43  lr: 0.000329  min_lr: 0.000000  loss: 1.4902 (1.5846)  class_acc: 0.8500 (0.8457)  loss_scale: 16384.0000 (20183.1884)  weight_decay: 0.0500 (0.0500)  time: 0.8544  data: 0.0002  max mem: 26773
Epoch: [16]  [2100/2669]  eta: 0:08:15  lr: 0.000327  min_lr: 0.000000  loss: 1.4199 (1.5846)  class_acc: 0.8500 (0.8456)  loss_scale: 16384.0000 (20002.3608)  weight_decay: 0.0500 (0.0500)  time: 0.8549  data: 0.0003  max mem: 26773
Epoch: [16]  [2200/2669]  eta: 0:06:47  lr: 0.000326  min_lr: 0.000000  loss: 1.6074 (1.5852)  class_acc: 0.8500 (0.8457)  loss_scale: 16384.0000 (19837.9646)  weight_decay: 0.0500 (0.0500)  time: 0.8497  data: 0.0002  max mem: 26773
Epoch: [16]  [2300/2669]  eta: 0:05:20  lr: 0.000324  min_lr: 0.000000  loss: 1.4492 (1.5857)  class_acc: 0.8500 (0.8454)  loss_scale: 16384.0000 (19687.8575)  weight_decay: 0.0500 (0.0500)  time: 0.8509  data: 0.0002  max mem: 26773
Epoch: [16]  [2400/2669]  eta: 0:03:53  lr: 0.000323  min_lr: 0.000000  loss: 1.3867 (1.5860)  class_acc: 0.8500 (0.8453)  loss_scale: 32768.0000 (20055.2170)  weight_decay: 0.0500 (0.0500)  time: 0.8504  data: 0.0002  max mem: 26773
Epoch: [16]  [2500/2669]  eta: 0:02:26  lr: 0.000322  min_lr: 0.000000  loss: 1.6494 (1.5856)  class_acc: 0.8000 (0.8453)  loss_scale: 32768.0000 (20563.5250)  weight_decay: 0.0500 (0.0500)  time: 0.8512  data: 0.0002  max mem: 26773
Epoch: [16]  [2600/2669]  eta: 0:00:59  lr: 0.000320  min_lr: 0.000000  loss: 1.6484 (1.5874)  class_acc: 0.8000 (0.8448)  loss_scale: 32768.0000 (21032.7474)  weight_decay: 0.0500 (0.0500)  time: 0.8545  data: 0.0003  max mem: 26773
Epoch: [16]  [2668/2669]  eta: 0:00:00  lr: 0.000319  min_lr: 0.000000  loss: 1.4688 (1.5868)  class_acc: 0.8500 (0.8452)  loss_scale: 32768.0000 (21327.4483)  weight_decay: 0.0500 (0.0500)  time: 0.8205  data: 0.0009  max mem: 26773
Epoch: [16] Total time: 0:38:34 (0.8672 s / it)
Averaged stats: lr: 0.000319  min_lr: 0.000000  loss: 1.4688 (1.6012)  class_acc: 0.8500 (0.8433)  loss_scale: 32768.0000 (21327.4483)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Test:  [  0/105]  eta: 0:05:34  loss: 0.2251 (0.2251)  acc1: 95.0000 (95.0000)  acc5: 100.0000 (100.0000)  time: 3.1869  data: 2.8772  max mem: 26773
Test:  [104/105]  eta: 0:00:00  loss: 0.4759 (0.5935)  acc1: 90.0000 (86.9482)  acc5: 100.0000 (98.0326)  time: 0.2869  data: 0.0002  max mem: 26773
Test: Total time: 0:00:34 (0.3329 s / it)
* Acc@1 87.244 Acc@5 98.027 loss 0.588
Accuracy of the network on the 50000 test images: 87.2%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Test:  [  0/105]  eta: 0:05:59  loss: 0.2494 (0.2494)  acc1: 90.0000 (90.0000)  acc5: 100.0000 (100.0000)  time: 3.4238  data: 3.1079  max mem: 26773
Test:  [104/105]  eta: 0:00:00  loss: 0.4000 (0.5003)  acc1: 90.0000 (88.5797)  acc5: 100.0000 (98.4165)  time: 0.2895  data: 0.0001  max mem: 26773
Test: Total time: 0:00:35 (0.3355 s / it)
* Acc@1 88.260 Acc@5 98.594 loss 0.489
EMA Accuracy of the network on the 50000 test images: 88.3%
Max accuracy: 88.26%
{"train_lr": 0.00033804058188193045, "train_min_lr": 7.108905754754107e-09, "train_loss": 1.601180107578047, "train_class_acc": 0.8433049246263893, "train_loss_scale": 21327.44827586207, "train_weight_decay": 0.04999999999999853, "test_loss": 0.4888446110847687, "test_acc1": 88.2597568777991, "test_acc5": 98.59444977607166, "epoch": 16, "n_parameters": 304532456}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Epoch: [17]  [   0/2669]  eta: 1 day, 4:52:02  lr: 0.000319  min_lr: 0.000000  loss: 1.3965 (1.3965)  class_acc: 0.9500 (0.9500)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 38.9368  data: 29.6300  max mem: 26773
Epoch: [17]  [ 100/2669]  eta: 0:52:22  lr: 0.000318  min_lr: 0.000000  loss: 1.6016 (1.5069)  class_acc: 0.8500 (0.8718)  loss_scale: 16384.0000 (25305.9802)  weight_decay: 0.0500 (0.0500)  time: 0.8484  data: 0.0003  max mem: 26773
Epoch: [17]  [ 200/2669]  eta: 0:42:41  lr: 0.000316  min_lr: 0.000000  loss: 1.5146 (1.5429)  class_acc: 0.8500 (0.8602)  loss_scale: 16384.0000 (20867.1841)  weight_decay: 0.0500 (0.0500)  time: 0.8500  data: 0.0003  max mem: 26773
Epoch: [17]  [ 300/2669]  eta: 0:38:31  lr: 0.000315  min_lr: 0.000000  loss: 1.4766 (1.5601)  class_acc: 0.8500 (0.8561)  loss_scale: 16384.0000 (19377.7542)  weight_decay: 0.0500 (0.0500)  time: 0.8492  data: 0.0003  max mem: 26773
Epoch: [17]  [ 400/2669]  eta: 0:35:43  lr: 0.000314  min_lr: 0.000000  loss: 1.4004 (1.5556)  class_acc: 0.9000 (0.8584)  loss_scale: 16384.0000 (18631.1820)  weight_decay: 0.0500 (0.0500)  time: 0.8509  data: 0.0003  max mem: 26773
Epoch: [17]  [ 500/2669]  eta: 0:33:28  lr: 0.000312  min_lr: 0.000000  loss: 1.4600 (1.5618)  class_acc: 0.8500 (0.8560)  loss_scale: 16384.0000 (18182.6427)  weight_decay: 0.0500 (0.0500)  time: 0.8482  data: 0.0003  max mem: 26773
Epoch: [17]  [ 600/2669]  eta: 0:31:29  lr: 0.000311  min_lr: 0.000000  loss: 1.5781 (1.5620)  class_acc: 0.8500 (0.8554)  loss_scale: 32768.0000 (18701.2047)  weight_decay: 0.0500 (0.0500)  time: 0.8493  data: 0.0003  max mem: 26773
Epoch: [17]  [ 700/2669]  eta: 0:29:40  lr: 0.000309  min_lr: 0.000000  loss: 1.4912 (1.5588)  class_acc: 0.8500 (0.8557)  loss_scale: 16384.0000 (19819.7318)  weight_decay: 0.0500 (0.0500)  time: 0.8497  data: 0.0003  max mem: 26773
Epoch: [17]  [ 800/2669]  eta: 0:27:57  lr: 0.000308  min_lr: 0.000000  loss: 1.5137 (1.5617)  class_acc: 0.8500 (0.8546)  loss_scale: 16384.0000 (19390.8015)  weight_decay: 0.0500 (0.0500)  time: 0.8494  data: 0.0003  max mem: 26773
Epoch: [17]  [ 900/2669]  eta: 0:26:19  lr: 0.000307  min_lr: 0.000000  loss: 1.3975 (1.5610)  class_acc: 0.9000 (0.8552)  loss_scale: 16384.0000 (19057.0832)  weight_decay: 0.0500 (0.0500)  time: 0.8675  data: 0.0002  max mem: 26773
Epoch: [17]  [1000/2669]  eta: 0:24:43  lr: 0.000305  min_lr: 0.000000  loss: 1.4541 (1.5563)  class_acc: 0.8500 (0.8566)  loss_scale: 16384.0000 (18790.0420)  weight_decay: 0.0500 (0.0500)  time: 0.8501  data: 0.0002  max mem: 26773
Epoch: [17]  [1100/2669]  eta: 0:23:09  lr: 0.000304  min_lr: 0.000000  loss: 1.4551 (1.5576)  class_acc: 0.8500 (0.8555)  loss_scale: 16384.0000 (18571.5095)  weight_decay: 0.0500 (0.0500)  time: 0.8512  data: 0.0003  max mem: 26773
Epoch: [17]  [1200/2669]  eta: 0:21:36  lr: 0.000302  min_lr: 0.000000  loss: 1.5986 (1.5594)  class_acc: 0.8500 (0.8548)  loss_scale: 32768.0000 (18689.4921)  weight_decay: 0.0500 (0.0500)  time: 0.8521  data: 0.0003  max mem: 26773
Epoch: [17]  [1300/2669]  eta: 0:20:05  lr: 0.000301  min_lr: 0.000000  loss: 1.7041 (1.5617)  class_acc: 0.8000 (0.8541)  loss_scale: 32768.0000 (19771.6218)  weight_decay: 0.0500 (0.0500)  time: 0.8507  data: 0.0003  max mem: 26773
Epoch: [17]  [1400/2669]  eta: 0:18:34  lr: 0.000300  min_lr: 0.000000  loss: 1.3789 (1.5582)  class_acc: 0.9000 (0.8549)  loss_scale: 32768.0000 (20699.2719)  weight_decay: 0.0500 (0.0500)  time: 0.8513  data: 0.0003  max mem: 26773
Epoch: [17]  [1500/2669]  eta: 0:17:04  lr: 0.000298  min_lr: 0.000000  loss: 1.5459 (1.5591)  class_acc: 0.8500 (0.8543)  loss_scale: 16384.0000 (21044.8714)  weight_decay: 0.0500 (0.0500)  time: 0.8484  data: 0.0003  max mem: 26773
Epoch: [17]  [1600/2669]  eta: 0:15:35  lr: 0.000297  min_lr: 0.000000  loss: 1.5967 (1.5612)  class_acc: 0.8500 (0.8543)  loss_scale: 16384.0000 (20753.7489)  weight_decay: 0.0500 (0.0500)  time: 0.8501  data: 0.0003  max mem: 26773
Epoch: [17]  [1700/2669]  eta: 0:14:06  lr: 0.000295  min_lr: 0.000000  loss: 1.5664 (1.5633)  class_acc: 0.8500 (0.8540)  loss_scale: 16384.0000 (20496.8560)  weight_decay: 0.0500 (0.0500)  time: 0.8479  data: 0.0003  max mem: 26773
Epoch: [17]  [1800/2669]  eta: 0:12:38  lr: 0.000294  min_lr: 0.000000  loss: 1.6387 (1.5688)  class_acc: 0.8000 (0.8525)  loss_scale: 16384.0000 (20268.4908)  weight_decay: 0.0500 (0.0500)  time: 0.8512  data: 0.0003  max mem: 26773
Epoch: [17]  [1900/2669]  eta: 0:11:10  lr: 0.000293  min_lr: 0.000000  loss: 1.5449 (1.5707)  class_acc: 0.8500 (0.8518)  loss_scale: 16384.0000 (20064.1515)  weight_decay: 0.0500 (0.0500)  time: 0.8495  data: 0.0003  max mem: 26773
Epoch: [17]  [2000/2669]  eta: 0:09:42  lr: 0.000291  min_lr: 0.000000  loss: 1.4961 (1.5720)  class_acc: 0.9000 (0.8517)  loss_scale: 32768.0000 (20093.1214)  weight_decay: 0.0500 (0.0500)  time: 0.8489  data: 0.0003  max mem: 26773
Epoch: [17]  [2100/2669]  eta: 0:08:14  lr: 0.000290  min_lr: 0.000000  loss: 1.5303 (1.5713)  class_acc: 0.8500 (0.8515)  loss_scale: 32768.0000 (20680.8034)  weight_decay: 0.0500 (0.0500)  time: 0.8484  data: 0.0003  max mem: 26773
Epoch: [17]  [2200/2669]  eta: 0:06:47  lr: 0.000288  min_lr: 0.000000  loss: 1.6230 (1.5735)  class_acc: 0.8500 (0.8508)  loss_scale: 16384.0000 (20485.5829)  weight_decay: 0.0500 (0.0500)  time: 0.8480  data: 0.0002  max mem: 26773
Epoch: [17]  [2300/2669]  eta: 0:05:20  lr: 0.000287  min_lr: 0.000000  loss: 1.5264 (1.5751)  class_acc: 0.9000 (0.8505)  loss_scale: 16384.0000 (20307.3307)  weight_decay: 0.0500 (0.0500)  time: 0.8513  data: 0.0002  max mem: 26773
Epoch: [17]  [2400/2669]  eta: 0:03:53  lr: 0.000285  min_lr: 0.000000  loss: 1.6045 (1.5772)  class_acc: 0.8000 (0.8499)  loss_scale: 16384.0000 (20143.9267)  weight_decay: 0.0500 (0.0500)  time: 0.8504  data: 0.0003  max mem: 26773
Epoch: [17]  [2500/2669]  eta: 0:02:26  lr: 0.000284  min_lr: 0.000000  loss: 1.4932 (1.5787)  class_acc: 0.8500 (0.8498)  loss_scale: 16384.0000 (19993.5898)  weight_decay: 0.0500 (0.0500)  time: 0.8517  data: 0.0003  max mem: 26773
Epoch: [17]  [2600/2669]  eta: 0:00:59  lr: 0.000283  min_lr: 0.000000  loss: 1.5625 (1.5794)  class_acc: 0.8500 (0.8495)  loss_scale: 16384.0000 (19854.8128)  weight_decay: 0.0500 (0.0500)  time: 0.8506  data: 0.0003  max mem: 26773
Epoch: [17]  [2668/2669]  eta: 0:00:00  lr: 0.000282  min_lr: 0.000000  loss: 1.3438 (1.5788)  class_acc: 0.9000 (0.8496)  loss_scale: 32768.0000 (20093.1214)  weight_decay: 0.0500 (0.0500)  time: 0.8198  data: 0.0021  max mem: 26773
Epoch: [17] Total time: 0:38:31 (0.8660 s / it)
Averaged stats: lr: 0.000282  min_lr: 0.000000  loss: 1.3438 (1.5819)  class_acc: 0.9000 (0.8487)  loss_scale: 32768.0000 (20093.1214)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Test:  [  0/105]  eta: 0:06:41  loss: 0.3297 (0.3297)  acc1: 90.0000 (90.0000)  acc5: 100.0000 (100.0000)  time: 3.8232  data: 3.5281  max mem: 26773
Test:  [104/105]  eta: 0:00:00  loss: 0.4505 (0.5970)  acc1: 90.0000 (87.8599)  acc5: 100.0000 (98.0326)  time: 0.2867  data: 0.0001  max mem: 26773
Test: Total time: 0:00:35 (0.3336 s / it)
* Acc@1 87.310 Acc@5 98.039 loss 0.597
Accuracy of the network on the 50000 test images: 87.3%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Test:  [  0/105]  eta: 0:05:52  loss: 0.2382 (0.2382)  acc1: 90.0000 (90.0000)  acc5: 100.0000 (100.0000)  time: 3.3615  data: 3.0521  max mem: 26773
Test:  [104/105]  eta: 0:00:00  loss: 0.4061 (0.4999)  acc1: 90.0000 (88.5317)  acc5: 100.0000 (98.5125)  time: 0.2893  data: 0.0001  max mem: 26773
Test: Total time: 0:00:34 (0.3313 s / it)
* Acc@1 88.306 Acc@5 98.576 loss 0.489
EMA Accuracy of the network on the 50000 test images: 88.3%
Max accuracy: 88.31%
{"train_lr": 0.0003005281945436273, "train_min_lr": 6.320029979132075e-09, "train_loss": 1.5819089814223748, "train_class_acc": 0.8487147198252748, "train_loss_scale": 20093.12143928036, "train_weight_decay": 0.04999999999999853, "test_loss": 0.488645484795173, "test_acc1": 88.305742162508, "test_acc5": 98.57645553422904, "epoch": 17, "n_parameters": 304532456}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Epoch: [18]  [   0/2669]  eta: 1 day, 5:37:29  lr: 0.000282  min_lr: 0.000000  loss: 1.7559 (1.7559)  class_acc: 0.8500 (0.8500)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 39.9585  data: 33.9318  max mem: 26773
Epoch: [18]  [ 100/2669]  eta: 0:53:06  lr: 0.000280  min_lr: 0.000000  loss: 1.4043 (1.5403)  class_acc: 0.8500 (0.8624)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8486  data: 0.0003  max mem: 26773
Epoch: [18]  [ 200/2669]  eta: 0:42:58  lr: 0.000279  min_lr: 0.000000  loss: 1.5977 (1.5534)  class_acc: 0.8500 (0.8547)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8475  data: 0.0002  max mem: 26773
Epoch: [18]  [ 300/2669]  eta: 0:38:42  lr: 0.000277  min_lr: 0.000000  loss: 1.4414 (1.5534)  class_acc: 0.8500 (0.8565)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8643  data: 0.0002  max mem: 26773
Epoch: [18]  [ 400/2669]  eta: 0:35:51  lr: 0.000276  min_lr: 0.000000  loss: 1.4268 (1.5433)  class_acc: 0.9000 (0.8591)  loss_scale: 32768.0000 (32686.2843)  weight_decay: 0.0500 (0.0500)  time: 0.8487  data: 0.0002  max mem: 26773
Epoch: [18]  [ 500/2669]  eta: 0:33:33  lr: 0.000275  min_lr: 0.000000  loss: 1.4883 (1.5453)  class_acc: 0.8500 (0.8580)  loss_scale: 16384.0000 (29432.3353)  weight_decay: 0.0500 (0.0500)  time: 0.8501  data: 0.0003  max mem: 26773
Epoch: [18]  [ 600/2669]  eta: 0:31:34  lr: 0.000273  min_lr: 0.000000  loss: 1.5703 (1.5515)  class_acc: 0.8500 (0.8559)  loss_scale: 16384.0000 (27261.2313)  weight_decay: 0.0500 (0.0500)  time: 0.8608  data: 0.0002  max mem: 26773
Epoch: [18]  [ 700/2669]  eta: 0:29:44  lr: 0.000272  min_lr: 0.000000  loss: 1.5566 (1.5490)  class_acc: 0.8500 (0.8566)  loss_scale: 16384.0000 (25709.5578)  weight_decay: 0.0500 (0.0500)  time: 0.8501  data: 0.0003  max mem: 26773
Epoch: [18]  [ 800/2669]  eta: 0:28:01  lr: 0.000270  min_lr: 0.000000  loss: 1.6074 (1.5503)  class_acc: 0.8500 (0.8559)  loss_scale: 16384.0000 (24545.3184)  weight_decay: 0.0500 (0.0500)  time: 0.8497  data: 0.0002  max mem: 26773
Epoch: [18]  [ 900/2669]  eta: 0:26:22  lr: 0.000269  min_lr: 0.000000  loss: 1.4805 (1.5500)  class_acc: 0.8500 (0.8559)  loss_scale: 16384.0000 (23639.5117)  weight_decay: 0.0500 (0.0500)  time: 0.8506  data: 0.0002  max mem: 26773
Epoch: [18]  [1000/2669]  eta: 0:24:45  lr: 0.000268  min_lr: 0.000000  loss: 1.4590 (1.5514)  class_acc: 0.8500 (0.8550)  loss_scale: 32768.0000 (24322.3017)  weight_decay: 0.0500 (0.0500)  time: 0.8527  data: 0.0002  max mem: 26773
Epoch: [18]  [1100/2669]  eta: 0:23:11  lr: 0.000266  min_lr: 0.000000  loss: 1.4863 (1.5494)  class_acc: 0.8500 (0.8556)  loss_scale: 32768.0000 (25089.3951)  weight_decay: 0.0500 (0.0500)  time: 0.8498  data: 0.0003  max mem: 26773
Epoch: [18]  [1200/2669]  eta: 0:21:38  lr: 0.000265  min_lr: 0.000000  loss: 1.5078 (1.5561)  class_acc: 0.8500 (0.8540)  loss_scale: 16384.0000 (25046.6478)  weight_decay: 0.0500 (0.0500)  time: 0.8482  data: 0.0002  max mem: 26773
Epoch: [18]  [1300/2669]  eta: 0:20:06  lr: 0.000263  min_lr: 0.000000  loss: 1.4414 (1.5577)  class_acc: 0.9000 (0.8533)  loss_scale: 16384.0000 (24380.8025)  weight_decay: 0.0500 (0.0500)  time: 0.8507  data: 0.0002  max mem: 26773
Epoch: [18]  [1400/2669]  eta: 0:18:35  lr: 0.000262  min_lr: 0.000000  loss: 1.6084 (1.5573)  class_acc: 0.8500 (0.8534)  loss_scale: 16384.0000 (23810.0100)  weight_decay: 0.0500 (0.0500)  time: 0.8522  data: 0.0003  max mem: 26773
Epoch: [18]  [1500/2669]  eta: 0:17:05  lr: 0.000261  min_lr: 0.000000  loss: 1.4229 (1.5551)  class_acc: 0.9000 (0.8539)  loss_scale: 16384.0000 (23315.2725)  weight_decay: 0.0500 (0.0500)  time: 0.8519  data: 0.0002  max mem: 26773
Epoch: [18]  [1600/2669]  eta: 0:15:36  lr: 0.000259  min_lr: 0.000000  loss: 1.5225 (1.5538)  class_acc: 0.9000 (0.8543)  loss_scale: 16384.0000 (22882.3385)  weight_decay: 0.0500 (0.0500)  time: 0.8527  data: 0.0002  max mem: 26773
Epoch: [18]  [1700/2669]  eta: 0:14:07  lr: 0.000258  min_lr: 0.000000  loss: 1.6113 (1.5544)  class_acc: 0.8500 (0.8542)  loss_scale: 8192.0000 (22047.6049)  weight_decay: 0.0500 (0.0500)  time: 0.8491  data: 0.0002  max mem: 26773
Epoch: [18]  [1800/2669]  eta: 0:12:38  lr: 0.000256  min_lr: 0.000000  loss: 1.5898 (1.5543)  class_acc: 0.9000 (0.8549)  loss_scale: 8192.0000 (21278.2765)  weight_decay: 0.0500 (0.0500)  time: 0.8563  data: 0.0003  max mem: 26773
Epoch: [18]  [1900/2669]  eta: 0:11:10  lr: 0.000255  min_lr: 0.000000  loss: 1.4268 (1.5529)  class_acc: 0.8500 (0.8552)  loss_scale: 8192.0000 (20589.8874)  weight_decay: 0.0500 (0.0500)  time: 0.8604  data: 0.0002  max mem: 26773
Epoch: [18]  [2000/2669]  eta: 0:09:42  lr: 0.000254  min_lr: 0.000000  loss: 1.4424 (1.5513)  class_acc: 0.8500 (0.8558)  loss_scale: 8192.0000 (19970.3028)  weight_decay: 0.0500 (0.0500)  time: 0.8502  data: 0.0002  max mem: 26773
Epoch: [18]  [2100/2669]  eta: 0:08:14  lr: 0.000252  min_lr: 0.000000  loss: 1.4238 (1.5515)  class_acc: 0.8500 (0.8562)  loss_scale: 8192.0000 (19409.6982)  weight_decay: 0.0500 (0.0500)  time: 0.8490  data: 0.0002  max mem: 26773
Epoch: [18]  [2200/2669]  eta: 0:06:47  lr: 0.000251  min_lr: 0.000000  loss: 1.3457 (1.5504)  class_acc: 0.9000 (0.8563)  loss_scale: 16384.0000 (19190.3462)  weight_decay: 0.0500 (0.0500)  time: 0.8499  data: 0.0002  max mem: 26773
Epoch: [18]  [2300/2669]  eta: 0:05:20  lr: 0.000249  min_lr: 0.000000  loss: 1.4746 (1.5492)  class_acc: 0.8500 (0.8565)  loss_scale: 16384.0000 (19068.3842)  weight_decay: 0.0500 (0.0500)  time: 0.8605  data: 0.0002  max mem: 26773
Epoch: [18]  [2400/2669]  eta: 0:03:53  lr: 0.000248  min_lr: 0.000000  loss: 1.4873 (1.5488)  class_acc: 0.8500 (0.8567)  loss_scale: 16384.0000 (18956.5814)  weight_decay: 0.0500 (0.0500)  time: 0.8492  data: 0.0003  max mem: 26773
Epoch: [18]  [2500/2669]  eta: 0:02:26  lr: 0.000247  min_lr: 0.000000  loss: 1.4512 (1.5477)  class_acc: 0.8500 (0.8570)  loss_scale: 16384.0000 (18853.7193)  weight_decay: 0.0500 (0.0500)  time: 0.8529  data: 0.0003  max mem: 26773
Epoch: [18]  [2600/2669]  eta: 0:00:59  lr: 0.000245  min_lr: 0.000000  loss: 1.5010 (1.5470)  class_acc: 0.8500 (0.8570)  loss_scale: 16384.0000 (18758.7666)  weight_decay: 0.0500 (0.0500)  time: 0.8627  data: 0.0002  max mem: 26773
Epoch: [18]  [2668/2669]  eta: 0:00:00  lr: 0.000244  min_lr: 0.000000  loss: 1.5557 (1.5469)  class_acc: 0.8500 (0.8570)  loss_scale: 32768.0000 (18901.7811)  weight_decay: 0.0500 (0.0500)  time: 0.8299  data: 0.0009  max mem: 26773
Epoch: [18] Total time: 0:38:32 (0.8664 s / it)
Averaged stats: lr: 0.000244  min_lr: 0.000000  loss: 1.5557 (1.5608)  class_acc: 0.8500 (0.8547)  loss_scale: 32768.0000 (18901.7811)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Test:  [  0/105]  eta: 0:05:19  loss: 0.3427 (0.3427)  acc1: 90.0000 (90.0000)  acc5: 100.0000 (100.0000)  time: 3.0436  data: 2.7482  max mem: 26773
Test:  [104/105]  eta: 0:00:00  loss: 0.4757 (0.6006)  acc1: 90.0000 (87.9079)  acc5: 100.0000 (98.3205)  time: 0.2870  data: 0.0001  max mem: 26773
Test: Total time: 0:00:34 (0.3312 s / it)
* Acc@1 87.222 Acc@5 98.055 loss 0.602
Accuracy of the network on the 50000 test images: 87.2%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Test:  [  0/105]  eta: 0:06:51  loss: 0.2274 (0.2274)  acc1: 90.0000 (90.0000)  acc5: 100.0000 (100.0000)  time: 3.9187  data: 3.6202  max mem: 26773
Test:  [104/105]  eta: 0:00:00  loss: 0.4077 (0.5011)  acc1: 90.0000 (88.4837)  acc5: 100.0000 (98.3685)  time: 0.2895  data: 0.0002  max mem: 26773
Test: Total time: 0:00:35 (0.3357 s / it)
* Acc@1 88.328 Acc@5 98.568 loss 0.490
EMA Accuracy of the network on the 50000 test images: 88.3%
Max accuracy: 88.33%
{"train_lr": 0.0002630153625605287, "train_min_lr": 5.5311448527453685e-09, "train_loss": 1.5607910613784906, "train_class_acc": 0.8547367088102001, "train_loss_scale": 18901.781109445277, "train_weight_decay": 0.04999999999999853, "test_loss": 0.49046163061367615, "test_acc1": 88.32773512476008, "test_acc5": 98.56845809341011, "epoch": 18, "n_parameters": 304532456}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Epoch: [19]  [   0/2669]  eta: 1 day, 5:20:18  lr: 0.000244  min_lr: 0.000000  loss: 2.1660 (2.1660)  class_acc: 0.7500 (0.7500)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 39.5723  data: 32.9627  max mem: 26773
Epoch: [19]  [ 100/2669]  eta: 0:53:05  lr: 0.000243  min_lr: 0.000000  loss: 1.4629 (1.5168)  class_acc: 0.9000 (0.8698)  loss_scale: 16384.0000 (29848.0792)  weight_decay: 0.0500 (0.0500)  time: 0.8564  data: 0.0003  max mem: 26773
Epoch: [19]  [ 200/2669]  eta: 0:43:02  lr: 0.000242  min_lr: 0.000000  loss: 1.4395 (1.5106)  class_acc: 0.8500 (0.8711)  loss_scale: 16384.0000 (23149.5323)  weight_decay: 0.0500 (0.0500)  time: 0.8591  data: 0.0003  max mem: 26773
Epoch: [19]  [ 300/2669]  eta: 0:38:44  lr: 0.000240  min_lr: 0.000000  loss: 1.4551 (1.5054)  class_acc: 0.9000 (0.8728)  loss_scale: 16384.0000 (20901.8472)  weight_decay: 0.0500 (0.0500)  time: 0.8496  data: 0.0003  max mem: 26773
Epoch: [19]  [ 400/2669]  eta: 0:35:53  lr: 0.000239  min_lr: 0.000000  loss: 1.4297 (1.5102)  class_acc: 0.9000 (0.8698)  loss_scale: 16384.0000 (19775.2020)  weight_decay: 0.0500 (0.0500)  time: 0.8517  data: 0.0003  max mem: 26773
Epoch: [19]  [ 500/2669]  eta: 0:33:36  lr: 0.000237  min_lr: 0.000000  loss: 1.5166 (1.5244)  class_acc: 0.8500 (0.8668)  loss_scale: 16384.0000 (19098.3154)  weight_decay: 0.0500 (0.0500)  time: 0.8518  data: 0.0003  max mem: 26773
Epoch: [19]  [ 600/2669]  eta: 0:31:36  lr: 0.000236  min_lr: 0.000000  loss: 1.4375 (1.5221)  class_acc: 0.8500 (0.8660)  loss_scale: 16384.0000 (18701.2047)  weight_decay: 0.0500 (0.0500)  time: 0.8515  data: 0.0003  max mem: 26773
Epoch: [19]  [ 700/2669]  eta: 0:29:46  lr: 0.000235  min_lr: 0.000000  loss: 1.5645 (1.5282)  class_acc: 0.8500 (0.8642)  loss_scale: 16384.0000 (18510.8816)  weight_decay: 0.0500 (0.0500)  time: 0.8605  data: 0.0002  max mem: 26773
Epoch: [19]  [ 800/2669]  eta: 0:28:02  lr: 0.000233  min_lr: 0.000000  loss: 1.4590 (1.5320)  class_acc: 0.9000 (0.8635)  loss_scale: 16384.0000 (18245.3533)  weight_decay: 0.0500 (0.0500)  time: 0.8491  data: 0.0003  max mem: 26773
Epoch: [19]  [ 900/2669]  eta: 0:26:23  lr: 0.000232  min_lr: 0.000000  loss: 1.3848 (1.5310)  class_acc: 0.9000 (0.8648)  loss_scale: 16384.0000 (18038.7658)  weight_decay: 0.0500 (0.0500)  time: 0.8513  data: 0.0003  max mem: 26773
Epoch: [19]  [1000/2669]  eta: 0:24:46  lr: 0.000231  min_lr: 0.000000  loss: 1.7148 (1.5351)  class_acc: 0.8500 (0.8639)  loss_scale: 16384.0000 (17873.4545)  weight_decay: 0.0500 (0.0500)  time: 0.8500  data: 0.0002  max mem: 26773
Epoch: [19]  [1100/2669]  eta: 0:23:12  lr: 0.000229  min_lr: 0.000000  loss: 1.5010 (1.5327)  class_acc: 0.8500 (0.8639)  loss_scale: 16384.0000 (17738.1726)  weight_decay: 0.0500 (0.0500)  time: 0.8612  data: 0.0002  max mem: 26773
Epoch: [19]  [1200/2669]  eta: 0:21:39  lr: 0.000228  min_lr: 0.000000  loss: 1.4033 (1.5281)  class_acc: 0.8500 (0.8645)  loss_scale: 16384.0000 (18498.5046)  weight_decay: 0.0500 (0.0500)  time: 0.8485  data: 0.0003  max mem: 26773
Epoch: [19]  [1300/2669]  eta: 0:20:07  lr: 0.000226  min_lr: 0.000000  loss: 1.4902 (1.5304)  class_acc: 0.8500 (0.8629)  loss_scale: 16384.0000 (18335.9754)  weight_decay: 0.0500 (0.0500)  time: 0.8525  data: 0.0003  max mem: 26773
Epoch: [19]  [1400/2669]  eta: 0:18:37  lr: 0.000225  min_lr: 0.000000  loss: 1.5703 (1.5316)  class_acc: 0.8500 (0.8624)  loss_scale: 16384.0000 (18196.6481)  weight_decay: 0.0500 (0.0500)  time: 0.8495  data: 0.0003  max mem: 26773
Epoch: [19]  [1500/2669]  eta: 0:17:07  lr: 0.000224  min_lr: 0.000000  loss: 1.4844 (1.5322)  class_acc: 0.8500 (0.8623)  loss_scale: 16384.0000 (18075.8854)  weight_decay: 0.0500 (0.0500)  time: 0.8493  data: 0.0003  max mem: 26773
Epoch: [19]  [1600/2669]  eta: 0:15:37  lr: 0.000222  min_lr: 0.000000  loss: 1.4834 (1.5349)  class_acc: 0.9000 (0.8616)  loss_scale: 16384.0000 (17970.2086)  weight_decay: 0.0500 (0.0500)  time: 0.8497  data: 0.0003  max mem: 26773
Epoch: [19]  [1700/2669]  eta: 0:14:08  lr: 0.000221  min_lr: 0.000000  loss: 1.5029 (1.5350)  class_acc: 0.8500 (0.8613)  loss_scale: 16384.0000 (17876.9571)  weight_decay: 0.0500 (0.0500)  time: 0.8493  data: 0.0003  max mem: 26773
Epoch: [19]  [1800/2669]  eta: 0:12:39  lr: 0.000220  min_lr: 0.000000  loss: 1.4395 (1.5355)  class_acc: 0.8500 (0.8609)  loss_scale: 16384.0000 (18157.9478)  weight_decay: 0.0500 (0.0500)  time: 0.8508  data: 0.0003  max mem: 26773
Epoch: [19]  [1900/2669]  eta: 0:11:11  lr: 0.000218  min_lr: 0.000000  loss: 1.6152 (1.5365)  class_acc: 0.8500 (0.8608)  loss_scale: 8192.0000 (17728.5050)  weight_decay: 0.0500 (0.0500)  time: 0.8510  data: 0.0002  max mem: 26773
Epoch: [19]  [2000/2669]  eta: 0:09:43  lr: 0.000217  min_lr: 0.000000  loss: 1.4570 (1.5371)  class_acc: 0.9000 (0.8606)  loss_scale: 8192.0000 (17251.9180)  weight_decay: 0.0500 (0.0500)  time: 0.8485  data: 0.0003  max mem: 26773
Epoch: [19]  [2100/2669]  eta: 0:08:15  lr: 0.000216  min_lr: 0.000000  loss: 1.5137 (1.5392)  class_acc: 0.8500 (0.8602)  loss_scale: 8192.0000 (16820.6987)  weight_decay: 0.0500 (0.0500)  time: 0.8512  data: 0.0003  max mem: 26773
Epoch: [19]  [2200/2669]  eta: 0:06:47  lr: 0.000214  min_lr: 0.000000  loss: 1.5352 (1.5386)  class_acc: 0.8500 (0.8604)  loss_scale: 8192.0000 (16428.6633)  weight_decay: 0.0500 (0.0500)  time: 0.8510  data: 0.0003  max mem: 26773
Epoch: [19]  [2300/2669]  eta: 0:05:20  lr: 0.000213  min_lr: 0.000000  loss: 1.4307 (1.5363)  class_acc: 0.9000 (0.8611)  loss_scale: 8192.0000 (16070.7032)  weight_decay: 0.0500 (0.0500)  time: 0.8503  data: 0.0003  max mem: 26773
Epoch: [19]  [2400/2669]  eta: 0:03:53  lr: 0.000212  min_lr: 0.000000  loss: 1.5322 (1.5363)  class_acc: 0.8500 (0.8613)  loss_scale: 16384.0000 (15954.0991)  weight_decay: 0.0500 (0.0500)  time: 0.8509  data: 0.0003  max mem: 26773
Epoch: [19]  [2500/2669]  eta: 0:02:26  lr: 0.000210  min_lr: 0.000000  loss: 1.4180 (1.5366)  class_acc: 0.8500 (0.8611)  loss_scale: 16384.0000 (15971.2883)  weight_decay: 0.0500 (0.0500)  time: 0.8520  data: 0.0003  max mem: 26773
Epoch: [19]  [2600/2669]  eta: 0:00:59  lr: 0.000209  min_lr: 0.000000  loss: 1.5117 (1.5364)  class_acc: 0.8500 (0.8613)  loss_scale: 16384.0000 (15987.1557)  weight_decay: 0.0500 (0.0500)  time: 0.8500  data: 0.0003  max mem: 26773
Epoch: [19]  [2668/2669]  eta: 0:00:00  lr: 0.000208  min_lr: 0.000000  loss: 1.5186 (1.5375)  class_acc: 0.8500 (0.8610)  loss_scale: 16384.0000 (15997.1214)  weight_decay: 0.0500 (0.0500)  time: 0.8269  data: 0.0015  max mem: 26773
Epoch: [19] Total time: 0:38:33 (0.8668 s / it)
Averaged stats: lr: 0.000208  min_lr: 0.000000  loss: 1.5186 (1.5441)  class_acc: 0.8500 (0.8595)  loss_scale: 16384.0000 (15997.1214)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Test:  [  0/105]  eta: 0:05:35  loss: 0.2775 (0.2775)  acc1: 90.0000 (90.0000)  acc5: 100.0000 (100.0000)  time: 3.1948  data: 2.8985  max mem: 26773
Test:  [104/105]  eta: 0:00:00  loss: 0.4503 (0.6129)  acc1: 90.0000 (87.8119)  acc5: 100.0000 (98.1766)  time: 0.2869  data: 0.0001  max mem: 26773
Test: Total time: 0:00:35 (0.3345 s / it)
* Acc@1 87.166 Acc@5 97.905 loss 0.613
Accuracy of the network on the 50000 test images: 87.2%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Test:  [  0/105]  eta: 0:05:57  loss: 0.2233 (0.2233)  acc1: 90.0000 (90.0000)  acc5: 100.0000 (100.0000)  time: 3.4034  data: 3.1049  max mem: 26773
Test:  [104/105]  eta: 0:00:00  loss: 0.4041 (0.5041)  acc1: 90.0000 (88.5797)  acc5: 100.0000 (98.4645)  time: 0.2896  data: 0.0002  max mem: 26773
Test: Total time: 0:00:35 (0.3399 s / it)
* Acc@1 88.302 Acc@5 98.546 loss 0.494
EMA Accuracy of the network on the 50000 test images: 88.3%
Max accuracy: 88.33%
{"train_lr": 0.00022609368570208736, "train_min_lr": 4.754691565294155e-09, "train_loss": 1.5440957347254107, "train_class_acc": 0.8594632518888712, "train_loss_scale": 15997.12143928036, "train_weight_decay": 0.04999999999999853, "test_loss": 0.49391905801400304, "test_acc1": 88.30174344209853, "test_acc5": 98.54646513115803, "epoch": 19, "n_parameters": 304532456}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Epoch: [20]  [   0/2669]  eta: 1 day, 4:56:34  lr: 0.000208  min_lr: 0.000000  loss: 1.5723 (1.5723)  class_acc: 0.8500 (0.8500)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 39.0386  data: 34.9643  max mem: 26773
Epoch: [20]  [ 100/2669]  eta: 0:52:27  lr: 0.000207  min_lr: 0.000000  loss: 1.4971 (1.5308)  class_acc: 0.9000 (0.8683)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8452  data: 0.0003  max mem: 26773
Epoch: [20]  [ 200/2669]  eta: 0:42:47  lr: 0.000205  min_lr: 0.000000  loss: 1.5352 (1.5138)  class_acc: 0.8500 (0.8699)  loss_scale: 32768.0000 (17851.2239)  weight_decay: 0.0500 (0.0500)  time: 0.8496  data: 0.0002  max mem: 26773
Epoch: [20]  [ 300/2669]  eta: 0:38:33  lr: 0.000204  min_lr: 0.000000  loss: 1.4150 (1.5141)  class_acc: 0.8500 (0.8683)  loss_scale: 32768.0000 (22698.0997)  weight_decay: 0.0500 (0.0500)  time: 0.8464  data: 0.0002  max mem: 26773
Epoch: [20]  [ 400/2669]  eta: 0:35:44  lr: 0.000203  min_lr: 0.000000  loss: 1.5391 (1.5188)  class_acc: 0.8500 (0.8672)  loss_scale: 16384.0000 (21123.5112)  weight_decay: 0.0500 (0.0500)  time: 0.8493  data: 0.0002  max mem: 26773
Epoch: [20]  [ 500/2669]  eta: 0:33:30  lr: 0.000201  min_lr: 0.000000  loss: 1.5254 (1.5216)  class_acc: 0.8500 (0.8663)  loss_scale: 16384.0000 (20177.5010)  weight_decay: 0.0500 (0.0500)  time: 0.8519  data: 0.0003  max mem: 26773
Epoch: [20]  [ 600/2669]  eta: 0:31:31  lr: 0.000200  min_lr: 0.000000  loss: 1.5088 (1.5164)  class_acc: 0.8500 (0.8676)  loss_scale: 8192.0000 (19191.9068)  weight_decay: 0.0500 (0.0500)  time: 0.8511  data: 0.0003  max mem: 26773
Epoch: [20]  [ 700/2669]  eta: 0:29:42  lr: 0.000199  min_lr: 0.000000  loss: 1.3555 (1.5112)  class_acc: 0.9000 (0.8691)  loss_scale: 8192.0000 (17622.7332)  weight_decay: 0.0500 (0.0500)  time: 0.8506  data: 0.0003  max mem: 26773
Epoch: [20]  [ 800/2669]  eta: 0:27:59  lr: 0.000197  min_lr: 0.000000  loss: 1.5645 (1.5137)  class_acc: 0.8500 (0.8685)  loss_scale: 8192.0000 (16445.3633)  weight_decay: 0.0500 (0.0500)  time: 0.8498  data: 0.0003  max mem: 26773
Epoch: [20]  [ 900/2669]  eta: 0:26:19  lr: 0.000196  min_lr: 0.000000  loss: 1.5029 (1.5188)  class_acc: 0.8500 (0.8674)  loss_scale: 8192.0000 (15529.3407)  weight_decay: 0.0500 (0.0500)  time: 0.8485  data: 0.0002  max mem: 26773
Epoch: [20]  [1000/2669]  eta: 0:24:43  lr: 0.000195  min_lr: 0.000000  loss: 1.6963 (1.5221)  class_acc: 0.8000 (0.8667)  loss_scale: 8192.0000 (14796.3397)  weight_decay: 0.0500 (0.0500)  time: 0.8487  data: 0.0005  max mem: 26773
Epoch: [20]  [1100/2669]  eta: 0:23:08  lr: 0.000193  min_lr: 0.000000  loss: 1.5205 (1.5211)  class_acc: 0.8500 (0.8671)  loss_scale: 8192.0000 (14270.8955)  weight_decay: 0.0500 (0.0500)  time: 0.8511  data: 0.0003  max mem: 26773
Epoch: [20]  [1200/2669]  eta: 0:21:36  lr: 0.000192  min_lr: 0.000000  loss: 1.6211 (1.5256)  class_acc: 0.8000 (0.8662)  loss_scale: 16384.0000 (14446.8410)  weight_decay: 0.0500 (0.0500)  time: 0.8502  data: 0.0002  max mem: 26773
Epoch: [20]  [1300/2669]  eta: 0:20:04  lr: 0.000191  min_lr: 0.000000  loss: 1.3984 (1.5263)  class_acc: 0.9000 (0.8658)  loss_scale: 16384.0000 (14595.7387)  weight_decay: 0.0500 (0.0500)  time: 0.8486  data: 0.0003  max mem: 26773
Epoch: [20]  [1400/2669]  eta: 0:18:34  lr: 0.000189  min_lr: 0.000000  loss: 1.3760 (1.5268)  class_acc: 0.9000 (0.8653)  loss_scale: 16384.0000 (14723.3804)  weight_decay: 0.0500 (0.0500)  time: 0.8575  data: 0.0003  max mem: 26773
Epoch: [20]  [1500/2669]  eta: 0:17:04  lr: 0.000188  min_lr: 0.000000  loss: 1.4443 (1.5267)  class_acc: 0.9000 (0.8657)  loss_scale: 16384.0000 (14834.0147)  weight_decay: 0.0500 (0.0500)  time: 0.8525  data: 0.0003  max mem: 26773
Epoch: [20]  [1600/2669]  eta: 0:15:35  lr: 0.000187  min_lr: 0.000000  loss: 1.5400 (1.5309)  class_acc: 0.8500 (0.8646)  loss_scale: 16384.0000 (14930.8282)  weight_decay: 0.0500 (0.0500)  time: 0.8499  data: 0.0003  max mem: 26773
Epoch: [20]  [1700/2669]  eta: 0:14:06  lr: 0.000185  min_lr: 0.000000  loss: 1.5273 (1.5313)  class_acc: 0.8000 (0.8643)  loss_scale: 16384.0000 (15632.7055)  weight_decay: 0.0500 (0.0500)  time: 0.8514  data: 0.0002  max mem: 26773
Epoch: [20]  [1800/2669]  eta: 0:12:37  lr: 0.000184  min_lr: 0.000000  loss: 1.5928 (1.5314)  class_acc: 0.8500 (0.8646)  loss_scale: 16384.0000 (15674.4209)  weight_decay: 0.0500 (0.0500)  time: 0.8489  data: 0.0002  max mem: 26773
Epoch: [20]  [1900/2669]  eta: 0:11:09  lr: 0.000183  min_lr: 0.000000  loss: 1.5713 (1.5306)  class_acc: 0.8500 (0.8647)  loss_scale: 16384.0000 (15711.7475)  weight_decay: 0.0500 (0.0500)  time: 0.8506  data: 0.0003  max mem: 26773
Epoch: [20]  [2000/2669]  eta: 0:09:42  lr: 0.000182  min_lr: 0.000000  loss: 1.4795 (1.5309)  class_acc: 0.8500 (0.8647)  loss_scale: 16384.0000 (15745.3433)  weight_decay: 0.0500 (0.0500)  time: 0.8526  data: 0.0003  max mem: 26773
Epoch: [20]  [2100/2669]  eta: 0:08:14  lr: 0.000180  min_lr: 0.000000  loss: 1.4590 (1.5309)  class_acc: 0.8500 (0.8647)  loss_scale: 16384.0000 (15775.7411)  weight_decay: 0.0500 (0.0500)  time: 0.8528  data: 0.0003  max mem: 26773
Epoch: [20]  [2200/2669]  eta: 0:06:47  lr: 0.000179  min_lr: 0.000000  loss: 1.5068 (1.5324)  class_acc: 0.9000 (0.8640)  loss_scale: 32768.0000 (15937.3667)  weight_decay: 0.0500 (0.0500)  time: 0.8530  data: 0.0003  max mem: 26773
Epoch: [20]  [2300/2669]  eta: 0:05:20  lr: 0.000178  min_lr: 0.000000  loss: 1.5283 (1.5331)  class_acc: 0.8500 (0.8636)  loss_scale: 32768.0000 (16668.8153)  weight_decay: 0.0500 (0.0500)  time: 0.8500  data: 0.0003  max mem: 26773
Epoch: [20]  [2400/2669]  eta: 0:03:53  lr: 0.000176  min_lr: 0.000000  loss: 1.4844 (1.5357)  class_acc: 0.8500 (0.8630)  loss_scale: 32768.0000 (17339.3353)  weight_decay: 0.0500 (0.0500)  time: 0.8522  data: 0.0003  max mem: 26773
Epoch: [20]  [2500/2669]  eta: 0:02:26  lr: 0.000175  min_lr: 0.000000  loss: 1.5840 (1.5365)  class_acc: 0.8500 (0.8627)  loss_scale: 32768.0000 (17956.2351)  weight_decay: 0.0500 (0.0500)  time: 0.8513  data: 0.0003  max mem: 26773
Epoch: [20]  [2600/2669]  eta: 0:00:59  lr: 0.000174  min_lr: 0.000000  loss: 1.5439 (1.5368)  class_acc: 0.9000 (0.8628)  loss_scale: 32768.0000 (18525.6993)  weight_decay: 0.0500 (0.0500)  time: 0.8490  data: 0.0003  max mem: 26773
Epoch: [20]  [2668/2669]  eta: 0:00:00  lr: 0.000173  min_lr: 0.000000  loss: 1.5166 (1.5370)  class_acc: 0.8500 (0.8627)  loss_scale: 16384.0000 (18680.7076)  weight_decay: 0.0500 (0.0500)  time: 0.8177  data: 0.0010  max mem: 26773
Epoch: [20] Total time: 0:38:31 (0.8661 s / it)
Averaged stats: lr: 0.000173  min_lr: 0.000000  loss: 1.5166 (1.5283)  class_acc: 0.8500 (0.8641)  loss_scale: 16384.0000 (18680.7076)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Test:  [  0/105]  eta: 0:06:43  loss: 0.2549 (0.2549)  acc1: 95.0000 (95.0000)  acc5: 100.0000 (100.0000)  time: 3.8442  data: 3.5430  max mem: 26773
Test:  [104/105]  eta: 0:00:00  loss: 0.4982 (0.6248)  acc1: 90.0000 (87.3800)  acc5: 100.0000 (97.9367)  time: 0.2867  data: 0.0002  max mem: 26773
Test: Total time: 0:00:34 (0.3327 s / it)
* Acc@1 87.222 Acc@5 97.937 loss 0.611
Accuracy of the network on the 50000 test images: 87.2%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Test:  [  0/105]  eta: 0:06:26  loss: 0.2195 (0.2195)  acc1: 90.0000 (90.0000)  acc5: 100.0000 (100.0000)  time: 3.6793  data: 3.3824  max mem: 26773
Test:  [104/105]  eta: 0:00:00  loss: 0.4049 (0.5087)  acc1: 90.0000 (88.5797)  acc5: 100.0000 (98.4645)  time: 0.2894  data: 0.0002  max mem: 26773
Test: Total time: 0:00:35 (0.3407 s / it)
* Acc@1 88.250 Acc@5 98.544 loss 0.499
EMA Accuracy of the network on the 50000 test images: 88.2%
Max accuracy: 88.33%
{"train_lr": 0.00019034544086830307, "train_min_lr": 4.0029152489523276e-09, "train_loss": 1.5282935772104183, "train_class_acc": 0.8640867213577569, "train_loss_scale": 18680.70764617691, "train_weight_decay": 0.04999999999999853, "test_loss": 0.4988790541857718, "test_acc1": 88.24976007677543, "test_acc5": 98.5444657709533, "epoch": 20, "n_parameters": 304532456}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Epoch: [21]  [   0/2669]  eta: 1 day, 5:35:19  lr: 0.000173  min_lr: 0.000000  loss: 1.4951 (1.4951)  class_acc: 0.8000 (0.8000)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 39.9101  data: 33.2039  max mem: 26773
Epoch: [21]  [ 100/2669]  eta: 0:52:52  lr: 0.000172  min_lr: 0.000000  loss: 1.5469 (1.5230)  class_acc: 0.8500 (0.8668)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8481  data: 0.0003  max mem: 26773
Epoch: [21]  [ 200/2669]  eta: 0:42:57  lr: 0.000170  min_lr: 0.000000  loss: 1.3506 (1.5130)  class_acc: 0.9000 (0.8632)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8583  data: 0.0003  max mem: 26773
Epoch: [21]  [ 300/2669]  eta: 0:38:38  lr: 0.000169  min_lr: 0.000000  loss: 1.4424 (1.5110)  class_acc: 0.9000 (0.8654)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8490  data: 0.0003  max mem: 26773
Epoch: [21]  [ 400/2669]  eta: 0:35:47  lr: 0.000168  min_lr: 0.000000  loss: 1.5303 (1.5099)  class_acc: 0.8500 (0.8656)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8492  data: 0.0003  max mem: 26773
Epoch: [21]  [ 500/2669]  eta: 0:33:32  lr: 0.000167  min_lr: 0.000000  loss: 1.4629 (1.5062)  class_acc: 0.9000 (0.8669)  loss_scale: 32768.0000 (16972.6467)  weight_decay: 0.0500 (0.0500)  time: 0.8487  data: 0.0002  max mem: 26773
Epoch: [21]  [ 600/2669]  eta: 0:31:34  lr: 0.000165  min_lr: 0.000000  loss: 1.3594 (1.5109)  class_acc: 0.9000 (0.8660)  loss_scale: 16384.0000 (18237.7637)  weight_decay: 0.0500 (0.0500)  time: 0.8486  data: 0.0002  max mem: 26773
Epoch: [21]  [ 700/2669]  eta: 0:29:44  lr: 0.000164  min_lr: 0.000000  loss: 1.3496 (1.5041)  class_acc: 0.9000 (0.8674)  loss_scale: 16384.0000 (17973.3181)  weight_decay: 0.0500 (0.0500)  time: 0.8508  data: 0.0002  max mem: 26773
Epoch: [21]  [ 800/2669]  eta: 0:28:01  lr: 0.000163  min_lr: 0.000000  loss: 1.4268 (1.5051)  class_acc: 0.8500 (0.8671)  loss_scale: 16384.0000 (17774.9014)  weight_decay: 0.0500 (0.0500)  time: 0.8536  data: 0.0003  max mem: 26773
Epoch: [21]  [ 900/2669]  eta: 0:26:21  lr: 0.000162  min_lr: 0.000000  loss: 1.4990 (1.5054)  class_acc: 0.8500 (0.8675)  loss_scale: 16384.0000 (17620.5283)  weight_decay: 0.0500 (0.0500)  time: 0.8503  data: 0.0002  max mem: 26773
Epoch: [21]  [1000/2669]  eta: 0:24:44  lr: 0.000160  min_lr: 0.000000  loss: 1.4531 (1.5071)  class_acc: 0.8500 (0.8675)  loss_scale: 16384.0000 (17496.9990)  weight_decay: 0.0500 (0.0500)  time: 0.8508  data: 0.0003  max mem: 26773
Epoch: [21]  [1100/2669]  eta: 0:23:11  lr: 0.000159  min_lr: 0.000000  loss: 1.3838 (1.5081)  class_acc: 0.9000 (0.8672)  loss_scale: 32768.0000 (17901.8638)  weight_decay: 0.0500 (0.0500)  time: 0.8543  data: 0.0002  max mem: 26773
Epoch: [21]  [1200/2669]  eta: 0:21:38  lr: 0.000158  min_lr: 0.000000  loss: 1.5381 (1.5093)  class_acc: 0.8500 (0.8673)  loss_scale: 32768.0000 (19112.3930)  weight_decay: 0.0500 (0.0500)  time: 0.8491  data: 0.0003  max mem: 26773
Epoch: [21]  [1300/2669]  eta: 0:20:06  lr: 0.000157  min_lr: 0.000000  loss: 1.5205 (1.5108)  class_acc: 0.8500 (0.8674)  loss_scale: 16384.0000 (18902.6779)  weight_decay: 0.0500 (0.0500)  time: 0.8494  data: 0.0002  max mem: 26773
Epoch: [21]  [1400/2669]  eta: 0:18:35  lr: 0.000155  min_lr: 0.000000  loss: 1.4824 (1.5131)  class_acc: 0.8500 (0.8667)  loss_scale: 16384.0000 (18722.9008)  weight_decay: 0.0500 (0.0500)  time: 0.8504  data: 0.0002  max mem: 26773
Epoch: [21]  [1500/2669]  eta: 0:17:05  lr: 0.000154  min_lr: 0.000000  loss: 1.4658 (1.5127)  class_acc: 0.8500 (0.8667)  loss_scale: 16384.0000 (18567.0779)  weight_decay: 0.0500 (0.0500)  time: 0.8542  data: 0.0003  max mem: 26773
Epoch: [21]  [1600/2669]  eta: 0:15:36  lr: 0.000153  min_lr: 0.000000  loss: 1.5195 (1.5148)  class_acc: 0.8500 (0.8662)  loss_scale: 16384.0000 (18430.7208)  weight_decay: 0.0500 (0.0500)  time: 0.8615  data: 0.0002  max mem: 26773
Epoch: [21]  [1700/2669]  eta: 0:14:07  lr: 0.000152  min_lr: 0.000000  loss: 1.4980 (1.5146)  class_acc: 0.8500 (0.8663)  loss_scale: 16384.0000 (18310.3962)  weight_decay: 0.0500 (0.0500)  time: 0.8488  data: 0.0003  max mem: 26773
Epoch: [21]  [1800/2669]  eta: 0:12:38  lr: 0.000150  min_lr: 0.000000  loss: 1.4609 (1.5140)  class_acc: 0.8500 (0.8664)  loss_scale: 16384.0000 (18276.2110)  weight_decay: 0.0500 (0.0500)  time: 0.8496  data: 0.0003  max mem: 26773
Epoch: [21]  [1900/2669]  eta: 0:11:10  lr: 0.000149  min_lr: 0.000000  loss: 1.4463 (1.5132)  class_acc: 0.8500 (0.8666)  loss_scale: 16384.0000 (18176.6733)  weight_decay: 0.0500 (0.0500)  time: 0.8483  data: 0.0003  max mem: 26773
Epoch: [21]  [2000/2669]  eta: 0:09:42  lr: 0.000148  min_lr: 0.000000  loss: 1.4678 (1.5119)  class_acc: 0.9000 (0.8672)  loss_scale: 16384.0000 (18087.0845)  weight_decay: 0.0500 (0.0500)  time: 0.8508  data: 0.0003  max mem: 26773
Epoch: [21]  [2100/2669]  eta: 0:08:14  lr: 0.000147  min_lr: 0.000000  loss: 1.4961 (1.5104)  class_acc: 0.8500 (0.8679)  loss_scale: 16384.0000 (18006.0238)  weight_decay: 0.0500 (0.0500)  time: 0.8511  data: 0.0003  max mem: 26773
Epoch: [21]  [2200/2669]  eta: 0:06:47  lr: 0.000146  min_lr: 0.000000  loss: 1.3604 (1.5081)  class_acc: 0.9000 (0.8684)  loss_scale: 16384.0000 (17932.3289)  weight_decay: 0.0500 (0.0500)  time: 0.8520  data: 0.0003  max mem: 26773
Epoch: [21]  [2300/2669]  eta: 0:05:20  lr: 0.000144  min_lr: 0.000000  loss: 1.4854 (1.5089)  class_acc: 0.8500 (0.8683)  loss_scale: 32768.0000 (18306.5033)  weight_decay: 0.0500 (0.0500)  time: 0.8504  data: 0.0003  max mem: 26773
Epoch: [21]  [2400/2669]  eta: 0:03:53  lr: 0.000143  min_lr: 0.000000  loss: 1.4795 (1.5096)  class_acc: 0.9000 (0.8681)  loss_scale: 32768.0000 (18908.8147)  weight_decay: 0.0500 (0.0500)  time: 0.8612  data: 0.0003  max mem: 26773
Epoch: [21]  [2500/2669]  eta: 0:02:26  lr: 0.000142  min_lr: 0.000000  loss: 1.5264 (1.5090)  class_acc: 0.8500 (0.8684)  loss_scale: 16384.0000 (18925.7801)  weight_decay: 0.0500 (0.0500)  time: 0.8515  data: 0.0003  max mem: 26773
Epoch: [21]  [2600/2669]  eta: 0:00:59  lr: 0.000141  min_lr: 0.000000  loss: 1.4600 (1.5083)  class_acc: 0.8500 (0.8685)  loss_scale: 16384.0000 (18828.0569)  weight_decay: 0.0500 (0.0500)  time: 0.8514  data: 0.0003  max mem: 26773
Epoch: [21]  [2668/2669]  eta: 0:00:00  lr: 0.000140  min_lr: 0.000000  loss: 1.4336 (1.5077)  class_acc: 0.9000 (0.8689)  loss_scale: 16384.0000 (18766.6807)  weight_decay: 0.0500 (0.0500)  time: 0.8172  data: 0.0013  max mem: 26773
Epoch: [21] Total time: 0:38:31 (0.8660 s / it)
Averaged stats: lr: 0.000140  min_lr: 0.000000  loss: 1.4336 (1.5108)  class_acc: 0.9000 (0.8688)  loss_scale: 16384.0000 (18766.6807)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Test:  [  0/105]  eta: 0:06:23  loss: 0.2971 (0.2971)  acc1: 90.0000 (90.0000)  acc5: 100.0000 (100.0000)  time: 3.6490  data: 3.3515  max mem: 26773
Test:  [104/105]  eta: 0:00:00  loss: 0.5691 (0.6400)  acc1: 85.0000 (87.3800)  acc5: 100.0000 (97.6967)  time: 0.2867  data: 0.0002  max mem: 26773
Test: Total time: 0:00:34 (0.3303 s / it)
* Acc@1 87.178 Acc@5 97.881 loss 0.627
Accuracy of the network on the 50000 test images: 87.2%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Test:  [  0/105]  eta: 0:06:43  loss: 0.2212 (0.2212)  acc1: 90.0000 (90.0000)  acc5: 100.0000 (100.0000)  time: 3.8447  data: 3.5465  max mem: 26773
Test:  [104/105]  eta: 0:00:00  loss: 0.4089 (0.5144)  acc1: 90.0000 (88.5797)  acc5: 100.0000 (98.4165)  time: 0.2895  data: 0.0002  max mem: 26773
Test: Total time: 0:00:35 (0.3350 s / it)
* Acc@1 88.192 Acc@5 98.502 loss 0.505
EMA Accuracy of the network on the 50000 test images: 88.2%
Max accuracy: 88.33%
{"train_lr": 0.00015633439923516912, "train_min_lr": 3.2876718653179215e-09, "train_loss": 1.5107618615008902, "train_class_acc": 0.8688085791159204, "train_loss_scale": 18766.680659670164, "train_weight_decay": 0.04999999999999853, "test_loss": 0.5049647977132172, "test_acc1": 88.19177863083813, "test_acc5": 98.50247920665387, "epoch": 21, "n_parameters": 304532456}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Epoch: [22]  [   0/2669]  eta: 1 day, 5:42:38  lr: 0.000140  min_lr: 0.000000  loss: 1.0820 (1.0820)  class_acc: 1.0000 (1.0000)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 40.0743  data: 31.9764  max mem: 26773
Epoch: [22]  [ 100/2669]  eta: 0:53:03  lr: 0.000139  min_lr: 0.000000  loss: 1.5039 (1.5122)  class_acc: 0.9000 (0.8787)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8502  data: 0.0003  max mem: 26773
Epoch: [22]  [ 200/2669]  eta: 0:42:59  lr: 0.000138  min_lr: 0.000000  loss: 1.4609 (1.5125)  class_acc: 0.8500 (0.8739)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8472  data: 0.0003  max mem: 26773
Epoch: [22]  [ 300/2669]  eta: 0:38:45  lr: 0.000136  min_lr: 0.000000  loss: 1.3896 (1.5106)  class_acc: 0.9000 (0.8728)  loss_scale: 32768.0000 (18234.6844)  weight_decay: 0.0500 (0.0500)  time: 0.8612  data: 0.0003  max mem: 26773
Epoch: [22]  [ 400/2669]  eta: 0:35:52  lr: 0.000135  min_lr: 0.000000  loss: 1.4766 (1.5146)  class_acc: 0.9000 (0.8737)  loss_scale: 32768.0000 (21858.9526)  weight_decay: 0.0500 (0.0500)  time: 0.8518  data: 0.0003  max mem: 26773
Epoch: [22]  [ 500/2669]  eta: 0:33:35  lr: 0.000134  min_lr: 0.000000  loss: 1.4277 (1.5215)  class_acc: 0.9000 (0.8719)  loss_scale: 32768.0000 (24036.4072)  weight_decay: 0.0500 (0.0500)  time: 0.8529  data: 0.0004  max mem: 26773
Epoch: [22]  [ 600/2669]  eta: 0:31:35  lr: 0.000133  min_lr: 0.000000  loss: 1.5273 (1.5191)  class_acc: 0.9000 (0.8720)  loss_scale: 32768.0000 (25489.2512)  weight_decay: 0.0500 (0.0500)  time: 0.8513  data: 0.0003  max mem: 26773
Epoch: [22]  [ 700/2669]  eta: 0:29:47  lr: 0.000132  min_lr: 0.000000  loss: 1.4404 (1.5169)  class_acc: 0.9000 (0.8721)  loss_scale: 16384.0000 (24985.0157)  weight_decay: 0.0500 (0.0500)  time: 0.8645  data: 0.0002  max mem: 26773
Epoch: [22]  [ 800/2669]  eta: 0:28:03  lr: 0.000131  min_lr: 0.000000  loss: 1.4277 (1.5112)  class_acc: 0.8500 (0.8730)  loss_scale: 16384.0000 (23911.2310)  weight_decay: 0.0500 (0.0500)  time: 0.8499  data: 0.0003  max mem: 26773
Epoch: [22]  [ 900/2669]  eta: 0:26:23  lr: 0.000129  min_lr: 0.000000  loss: 1.6934 (1.5132)  class_acc: 0.8500 (0.8732)  loss_scale: 16384.0000 (23075.8002)  weight_decay: 0.0500 (0.0500)  time: 0.8520  data: 0.0002  max mem: 26773
Epoch: [22]  [1000/2669]  eta: 0:24:47  lr: 0.000128  min_lr: 0.000000  loss: 1.3857 (1.5072)  class_acc: 0.9000 (0.8740)  loss_scale: 16384.0000 (22407.2887)  weight_decay: 0.0500 (0.0500)  time: 0.8643  data: 0.0002  max mem: 26773
Epoch: [22]  [1100/2669]  eta: 0:23:12  lr: 0.000127  min_lr: 0.000000  loss: 1.5840 (1.5065)  class_acc: 0.8500 (0.8744)  loss_scale: 16384.0000 (21860.2144)  weight_decay: 0.0500 (0.0500)  time: 0.8523  data: 0.0003  max mem: 26773
Epoch: [22]  [1200/2669]  eta: 0:21:39  lr: 0.000126  min_lr: 0.000000  loss: 1.5615 (1.5065)  class_acc: 0.9000 (0.8741)  loss_scale: 8192.0000 (21035.9101)  weight_decay: 0.0500 (0.0500)  time: 0.8509  data: 0.0003  max mem: 26773
Epoch: [22]  [1300/2669]  eta: 0:20:08  lr: 0.000125  min_lr: 0.000000  loss: 1.3760 (1.5023)  class_acc: 0.9000 (0.8746)  loss_scale: 8192.0000 (20048.6764)  weight_decay: 0.0500 (0.0500)  time: 0.8502  data: 0.0002  max mem: 26773
Epoch: [22]  [1400/2669]  eta: 0:18:36  lr: 0.000124  min_lr: 0.000000  loss: 1.4482 (1.5005)  class_acc: 0.8500 (0.8747)  loss_scale: 8192.0000 (19202.3754)  weight_decay: 0.0500 (0.0500)  time: 0.8495  data: 0.0003  max mem: 26773
Epoch: [22]  [1500/2669]  eta: 0:17:06  lr: 0.000123  min_lr: 0.000000  loss: 1.5469 (1.5008)  class_acc: 0.8500 (0.8746)  loss_scale: 8192.0000 (18468.8394)  weight_decay: 0.0500 (0.0500)  time: 0.8479  data: 0.0003  max mem: 26773
Epoch: [22]  [1600/2669]  eta: 0:15:36  lr: 0.000121  min_lr: 0.000000  loss: 1.5020 (1.5019)  class_acc: 0.9000 (0.8746)  loss_scale: 8192.0000 (17826.9382)  weight_decay: 0.0500 (0.0500)  time: 0.8493  data: 0.0003  max mem: 26773
Epoch: [22]  [1700/2669]  eta: 0:14:07  lr: 0.000120  min_lr: 0.000000  loss: 1.4580 (1.5016)  class_acc: 0.8500 (0.8744)  loss_scale: 16384.0000 (17443.5179)  weight_decay: 0.0500 (0.0500)  time: 0.8514  data: 0.0003  max mem: 26773
Epoch: [22]  [1800/2669]  eta: 0:12:39  lr: 0.000119  min_lr: 0.000000  loss: 1.5508 (1.5032)  class_acc: 0.8500 (0.8739)  loss_scale: 16384.0000 (17384.6885)  weight_decay: 0.0500 (0.0500)  time: 0.8590  data: 0.0002  max mem: 26773
Epoch: [22]  [1900/2669]  eta: 0:11:11  lr: 0.000118  min_lr: 0.000000  loss: 1.3467 (1.5010)  class_acc: 0.9000 (0.8745)  loss_scale: 16384.0000 (17332.0484)  weight_decay: 0.0500 (0.0500)  time: 0.8511  data: 0.0002  max mem: 26773
Epoch: [22]  [2000/2669]  eta: 0:09:43  lr: 0.000117  min_lr: 0.000000  loss: 1.4971 (1.5016)  class_acc: 0.9000 (0.8744)  loss_scale: 16384.0000 (17284.6697)  weight_decay: 0.0500 (0.0500)  time: 0.8498  data: 0.0002  max mem: 26773
Epoch: [22]  [2100/2669]  eta: 0:08:15  lr: 0.000116  min_lr: 0.000000  loss: 1.4287 (1.5024)  class_acc: 0.8500 (0.8740)  loss_scale: 16384.0000 (17241.8010)  weight_decay: 0.0500 (0.0500)  time: 0.8505  data: 0.0002  max mem: 26773
Epoch: [22]  [2200/2669]  eta: 0:06:48  lr: 0.000115  min_lr: 0.000000  loss: 1.4688 (1.5031)  class_acc: 0.8500 (0.8736)  loss_scale: 32768.0000 (17396.3689)  weight_decay: 0.0500 (0.0500)  time: 0.8561  data: 0.0003  max mem: 26773
Epoch: [22]  [2300/2669]  eta: 0:05:20  lr: 0.000114  min_lr: 0.000000  loss: 1.4229 (1.5038)  class_acc: 0.8500 (0.8733)  loss_scale: 16384.0000 (17395.0943)  weight_decay: 0.0500 (0.0500)  time: 0.8510  data: 0.0002  max mem: 26773
Epoch: [22]  [2400/2669]  eta: 0:03:53  lr: 0.000113  min_lr: 0.000000  loss: 1.5518 (1.5044)  class_acc: 0.8500 (0.8728)  loss_scale: 16384.0000 (17352.9829)  weight_decay: 0.0500 (0.0500)  time: 0.8533  data: 0.0003  max mem: 26773
Epoch: [22]  [2500/2669]  eta: 0:02:26  lr: 0.000111  min_lr: 0.000000  loss: 1.5352 (1.5037)  class_acc: 0.9000 (0.8729)  loss_scale: 16384.0000 (17314.2391)  weight_decay: 0.0500 (0.0500)  time: 0.8491  data: 0.0003  max mem: 26773
Epoch: [22]  [2600/2669]  eta: 0:00:59  lr: 0.000110  min_lr: 0.000000  loss: 1.3330 (1.5049)  class_acc: 0.9000 (0.8726)  loss_scale: 16384.0000 (17278.4744)  weight_decay: 0.0500 (0.0500)  time: 0.8642  data: 0.0003  max mem: 26773
Epoch: [22]  [2668/2669]  eta: 0:00:00  lr: 0.000110  min_lr: 0.000000  loss: 1.5977 (1.5059)  class_acc: 0.8500 (0.8723)  loss_scale: 16384.0000 (17256.0120)  weight_decay: 0.0500 (0.0500)  time: 0.8176  data: 0.0009  max mem: 26773
Epoch: [22] Total time: 0:38:34 (0.8672 s / it)
Averaged stats: lr: 0.000110  min_lr: 0.000000  loss: 1.5977 (1.4964)  class_acc: 0.8500 (0.8735)  loss_scale: 16384.0000 (17256.0120)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Test:  [  0/105]  eta: 0:06:47  loss: 0.3030 (0.3030)  acc1: 90.0000 (90.0000)  acc5: 100.0000 (100.0000)  time: 3.8802  data: 3.5783  max mem: 26773
Test:  [104/105]  eta: 0:00:00  loss: 0.5262 (0.6256)  acc1: 90.0000 (87.5720)  acc5: 100.0000 (97.7927)  time: 0.2870  data: 0.0001  max mem: 26773
Test: Total time: 0:00:35 (0.3343 s / it)
* Acc@1 87.008 Acc@5 97.827 loss 0.632
Accuracy of the network on the 50000 test images: 87.0%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Test:  [  0/105]  eta: 0:08:17  loss: 0.2219 (0.2219)  acc1: 90.0000 (90.0000)  acc5: 100.0000 (100.0000)  time: 4.7424  data: 4.4446  max mem: 26773
Test:  [104/105]  eta: 0:00:00  loss: 0.4137 (0.5212)  acc1: 90.0000 (88.7236)  acc5: 100.0000 (98.4645)  time: 0.2896  data: 0.0002  max mem: 26773
Test: Total time: 0:00:36 (0.3434 s / it)
* Acc@1 88.180 Acc@5 98.468 loss 0.512
EMA Accuracy of the network on the 50000 test images: 88.2%
Max accuracy: 88.33%
{"train_lr": 0.00012459693524645259, "train_min_lr": 2.6202412298166185e-09, "train_loss": 1.4963597302911045, "train_class_acc": 0.8734781190819074, "train_loss_scale": 17256.011994002998, "train_weight_decay": 0.04999999999999853, "test_loss": 0.5120967434600942, "test_acc1": 88.17978246960972, "test_acc5": 98.46849008317338, "epoch": 22, "n_parameters": 304532456}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Epoch: [23]  [   0/2669]  eta: 1 day, 4:08:07  lr: 0.000110  min_lr: 0.000000  loss: 1.7471 (1.7471)  class_acc: 0.9000 (0.9000)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 37.9495  data: 33.8597  max mem: 26773
Epoch: [23]  [ 100/2669]  eta: 0:52:31  lr: 0.000109  min_lr: 0.000000  loss: 1.4570 (1.4647)  class_acc: 0.9000 (0.8832)  loss_scale: 32768.0000 (23846.0198)  weight_decay: 0.0500 (0.0500)  time: 0.8468  data: 0.0003  max mem: 26773
Epoch: [23]  [ 200/2669]  eta: 0:42:49  lr: 0.000107  min_lr: 0.000000  loss: 1.3330 (1.4529)  class_acc: 0.9000 (0.8823)  loss_scale: 16384.0000 (22578.9453)  weight_decay: 0.0500 (0.0500)  time: 0.8588  data: 0.0002  max mem: 26773
Epoch: [23]  [ 300/2669]  eta: 0:38:34  lr: 0.000106  min_lr: 0.000000  loss: 1.5312 (1.4614)  class_acc: 0.8500 (0.8827)  loss_scale: 16384.0000 (20520.8239)  weight_decay: 0.0500 (0.0500)  time: 0.8503  data: 0.0003  max mem: 26773
Epoch: [23]  [ 400/2669]  eta: 0:35:44  lr: 0.000105  min_lr: 0.000000  loss: 1.3457 (1.4658)  class_acc: 0.9000 (0.8820)  loss_scale: 16384.0000 (19489.1970)  weight_decay: 0.0500 (0.0500)  time: 0.8517  data: 0.0002  max mem: 26773
Epoch: [23]  [ 500/2669]  eta: 0:33:29  lr: 0.000104  min_lr: 0.000000  loss: 1.5117 (1.4640)  class_acc: 0.8500 (0.8821)  loss_scale: 16384.0000 (18869.3972)  weight_decay: 0.0500 (0.0500)  time: 0.8504  data: 0.0002  max mem: 26773
Epoch: [23]  [ 600/2669]  eta: 0:31:30  lr: 0.000103  min_lr: 0.000000  loss: 1.5195 (1.4714)  class_acc: 0.8500 (0.8803)  loss_scale: 16384.0000 (18455.8536)  weight_decay: 0.0500 (0.0500)  time: 0.8493  data: 0.0002  max mem: 26773
Epoch: [23]  [ 700/2669]  eta: 0:29:41  lr: 0.000102  min_lr: 0.000000  loss: 1.5352 (1.4749)  class_acc: 0.9000 (0.8797)  loss_scale: 32768.0000 (19422.4023)  weight_decay: 0.0500 (0.0500)  time: 0.8519  data: 0.0003  max mem: 26773
Epoch: [23]  [ 800/2669]  eta: 0:27:58  lr: 0.000101  min_lr: 0.000000  loss: 1.4277 (1.4791)  class_acc: 0.9000 (0.8787)  loss_scale: 32768.0000 (21088.5194)  weight_decay: 0.0500 (0.0500)  time: 0.8509  data: 0.0003  max mem: 26773
Epoch: [23]  [ 900/2669]  eta: 0:26:19  lr: 0.000100  min_lr: 0.000000  loss: 1.4111 (1.4809)  class_acc: 0.9000 (0.8787)  loss_scale: 16384.0000 (21693.7980)  weight_decay: 0.0500 (0.0500)  time: 0.8493  data: 0.0002  max mem: 26773
Epoch: [23]  [1000/2669]  eta: 0:24:43  lr: 0.000099  min_lr: 0.000000  loss: 1.3887 (1.4790)  class_acc: 0.9000 (0.8790)  loss_scale: 16384.0000 (21163.3487)  weight_decay: 0.0500 (0.0500)  time: 0.8583  data: 0.0003  max mem: 26773
Epoch: [23]  [1100/2669]  eta: 0:23:09  lr: 0.000098  min_lr: 0.000000  loss: 1.3594 (1.4811)  class_acc: 0.9000 (0.8786)  loss_scale: 16384.0000 (20729.2570)  weight_decay: 0.0500 (0.0500)  time: 0.8493  data: 0.0002  max mem: 26773
Epoch: [23]  [1200/2669]  eta: 0:21:36  lr: 0.000097  min_lr: 0.000000  loss: 1.5596 (1.4823)  class_acc: 0.8500 (0.8779)  loss_scale: 16384.0000 (20367.4538)  weight_decay: 0.0500 (0.0500)  time: 0.8483  data: 0.0003  max mem: 26773
Epoch: [23]  [1300/2669]  eta: 0:20:05  lr: 0.000096  min_lr: 0.000000  loss: 1.4570 (1.4844)  class_acc: 0.8500 (0.8775)  loss_scale: 16384.0000 (20061.2698)  weight_decay: 0.0500 (0.0500)  time: 0.8609  data: 0.0002  max mem: 26773
Epoch: [23]  [1400/2669]  eta: 0:18:34  lr: 0.000095  min_lr: 0.000000  loss: 1.4697 (1.4841)  class_acc: 0.9000 (0.8776)  loss_scale: 32768.0000 (20056.0742)  weight_decay: 0.0500 (0.0500)  time: 0.8494  data: 0.0002  max mem: 26773
Epoch: [23]  [1500/2669]  eta: 0:17:04  lr: 0.000094  min_lr: 0.000000  loss: 1.4639 (1.4825)  class_acc: 0.9000 (0.8779)  loss_scale: 32768.0000 (20902.9714)  weight_decay: 0.0500 (0.0500)  time: 0.8509  data: 0.0002  max mem: 26773
Epoch: [23]  [1600/2669]  eta: 0:15:35  lr: 0.000093  min_lr: 0.000000  loss: 1.4150 (1.4824)  class_acc: 0.9000 (0.8780)  loss_scale: 16384.0000 (20682.1137)  weight_decay: 0.0500 (0.0500)  time: 0.8503  data: 0.0002  max mem: 26773
Epoch: [23]  [1700/2669]  eta: 0:14:06  lr: 0.000092  min_lr: 0.000000  loss: 1.6270 (1.4836)  class_acc: 0.8500 (0.8775)  loss_scale: 16384.0000 (20429.4321)  weight_decay: 0.0500 (0.0500)  time: 0.8499  data: 0.0003  max mem: 26773
Epoch: [23]  [1800/2669]  eta: 0:12:38  lr: 0.000091  min_lr: 0.000000  loss: 1.4580 (1.4828)  class_acc: 0.9000 (0.8778)  loss_scale: 16384.0000 (20204.8107)  weight_decay: 0.0500 (0.0500)  time: 0.8492  data: 0.0002  max mem: 26773
Epoch: [23]  [1900/2669]  eta: 0:11:10  lr: 0.000090  min_lr: 0.000000  loss: 1.5156 (1.4818)  class_acc: 0.8500 (0.8780)  loss_scale: 16384.0000 (20003.8211)  weight_decay: 0.0500 (0.0500)  time: 0.8522  data: 0.0003  max mem: 26773
Epoch: [23]  [2000/2669]  eta: 0:09:42  lr: 0.000089  min_lr: 0.000000  loss: 1.5146 (1.4819)  class_acc: 0.8500 (0.8783)  loss_scale: 16384.0000 (19822.9205)  weight_decay: 0.0500 (0.0500)  time: 0.8496  data: 0.0003  max mem: 26773
Epoch: [23]  [2100/2669]  eta: 0:08:14  lr: 0.000088  min_lr: 0.000000  loss: 1.4004 (1.4816)  class_acc: 0.8500 (0.8782)  loss_scale: 16384.0000 (19846.3970)  weight_decay: 0.0500 (0.0500)  time: 0.8491  data: 0.0002  max mem: 26773
Epoch: [23]  [2200/2669]  eta: 0:06:47  lr: 0.000087  min_lr: 0.000000  loss: 1.5527 (1.4806)  class_acc: 0.8500 (0.8784)  loss_scale: 16384.0000 (19689.0868)  weight_decay: 0.0500 (0.0500)  time: 0.8503  data: 0.0002  max mem: 26773
Epoch: [23]  [2300/2669]  eta: 0:05:20  lr: 0.000086  min_lr: 0.000000  loss: 1.5225 (1.4818)  class_acc: 0.9000 (0.8782)  loss_scale: 16384.0000 (19545.4498)  weight_decay: 0.0500 (0.0500)  time: 0.8493  data: 0.0002  max mem: 26773
Epoch: [23]  [2400/2669]  eta: 0:03:53  lr: 0.000085  min_lr: 0.000000  loss: 1.4541 (1.4822)  class_acc: 0.9000 (0.8781)  loss_scale: 16384.0000 (19413.7776)  weight_decay: 0.0500 (0.0500)  time: 0.8496  data: 0.0002  max mem: 26773
Epoch: [23]  [2500/2669]  eta: 0:02:26  lr: 0.000084  min_lr: 0.000000  loss: 1.5303 (1.4822)  class_acc: 0.9000 (0.8780)  loss_scale: 16384.0000 (19292.6349)  weight_decay: 0.0500 (0.0500)  time: 0.8489  data: 0.0002  max mem: 26773
Epoch: [23]  [2600/2669]  eta: 0:00:59  lr: 0.000083  min_lr: 0.000000  loss: 1.4111 (1.4830)  class_acc: 0.9000 (0.8778)  loss_scale: 32768.0000 (19420.1738)  weight_decay: 0.0500 (0.0500)  time: 0.8539  data: 0.0002  max mem: 26773
Epoch: [23]  [2668/2669]  eta: 0:00:00  lr: 0.000082  min_lr: 0.000000  loss: 1.4385 (1.4844)  class_acc: 0.9000 (0.8777)  loss_scale: 16384.0000 (19601.8471)  weight_decay: 0.0500 (0.0500)  time: 0.8189  data: 0.0008  max mem: 26773
Epoch: [23] Total time: 0:38:30 (0.8658 s / it)
Averaged stats: lr: 0.000082  min_lr: 0.000000  loss: 1.4385 (1.4846)  class_acc: 0.9000 (0.8768)  loss_scale: 16384.0000 (19601.8471)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Test:  [  0/105]  eta: 0:07:03  loss: 0.2624 (0.2624)  acc1: 95.0000 (95.0000)  acc5: 100.0000 (100.0000)  time: 4.0327  data: 3.7369  max mem: 26773
Test:  [104/105]  eta: 0:00:00  loss: 0.4611 (0.6317)  acc1: 85.0000 (87.7639)  acc5: 100.0000 (97.8407)  time: 0.2867  data: 0.0001  max mem: 26773
Test: Total time: 0:00:35 (0.3344 s / it)
* Acc@1 87.138 Acc@5 97.759 loss 0.639
Accuracy of the network on the 50000 test images: 87.1%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Test:  [  0/105]  eta: 0:06:07  loss: 0.2228 (0.2228)  acc1: 90.0000 (90.0000)  acc5: 100.0000 (100.0000)  time: 3.4953  data: 3.1970  max mem: 26773
Test:  [104/105]  eta: 0:00:00  loss: 0.4169 (0.5279)  acc1: 90.0000 (88.7716)  acc5: 100.0000 (98.4645)  time: 0.2893  data: 0.0001  max mem: 26773
Test: Total time: 0:00:34 (0.3328 s / it)
* Acc@1 88.152 Acc@5 98.446 loss 0.520
EMA Accuracy of the network on the 50000 test images: 88.2%
Max accuracy: 88.33%
{"train_lr": 9.563356766829636e-05, "train_min_lr": 2.011149122273938e-09, "train_loss": 1.4845676374459256, "train_class_acc": 0.8768131702250314, "train_loss_scale": 19601.84707646177, "train_weight_decay": 0.04999999999999853, "test_loss": 0.5199598911972273, "test_acc1": 88.15179142674344, "test_acc5": 98.44649712092131, "epoch": 23, "n_parameters": 304532456}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Epoch: [24]  [   0/2669]  eta: 1 day, 5:47:23  lr: 0.000082  min_lr: 0.000000  loss: 1.1289 (1.1289)  class_acc: 1.0000 (1.0000)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 40.1813  data: 31.8294  max mem: 26773
Epoch: [24]  [ 100/2669]  eta: 0:52:48  lr: 0.000081  min_lr: 0.000000  loss: 1.4277 (1.5030)  class_acc: 0.8500 (0.8718)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8440  data: 0.0003  max mem: 26773
Epoch: [24]  [ 200/2669]  eta: 0:42:59  lr: 0.000080  min_lr: 0.000000  loss: 1.3896 (1.4719)  class_acc: 0.8500 (0.8811)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8497  data: 0.0003  max mem: 26773
Epoch: [24]  [ 300/2669]  eta: 0:38:40  lr: 0.000079  min_lr: 0.000000  loss: 1.5977 (1.4787)  class_acc: 0.8500 (0.8787)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8485  data: 0.0003  max mem: 26773
Epoch: [24]  [ 400/2669]  eta: 0:35:50  lr: 0.000078  min_lr: 0.000000  loss: 1.4209 (1.4776)  class_acc: 0.9000 (0.8778)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8482  data: 0.0003  max mem: 26773
Epoch: [24]  [ 500/2669]  eta: 0:33:32  lr: 0.000077  min_lr: 0.000000  loss: 1.4395 (1.4812)  class_acc: 0.9000 (0.8758)  loss_scale: 16384.0000 (16711.0259)  weight_decay: 0.0500 (0.0500)  time: 0.8519  data: 0.0002  max mem: 26773
Epoch: [24]  [ 600/2669]  eta: 0:31:32  lr: 0.000076  min_lr: 0.000000  loss: 1.3711 (1.4752)  class_acc: 0.8500 (0.8758)  loss_scale: 16384.0000 (17038.2696)  weight_decay: 0.0500 (0.0500)  time: 0.8514  data: 0.0002  max mem: 26773
Epoch: [24]  [ 700/2669]  eta: 0:29:43  lr: 0.000076  min_lr: 0.000000  loss: 1.3418 (1.4719)  class_acc: 0.9000 (0.8774)  loss_scale: 16384.0000 (16944.9358)  weight_decay: 0.0500 (0.0500)  time: 0.8501  data: 0.0002  max mem: 26773
Epoch: [24]  [ 800/2669]  eta: 0:27:59  lr: 0.000075  min_lr: 0.000000  loss: 1.4570 (1.4781)  class_acc: 0.9000 (0.8767)  loss_scale: 16384.0000 (16874.9064)  weight_decay: 0.0500 (0.0500)  time: 0.8495  data: 0.0002  max mem: 26773
Epoch: [24]  [ 900/2669]  eta: 0:26:20  lr: 0.000074  min_lr: 0.000000  loss: 1.4326 (1.4770)  class_acc: 0.9000 (0.8776)  loss_scale: 16384.0000 (16820.4218)  weight_decay: 0.0500 (0.0500)  time: 0.8492  data: 0.0003  max mem: 26773
Epoch: [24]  [1000/2669]  eta: 0:24:44  lr: 0.000073  min_lr: 0.000000  loss: 1.5547 (1.4844)  class_acc: 0.8500 (0.8758)  loss_scale: 16384.0000 (16776.8232)  weight_decay: 0.0500 (0.0500)  time: 0.8504  data: 0.0002  max mem: 26773
Epoch: [24]  [1100/2669]  eta: 0:23:09  lr: 0.000072  min_lr: 0.000000  loss: 1.3525 (1.4829)  class_acc: 0.9000 (0.8769)  loss_scale: 16384.0000 (17276.8610)  weight_decay: 0.0500 (0.0500)  time: 0.8501  data: 0.0003  max mem: 26773
Epoch: [24]  [1200/2669]  eta: 0:21:37  lr: 0.000071  min_lr: 0.000000  loss: 1.4023 (1.4837)  class_acc: 0.9000 (0.8771)  loss_scale: 16384.0000 (17202.5179)  weight_decay: 0.0500 (0.0500)  time: 0.8561  data: 0.0002  max mem: 26773
Epoch: [24]  [1300/2669]  eta: 0:20:05  lr: 0.000070  min_lr: 0.000000  loss: 1.4453 (1.4852)  class_acc: 0.9000 (0.8766)  loss_scale: 16384.0000 (17139.6034)  weight_decay: 0.0500 (0.0500)  time: 0.8517  data: 0.0002  max mem: 26773
Epoch: [24]  [1400/2669]  eta: 0:18:34  lr: 0.000069  min_lr: 0.000000  loss: 1.4678 (1.4840)  class_acc: 0.9000 (0.8772)  loss_scale: 8192.0000 (16606.1956)  weight_decay: 0.0500 (0.0500)  time: 0.8493  data: 0.0002  max mem: 26773
Epoch: [24]  [1500/2669]  eta: 0:17:04  lr: 0.000068  min_lr: 0.000000  loss: 1.3828 (1.4850)  class_acc: 0.9000 (0.8766)  loss_scale: 8192.0000 (16045.6229)  weight_decay: 0.0500 (0.0500)  time: 0.8484  data: 0.0002  max mem: 26773
Epoch: [24]  [1600/2669]  eta: 0:15:35  lr: 0.000067  min_lr: 0.000000  loss: 1.4512 (1.4829)  class_acc: 0.8500 (0.8769)  loss_scale: 8192.0000 (15555.0781)  weight_decay: 0.0500 (0.0500)  time: 0.8483  data: 0.0003  max mem: 26773
Epoch: [24]  [1700/2669]  eta: 0:14:06  lr: 0.000066  min_lr: 0.000000  loss: 1.5029 (1.4843)  class_acc: 0.9000 (0.8767)  loss_scale: 8192.0000 (15122.2105)  weight_decay: 0.0500 (0.0500)  time: 0.8480  data: 0.0002  max mem: 26773
Epoch: [24]  [1800/2669]  eta: 0:12:37  lr: 0.000066  min_lr: 0.000000  loss: 1.4639 (1.4827)  class_acc: 0.8500 (0.8767)  loss_scale: 8192.0000 (14737.4125)  weight_decay: 0.0500 (0.0500)  time: 0.8499  data: 0.0002  max mem: 26773
Epoch: [24]  [1900/2669]  eta: 0:11:09  lr: 0.000065  min_lr: 0.000000  loss: 1.3818 (1.4837)  class_acc: 0.8500 (0.8766)  loss_scale: 16384.0000 (14677.5129)  weight_decay: 0.0500 (0.0500)  time: 0.8497  data: 0.0003  max mem: 26773
Epoch: [24]  [2000/2669]  eta: 0:09:42  lr: 0.000064  min_lr: 0.000000  loss: 1.4316 (1.4858)  class_acc: 0.9000 (0.8762)  loss_scale: 16384.0000 (14762.7946)  weight_decay: 0.0500 (0.0500)  time: 0.8617  data: 0.0003  max mem: 26773
Epoch: [24]  [2100/2669]  eta: 0:08:14  lr: 0.000063  min_lr: 0.000000  loss: 1.4268 (1.4857)  class_acc: 0.9000 (0.8764)  loss_scale: 16384.0000 (14839.9581)  weight_decay: 0.0500 (0.0500)  time: 0.8506  data: 0.0002  max mem: 26773
Epoch: [24]  [2200/2669]  eta: 0:06:47  lr: 0.000062  min_lr: 0.000000  loss: 1.4531 (1.4861)  class_acc: 0.9000 (0.8764)  loss_scale: 16384.0000 (14910.1100)  weight_decay: 0.0500 (0.0500)  time: 0.8494  data: 0.0002  max mem: 26773
Epoch: [24]  [2300/2669]  eta: 0:05:20  lr: 0.000061  min_lr: 0.000000  loss: 1.3750 (1.4836)  class_acc: 0.8500 (0.8769)  loss_scale: 16384.0000 (14974.1643)  weight_decay: 0.0500 (0.0500)  time: 0.8519  data: 0.0002  max mem: 26773
Epoch: [24]  [2400/2669]  eta: 0:03:53  lr: 0.000060  min_lr: 0.000000  loss: 1.3975 (1.4838)  class_acc: 0.9000 (0.8771)  loss_scale: 16384.0000 (15333.1312)  weight_decay: 0.0500 (0.0500)  time: 0.8479  data: 0.0003  max mem: 26773
Epoch: [24]  [2500/2669]  eta: 0:02:26  lr: 0.000060  min_lr: 0.000000  loss: 1.4863 (1.4826)  class_acc: 0.8500 (0.8771)  loss_scale: 16384.0000 (15375.1491)  weight_decay: 0.0500 (0.0500)  time: 0.8503  data: 0.0003  max mem: 26773
Epoch: [24]  [2600/2669]  eta: 0:00:59  lr: 0.000059  min_lr: 0.000000  loss: 1.3711 (1.4818)  class_acc: 0.8500 (0.8771)  loss_scale: 16384.0000 (15413.9362)  weight_decay: 0.0500 (0.0500)  time: 0.8494  data: 0.0003  max mem: 26773
Epoch: [24]  [2668/2669]  eta: 0:00:00  lr: 0.000058  min_lr: 0.000000  loss: 1.3740 (1.4828)  class_acc: 0.8500 (0.8767)  loss_scale: 16384.0000 (15438.2969)  weight_decay: 0.0500 (0.0500)  time: 0.8264  data: 0.0008  max mem: 26773
Epoch: [24] Total time: 0:38:31 (0.8660 s / it)
Averaged stats: lr: 0.000058  min_lr: 0.000000  loss: 1.3740 (1.4758)  class_acc: 0.8500 (0.8793)  loss_scale: 16384.0000 (15438.2969)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Test:  [  0/105]  eta: 0:06:15  loss: 0.2971 (0.2971)  acc1: 95.0000 (95.0000)  acc5: 100.0000 (100.0000)  time: 3.5772  data: 3.2778  max mem: 26773
Test:  [104/105]  eta: 0:00:00  loss: 0.4625 (0.6453)  acc1: 90.0000 (87.3321)  acc5: 100.0000 (97.7927)  time: 0.2866  data: 0.0001  max mem: 26773
Test: Total time: 0:00:34 (0.3305 s / it)
* Acc@1 87.104 Acc@5 97.725 loss 0.649
Accuracy of the network on the 50000 test images: 87.1%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Test:  [  0/105]  eta: 0:05:16  loss: 0.2246 (0.2246)  acc1: 90.0000 (90.0000)  acc5: 100.0000 (100.0000)  time: 3.0108  data: 2.7122  max mem: 26773
Test:  [104/105]  eta: 0:00:00  loss: 0.4172 (0.5349)  acc1: 90.0000 (88.6756)  acc5: 100.0000 (98.4645)  time: 0.2894  data: 0.0002  max mem: 26773
Test: Total time: 0:00:34 (0.3285 s / it)
* Acc@1 88.052 Acc@5 98.405 loss 0.528
EMA Accuracy of the network on the 50000 test images: 88.1%
Max accuracy: 88.33%
{"train_lr": 6.990106610928553e-05, "train_min_lr": 1.470001289079869e-09, "train_loss": 1.4757633542847717, "train_class_acc": 0.8792994269972731, "train_loss_scale": 15438.296851574212, "train_weight_decay": 0.04999999999999853, "test_loss": 0.5284827092869414, "test_acc1": 88.05182341650672, "test_acc5": 98.40451055662189, "epoch": 24, "n_parameters": 304532456}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Epoch: [25]  [   0/2669]  eta: 1 day, 4:16:35  lr: 0.000058  min_lr: 0.000000  loss: 1.4473 (1.4473)  class_acc: 0.9000 (0.9000)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 38.1400  data: 29.2533  max mem: 26773
Epoch: [25]  [ 100/2669]  eta: 0:52:40  lr: 0.000057  min_lr: 0.000000  loss: 1.3682 (1.4520)  class_acc: 0.9000 (0.8822)  loss_scale: 8192.0000 (10381.9406)  weight_decay: 0.0500 (0.0500)  time: 0.8475  data: 0.0003  max mem: 26773
Epoch: [25]  [ 200/2669]  eta: 0:42:48  lr: 0.000057  min_lr: 0.000000  loss: 1.4004 (1.4666)  class_acc: 0.9000 (0.8769)  loss_scale: 8192.0000 (9292.4179)  weight_decay: 0.0500 (0.0500)  time: 0.8505  data: 0.0003  max mem: 26773
Epoch: [25]  [ 300/2669]  eta: 0:38:34  lr: 0.000056  min_lr: 0.000000  loss: 1.3564 (1.4554)  class_acc: 0.9000 (0.8799)  loss_scale: 8192.0000 (8926.8306)  weight_decay: 0.0500 (0.0500)  time: 0.8568  data: 0.0002  max mem: 26773
Epoch: [25]  [ 400/2669]  eta: 0:35:43  lr: 0.000055  min_lr: 0.000000  loss: 1.4004 (1.4616)  class_acc: 0.8500 (0.8798)  loss_scale: 8192.0000 (8743.5810)  weight_decay: 0.0500 (0.0500)  time: 0.8491  data: 0.0002  max mem: 26773
Epoch: [25]  [ 500/2669]  eta: 0:33:28  lr: 0.000054  min_lr: 0.000000  loss: 1.4639 (1.4647)  class_acc: 0.9000 (0.8800)  loss_scale: 8192.0000 (8633.4850)  weight_decay: 0.0500 (0.0500)  time: 0.8479  data: 0.0003  max mem: 26773
Epoch: [25]  [ 600/2669]  eta: 0:31:30  lr: 0.000053  min_lr: 0.000000  loss: 1.3408 (1.4627)  class_acc: 0.9000 (0.8809)  loss_scale: 16384.0000 (9350.6023)  weight_decay: 0.0500 (0.0500)  time: 0.8615  data: 0.0002  max mem: 26773
Epoch: [25]  [ 700/2669]  eta: 0:29:42  lr: 0.000053  min_lr: 0.000000  loss: 1.4453 (1.4615)  class_acc: 0.8500 (0.8809)  loss_scale: 16384.0000 (10353.9401)  weight_decay: 0.0500 (0.0500)  time: 0.8498  data: 0.0003  max mem: 26773
Epoch: [25]  [ 800/2669]  eta: 0:27:58  lr: 0.000052  min_lr: 0.000000  loss: 1.3896 (1.4595)  class_acc: 0.8500 (0.8819)  loss_scale: 16384.0000 (11106.7566)  weight_decay: 0.0500 (0.0500)  time: 0.8496  data: 0.0003  max mem: 26773
Epoch: [25]  [ 900/2669]  eta: 0:26:20  lr: 0.000051  min_lr: 0.000000  loss: 1.4541 (1.4607)  class_acc: 0.8500 (0.8819)  loss_scale: 16384.0000 (11692.4661)  weight_decay: 0.0500 (0.0500)  time: 0.8625  data: 0.0003  max mem: 26773
Epoch: [25]  [1000/2669]  eta: 0:24:44  lr: 0.000050  min_lr: 0.000000  loss: 1.6113 (1.4641)  class_acc: 0.8500 (0.8811)  loss_scale: 16384.0000 (12161.1508)  weight_decay: 0.0500 (0.0500)  time: 0.8668  data: 0.0002  max mem: 26773
Epoch: [25]  [1100/2669]  eta: 0:23:10  lr: 0.000049  min_lr: 0.000000  loss: 1.3896 (1.4613)  class_acc: 0.9000 (0.8819)  loss_scale: 32768.0000 (13229.2243)  weight_decay: 0.0500 (0.0500)  time: 0.8520  data: 0.0002  max mem: 26773
Epoch: [25]  [1200/2669]  eta: 0:21:37  lr: 0.000049  min_lr: 0.000000  loss: 1.5605 (1.4678)  class_acc: 0.8500 (0.8800)  loss_scale: 32768.0000 (14856.0999)  weight_decay: 0.0500 (0.0500)  time: 0.8630  data: 0.0003  max mem: 26773
Epoch: [25]  [1300/2669]  eta: 0:20:05  lr: 0.000048  min_lr: 0.000000  loss: 1.3945 (1.4679)  class_acc: 0.9000 (0.8796)  loss_scale: 16384.0000 (15855.0776)  weight_decay: 0.0500 (0.0500)  time: 0.8496  data: 0.0002  max mem: 26773
Epoch: [25]  [1400/2669]  eta: 0:18:35  lr: 0.000047  min_lr: 0.000000  loss: 1.3555 (1.4682)  class_acc: 0.9000 (0.8798)  loss_scale: 8192.0000 (15810.9693)  weight_decay: 0.0500 (0.0500)  time: 0.8556  data: 0.0003  max mem: 26773
Epoch: [25]  [1500/2669]  eta: 0:17:05  lr: 0.000046  min_lr: 0.000000  loss: 1.4395 (1.4670)  class_acc: 0.9000 (0.8802)  loss_scale: 8192.0000 (15303.3764)  weight_decay: 0.0500 (0.0500)  time: 0.8481  data: 0.0003  max mem: 26773
Epoch: [25]  [1600/2669]  eta: 0:15:35  lr: 0.000046  min_lr: 0.000000  loss: 1.5566 (1.4675)  class_acc: 0.8500 (0.8803)  loss_scale: 8192.0000 (14859.1930)  weight_decay: 0.0500 (0.0500)  time: 0.8492  data: 0.0003  max mem: 26773
Epoch: [25]  [1700/2669]  eta: 0:14:06  lr: 0.000045  min_lr: 0.000000  loss: 1.4141 (1.4703)  class_acc: 0.9000 (0.8795)  loss_scale: 8192.0000 (14467.2357)  weight_decay: 0.0500 (0.0500)  time: 0.8479  data: 0.0003  max mem: 26773
Epoch: [25]  [1800/2669]  eta: 0:12:38  lr: 0.000044  min_lr: 0.000000  loss: 1.3955 (1.4692)  class_acc: 0.9000 (0.8798)  loss_scale: 8192.0000 (14118.8051)  weight_decay: 0.0500 (0.0500)  time: 0.8481  data: 0.0003  max mem: 26773
Epoch: [25]  [1900/2669]  eta: 0:11:09  lr: 0.000043  min_lr: 0.000000  loss: 1.3848 (1.4695)  class_acc: 0.9000 (0.8798)  loss_scale: 8192.0000 (13807.0321)  weight_decay: 0.0500 (0.0500)  time: 0.8480  data: 0.0003  max mem: 26773
Epoch: [25]  [2000/2669]  eta: 0:09:42  lr: 0.000043  min_lr: 0.000000  loss: 1.5859 (1.4705)  class_acc: 0.8500 (0.8798)  loss_scale: 16384.0000 (13927.6282)  weight_decay: 0.0500 (0.0500)  time: 0.8500  data: 0.0003  max mem: 26773
Epoch: [25]  [2100/2669]  eta: 0:08:14  lr: 0.000042  min_lr: 0.000000  loss: 1.2637 (1.4708)  class_acc: 0.9000 (0.8798)  loss_scale: 16384.0000 (14044.5426)  weight_decay: 0.0500 (0.0500)  time: 0.8508  data: 0.0003  max mem: 26773
Epoch: [25]  [2200/2669]  eta: 0:06:47  lr: 0.000041  min_lr: 0.000000  loss: 1.3877 (1.4706)  class_acc: 0.8500 (0.8799)  loss_scale: 16384.0000 (14150.8333)  weight_decay: 0.0500 (0.0500)  time: 0.8503  data: 0.0003  max mem: 26773
Epoch: [25]  [2300/2669]  eta: 0:05:20  lr: 0.000041  min_lr: 0.000000  loss: 1.4160 (1.4697)  class_acc: 0.9000 (0.8804)  loss_scale: 16384.0000 (14247.8853)  weight_decay: 0.0500 (0.0500)  time: 0.8503  data: 0.0004  max mem: 26773
Epoch: [25]  [2400/2669]  eta: 0:03:53  lr: 0.000040  min_lr: 0.000000  loss: 1.3916 (1.4685)  class_acc: 0.9000 (0.8804)  loss_scale: 16384.0000 (14336.8530)  weight_decay: 0.0500 (0.0500)  time: 0.8497  data: 0.0003  max mem: 26773
Epoch: [25]  [2500/2669]  eta: 0:02:26  lr: 0.000039  min_lr: 0.000000  loss: 1.4707 (1.4682)  class_acc: 0.9000 (0.8807)  loss_scale: 32768.0000 (14982.0904)  weight_decay: 0.0500 (0.0500)  time: 0.8518  data: 0.0003  max mem: 26773
Epoch: [25]  [2600/2669]  eta: 0:00:59  lr: 0.000039  min_lr: 0.000000  loss: 1.4863 (1.4695)  class_acc: 0.8500 (0.8807)  loss_scale: 16384.0000 (15224.9627)  weight_decay: 0.0500 (0.0500)  time: 0.8496  data: 0.0003  max mem: 26773
Epoch: [25]  [2668/2669]  eta: 0:00:00  lr: 0.000038  min_lr: 0.000000  loss: 1.2363 (1.4688)  class_acc: 0.9000 (0.8812)  loss_scale: 16384.0000 (15254.0690)  weight_decay: 0.0500 (0.0500)  time: 0.8178  data: 0.0016  max mem: 26773
Epoch: [25] Total time: 0:38:30 (0.8657 s / it)
Averaged stats: lr: 0.000038  min_lr: 0.000000  loss: 1.2363 (1.4661)  class_acc: 0.9000 (0.8819)  loss_scale: 16384.0000 (15254.0690)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Test:  [  0/105]  eta: 0:06:41  loss: 0.2976 (0.2976)  acc1: 90.0000 (90.0000)  acc5: 100.0000 (100.0000)  time: 3.8206  data: 3.5197  max mem: 26773
Test:  [104/105]  eta: 0:00:00  loss: 0.4640 (0.6456)  acc1: 90.0000 (87.4280)  acc5: 100.0000 (97.6967)  time: 0.2870  data: 0.0001  max mem: 26773
Test: Total time: 0:00:35 (0.3346 s / it)
* Acc@1 87.044 Acc@5 97.737 loss 0.647
Accuracy of the network on the 50000 test images: 87.0%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Test:  [  0/105]  eta: 0:06:58  loss: 0.2275 (0.2275)  acc1: 95.0000 (95.0000)  acc5: 100.0000 (100.0000)  time: 3.9827  data: 3.6740  max mem: 26773
Test:  [104/105]  eta: 0:00:00  loss: 0.4205 (0.5420)  acc1: 90.0000 (88.8196)  acc5: 100.0000 (98.5125)  time: 0.2895  data: 0.0002  max mem: 26773
Test: Total time: 0:00:35 (0.3392 s / it)
* Acc@1 87.994 Acc@5 98.363 loss 0.537
EMA Accuracy of the network on the 50000 test images: 88.0%
Max accuracy: 88.33%
{"train_lr": 4.7805247490856985e-05, "train_min_lr": 1.0053319548299093e-09, "train_loss": 1.466102779298827, "train_class_acc": 0.8819074978319363, "train_loss_scale": 15254.068965517241, "train_weight_decay": 0.04999999999999853, "test_loss": 0.5373828095516988, "test_acc1": 87.99384197056942, "test_acc5": 98.36252399232245, "epoch": 25, "n_parameters": 304532456}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Epoch: [26]  [   0/2669]  eta: 1 day, 4:57:27  lr: 0.000038  min_lr: 0.000000  loss: 1.6777 (1.6777)  class_acc: 0.7500 (0.7500)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 39.0587  data: 35.9047  max mem: 26773
Epoch: [26]  [ 100/2669]  eta: 0:52:25  lr: 0.000037  min_lr: 0.000000  loss: 1.4062 (1.4464)  class_acc: 0.9000 (0.8827)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8471  data: 0.0003  max mem: 26773
Epoch: [26]  [ 200/2669]  eta: 0:42:39  lr: 0.000037  min_lr: 0.000000  loss: 1.5391 (1.4668)  class_acc: 0.8500 (0.8808)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8515  data: 0.0003  max mem: 26773
Epoch: [26]  [ 300/2669]  eta: 0:38:28  lr: 0.000036  min_lr: 0.000000  loss: 1.3672 (1.4624)  class_acc: 0.9000 (0.8821)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8488  data: 0.0003  max mem: 26773
Epoch: [26]  [ 400/2669]  eta: 0:35:40  lr: 0.000035  min_lr: 0.000000  loss: 1.3916 (1.4597)  class_acc: 0.9000 (0.8819)  loss_scale: 32768.0000 (17282.8728)  weight_decay: 0.0500 (0.0500)  time: 0.8513  data: 0.0002  max mem: 26773
Epoch: [26]  [ 500/2669]  eta: 0:33:26  lr: 0.000035  min_lr: 0.000000  loss: 1.3564 (1.4618)  class_acc: 0.9000 (0.8824)  loss_scale: 16384.0000 (19000.2076)  weight_decay: 0.0500 (0.0500)  time: 0.8475  data: 0.0002  max mem: 26773
Epoch: [26]  [ 600/2669]  eta: 0:31:30  lr: 0.000034  min_lr: 0.000000  loss: 1.3564 (1.4643)  class_acc: 0.9000 (0.8819)  loss_scale: 16384.0000 (18564.8985)  weight_decay: 0.0500 (0.0500)  time: 0.8638  data: 0.0003  max mem: 26773
Epoch: [26]  [ 700/2669]  eta: 0:29:40  lr: 0.000033  min_lr: 0.000000  loss: 1.3701 (1.4564)  class_acc: 0.9000 (0.8842)  loss_scale: 16384.0000 (18253.7860)  weight_decay: 0.0500 (0.0500)  time: 0.8491  data: 0.0002  max mem: 26773
Epoch: [26]  [ 800/2669]  eta: 0:27:58  lr: 0.000033  min_lr: 0.000000  loss: 1.3896 (1.4585)  class_acc: 0.9000 (0.8845)  loss_scale: 16384.0000 (18020.3546)  weight_decay: 0.0500 (0.0500)  time: 0.8488  data: 0.0003  max mem: 26773
Epoch: [26]  [ 900/2669]  eta: 0:26:19  lr: 0.000032  min_lr: 0.000000  loss: 1.2539 (1.4579)  class_acc: 0.9000 (0.8837)  loss_scale: 16384.0000 (17838.7392)  weight_decay: 0.0500 (0.0500)  time: 0.8531  data: 0.0002  max mem: 26773
Epoch: [26]  [1000/2669]  eta: 0:24:43  lr: 0.000032  min_lr: 0.000000  loss: 1.4307 (1.4559)  class_acc: 0.8500 (0.8848)  loss_scale: 32768.0000 (18118.9690)  weight_decay: 0.0500 (0.0500)  time: 0.8491  data: 0.0003  max mem: 26773
Epoch: [26]  [1100/2669]  eta: 0:23:09  lr: 0.000031  min_lr: 0.000000  loss: 1.3486 (1.4597)  class_acc: 0.9000 (0.8841)  loss_scale: 32768.0000 (19449.4896)  weight_decay: 0.0500 (0.0500)  time: 0.8511  data: 0.0003  max mem: 26773
Epoch: [26]  [1200/2669]  eta: 0:21:36  lr: 0.000030  min_lr: 0.000000  loss: 1.4307 (1.4628)  class_acc: 0.8500 (0.8831)  loss_scale: 16384.0000 (19712.6395)  weight_decay: 0.0500 (0.0500)  time: 0.8478  data: 0.0003  max mem: 26773
Epoch: [26]  [1300/2669]  eta: 0:20:04  lr: 0.000030  min_lr: 0.000000  loss: 1.4688 (1.4631)  class_acc: 0.9000 (0.8829)  loss_scale: 16384.0000 (19456.7871)  weight_decay: 0.0500 (0.0500)  time: 0.8480  data: 0.0002  max mem: 26773
Epoch: [26]  [1400/2669]  eta: 0:18:33  lr: 0.000029  min_lr: 0.000000  loss: 1.4561 (1.4627)  class_acc: 0.8500 (0.8827)  loss_scale: 16384.0000 (19237.4590)  weight_decay: 0.0500 (0.0500)  time: 0.8490  data: 0.0003  max mem: 26773
Epoch: [26]  [1500/2669]  eta: 0:17:03  lr: 0.000029  min_lr: 0.000000  loss: 1.3711 (1.4609)  class_acc: 0.9000 (0.8833)  loss_scale: 16384.0000 (19047.3551)  weight_decay: 0.0500 (0.0500)  time: 0.8484  data: 0.0003  max mem: 26773
Epoch: [26]  [1600/2669]  eta: 0:15:34  lr: 0.000028  min_lr: 0.000000  loss: 1.3633 (1.4632)  class_acc: 0.9000 (0.8829)  loss_scale: 16384.0000 (18880.9994)  weight_decay: 0.0500 (0.0500)  time: 0.8529  data: 0.0003  max mem: 26773
Epoch: [26]  [1700/2669]  eta: 0:14:06  lr: 0.000027  min_lr: 0.000000  loss: 1.3828 (1.4613)  class_acc: 0.9000 (0.8835)  loss_scale: 32768.0000 (19177.2745)  weight_decay: 0.0500 (0.0500)  time: 0.8491  data: 0.0003  max mem: 26773
Epoch: [26]  [1800/2669]  eta: 0:12:37  lr: 0.000027  min_lr: 0.000000  loss: 1.4365 (1.4604)  class_acc: 0.8500 (0.8839)  loss_scale: 16384.0000 (19404.2599)  weight_decay: 0.0500 (0.0500)  time: 0.8495  data: 0.0003  max mem: 26773
Epoch: [26]  [1900/2669]  eta: 0:11:09  lr: 0.000026  min_lr: 0.000000  loss: 1.4951 (1.4599)  class_acc: 0.9000 (0.8840)  loss_scale: 16384.0000 (19245.3824)  weight_decay: 0.0500 (0.0500)  time: 0.8622  data: 0.0003  max mem: 26773
Epoch: [26]  [2000/2669]  eta: 0:09:42  lr: 0.000026  min_lr: 0.000000  loss: 1.4229 (1.4594)  class_acc: 0.9000 (0.8844)  loss_scale: 16384.0000 (19102.3848)  weight_decay: 0.0500 (0.0500)  time: 0.8654  data: 0.0002  max mem: 26773
Epoch: [26]  [2100/2669]  eta: 0:08:14  lr: 0.000025  min_lr: 0.000000  loss: 1.4580 (1.4600)  class_acc: 0.8500 (0.8840)  loss_scale: 16384.0000 (18972.9995)  weight_decay: 0.0500 (0.0500)  time: 0.8508  data: 0.0003  max mem: 26773
Epoch: [26]  [2200/2669]  eta: 0:06:47  lr: 0.000025  min_lr: 0.000000  loss: 1.5049 (1.4595)  class_acc: 0.8500 (0.8843)  loss_scale: 16384.0000 (18855.3712)  weight_decay: 0.0500 (0.0500)  time: 0.8492  data: 0.0003  max mem: 26773
Epoch: [26]  [2300/2669]  eta: 0:05:20  lr: 0.000024  min_lr: 0.000000  loss: 1.5039 (1.4600)  class_acc: 0.9000 (0.8843)  loss_scale: 32768.0000 (19047.0230)  weight_decay: 0.0500 (0.0500)  time: 0.8500  data: 0.0003  max mem: 26773
Epoch: [26]  [2400/2669]  eta: 0:03:53  lr: 0.000023  min_lr: 0.000000  loss: 1.5469 (1.4596)  class_acc: 0.8500 (0.8844)  loss_scale: 32768.0000 (19618.4923)  weight_decay: 0.0500 (0.0500)  time: 0.8505  data: 0.0003  max mem: 26773
Epoch: [26]  [2500/2669]  eta: 0:02:26  lr: 0.000023  min_lr: 0.000000  loss: 1.5088 (1.4596)  class_acc: 0.8500 (0.8842)  loss_scale: 32768.0000 (20144.2623)  weight_decay: 0.0500 (0.0500)  time: 0.8504  data: 0.0003  max mem: 26773
Epoch: [26]  [2600/2669]  eta: 0:00:59  lr: 0.000022  min_lr: 0.000000  loss: 1.4297 (1.4599)  class_acc: 0.9000 (0.8842)  loss_scale: 32768.0000 (20629.6040)  weight_decay: 0.0500 (0.0500)  time: 0.8515  data: 0.0003  max mem: 26773
Epoch: [26]  [2668/2669]  eta: 0:00:00  lr: 0.000022  min_lr: 0.000000  loss: 1.4219 (1.4600)  class_acc: 0.9000 (0.8841)  loss_scale: 16384.0000 (20608.9595)  weight_decay: 0.0500 (0.0500)  time: 0.8154  data: 0.0009  max mem: 26773
Epoch: [26] Total time: 0:38:30 (0.8656 s / it)
Averaged stats: lr: 0.000022  min_lr: 0.000000  loss: 1.4219 (1.4611)  class_acc: 0.9000 (0.8835)  loss_scale: 16384.0000 (20608.9595)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Test:  [  0/105]  eta: 0:07:21  loss: 0.3229 (0.3229)  acc1: 95.0000 (95.0000)  acc5: 100.0000 (100.0000)  time: 4.2053  data: 3.8924  max mem: 26773
Test:  [104/105]  eta: 0:00:00  loss: 0.4506 (0.6473)  acc1: 90.0000 (87.9079)  acc5: 100.0000 (97.5528)  time: 0.2869  data: 0.0001  max mem: 26773
Test: Total time: 0:00:35 (0.3356 s / it)
* Acc@1 87.100 Acc@5 97.707 loss 0.651
Accuracy of the network on the 50000 test images: 87.1%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Test:  [  0/105]  eta: 0:06:31  loss: 0.2317 (0.2317)  acc1: 95.0000 (95.0000)  acc5: 100.0000 (100.0000)  time: 3.7310  data: 3.4332  max mem: 26773
Test:  [104/105]  eta: 0:00:00  loss: 0.4214 (0.5500)  acc1: 90.0000 (88.6756)  acc5: 100.0000 (98.4645)  time: 0.2894  data: 0.0001  max mem: 26773
Test: Total time: 0:00:35 (0.3348 s / it)
* Acc@1 87.916 Acc@5 98.333 loss 0.546
EMA Accuracy of the network on the 50000 test images: 87.9%
Max accuracy: 88.33%
{"train_lr": 2.9694576072026487e-05, "train_min_lr": 6.244692325051848e-10, "train_loss": 1.4611061321146068, "train_class_acc": 0.8834996703502046, "train_loss_scale": 20608.95952023988, "train_weight_decay": 0.04999999999999853, "test_loss": 0.5463529377643551, "test_acc1": 87.91586692258477, "test_acc5": 98.33253358925144, "epoch": 26, "n_parameters": 304532456}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Epoch: [27]  [   0/2669]  eta: 1 day, 6:52:49  lr: 0.000022  min_lr: 0.000000  loss: 1.2695 (1.2695)  class_acc: 0.9500 (0.9500)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 41.6522  data: 35.0349  max mem: 26773
Epoch: [27]  [ 100/2669]  eta: 0:53:28  lr: 0.000022  min_lr: 0.000000  loss: 1.4863 (1.4759)  class_acc: 0.8500 (0.8807)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8458  data: 0.0003  max mem: 26773
Epoch: [27]  [ 200/2669]  eta: 0:43:13  lr: 0.000021  min_lr: 0.000000  loss: 1.3369 (1.4551)  class_acc: 0.9000 (0.8856)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8463  data: 0.0003  max mem: 26773
Epoch: [27]  [ 300/2669]  eta: 0:38:51  lr: 0.000021  min_lr: 0.000000  loss: 1.4492 (1.4560)  class_acc: 0.9000 (0.8849)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8505  data: 0.0002  max mem: 26773
Epoch: [27]  [ 400/2669]  eta: 0:35:59  lr: 0.000020  min_lr: 0.000000  loss: 1.3486 (1.4578)  class_acc: 0.9000 (0.8855)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8497  data: 0.0004  max mem: 26773
Epoch: [27]  [ 500/2669]  eta: 0:33:40  lr: 0.000020  min_lr: 0.000000  loss: 1.4062 (1.4507)  class_acc: 0.9000 (0.8863)  loss_scale: 32768.0000 (17626.6986)  weight_decay: 0.0500 (0.0500)  time: 0.8517  data: 0.0003  max mem: 26773
Epoch: [27]  [ 600/2669]  eta: 0:31:38  lr: 0.000019  min_lr: 0.000000  loss: 1.4326 (1.4497)  class_acc: 0.9000 (0.8863)  loss_scale: 16384.0000 (17692.5391)  weight_decay: 0.0500 (0.0500)  time: 0.8500  data: 0.0003  max mem: 26773
Epoch: [27]  [ 700/2669]  eta: 0:29:47  lr: 0.000019  min_lr: 0.000000  loss: 1.3086 (1.4561)  class_acc: 0.9000 (0.8836)  loss_scale: 16384.0000 (17505.8716)  weight_decay: 0.0500 (0.0500)  time: 0.8506  data: 0.0002  max mem: 26773
Epoch: [27]  [ 800/2669]  eta: 0:28:04  lr: 0.000018  min_lr: 0.000000  loss: 1.4307 (1.4558)  class_acc: 0.9000 (0.8848)  loss_scale: 16384.0000 (17365.8127)  weight_decay: 0.0500 (0.0500)  time: 0.8608  data: 0.0002  max mem: 26773
Epoch: [27]  [ 900/2669]  eta: 0:26:24  lr: 0.000018  min_lr: 0.000000  loss: 1.3594 (1.4598)  class_acc: 0.9000 (0.8844)  loss_scale: 16384.0000 (17256.8435)  weight_decay: 0.0500 (0.0500)  time: 0.8532  data: 0.0002  max mem: 26773
Epoch: [27]  [1000/2669]  eta: 0:24:47  lr: 0.000017  min_lr: 0.000000  loss: 1.4365 (1.4594)  class_acc: 0.9000 (0.8842)  loss_scale: 16384.0000 (17169.6464)  weight_decay: 0.0500 (0.0500)  time: 0.8523  data: 0.0002  max mem: 26773
Epoch: [27]  [1100/2669]  eta: 0:23:12  lr: 0.000017  min_lr: 0.000000  loss: 1.4561 (1.4576)  class_acc: 0.9000 (0.8842)  loss_scale: 32768.0000 (18199.4841)  weight_decay: 0.0500 (0.0500)  time: 0.8506  data: 0.0002  max mem: 26773
Epoch: [27]  [1200/2669]  eta: 0:21:39  lr: 0.000016  min_lr: 0.000000  loss: 1.3770 (1.4556)  class_acc: 0.9000 (0.8848)  loss_scale: 16384.0000 (19221.5287)  weight_decay: 0.0500 (0.0500)  time: 0.8491  data: 0.0002  max mem: 26773
Epoch: [27]  [1300/2669]  eta: 0:20:07  lr: 0.000016  min_lr: 0.000000  loss: 1.5527 (1.4551)  class_acc: 0.8500 (0.8847)  loss_scale: 8192.0000 (18789.3374)  weight_decay: 0.0500 (0.0500)  time: 0.8499  data: 0.0003  max mem: 26773
Epoch: [27]  [1400/2669]  eta: 0:18:36  lr: 0.000015  min_lr: 0.000000  loss: 1.4268 (1.4566)  class_acc: 0.8500 (0.8845)  loss_scale: 8192.0000 (18032.9251)  weight_decay: 0.0500 (0.0500)  time: 0.8492  data: 0.0003  max mem: 26773
Epoch: [27]  [1500/2669]  eta: 0:17:06  lr: 0.000015  min_lr: 0.000000  loss: 1.2725 (1.4567)  class_acc: 0.9000 (0.8847)  loss_scale: 8192.0000 (17377.3005)  weight_decay: 0.0500 (0.0500)  time: 0.8615  data: 0.0003  max mem: 26773
Epoch: [27]  [1600/2669]  eta: 0:15:36  lr: 0.000015  min_lr: 0.000000  loss: 1.3867 (1.4556)  class_acc: 0.9000 (0.8848)  loss_scale: 8192.0000 (16803.5778)  weight_decay: 0.0500 (0.0500)  time: 0.8498  data: 0.0002  max mem: 26773
Epoch: [27]  [1700/2669]  eta: 0:14:07  lr: 0.000014  min_lr: 0.000000  loss: 1.3428 (1.4547)  class_acc: 0.9000 (0.8850)  loss_scale: 8192.0000 (16297.3122)  weight_decay: 0.0500 (0.0500)  time: 0.8512  data: 0.0003  max mem: 26773
Epoch: [27]  [1800/2669]  eta: 0:12:39  lr: 0.000014  min_lr: 0.000000  loss: 1.5312 (1.4522)  class_acc: 0.9000 (0.8856)  loss_scale: 16384.0000 (15929.1416)  weight_decay: 0.0500 (0.0500)  time: 0.8638  data: 0.0002  max mem: 26773
Epoch: [27]  [1900/2669]  eta: 0:11:10  lr: 0.000013  min_lr: 0.000000  loss: 1.3838 (1.4498)  class_acc: 0.9000 (0.8865)  loss_scale: 16384.0000 (15953.0689)  weight_decay: 0.0500 (0.0500)  time: 0.8591  data: 0.0002  max mem: 26773
Epoch: [27]  [2000/2669]  eta: 0:09:43  lr: 0.000013  min_lr: 0.000000  loss: 1.3203 (1.4507)  class_acc: 0.9000 (0.8862)  loss_scale: 16384.0000 (15974.6047)  weight_decay: 0.0500 (0.0500)  time: 0.8500  data: 0.0002  max mem: 26773
Epoch: [27]  [2100/2669]  eta: 0:08:15  lr: 0.000013  min_lr: 0.000000  loss: 1.4102 (1.4504)  class_acc: 0.9000 (0.8862)  loss_scale: 16384.0000 (15994.0904)  weight_decay: 0.0500 (0.0500)  time: 0.8524  data: 0.0003  max mem: 26773
Epoch: [27]  [2200/2669]  eta: 0:06:47  lr: 0.000012  min_lr: 0.000000  loss: 1.3242 (1.4505)  class_acc: 0.9000 (0.8863)  loss_scale: 16384.0000 (16011.8055)  weight_decay: 0.0500 (0.0500)  time: 0.8508  data: 0.0003  max mem: 26773
Epoch: [27]  [2300/2669]  eta: 0:05:20  lr: 0.000012  min_lr: 0.000000  loss: 1.4385 (1.4497)  class_acc: 0.9000 (0.8869)  loss_scale: 16384.0000 (16070.7032)  weight_decay: 0.0500 (0.0500)  time: 0.8505  data: 0.0003  max mem: 26773
Epoch: [27]  [2400/2669]  eta: 0:03:53  lr: 0.000011  min_lr: 0.000000  loss: 1.4668 (1.4511)  class_acc: 0.9000 (0.8867)  loss_scale: 32768.0000 (16766.1341)  weight_decay: 0.0500 (0.0500)  time: 0.8507  data: 0.0002  max mem: 26773
Epoch: [27]  [2500/2669]  eta: 0:02:26  lr: 0.000011  min_lr: 0.000000  loss: 1.4180 (1.4520)  class_acc: 0.9000 (0.8866)  loss_scale: 16384.0000 (16894.9764)  weight_decay: 0.0500 (0.0500)  time: 0.8528  data: 0.0003  max mem: 26773
Epoch: [27]  [2600/2669]  eta: 0:00:59  lr: 0.000011  min_lr: 0.000000  loss: 1.4131 (1.4519)  class_acc: 0.9000 (0.8862)  loss_scale: 16384.0000 (16875.3310)  weight_decay: 0.0500 (0.0500)  time: 0.8510  data: 0.0003  max mem: 26773
Epoch: [27]  [2668/2669]  eta: 0:00:00  lr: 0.000010  min_lr: 0.000000  loss: 1.4072 (1.4522)  class_acc: 0.9000 (0.8861)  loss_scale: 16384.0000 (16862.9925)  weight_decay: 0.0500 (0.0500)  time: 0.8316  data: 0.0003  max mem: 26773
Epoch: [27] Total time: 0:38:33 (0.8667 s / it)
Averaged stats: lr: 0.000010  min_lr: 0.000000  loss: 1.4072 (1.4568)  class_acc: 0.9000 (0.8848)  loss_scale: 16384.0000 (16862.9925)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Test:  [  0/105]  eta: 0:06:15  loss: 0.2984 (0.2984)  acc1: 95.0000 (95.0000)  acc5: 100.0000 (100.0000)  time: 3.5716  data: 3.2638  max mem: 26773
Test:  [104/105]  eta: 0:00:00  loss: 0.4704 (0.6495)  acc1: 90.0000 (87.9559)  acc5: 100.0000 (97.5048)  time: 0.2867  data: 0.0002  max mem: 26773
Test: Total time: 0:00:34 (0.3313 s / it)
* Acc@1 87.128 Acc@5 97.689 loss 0.653
Accuracy of the network on the 50000 test images: 87.1%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Test:  [  0/105]  eta: 0:06:09  loss: 0.2364 (0.2364)  acc1: 95.0000 (95.0000)  acc5: 100.0000 (100.0000)  time: 3.5223  data: 3.2226  max mem: 26773
Test:  [104/105]  eta: 0:00:00  loss: 0.4227 (0.5578)  acc1: 90.0000 (88.6276)  acc5: 100.0000 (98.4165)  time: 0.2895  data: 0.0002  max mem: 26773
Test: Total time: 0:00:35 (0.3374 s / it)
* Acc@1 87.866 Acc@5 98.289 loss 0.555
EMA Accuracy of the network on the 50000 test images: 87.9%
Max accuracy: 88.33%
{"train_lr": 1.585466795985902e-05, "train_min_lr": 3.334195547531159e-10, "train_loss": 1.4567834795980916, "train_class_acc": 0.884809171916849, "train_loss_scale": 16862.992503748126, "train_weight_decay": 0.04999999999999853, "test_loss": 0.5552808794148621, "test_acc1": 87.86588291746641, "test_acc5": 98.28854766474728, "epoch": 27, "n_parameters": 304532456}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Epoch: [28]  [   0/2669]  eta: 1 day, 5:15:02  lr: 0.000010  min_lr: 0.000000  loss: 1.1172 (1.1172)  class_acc: 0.9500 (0.9500)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 39.4538  data: 32.3820  max mem: 26773
Epoch: [28]  [ 100/2669]  eta: 0:52:43  lr: 0.000010  min_lr: 0.000000  loss: 1.5049 (1.4794)  class_acc: 0.9000 (0.8842)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8482  data: 0.0002  max mem: 26773
Epoch: [28]  [ 200/2669]  eta: 0:42:49  lr: 0.000010  min_lr: 0.000000  loss: 1.3594 (1.4730)  class_acc: 0.9000 (0.8826)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8487  data: 0.0002  max mem: 26773
Epoch: [28]  [ 300/2669]  eta: 0:38:38  lr: 0.000009  min_lr: 0.000000  loss: 1.3594 (1.4696)  class_acc: 0.9000 (0.8812)  loss_scale: 32768.0000 (18016.9568)  weight_decay: 0.0500 (0.0500)  time: 0.8610  data: 0.0003  max mem: 26773
Epoch: [28]  [ 400/2669]  eta: 0:35:49  lr: 0.000009  min_lr: 0.000000  loss: 1.4072 (1.4748)  class_acc: 0.9000 (0.8793)  loss_scale: 32768.0000 (21695.5212)  weight_decay: 0.0500 (0.0500)  time: 0.8523  data: 0.0003  max mem: 26773
Epoch: [28]  [ 500/2669]  eta: 0:33:32  lr: 0.000009  min_lr: 0.000000  loss: 1.4121 (1.4784)  class_acc: 0.9000 (0.8794)  loss_scale: 32768.0000 (23905.5968)  weight_decay: 0.0500 (0.0500)  time: 0.8497  data: 0.0002  max mem: 26773
Epoch: [28]  [ 600/2669]  eta: 0:31:33  lr: 0.000008  min_lr: 0.000000  loss: 1.3818 (1.4771)  class_acc: 0.9000 (0.8806)  loss_scale: 16384.0000 (23580.9651)  weight_decay: 0.0500 (0.0500)  time: 0.8501  data: 0.0002  max mem: 26773
Epoch: [28]  [ 700/2669]  eta: 0:29:43  lr: 0.000008  min_lr: 0.000000  loss: 1.4268 (1.4775)  class_acc: 0.8500 (0.8807)  loss_scale: 16384.0000 (22554.2939)  weight_decay: 0.0500 (0.0500)  time: 0.8506  data: 0.0003  max mem: 26773
Epoch: [28]  [ 800/2669]  eta: 0:28:00  lr: 0.000008  min_lr: 0.000000  loss: 1.3906 (1.4761)  class_acc: 0.9000 (0.8817)  loss_scale: 16384.0000 (21783.9700)  weight_decay: 0.0500 (0.0500)  time: 0.8500  data: 0.0002  max mem: 26773
Epoch: [28]  [ 900/2669]  eta: 0:26:20  lr: 0.000008  min_lr: 0.000000  loss: 1.3887 (1.4760)  class_acc: 0.9000 (0.8810)  loss_scale: 16384.0000 (21184.6393)  weight_decay: 0.0500 (0.0500)  time: 0.8494  data: 0.0003  max mem: 26773
Epoch: [28]  [1000/2669]  eta: 0:24:44  lr: 0.000007  min_lr: 0.000000  loss: 1.5039 (1.4711)  class_acc: 0.8500 (0.8815)  loss_scale: 16384.0000 (20705.0549)  weight_decay: 0.0500 (0.0500)  time: 0.8500  data: 0.0003  max mem: 26773
Epoch: [28]  [1100/2669]  eta: 0:23:09  lr: 0.000007  min_lr: 0.000000  loss: 1.3818 (1.4713)  class_acc: 0.9000 (0.8812)  loss_scale: 32768.0000 (21056.6394)  weight_decay: 0.0500 (0.0500)  time: 0.8527  data: 0.0003  max mem: 26773
Epoch: [28]  [1200/2669]  eta: 0:21:36  lr: 0.000007  min_lr: 0.000000  loss: 1.3750 (1.4719)  class_acc: 0.9000 (0.8815)  loss_scale: 16384.0000 (21404.2431)  weight_decay: 0.0500 (0.0500)  time: 0.8488  data: 0.0002  max mem: 26773
Epoch: [28]  [1300/2669]  eta: 0:20:05  lr: 0.000006  min_lr: 0.000000  loss: 1.3545 (1.4714)  class_acc: 0.9000 (0.8811)  loss_scale: 16384.0000 (21018.3674)  weight_decay: 0.0500 (0.0500)  time: 0.8517  data: 0.0002  max mem: 26773
Epoch: [28]  [1400/2669]  eta: 0:18:35  lr: 0.000006  min_lr: 0.000000  loss: 1.4639 (1.4719)  class_acc: 0.8500 (0.8808)  loss_scale: 16384.0000 (20687.5774)  weight_decay: 0.0500 (0.0500)  time: 0.8718  data: 0.0003  max mem: 26773
Epoch: [28]  [1500/2669]  eta: 0:17:05  lr: 0.000006  min_lr: 0.000000  loss: 1.3223 (1.4674)  class_acc: 0.8500 (0.8820)  loss_scale: 16384.0000 (20400.8634)  weight_decay: 0.0500 (0.0500)  time: 0.8482  data: 0.0003  max mem: 26773
Epoch: [28]  [1600/2669]  eta: 0:15:35  lr: 0.000006  min_lr: 0.000000  loss: 1.5068 (1.4638)  class_acc: 0.8500 (0.8822)  loss_scale: 16384.0000 (20149.9663)  weight_decay: 0.0500 (0.0500)  time: 0.8480  data: 0.0003  max mem: 26773
Epoch: [28]  [1700/2669]  eta: 0:14:06  lr: 0.000005  min_lr: 0.000000  loss: 1.3730 (1.4648)  class_acc: 0.9000 (0.8818)  loss_scale: 32768.0000 (20217.5285)  weight_decay: 0.0500 (0.0500)  time: 0.8479  data: 0.0002  max mem: 26773
Epoch: [28]  [1800/2669]  eta: 0:12:37  lr: 0.000005  min_lr: 0.000000  loss: 1.3779 (1.4622)  class_acc: 0.9000 (0.8827)  loss_scale: 16384.0000 (20423.1427)  weight_decay: 0.0500 (0.0500)  time: 0.8502  data: 0.0002  max mem: 26773
Epoch: [28]  [1900/2669]  eta: 0:11:09  lr: 0.000005  min_lr: 0.000000  loss: 1.4014 (1.4625)  class_acc: 0.9000 (0.8827)  loss_scale: 16384.0000 (20210.6681)  weight_decay: 0.0500 (0.0500)  time: 0.8477  data: 0.0003  max mem: 26773
Epoch: [28]  [2000/2669]  eta: 0:09:42  lr: 0.000005  min_lr: 0.000000  loss: 1.3799 (1.4625)  class_acc: 0.9000 (0.8830)  loss_scale: 16384.0000 (20019.4303)  weight_decay: 0.0500 (0.0500)  time: 0.8495  data: 0.0002  max mem: 26773
Epoch: [28]  [2100/2669]  eta: 0:08:14  lr: 0.000004  min_lr: 0.000000  loss: 1.4229 (1.4615)  class_acc: 0.8500 (0.8834)  loss_scale: 16384.0000 (19846.3970)  weight_decay: 0.0500 (0.0500)  time: 0.8529  data: 0.0002  max mem: 26773
Epoch: [28]  [2200/2669]  eta: 0:06:47  lr: 0.000004  min_lr: 0.000000  loss: 1.3389 (1.4618)  class_acc: 0.9000 (0.8832)  loss_scale: 16384.0000 (19689.0868)  weight_decay: 0.0500 (0.0500)  time: 0.8492  data: 0.0003  max mem: 26773
Epoch: [28]  [2300/2669]  eta: 0:05:20  lr: 0.000004  min_lr: 0.000000  loss: 1.4219 (1.4604)  class_acc: 0.8500 (0.8836)  loss_scale: 32768.0000 (19816.0243)  weight_decay: 0.0500 (0.0500)  time: 0.8507  data: 0.0002  max mem: 26773
Epoch: [28]  [2400/2669]  eta: 0:03:53  lr: 0.000004  min_lr: 0.000000  loss: 1.4102 (1.4602)  class_acc: 0.9000 (0.8837)  loss_scale: 16384.0000 (19686.7305)  weight_decay: 0.0500 (0.0500)  time: 0.8494  data: 0.0002  max mem: 26773
Epoch: [28]  [2500/2669]  eta: 0:02:26  lr: 0.000004  min_lr: 0.000000  loss: 1.2998 (1.4591)  class_acc: 0.9000 (0.8842)  loss_scale: 16384.0000 (19554.6741)  weight_decay: 0.0500 (0.0500)  time: 0.8639  data: 0.0002  max mem: 26773
Epoch: [28]  [2600/2669]  eta: 0:00:59  lr: 0.000003  min_lr: 0.000000  loss: 1.3740 (1.4585)  class_acc: 0.9000 (0.8845)  loss_scale: 16384.0000 (19432.7720)  weight_decay: 0.0500 (0.0500)  time: 0.8524  data: 0.0002  max mem: 26773
Epoch: [28]  [2668/2669]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000000  loss: 1.5967 (1.4594)  class_acc: 0.8500 (0.8845)  loss_scale: 16384.0000 (19356.2099)  weight_decay: 0.0500 (0.0500)  time: 0.8142  data: 0.0009  max mem: 26773
Epoch: [28] Total time: 0:38:29 (0.8655 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000000  loss: 1.5967 (1.4541)  class_acc: 0.8500 (0.8857)  loss_scale: 16384.0000 (19356.2099)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Test:  [  0/105]  eta: 0:06:12  loss: 0.3102 (0.3102)  acc1: 95.0000 (95.0000)  acc5: 100.0000 (100.0000)  time: 3.5522  data: 3.2568  max mem: 26773
Test:  [104/105]  eta: 0:00:00  loss: 0.4598 (0.6494)  acc1: 85.0000 (87.7639)  acc5: 100.0000 (97.5528)  time: 0.2866  data: 0.0001  max mem: 26773
Test: Total time: 0:00:34 (0.3326 s / it)
* Acc@1 87.128 Acc@5 97.701 loss 0.653
Accuracy of the network on the 50000 test images: 87.1%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Test:  [  0/105]  eta: 0:06:12  loss: 0.2407 (0.2407)  acc1: 95.0000 (95.0000)  acc5: 100.0000 (100.0000)  time: 3.5456  data: 3.2475  max mem: 26773
Test:  [104/105]  eta: 0:00:00  loss: 0.4230 (0.5658)  acc1: 90.0000 (88.6276)  acc5: 100.0000 (98.3205)  time: 0.2895  data: 0.0002  max mem: 26773
Test: Total time: 0:00:35 (0.3353 s / it)
* Acc@1 87.810 Acc@5 98.231 loss 0.564
EMA Accuracy of the network on the 50000 test images: 87.8%
Max accuracy: 88.33%
{"train_lr": 6.503786772843638e-06, "train_min_lr": 1.3677294885650957e-10, "train_loss": 1.4540562055398083, "train_class_acc": 0.8857180922506735, "train_loss_scale": 19356.209895052474, "train_weight_decay": 0.04999999999999853, "test_loss": 0.5639824760989064, "test_acc1": 87.80990083173384, "test_acc5": 98.23056621880998, "epoch": 28, "n_parameters": 304532456}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Epoch: [29]  [   0/2669]  eta: 1 day, 5:13:47  lr: 0.000003  min_lr: 0.000000  loss: 1.5537 (1.5537)  class_acc: 0.9000 (0.9000)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 39.4259  data: 34.4788  max mem: 26773
Epoch: [29]  [ 100/2669]  eta: 0:52:36  lr: 0.000003  min_lr: 0.000000  loss: 1.5098 (1.5001)  class_acc: 0.8500 (0.8703)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8458  data: 0.0002  max mem: 26773
Epoch: [29]  [ 200/2669]  eta: 0:42:49  lr: 0.000003  min_lr: 0.000000  loss: 1.3818 (1.4897)  class_acc: 0.9000 (0.8734)  loss_scale: 16384.0000 (19644.4975)  weight_decay: 0.0500 (0.0500)  time: 0.8570  data: 0.0002  max mem: 26773
Epoch: [29]  [ 300/2669]  eta: 0:38:35  lr: 0.000003  min_lr: 0.000000  loss: 1.5146 (1.4773)  class_acc: 0.9000 (0.8792)  loss_scale: 16384.0000 (18561.2757)  weight_decay: 0.0500 (0.0500)  time: 0.8500  data: 0.0002  max mem: 26773
Epoch: [29]  [ 400/2669]  eta: 0:35:46  lr: 0.000003  min_lr: 0.000000  loss: 1.2441 (1.4622)  class_acc: 0.9500 (0.8820)  loss_scale: 16384.0000 (18018.3142)  weight_decay: 0.0500 (0.0500)  time: 0.8516  data: 0.0002  max mem: 26773
Epoch: [29]  [ 500/2669]  eta: 0:33:29  lr: 0.000003  min_lr: 0.000000  loss: 1.4756 (1.4674)  class_acc: 0.8500 (0.8815)  loss_scale: 8192.0000 (16939.9441)  weight_decay: 0.0500 (0.0500)  time: 0.8501  data: 0.0002  max mem: 26773
Epoch: [29]  [ 600/2669]  eta: 0:31:30  lr: 0.000002  min_lr: 0.000000  loss: 1.3662 (1.4612)  class_acc: 0.9000 (0.8824)  loss_scale: 8192.0000 (15484.3794)  weight_decay: 0.0500 (0.0500)  time: 0.8518  data: 0.0002  max mem: 26773
Epoch: [29]  [ 700/2669]  eta: 0:29:41  lr: 0.000002  min_lr: 0.000000  loss: 1.3184 (1.4549)  class_acc: 0.9000 (0.8830)  loss_scale: 8192.0000 (14444.0970)  weight_decay: 0.0500 (0.0500)  time: 0.8491  data: 0.0002  max mem: 26773
Epoch: [29]  [ 800/2669]  eta: 0:27:58  lr: 0.000002  min_lr: 0.000000  loss: 1.4014 (1.4572)  class_acc: 0.9000 (0.8825)  loss_scale: 8192.0000 (13663.5605)  weight_decay: 0.0500 (0.0500)  time: 0.8501  data: 0.0002  max mem: 26773
Epoch: [29]  [ 900/2669]  eta: 0:26:18  lr: 0.000002  min_lr: 0.000000  loss: 1.3896 (1.4561)  class_acc: 0.9000 (0.8829)  loss_scale: 8192.0000 (13056.2841)  weight_decay: 0.0500 (0.0500)  time: 0.8492  data: 0.0002  max mem: 26773
Epoch: [29]  [1000/2669]  eta: 0:24:42  lr: 0.000002  min_lr: 0.000000  loss: 1.4521 (1.4543)  class_acc: 0.8500 (0.8835)  loss_scale: 16384.0000 (12815.8561)  weight_decay: 0.0500 (0.0500)  time: 0.8493  data: 0.0002  max mem: 26773
Epoch: [29]  [1100/2669]  eta: 0:23:08  lr: 0.000002  min_lr: 0.000000  loss: 1.3428 (1.4524)  class_acc: 0.9000 (0.8842)  loss_scale: 16384.0000 (13139.9382)  weight_decay: 0.0500 (0.0500)  time: 0.8504  data: 0.0002  max mem: 26773
Epoch: [29]  [1200/2669]  eta: 0:21:35  lr: 0.000002  min_lr: 0.000000  loss: 1.3281 (1.4446)  class_acc: 0.9000 (0.8860)  loss_scale: 16384.0000 (13410.0516)  weight_decay: 0.0500 (0.0500)  time: 0.8503  data: 0.0002  max mem: 26773
Epoch: [29]  [1300/2669]  eta: 0:20:04  lr: 0.000002  min_lr: 0.000000  loss: 1.3486 (1.4426)  class_acc: 0.8500 (0.8860)  loss_scale: 16384.0000 (13638.6410)  weight_decay: 0.0500 (0.0500)  time: 0.8481  data: 0.0002  max mem: 26773
Epoch: [29]  [1400/2669]  eta: 0:18:33  lr: 0.000002  min_lr: 0.000000  loss: 1.3809 (1.4440)  class_acc: 0.9000 (0.8862)  loss_scale: 16384.0000 (13834.5981)  weight_decay: 0.0500 (0.0500)  time: 0.8458  data: 0.0003  max mem: 26773
Epoch: [29]  [1500/2669]  eta: 0:17:03  lr: 0.000001  min_lr: 0.000000  loss: 1.3418 (1.4417)  class_acc: 0.9000 (0.8870)  loss_scale: 32768.0000 (14200.9221)  weight_decay: 0.0500 (0.0500)  time: 0.8501  data: 0.0002  max mem: 26773
Epoch: [29]  [1600/2669]  eta: 0:15:34  lr: 0.000001  min_lr: 0.000000  loss: 1.5684 (1.4423)  class_acc: 0.8500 (0.8869)  loss_scale: 32768.0000 (15299.2380)  weight_decay: 0.0500 (0.0500)  time: 0.8485  data: 0.0002  max mem: 26773
Epoch: [29]  [1700/2669]  eta: 0:14:05  lr: 0.000001  min_lr: 0.000000  loss: 1.4814 (1.4409)  class_acc: 0.8500 (0.8873)  loss_scale: 16384.0000 (15363.0100)  weight_decay: 0.0500 (0.0500)  time: 0.8617  data: 0.0002  max mem: 26773
Epoch: [29]  [1800/2669]  eta: 0:12:37  lr: 0.000001  min_lr: 0.000000  loss: 1.3545 (1.4405)  class_acc: 0.9000 (0.8874)  loss_scale: 16384.0000 (15419.7002)  weight_decay: 0.0500 (0.0500)  time: 0.8482  data: 0.0002  max mem: 26773
Epoch: [29]  [1900/2669]  eta: 0:11:09  lr: 0.000001  min_lr: 0.000000  loss: 1.5137 (1.4424)  class_acc: 0.8500 (0.8869)  loss_scale: 16384.0000 (15470.4261)  weight_decay: 0.0500 (0.0500)  time: 0.8487  data: 0.0002  max mem: 26773
Epoch: [29]  [2000/2669]  eta: 0:09:41  lr: 0.000001  min_lr: 0.000000  loss: 1.2949 (1.4402)  class_acc: 0.9000 (0.8874)  loss_scale: 16384.0000 (15516.0820)  weight_decay: 0.0500 (0.0500)  time: 0.8488  data: 0.0002  max mem: 26773
Epoch: [29]  [2100/2669]  eta: 0:08:14  lr: 0.000001  min_lr: 0.000000  loss: 1.3760 (1.4392)  class_acc: 0.9000 (0.8876)  loss_scale: 16384.0000 (15557.3917)  weight_decay: 0.0500 (0.0500)  time: 0.8468  data: 0.0002  max mem: 26773
Epoch: [29]  [2200/2669]  eta: 0:06:46  lr: 0.000001  min_lr: 0.000000  loss: 1.4258 (1.4417)  class_acc: 0.8500 (0.8868)  loss_scale: 16384.0000 (15684.2744)  weight_decay: 0.0500 (0.0500)  time: 0.8468  data: 0.0002  max mem: 26773
Epoch: [29]  [2300/2669]  eta: 0:05:19  lr: 0.000001  min_lr: 0.000000  loss: 1.3779 (1.4409)  class_acc: 0.9000 (0.8873)  loss_scale: 16384.0000 (15714.6841)  weight_decay: 0.0500 (0.0500)  time: 0.8622  data: 0.0003  max mem: 26773
Epoch: [29]  [2400/2669]  eta: 0:03:53  lr: 0.000001  min_lr: 0.000000  loss: 1.4639 (1.4413)  class_acc: 0.9000 (0.8877)  loss_scale: 16384.0000 (15742.5606)  weight_decay: 0.0500 (0.0500)  time: 0.8612  data: 0.0002  max mem: 26773
Epoch: [29]  [2500/2669]  eta: 0:02:26  lr: 0.000001  min_lr: 0.000000  loss: 1.3955 (1.4413)  class_acc: 0.9000 (0.8876)  loss_scale: 16384.0000 (15768.2079)  weight_decay: 0.0500 (0.0500)  time: 0.8483  data: 0.0003  max mem: 26773
Epoch: [29]  [2600/2669]  eta: 0:00:59  lr: 0.000001  min_lr: 0.000000  loss: 1.4678 (1.4419)  class_acc: 0.9000 (0.8877)  loss_scale: 16384.0000 (15791.8831)  weight_decay: 0.0500 (0.0500)  time: 0.8496  data: 0.0002  max mem: 26773
Epoch: [29]  [2668/2669]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.4180 (1.4433)  class_acc: 0.9000 (0.8873)  loss_scale: 32768.0000 (15984.8396)  weight_decay: 0.0500 (0.0500)  time: 0.8182  data: 0.0009  max mem: 26773
Epoch: [29] Total time: 0:38:28 (0.8651 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.4180 (1.4521)  class_acc: 0.9000 (0.8860)  loss_scale: 32768.0000 (15984.8396)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Test:  [  0/105]  eta: 0:06:16  loss: 0.3117 (0.3117)  acc1: 95.0000 (95.0000)  acc5: 100.0000 (100.0000)  time: 3.5846  data: 3.2887  max mem: 26773
Test:  [104/105]  eta: 0:00:00  loss: 0.4672 (0.6497)  acc1: 85.0000 (87.7159)  acc5: 100.0000 (97.5528)  time: 0.2867  data: 0.0001  max mem: 26773
Test: Total time: 0:00:34 (0.3331 s / it)
* Acc@1 87.110 Acc@5 97.689 loss 0.654
Accuracy of the network on the 50000 test images: 87.1%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
Test:  [  0/105]  eta: 0:09:01  loss: 0.2453 (0.2453)  acc1: 95.0000 (95.0000)  acc5: 100.0000 (100.0000)  time: 5.1571  data: 4.8567  max mem: 26773
Test:  [104/105]  eta: 0:00:00  loss: 0.4253 (0.5733)  acc1: 90.0000 (88.4837)  acc5: 100.0000 (98.3205)  time: 0.2894  data: 0.0001  max mem: 26773
Test: Total time: 0:00:36 (0.3473 s / it)
* Acc@1 87.730 Acc@5 98.195 loss 0.572
EMA Accuracy of the network on the 50000 test images: 87.7%
Max accuracy: 88.33%
{"train_lr": 1.7894014932455233e-06, "train_min_lr": 3.763064926134158e-11, "train_loss": 1.452066462496291, "train_class_acc": 0.886000763550438, "train_loss_scale": 15984.839580209895, "train_weight_decay": 0.04999999999999853, "test_loss": 0.5723392812564733, "test_acc1": 87.72992642354447, "test_acc5": 98.19457773512477, "epoch": 29, "n_parameters": 304532456}
Training time 19:51:05
