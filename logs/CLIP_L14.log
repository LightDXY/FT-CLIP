/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
**********find WORLD_SIZE 8 in env**********
**********find WORLD_SIZE 8 in env**********
**********find WORLD_SIZE 8 in env**********
**********find WORLD_SIZE 8 in env**********
**********find WORLD_SIZE 8 in env**********
**********find WORLD_SIZE 8 in env**********
**********find WORLD_SIZE 8 in env**********
**********find WORLD_SIZE 8 in env**********
************World_size is 8, current rank 0 ***********, local rank 0
************World_size is 8, current rank 5 ***********, local rank 5************World_size is 8, current rank 6 ***********, local rank 6
************World_size is 8, current rank 7 ***********, local rank 7

************World_size is 8, current rank 2 ***********, local rank 2
************World_size is 8, current rank 3 ***********, local rank 3
************World_size is 8, current rank 1 ***********, local rank 1
************World_size is 8, current rank 4 ***********, local rank 4
speec363a0001JL:64161:64161 [0] NCCL INFO Bootstrap : Using eth0:192.168.0.162<0>
speec363a0001JL:64161:64161 [0] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v4 symbol.
speec363a0001JL:64161:64161 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
speec363a0001JL:64161:64161 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [4]mlx5_4:1/IB [5]mlx5_5:1/IB [6]mlx5_6:1/IB [7]mlx5_7:1/IB ; OOB eth0:192.168.0.162<0>
speec363a0001JL:64161:64161 [0] NCCL INFO Using network IB
NCCL version 2.10.3+cuda11.3
speec363a0001JL:64162:64162 [1] NCCL INFO Bootstrap : Using eth0:192.168.0.162<0>
speec363a0001JL:64163:64163 [2] NCCL INFO Bootstrap : Using eth0:192.168.0.162<0>
speec363a0001JL:64168:64168 [7] NCCL INFO Bootstrap : Using eth0:192.168.0.162<0>
speec363a0001JL:64162:64162 [1] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v4 symbol.
speec363a0001JL:64163:64163 [2] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v4 symbol.
speec363a0001JL:64168:64168 [7] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v4 symbol.
speec363a0001JL:64162:64162 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
speec363a0001JL:64168:64168 [7] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
speec363a0001JL:64163:64163 [2] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
speec363a0001JL:64165:64165 [4] NCCL INFO Bootstrap : Using eth0:192.168.0.162<0>
speec363a0001JL:64165:64165 [4] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v4 symbol.
speec363a0001JL:64165:64165 [4] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
speec363a0001JL:64162:64162 [1] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [4]mlx5_4:1/IB [5]mlx5_5:1/IB [6]mlx5_6:1/IB [7]mlx5_7:1/IB ; OOB eth0:192.168.0.162<0>
speec363a0001JL:64162:64162 [1] NCCL INFO Using network IB
speec363a0001JL:64168:64168 [7] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [4]mlx5_4:1/IB [5]mlx5_5:1/IB [6]mlx5_6:1/IB [7]mlx5_7:1/IB ; OOB eth0:192.168.0.162<0>
speec363a0001JL:64163:64163 [2] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [4]mlx5_4:1/IB [5]mlx5_5:1/IB [6]mlx5_6:1/IB [7]mlx5_7:1/IB ; OOB eth0:192.168.0.162<0>
speec363a0001JL:64168:64168 [7] NCCL INFO Using network IB
speec363a0001JL:64163:64163 [2] NCCL INFO Using network IB
speec363a0001JL:64167:64167 [6] NCCL INFO Bootstrap : Using eth0:192.168.0.162<0>
speec363a0001JL:64164:64164 [3] NCCL INFO Bootstrap : Using eth0:192.168.0.162<0>
speec363a0001JL:64167:64167 [6] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v4 symbol.
speec363a0001JL:64167:64167 [6] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
speec363a0001JL:64164:64164 [3] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v4 symbol.
speec363a0001JL:64164:64164 [3] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
speec363a0001JL:64165:64165 [4] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [4]mlx5_4:1/IB [5]mlx5_5:1/IB [6]mlx5_6:1/IB [7]mlx5_7:1/IB ; OOB eth0:192.168.0.162<0>
speec363a0001JL:64165:64165 [4] NCCL INFO Using network IB
speec363a0001JL:64166:64166 [5] NCCL INFO Bootstrap : Using eth0:192.168.0.162<0>
speec363a0001JL:64166:64166 [5] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v4 symbol.
speec363a0001JL:64166:64166 [5] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
speec363a0001JL:64167:64167 [6] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [4]mlx5_4:1/IB [5]mlx5_5:1/IB [6]mlx5_6:1/IB [7]mlx5_7:1/IB ; OOB eth0:192.168.0.162<0>
speec363a0001JL:64167:64167 [6] NCCL INFO Using network IB
speec363a0001JL:64164:64164 [3] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [4]mlx5_4:1/IB [5]mlx5_5:1/IB [6]mlx5_6:1/IB [7]mlx5_7:1/IB ; OOB eth0:192.168.0.162<0>
speec363a0001JL:64164:64164 [3] NCCL INFO Using network IB
speec363a0001JL:64166:64166 [5] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [4]mlx5_4:1/IB [5]mlx5_5:1/IB [6]mlx5_6:1/IB [7]mlx5_7:1/IB ; OOB eth0:192.168.0.162<0>
speec363a0001JL:64166:64166 [5] NCCL INFO Using network IB
speec363a0001JL:64168:64456 [7] NCCL INFO NCCL_IB_TIMEOUT set by environment to 60.
speec363a0001JL:64162:64454 [1] NCCL INFO NCCL_IB_TIMEOUT set by environment to 60.
speec363a0001JL:64163:64457 [2] NCCL INFO NCCL_IB_TIMEOUT set by environment to 60.
speec363a0001JL:64167:64482 [6] NCCL INFO NCCL_IB_TIMEOUT set by environment to 60.
speec363a0001JL:64161:64427 [0] NCCL INFO NCCL_IB_TIMEOUT set by environment to 60.
speec363a0001JL:64165:64467 [4] NCCL INFO NCCL_IB_TIMEOUT set by environment to 60.
speec363a0001JL:64166:64490 [5] NCCL INFO NCCL_IB_TIMEOUT set by environment to 60.
speec363a0001JL:64164:64486 [3] NCCL INFO NCCL_IB_TIMEOUT set by environment to 60.
speec363a0001JL:64167:64482 [6] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
speec363a0001JL:64168:64456 [7] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
speec363a0001JL:64165:64467 [4] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
speec363a0001JL:64161:64427 [0] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
speec363a0001JL:64166:64490 [5] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
speec363a0001JL:64162:64454 [1] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
speec363a0001JL:64164:64486 [3] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
speec363a0001JL:64163:64457 [2] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
speec363a0001JL:64164:64486 [3] NCCL INFO Trees [0] 2/-1/-1->3->4 [1] 2/-1/-1->3->4 [2] 2/-1/-1->3->4 [3] 2/-1/-1->3->4 [4] 2/-1/-1->3->4 [5] 2/-1/-1->3->4 [6] 2/-1/-1->3->4 [7] 2/-1/-1->3->4 [8] 2/-1/-1->3->4 [9] 2/-1/-1->3->4 [10] 2/-1/-1->3->4 [11] 2/-1/-1->3->4 [12] 2/-1/-1->3->4 [13] 2/-1/-1->3->4 [14] 2/-1/-1->3->4 [15] 2/-1/-1->3->4 [16] 2/-1/-1->3->4 [17] 2/-1/-1->3->4 [18] 2/-1/-1->3->4 [19] 2/-1/-1->3->4 [20] 2/-1/-1->3->4 [21] 2/-1/-1->3->4 [22] 2/-1/-1->3->4 [23] 2/-1/-1->3->4
speec363a0001JL:64164:64486 [3] NCCL INFO Setting affinity for GPU 4 to ffff,0000ffff
speec363a0001JL:64165:64467 [4] NCCL INFO Trees [0] 3/-1/-1->4->5 [1] 3/-1/-1->4->5 [2] 3/-1/-1->4->5 [3] 3/-1/-1->4->5 [4] 3/-1/-1->4->5 [5] 3/-1/-1->4->5 [6] 3/-1/-1->4->5 [7] 3/-1/-1->4->5 [8] 3/-1/-1->4->5 [9] 3/-1/-1->4->5 [10] 3/-1/-1->4->5 [11] 3/-1/-1->4->5 [12] 3/-1/-1->4->5 [13] 3/-1/-1->4->5 [14] 3/-1/-1->4->5 [15] 3/-1/-1->4->5 [16] 3/-1/-1->4->5 [17] 3/-1/-1->4->5 [18] 3/-1/-1->4->5 [19] 3/-1/-1->4->5 [20] 3/-1/-1->4->5 [21] 3/-1/-1->4->5 [22] 3/-1/-1->4->5 [23] 3/-1/-1->4->5
speec363a0001JL:64166:64490 [5] NCCL INFO Trees [0] 4/-1/-1->5->6 [1] 4/-1/-1->5->6 [2] 4/-1/-1->5->6 [3] 4/-1/-1->5->6 [4] 4/-1/-1->5->6 [5] 4/-1/-1->5->6 [6] 4/-1/-1->5->6 [7] 4/-1/-1->5->6 [8] 4/-1/-1->5->6 [9] 4/-1/-1->5->6 [10] 4/-1/-1->5->6 [11] 4/-1/-1->5->6 [12] 4/-1/-1->5->6 [13] 4/-1/-1->5->6 [14] 4/-1/-1->5->6 [15] 4/-1/-1->5->6 [16] 4/-1/-1->5->6 [17] 4/-1/-1->5->6 [18] 4/-1/-1->5->6 [19] 4/-1/-1->5->6 [20] 4/-1/-1->5->6 [21] 4/-1/-1->5->6 [22] 4/-1/-1->5->6 [23] 4/-1/-1->5->6
speec363a0001JL:64165:64467 [4] NCCL INFO Setting affinity for GPU 3 to ffff,0000ffff
speec363a0001JL:64166:64490 [5] NCCL INFO Setting affinity for GPU 2 to ffff,0000ffff
speec363a0001JL:64167:64482 [6] NCCL INFO Trees [0] 5/-1/-1->6->7 [1] 5/-1/-1->6->7 [2] 5/-1/-1->6->7 [3] 5/-1/-1->6->7 [4] 5/-1/-1->6->7 [5] 5/-1/-1->6->7 [6] 5/-1/-1->6->7 [7] 5/-1/-1->6->7 [8] 5/-1/-1->6->7 [9] 5/-1/-1->6->7 [10] 5/-1/-1->6->7 [11] 5/-1/-1->6->7 [12] 5/-1/-1->6->7 [13] 5/-1/-1->6->7 [14] 5/-1/-1->6->7 [15] 5/-1/-1->6->7 [16] 5/-1/-1->6->7 [17] 5/-1/-1->6->7 [18] 5/-1/-1->6->7 [19] 5/-1/-1->6->7 [20] 5/-1/-1->6->7 [21] 5/-1/-1->6->7 [22] 5/-1/-1->6->7 [23] 5/-1/-1->6->7
speec363a0001JL:64167:64482 [6] NCCL INFO Setting affinity for GPU 1 to ffff,0000ffff
speec363a0001JL:64168:64456 [7] NCCL INFO Trees [0] 6/-1/-1->7->-1 [1] 6/-1/-1->7->-1 [2] 6/-1/-1->7->-1 [3] 6/-1/-1->7->-1 [4] 6/-1/-1->7->-1 [5] 6/-1/-1->7->-1 [6] 6/-1/-1->7->-1 [7] 6/-1/-1->7->-1 [8] 6/-1/-1->7->-1 [9] 6/-1/-1->7->-1 [10] 6/-1/-1->7->-1 [11] 6/-1/-1->7->-1 [12] 6/-1/-1->7->-1 [13] 6/-1/-1->7->-1 [14] 6/-1/-1->7->-1 [15] 6/-1/-1->7->-1 [16] 6/-1/-1->7->-1 [17] 6/-1/-1->7->-1 [18] 6/-1/-1->7->-1 [19] 6/-1/-1->7->-1 [20] 6/-1/-1->7->-1 [21] 6/-1/-1->7->-1 [22] 6/-1/-1->7->-1 [23] 6/-1/-1->7->-1
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 00/24 :    0   7   6   5   4   3   2   1
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 01/24 :    0   7   6   5   4   3   2   1
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 02/24 :    0   7   6   5   4   3   2   1
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 03/24 :    0   7   6   5   4   3   2   1
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 04/24 :    0   7   6   5   4   3   2   1
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 05/24 :    0   7   6   5   4   3   2   1
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 06/24 :    0   7   6   5   4   3   2   1
speec363a0001JL:64162:64454 [1] NCCL INFO Trees [0] 0/-1/-1->1->2 [1] 0/-1/-1->1->2 [2] 0/-1/-1->1->2 [3] 0/-1/-1->1->2 [4] 0/-1/-1->1->2 [5] 0/-1/-1->1->2 [6] 0/-1/-1->1->2 [7] 0/-1/-1->1->2 [8] 0/-1/-1->1->2 [9] 0/-1/-1->1->2 [10] 0/-1/-1->1->2 [11] 0/-1/-1->1->2 [12] 0/-1/-1->1->2 [13] 0/-1/-1->1->2 [14] 0/-1/-1->1->2 [15] 0/-1/-1->1->2 [16] 0/-1/-1->1->2 [17] 0/-1/-1->1->2 [18] 0/-1/-1->1->2 [19] 0/-1/-1->1->2 [20] 0/-1/-1->1->2 [21] 0/-1/-1->1->2 [22] 0/-1/-1->1->2 [23] 0/-1/-1->1->2
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 07/24 :    0   7   6   5   4   3   2   1
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 08/24 :    0   7   6   5   4   3   2   1
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 09/24 :    0   7   6   5   4   3   2   1
speec363a0001JL:64162:64454 [1] NCCL INFO Setting affinity for GPU 6 to ffff,0000ffff
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 10/24 :    0   7   6   5   4   3   2   1
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 11/24 :    0   7   6   5   4   3   2   1
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 12/24 :    0   7   6   5   4   3   2   1
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 13/24 :    0   7   6   5   4   3   2   1
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 14/24 :    0   7   6   5   4   3   2   1
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 15/24 :    0   7   6   5   4   3   2   1
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 16/24 :    0   7   6   5   4   3   2   1
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 17/24 :    0   7   6   5   4   3   2   1
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 18/24 :    0   7   6   5   4   3   2   1
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 19/24 :    0   7   6   5   4   3   2   1
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 20/24 :    0   7   6   5   4   3   2   1
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 21/24 :    0   7   6   5   4   3   2   1
speec363a0001JL:64163:64457 [2] NCCL INFO Trees [0] 1/-1/-1->2->3 [1] 1/-1/-1->2->3 [2] 1/-1/-1->2->3 [3] 1/-1/-1->2->3 [4] 1/-1/-1->2->3 [5] 1/-1/-1->2->3 [6] 1/-1/-1->2->3 [7] 1/-1/-1->2->3 [8] 1/-1/-1->2->3 [9] 1/-1/-1->2->3 [10] 1/-1/-1->2->3 [11] 1/-1/-1->2->3 [12] 1/-1/-1->2->3 [13] 1/-1/-1->2->3 [14] 1/-1/-1->2->3 [15] 1/-1/-1->2->3 [16] 1/-1/-1->2->3 [17] 1/-1/-1->2->3 [18] 1/-1/-1->2->3 [19] 1/-1/-1->2->3 [20] 1/-1/-1->2->3 [21] 1/-1/-1->2->3 [22] 1/-1/-1->2->3 [23] 1/-1/-1->2->3
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 22/24 :    0   7   6   5   4   3   2   1
speec363a0001JL:64168:64456 [7] NCCL INFO Setting affinity for GPU 0 to ffff,0000ffff
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 23/24 :    0   7   6   5   4   3   2   1
speec363a0001JL:64163:64457 [2] NCCL INFO Setting affinity for GPU 5 to ffff,0000ffff
speec363a0001JL:64161:64427 [0] NCCL INFO Trees [0] -1/-1/-1->0->1 [1] -1/-1/-1->0->1 [2] -1/-1/-1->0->1 [3] -1/-1/-1->0->1 [4] -1/-1/-1->0->1 [5] -1/-1/-1->0->1 [6] -1/-1/-1->0->1 [7] -1/-1/-1->0->1 [8] -1/-1/-1->0->1 [9] -1/-1/-1->0->1 [10] -1/-1/-1->0->1 [11] -1/-1/-1->0->1 [12] -1/-1/-1->0->1 [13] -1/-1/-1->0->1 [14] -1/-1/-1->0->1 [15] -1/-1/-1->0->1 [16] -1/-1/-1->0->1 [17] -1/-1/-1->0->1 [18] -1/-1/-1->0->1 [19] -1/-1/-1->0->1 [20] -1/-1/-1->0->1 [21] -1/-1/-1->0->1 [22] -1/-1/-1->0->1 [23] -1/-1/-1->0->1
speec363a0001JL:64161:64427 [0] NCCL INFO Setting affinity for GPU 7 to ffff,0000ffff
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 00 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 01 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64165:64467 [4] NCCL INFO Channel 00 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64163:64457 [2] NCCL INFO Channel 00 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64166:64490 [5] NCCL INFO Channel 00 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64167:64482 [6] NCCL INFO Channel 00 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64164:64486 [3] NCCL INFO Channel 00 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 02 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64165:64467 [4] NCCL INFO Channel 01 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64163:64457 [2] NCCL INFO Channel 01 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64166:64490 [5] NCCL INFO Channel 01 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64162:64454 [1] NCCL INFO Channel 00 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JL:64167:64482 [6] NCCL INFO Channel 01 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64164:64486 [3] NCCL INFO Channel 01 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64168:64456 [7] NCCL INFO Channel 00 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 03 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64163:64457 [2] NCCL INFO Channel 02 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64165:64467 [4] NCCL INFO Channel 02 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64166:64490 [5] NCCL INFO Channel 02 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64167:64482 [6] NCCL INFO Channel 02 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64164:64486 [3] NCCL INFO Channel 02 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64162:64454 [1] NCCL INFO Channel 01 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JL:64168:64456 [7] NCCL INFO Channel 01 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 04 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64163:64457 [2] NCCL INFO Channel 03 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64165:64467 [4] NCCL INFO Channel 03 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64166:64490 [5] NCCL INFO Channel 03 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64164:64486 [3] NCCL INFO Channel 03 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64167:64482 [6] NCCL INFO Channel 03 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64162:64454 [1] NCCL INFO Channel 02 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JL:64168:64456 [7] NCCL INFO Channel 02 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 05 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64163:64457 [2] NCCL INFO Channel 04 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64165:64467 [4] NCCL INFO Channel 04 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64166:64490 [5] NCCL INFO Channel 04 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64164:64486 [3] NCCL INFO Channel 04 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64167:64482 [6] NCCL INFO Channel 04 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64162:64454 [1] NCCL INFO Channel 03 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JL:64168:64456 [7] NCCL INFO Channel 03 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 06 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64163:64457 [2] NCCL INFO Channel 05 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64165:64467 [4] NCCL INFO Channel 05 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64166:64490 [5] NCCL INFO Channel 05 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64164:64486 [3] NCCL INFO Channel 05 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64167:64482 [6] NCCL INFO Channel 05 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64162:64454 [1] NCCL INFO Channel 04 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JL:64168:64456 [7] NCCL INFO Channel 04 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 07 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64163:64457 [2] NCCL INFO Channel 06 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64165:64467 [4] NCCL INFO Channel 06 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64166:64490 [5] NCCL INFO Channel 06 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64164:64486 [3] NCCL INFO Channel 06 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64167:64482 [6] NCCL INFO Channel 06 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64162:64454 [1] NCCL INFO Channel 05 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JL:64168:64456 [7] NCCL INFO Channel 05 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 08 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64163:64457 [2] NCCL INFO Channel 07 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64165:64467 [4] NCCL INFO Channel 07 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64166:64490 [5] NCCL INFO Channel 07 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64164:64486 [3] NCCL INFO Channel 07 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64167:64482 [6] NCCL INFO Channel 07 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64162:64454 [1] NCCL INFO Channel 06 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JL:64168:64456 [7] NCCL INFO Channel 06 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 09 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64163:64457 [2] NCCL INFO Channel 08 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64165:64467 [4] NCCL INFO Channel 08 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64166:64490 [5] NCCL INFO Channel 08 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64164:64486 [3] NCCL INFO Channel 08 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64167:64482 [6] NCCL INFO Channel 08 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64162:64454 [1] NCCL INFO Channel 07 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JL:64168:64456 [7] NCCL INFO Channel 07 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 10 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64163:64457 [2] NCCL INFO Channel 09 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64165:64467 [4] NCCL INFO Channel 09 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64166:64490 [5] NCCL INFO Channel 09 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64164:64486 [3] NCCL INFO Channel 09 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64167:64482 [6] NCCL INFO Channel 09 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64162:64454 [1] NCCL INFO Channel 08 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JL:64168:64456 [7] NCCL INFO Channel 08 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 11 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64163:64457 [2] NCCL INFO Channel 10 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64165:64467 [4] NCCL INFO Channel 10 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64166:64490 [5] NCCL INFO Channel 10 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64164:64486 [3] NCCL INFO Channel 10 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64167:64482 [6] NCCL INFO Channel 10 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64162:64454 [1] NCCL INFO Channel 09 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JL:64168:64456 [7] NCCL INFO Channel 09 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 12 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64163:64457 [2] NCCL INFO Channel 11 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64165:64467 [4] NCCL INFO Channel 11 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64166:64490 [5] NCCL INFO Channel 11 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64164:64486 [3] NCCL INFO Channel 11 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64167:64482 [6] NCCL INFO Channel 11 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64162:64454 [1] NCCL INFO Channel 10 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JL:64168:64456 [7] NCCL INFO Channel 10 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 13 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64163:64457 [2] NCCL INFO Channel 12 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64165:64467 [4] NCCL INFO Channel 12 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64166:64490 [5] NCCL INFO Channel 12 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64164:64486 [3] NCCL INFO Channel 12 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64167:64482 [6] NCCL INFO Channel 12 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64162:64454 [1] NCCL INFO Channel 11 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JL:64168:64456 [7] NCCL INFO Channel 11 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 14 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64163:64457 [2] NCCL INFO Channel 13 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64165:64467 [4] NCCL INFO Channel 13 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64166:64490 [5] NCCL INFO Channel 13 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64164:64486 [3] NCCL INFO Channel 13 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64167:64482 [6] NCCL INFO Channel 13 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64162:64454 [1] NCCL INFO Channel 12 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JL:64168:64456 [7] NCCL INFO Channel 12 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 15 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64163:64457 [2] NCCL INFO Channel 14 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64165:64467 [4] NCCL INFO Channel 14 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64166:64490 [5] NCCL INFO Channel 14 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64164:64486 [3] NCCL INFO Channel 14 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64167:64482 [6] NCCL INFO Channel 14 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64162:64454 [1] NCCL INFO Channel 13 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JL:64168:64456 [7] NCCL INFO Channel 13 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 16 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64163:64457 [2] NCCL INFO Channel 15 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64165:64467 [4] NCCL INFO Channel 15 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64166:64490 [5] NCCL INFO Channel 15 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64164:64486 [3] NCCL INFO Channel 15 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64167:64482 [6] NCCL INFO Channel 15 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64162:64454 [1] NCCL INFO Channel 14 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JL:64168:64456 [7] NCCL INFO Channel 14 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 17 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64163:64457 [2] NCCL INFO Channel 16 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64165:64467 [4] NCCL INFO Channel 16 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64166:64490 [5] NCCL INFO Channel 16 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64164:64486 [3] NCCL INFO Channel 16 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64167:64482 [6] NCCL INFO Channel 16 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64162:64454 [1] NCCL INFO Channel 15 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JL:64168:64456 [7] NCCL INFO Channel 15 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 18 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64163:64457 [2] NCCL INFO Channel 17 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64165:64467 [4] NCCL INFO Channel 17 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64166:64490 [5] NCCL INFO Channel 17 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64164:64486 [3] NCCL INFO Channel 17 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64167:64482 [6] NCCL INFO Channel 17 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64162:64454 [1] NCCL INFO Channel 16 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JL:64168:64456 [7] NCCL INFO Channel 16 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 19 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64163:64457 [2] NCCL INFO Channel 18 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64165:64467 [4] NCCL INFO Channel 18 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64166:64490 [5] NCCL INFO Channel 18 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64164:64486 [3] NCCL INFO Channel 18 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64167:64482 [6] NCCL INFO Channel 18 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64162:64454 [1] NCCL INFO Channel 17 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JL:64168:64456 [7] NCCL INFO Channel 17 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 20 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64163:64457 [2] NCCL INFO Channel 19 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64165:64467 [4] NCCL INFO Channel 19 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64166:64490 [5] NCCL INFO Channel 19 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64164:64486 [3] NCCL INFO Channel 19 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64167:64482 [6] NCCL INFO Channel 19 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64162:64454 [1] NCCL INFO Channel 18 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JL:64168:64456 [7] NCCL INFO Channel 18 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 21 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64163:64457 [2] NCCL INFO Channel 20 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64165:64467 [4] NCCL INFO Channel 20 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64166:64490 [5] NCCL INFO Channel 20 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64164:64486 [3] NCCL INFO Channel 20 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64167:64482 [6] NCCL INFO Channel 20 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64162:64454 [1] NCCL INFO Channel 19 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JL:64168:64456 [7] NCCL INFO Channel 19 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64163:64457 [2] NCCL INFO Channel 21 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64165:64467 [4] NCCL INFO Channel 21 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 22 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64166:64490 [5] NCCL INFO Channel 21 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64164:64486 [3] NCCL INFO Channel 21 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64167:64482 [6] NCCL INFO Channel 21 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64162:64454 [1] NCCL INFO Channel 20 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JL:64168:64456 [7] NCCL INFO Channel 20 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64163:64457 [2] NCCL INFO Channel 22 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64165:64467 [4] NCCL INFO Channel 22 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 23 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64166:64490 [5] NCCL INFO Channel 22 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64164:64486 [3] NCCL INFO Channel 22 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64167:64482 [6] NCCL INFO Channel 22 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64162:64454 [1] NCCL INFO Channel 21 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JL:64168:64456 [7] NCCL INFO Channel 21 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64163:64457 [2] NCCL INFO Channel 23 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64165:64467 [4] NCCL INFO Channel 23 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64166:64490 [5] NCCL INFO Channel 23 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64164:64486 [3] NCCL INFO Channel 23 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64167:64482 [6] NCCL INFO Channel 23 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64162:64454 [1] NCCL INFO Channel 22 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JL:64168:64456 [7] NCCL INFO Channel 22 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64162:64454 [1] NCCL INFO Channel 23 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JL:64168:64456 [7] NCCL INFO Channel 23 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64165:64467 [4] NCCL INFO Connected all rings
speec363a0001JL:64166:64490 [5] NCCL INFO Connected all rings
speec363a0001JL:64164:64486 [3] NCCL INFO Connected all rings
speec363a0001JL:64163:64457 [2] NCCL INFO Connected all rings
speec363a0001JL:64162:64454 [1] NCCL INFO Connected all rings
speec363a0001JL:64167:64482 [6] NCCL INFO Connected all rings
speec363a0001JL:64168:64456 [7] NCCL INFO Connected all rings
speec363a0001JL:64161:64427 [0] NCCL INFO Connected all rings
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 00 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 01 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 02 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 03 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 04 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 05 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 06 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 07 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 08 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 09 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 10 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 11 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 12 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 13 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 14 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 15 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 16 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 17 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 18 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 19 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 20 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 21 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 22 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64165:64467 [4] NCCL INFO Channel 00 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64166:64490 [5] NCCL INFO Channel 00 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64164:64486 [3] NCCL INFO Channel 00 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64161:64427 [0] NCCL INFO Channel 23 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64163:64457 [2] NCCL INFO Channel 00 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64165:64467 [4] NCCL INFO Channel 01 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64166:64490 [5] NCCL INFO Channel 01 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64162:64454 [1] NCCL INFO Channel 00 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64167:64482 [6] NCCL INFO Channel 00 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64164:64486 [3] NCCL INFO Channel 01 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64163:64457 [2] NCCL INFO Channel 01 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64165:64467 [4] NCCL INFO Channel 02 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64166:64490 [5] NCCL INFO Channel 02 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64162:64454 [1] NCCL INFO Channel 01 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64167:64482 [6] NCCL INFO Channel 01 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64164:64486 [3] NCCL INFO Channel 02 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64163:64457 [2] NCCL INFO Channel 02 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64165:64467 [4] NCCL INFO Channel 03 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64166:64490 [5] NCCL INFO Channel 03 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64162:64454 [1] NCCL INFO Channel 02 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64167:64482 [6] NCCL INFO Channel 02 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64164:64486 [3] NCCL INFO Channel 03 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64163:64457 [2] NCCL INFO Channel 03 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64165:64467 [4] NCCL INFO Channel 04 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64166:64490 [5] NCCL INFO Channel 04 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64162:64454 [1] NCCL INFO Channel 03 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64167:64482 [6] NCCL INFO Channel 03 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64164:64486 [3] NCCL INFO Channel 04 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64163:64457 [2] NCCL INFO Channel 04 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64165:64467 [4] NCCL INFO Channel 05 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64166:64490 [5] NCCL INFO Channel 05 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64162:64454 [1] NCCL INFO Channel 04 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64167:64482 [6] NCCL INFO Channel 04 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64164:64486 [3] NCCL INFO Channel 05 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64163:64457 [2] NCCL INFO Channel 05 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64165:64467 [4] NCCL INFO Channel 06 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64162:64454 [1] NCCL INFO Channel 05 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64166:64490 [5] NCCL INFO Channel 06 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64167:64482 [6] NCCL INFO Channel 05 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64164:64486 [3] NCCL INFO Channel 06 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64163:64457 [2] NCCL INFO Channel 06 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64165:64467 [4] NCCL INFO Channel 07 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64162:64454 [1] NCCL INFO Channel 06 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64166:64490 [5] NCCL INFO Channel 07 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64167:64482 [6] NCCL INFO Channel 06 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64164:64486 [3] NCCL INFO Channel 07 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64163:64457 [2] NCCL INFO Channel 07 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64162:64454 [1] NCCL INFO Channel 07 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64165:64467 [4] NCCL INFO Channel 08 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64166:64490 [5] NCCL INFO Channel 08 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64167:64482 [6] NCCL INFO Channel 07 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64164:64486 [3] NCCL INFO Channel 08 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64163:64457 [2] NCCL INFO Channel 08 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64162:64454 [1] NCCL INFO Channel 08 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64165:64467 [4] NCCL INFO Channel 09 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64166:64490 [5] NCCL INFO Channel 09 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64167:64482 [6] NCCL INFO Channel 08 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64164:64486 [3] NCCL INFO Channel 09 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64163:64457 [2] NCCL INFO Channel 09 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64162:64454 [1] NCCL INFO Channel 09 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64165:64467 [4] NCCL INFO Channel 10 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64166:64490 [5] NCCL INFO Channel 10 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64167:64482 [6] NCCL INFO Channel 09 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64164:64486 [3] NCCL INFO Channel 10 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64163:64457 [2] NCCL INFO Channel 10 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64162:64454 [1] NCCL INFO Channel 10 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64165:64467 [4] NCCL INFO Channel 11 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64166:64490 [5] NCCL INFO Channel 11 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64167:64482 [6] NCCL INFO Channel 10 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64164:64486 [3] NCCL INFO Channel 11 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64163:64457 [2] NCCL INFO Channel 11 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64165:64467 [4] NCCL INFO Channel 12 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64162:64454 [1] NCCL INFO Channel 11 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64166:64490 [5] NCCL INFO Channel 12 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64167:64482 [6] NCCL INFO Channel 11 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64164:64486 [3] NCCL INFO Channel 12 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64163:64457 [2] NCCL INFO Channel 12 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64162:64454 [1] NCCL INFO Channel 12 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64165:64467 [4] NCCL INFO Channel 13 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64166:64490 [5] NCCL INFO Channel 13 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64167:64482 [6] NCCL INFO Channel 12 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64164:64486 [3] NCCL INFO Channel 13 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64163:64457 [2] NCCL INFO Channel 13 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64162:64454 [1] NCCL INFO Channel 13 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64165:64467 [4] NCCL INFO Channel 14 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64166:64490 [5] NCCL INFO Channel 14 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64167:64482 [6] NCCL INFO Channel 13 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64164:64486 [3] NCCL INFO Channel 14 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64163:64457 [2] NCCL INFO Channel 14 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64162:64454 [1] NCCL INFO Channel 14 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64165:64467 [4] NCCL INFO Channel 15 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64166:64490 [5] NCCL INFO Channel 15 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64167:64482 [6] NCCL INFO Channel 14 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64164:64486 [3] NCCL INFO Channel 15 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64163:64457 [2] NCCL INFO Channel 15 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64162:64454 [1] NCCL INFO Channel 15 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64165:64467 [4] NCCL INFO Channel 16 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64166:64490 [5] NCCL INFO Channel 16 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64167:64482 [6] NCCL INFO Channel 15 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64164:64486 [3] NCCL INFO Channel 16 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64163:64457 [2] NCCL INFO Channel 16 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64165:64467 [4] NCCL INFO Channel 17 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64162:64454 [1] NCCL INFO Channel 16 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64166:64490 [5] NCCL INFO Channel 17 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64167:64482 [6] NCCL INFO Channel 16 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64164:64486 [3] NCCL INFO Channel 17 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64163:64457 [2] NCCL INFO Channel 17 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64165:64467 [4] NCCL INFO Channel 18 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64162:64454 [1] NCCL INFO Channel 17 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64166:64490 [5] NCCL INFO Channel 18 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64167:64482 [6] NCCL INFO Channel 17 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64164:64486 [3] NCCL INFO Channel 18 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64163:64457 [2] NCCL INFO Channel 18 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64162:64454 [1] NCCL INFO Channel 18 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64165:64467 [4] NCCL INFO Channel 19 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64166:64490 [5] NCCL INFO Channel 19 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64167:64482 [6] NCCL INFO Channel 18 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64164:64486 [3] NCCL INFO Channel 19 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64163:64457 [2] NCCL INFO Channel 19 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64162:64454 [1] NCCL INFO Channel 19 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64165:64467 [4] NCCL INFO Channel 20 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64166:64490 [5] NCCL INFO Channel 20 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64167:64482 [6] NCCL INFO Channel 19 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64164:64486 [3] NCCL INFO Channel 20 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64163:64457 [2] NCCL INFO Channel 20 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64162:64454 [1] NCCL INFO Channel 20 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64165:64467 [4] NCCL INFO Channel 21 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64166:64490 [5] NCCL INFO Channel 21 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64167:64482 [6] NCCL INFO Channel 20 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64164:64486 [3] NCCL INFO Channel 21 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64163:64457 [2] NCCL INFO Channel 21 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64165:64467 [4] NCCL INFO Channel 22 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64162:64454 [1] NCCL INFO Channel 21 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64166:64490 [5] NCCL INFO Channel 22 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64167:64482 [6] NCCL INFO Channel 21 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64164:64486 [3] NCCL INFO Channel 22 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64163:64457 [2] NCCL INFO Channel 22 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64165:64467 [4] NCCL INFO Channel 23 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64162:64454 [1] NCCL INFO Channel 22 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64166:64490 [5] NCCL INFO Channel 23 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64167:64482 [6] NCCL INFO Channel 22 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64164:64486 [3] NCCL INFO Channel 23 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64163:64457 [2] NCCL INFO Channel 23 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64162:64454 [1] NCCL INFO Channel 23 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64167:64482 [6] NCCL INFO Channel 23 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64161:64427 [0] NCCL INFO Connected all trees
speec363a0001JL:64161:64427 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
speec363a0001JL:64168:64456 [7] NCCL INFO Connected all trees
speec363a0001JL:64168:64456 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
speec363a0001JL:64161:64427 [0] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
speec363a0001JL:64168:64456 [7] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
speec363a0001JL:64165:64467 [4] NCCL INFO Connected all trees
speec363a0001JL:64165:64467 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
speec363a0001JL:64164:64486 [3] NCCL INFO Connected all trees
speec363a0001JL:64164:64486 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
speec363a0001JL:64163:64457 [2] NCCL INFO Connected all trees
speec363a0001JL:64163:64457 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
speec363a0001JL:64162:64454 [1] NCCL INFO Connected all trees
speec363a0001JL:64162:64454 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
speec363a0001JL:64166:64490 [5] NCCL INFO Connected all trees
speec363a0001JL:64166:64490 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
speec363a0001JL:64167:64482 [6] NCCL INFO Connected all trees
speec363a0001JL:64167:64482 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
speec363a0001JL:64165:64467 [4] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
speec363a0001JL:64164:64486 [3] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
speec363a0001JL:64163:64457 [2] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
speec363a0001JL:64162:64454 [1] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
speec363a0001JL:64166:64490 [5] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
speec363a0001JL:64167:64482 [6] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
speec363a0001JL:64167:64482 [6] NCCL INFO comm 0x7f3674002fb0 rank 6 nranks 8 cudaDev 6 busId 200000 - Init COMPLETE
speec363a0001JL:64162:64454 [1] NCCL INFO comm 0x7f0fa0002fb0 rank 1 nranks 8 cudaDev 1 busId d00000 - Init COMPLETE
speec363a0001JL:64168:64456 [7] NCCL INFO comm 0x7fdba0002fb0 rank 7 nranks 8 cudaDev 7 busId 100000 - Init COMPLETE
speec363a0001JL:64165:64467 [4] NCCL INFO comm 0x7ff428002fb0 rank 4 nranks 8 cudaDev 4 busId 400000 - Init COMPLETE
speec363a0001JL:64161:64427 [0] NCCL INFO comm 0x7fe964002fb0 rank 0 nranks 8 cudaDev 0 busId e00000 - Init COMPLETE
speec363a0001JL:64163:64457 [2] NCCL INFO comm 0x7fbe78002fb0 rank 2 nranks 8 cudaDev 2 busId c00000 - Init COMPLETE
speec363a0001JL:64164:64486 [3] NCCL INFO comm 0x7fa170002fb0 rank 3 nranks 8 cudaDev 3 busId b00000 - Init COMPLETE
speec363a0001JL:64166:64490 [5] NCCL INFO comm 0x7f0f8c002fb0 rank 5 nranks 8 cudaDev 5 busId 300000 - Init COMPLETE
speec363a0001JL:64161:64161 [0] NCCL INFO Launch mode Parallel
Namespace(aa='rand-m9-mstd0.5-inc1', abs_pos_emb=True, attn_drop_rate=0.0, auto_resume=True, backbone_decay=1.0, batch_size=64, bce_loss=False, clip_grad=None, clip_mean_and_std=True, color_jitter=0.4, crop_pct=None, crop_scale=0.08, cutmix=0.0, cutmix_minmax=None, data='imagenet', data_path='/tmp/DATASET/imagenet', data_set='IMNET', deepscale=False, deepscale_config=None, deepspeed=False, deepspeed_config=None, deepspeed_mpi=False, device='cuda:0', disable_eval_during_finetuning=False, disable_weight_decay_on_rel_pos_bias=False, dist_eval=True, dist_on_itp=False, dist_url='env://', distributed=True, drop=0.0, drop_path=0.0, enable_deepspeed=True, epochs=30, eval=False, eval_all=True, eval_data_path=None, finetune='OUTPUT/SLIP/declip_model/clip_vitL14.pth', imagenet_default_mean_and_std=False, init_scale=0.001, input_size=224, layer_decay=0.65, layer_scale_init_value=0.0, local_rank=0, log_dir=None, lr=0.0004, min_lr=1e-06, mixup=0.0, mixup_mode='batch', mixup_prob=1.0, mixup_switch_prob=0.5, model='declip_L14', model_ema=True, model_ema_decay=0.9998, model_ema_force_cpu=False, model_key='state', model_prefix='visual.', momentum=0.9, nb_classes=1000, num_gpu=1, num_workers=8, opt='adamw', opt_betas=None, opt_eps=1e-08, output_dir='OUTPUT/SLIP/maskclip_ft/CLIP_openai_Large_P14/FT30_4E4_D065_EMA98_DPR00', pin_mem=True, rank=0, recount=1, rel_pos_bias=False, remode='pixel', reprob=0.25, resplit=False, resume='', save_ckpt=True, save_ckpt_freq=5, seed=0, smoothing=0.1, src=False, start_epoch=0, three_aug=False, train_interpolation='bicubic', train_set='zip', update_freq=4, use_mean_pooling=True, warmup_epochs=5, warmup_lr=1e-06, warmup_steps=-1, weight_decay=0.05, weight_decay_end=None, world_size=8)
use crop scale (0.08, 1)
Transform = 
RandomResizedCropAndInterpolation(size=(224, 224), scale=(0.08, 1), ratio=(0.75, 1.3333), interpolation=PIL.Image.BICUBIC)
RandomHorizontalFlip(p=0.5)
<timm.data.auto_augment.RandAugment object at 0x7feab7973b80>
ToTensor()
Normalize(mean=tensor([0.4815, 0.4578, 0.4082]), std=tensor([0.2686, 0.2613, 0.2758]))
<timm.data.random_erasing.RandomErasing object at 0x7feab7973ee0>
---------------------------
Only ImageNet-1K, length 1281167
USE ZIP DATALOADER
Number of the class = 1000
Set crop pct to 1 for clip task
Transform = 
Resize(size=224, interpolation=bicubic, max_size=None, antialias=None)
CenterCrop(size=(224, 224))
ToTensor()
Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------------------------
Only ImageNet-1K, length 1281167
Only ImageNet-1K, length 1281167
Only ImageNet-1K, length 1281167
Only ImageNet-1K, length 1281167
Only ImageNet-1K, length 1281167
Only ImageNet-1K, length 1281167
Only ImageNet-1K, length 1281167
Only ImageNet-1K, length 50000
USE ZIP DATALOADER
Number of the class = 1000
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7feab7b5c310>
No Layer Scale
{'num_classes': 1000, 'drop_rate': 0.0, 'drop_path_rate': 0.0, 'attn_drop_rate': 0.0, 'use_mean_pooling': True, 'init_scale': 0.001, 'use_rel_pos_bias': False, 'use_abs_pos_emb': True}
drop_path_rate: 0.0
layer_scale: False
Using DPR [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Only ImageNet-1K, length 50000
Only ImageNet-1K, length 50000
Only ImageNet-1K, length 50000
Only ImageNet-1K, length 50000
Only ImageNet-1K, length 50000
Only ImageNet-1K, length 50000
Only ImageNet-1K, length 50000
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Patch size = (14, 14)
Load ckpt from OUTPUT/SLIP/declip_model/clip_vitL14.pth
Weights of VisualTransformer not initialized from pretrained model: ['visual.patch_embed.proj.weight', 'visual.patch_embed.proj.bias', 'visual.fc_norm.weight', 'visual.fc_norm.bias', 'visual.head.weight', 'visual.head.bias']
Weights from pretrained model not used in VisualTransformer: ['visual.proj', 'visual.ln_post.weight', 'visual.ln_post.bias']
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Using EMA with decay = 0.99980000
Model = VisualTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
  )
  (conv1): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
  (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (transformer): Transformer(
    (dropout): Dropout(p=0, inplace=False)
    (resblocks): Sequential(
      (0): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (2): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (3): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (4): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (5): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (6): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (7): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (8): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (9): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (10): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (11): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (12): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (13): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (14): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (15): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (16): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (17): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (18): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (19): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (20): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (21): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (22): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (23): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
    )
  )
  (fc_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (head): Linear(in_features=1024, out_features=1000, bias=True)
)
number of params: 303602664
LR = 0.00040000
Batch size = 2048
Update frequent = 4
Number of training examples = 1281167
Number of training training per epoch = 625
Assigned values = [2.1029740616282293e-05, 3.2353447101972754e-05, 4.977453400303501e-05, 7.65762061585154e-05, 0.00011780954793617752, 0.00018124545836335003, 0.0002788391667128462, 0.0004289833334043787, 0.0006599743590836596, 0.0010153451678210146, 0.0015620694889554071, 0.002403183829162165, 0.003697205891018715, 0.005688009063105715, 0.008750783174008792, 0.013462743344628911, 0.02071191283789063, 0.03186448128906251, 0.049022278906250015, 0.07541889062500001, 0.11602906250000002, 0.17850625000000003, 0.274625, 0.42250000000000004, 0.65, 1.0]
Param groups = {
  "layer_0_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "class_embedding",
      "positional_embedding",
      "ln_pre.weight",
      "ln_pre.bias"
    ],
    "lr_scale": 2.1029740616282293e-05
  },
  "layer_1_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.0.attn.in_proj_weight",
      "transformer.resblocks.0.attn.out_proj.weight",
      "transformer.resblocks.0.mlp.c_fc.weight",
      "transformer.resblocks.0.mlp.c_proj.weight"
    ],
    "lr_scale": 3.2353447101972754e-05
  },
  "layer_1_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.0.attn.in_proj_bias",
      "transformer.resblocks.0.attn.out_proj.bias",
      "transformer.resblocks.0.ln_1.weight",
      "transformer.resblocks.0.ln_1.bias",
      "transformer.resblocks.0.mlp.c_fc.bias",
      "transformer.resblocks.0.mlp.c_proj.bias",
      "transformer.resblocks.0.ln_2.weight",
      "transformer.resblocks.0.ln_2.bias"
    ],
    "lr_scale": 3.2353447101972754e-05
  },
  "layer_2_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.1.attn.in_proj_weight",
      "transformer.resblocks.1.attn.out_proj.weight",
      "transformer.resblocks.1.mlp.c_fc.weight",
      "transformer.resblocks.1.mlp.c_proj.weight"
    ],
    "lr_scale": 4.977453400303501e-05
  },
  "layer_2_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.1.attn.in_proj_bias",
      "transformer.resblocks.1.attn.out_proj.bias",
      "transformer.resblocks.1.ln_1.weight",
      "transformer.resblocks.1.ln_1.bias",
      "transformer.resblocks.1.mlp.c_fc.bias",
      "transformer.resblocks.1.mlp.c_proj.bias",
      "transformer.resblocks.1.ln_2.weight",
      "transformer.resblocks.1.ln_2.bias"
    ],
    "lr_scale": 4.977453400303501e-05
  },
  "layer_3_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.2.attn.in_proj_weight",
      "transformer.resblocks.2.attn.out_proj.weight",
      "transformer.resblocks.2.mlp.c_fc.weight",
      "transformer.resblocks.2.mlp.c_proj.weight"
    ],
    "lr_scale": 7.65762061585154e-05
  },
  "layer_3_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.2.attn.in_proj_bias",
      "transformer.resblocks.2.attn.out_proj.bias",
      "transformer.resblocks.2.ln_1.weight",
      "transformer.resblocks.2.ln_1.bias",
      "transformer.resblocks.2.mlp.c_fc.bias",
      "transformer.resblocks.2.mlp.c_proj.bias",
      "transformer.resblocks.2.ln_2.weight",
      "transformer.resblocks.2.ln_2.bias"
    ],
    "lr_scale": 7.65762061585154e-05
  },
  "layer_4_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.3.attn.in_proj_weight",
      "transformer.resblocks.3.attn.out_proj.weight",
      "transformer.resblocks.3.mlp.c_fc.weight",
      "transformer.resblocks.3.mlp.c_proj.weight"
    ],
    "lr_scale": 0.00011780954793617752
  },
  "layer_4_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.3.attn.in_proj_bias",
      "transformer.resblocks.3.attn.out_proj.bias",
      "transformer.resblocks.3.ln_1.weight",
      "transformer.resblocks.3.ln_1.bias",
      "transformer.resblocks.3.mlp.c_fc.bias",
      "transformer.resblocks.3.mlp.c_proj.bias",
      "transformer.resblocks.3.ln_2.weight",
      "transformer.resblocks.3.ln_2.bias"
    ],
    "lr_scale": 0.00011780954793617752
  },
  "layer_5_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.4.attn.in_proj_weight",
      "transformer.resblocks.4.attn.out_proj.weight",
      "transformer.resblocks.4.mlp.c_fc.weight",
      "transformer.resblocks.4.mlp.c_proj.weight"
    ],
    "lr_scale": 0.00018124545836335003
  },
  "layer_5_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.4.attn.in_proj_bias",
      "transformer.resblocks.4.attn.out_proj.bias",
      "transformer.resblocks.4.ln_1.weight",
      "transformer.resblocks.4.ln_1.bias",
      "transformer.resblocks.4.mlp.c_fc.bias",
      "transformer.resblocks.4.mlp.c_proj.bias",
      "transformer.resblocks.4.ln_2.weight",
      "transformer.resblocks.4.ln_2.bias"
    ],
    "lr_scale": 0.00018124545836335003
  },
  "layer_6_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.5.attn.in_proj_weight",
      "transformer.resblocks.5.attn.out_proj.weight",
      "transformer.resblocks.5.mlp.c_fc.weight",
      "transformer.resblocks.5.mlp.c_proj.weight"
    ],
    "lr_scale": 0.0002788391667128462
  },
  "layer_6_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.5.attn.in_proj_bias",
      "transformer.resblocks.5.attn.out_proj.bias",
      "transformer.resblocks.5.ln_1.weight",
      "transformer.resblocks.5.ln_1.bias",
      "transformer.resblocks.5.mlp.c_fc.bias",
      "transformer.resblocks.5.mlp.c_proj.bias",
      "transformer.resblocks.5.ln_2.weight",
      "transformer.resblocks.5.ln_2.bias"
    ],
    "lr_scale": 0.0002788391667128462
  },
  "layer_7_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.6.attn.in_proj_weight",
      "transformer.resblocks.6.attn.out_proj.weight",
      "transformer.resblocks.6.mlp.c_fc.weight",
      "transformer.resblocks.6.mlp.c_proj.weight"
    ],
    "lr_scale": 0.0004289833334043787
  },
  "layer_7_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.6.attn.in_proj_bias",
      "transformer.resblocks.6.attn.out_proj.bias",
      "transformer.resblocks.6.ln_1.weight",
      "transformer.resblocks.6.ln_1.bias",
      "transformer.resblocks.6.mlp.c_fc.bias",
      "transformer.resblocks.6.mlp.c_proj.bias",
      "transformer.resblocks.6.ln_2.weight",
      "transformer.resblocks.6.ln_2.bias"
    ],
    "lr_scale": 0.0004289833334043787
  },
  "layer_8_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.7.attn.in_proj_weight",
      "transformer.resblocks.7.attn.out_proj.weight",
      "transformer.resblocks.7.mlp.c_fc.weight",
      "transformer.resblocks.7.mlp.c_proj.weight"
    ],
    "lr_scale": 0.0006599743590836596
  },
  "layer_8_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.7.attn.in_proj_bias",
      "transformer.resblocks.7.attn.out_proj.bias",
      "transformer.resblocks.7.ln_1.weight",
      "transformer.resblocks.7.ln_1.bias",
      "transformer.resblocks.7.mlp.c_fc.bias",
      "transformer.resblocks.7.mlp.c_proj.bias",
      "transformer.resblocks.7.ln_2.weight",
      "transformer.resblocks.7.ln_2.bias"
    ],
    "lr_scale": 0.0006599743590836596
  },
  "layer_9_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.8.attn.in_proj_weight",
      "transformer.resblocks.8.attn.out_proj.weight",
      "transformer.resblocks.8.mlp.c_fc.weight",
      "transformer.resblocks.8.mlp.c_proj.weight"
    ],
    "lr_scale": 0.0010153451678210146
  },
  "layer_9_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.8.attn.in_proj_bias",
      "transformer.resblocks.8.attn.out_proj.bias",
      "transformer.resblocks.8.ln_1.weight",
      "transformer.resblocks.8.ln_1.bias",
      "transformer.resblocks.8.mlp.c_fc.bias",
      "transformer.resblocks.8.mlp.c_proj.bias",
      "transformer.resblocks.8.ln_2.weight",
      "transformer.resblocks.8.ln_2.bias"
    ],
    "lr_scale": 0.0010153451678210146
  },
  "layer_10_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.9.attn.in_proj_weight",
      "transformer.resblocks.9.attn.out_proj.weight",
      "transformer.resblocks.9.mlp.c_fc.weight",
      "transformer.resblocks.9.mlp.c_proj.weight"
    ],
    "lr_scale": 0.0015620694889554071
  },
  "layer_10_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.9.attn.in_proj_bias",
      "transformer.resblocks.9.attn.out_proj.bias",
      "transformer.resblocks.9.ln_1.weight",
      "transformer.resblocks.9.ln_1.bias",
      "transformer.resblocks.9.mlp.c_fc.bias",
      "transformer.resblocks.9.mlp.c_proj.bias",
      "transformer.resblocks.9.ln_2.weight",
      "transformer.resblocks.9.ln_2.bias"
    ],
    "lr_scale": 0.0015620694889554071
  },
  "layer_11_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.10.attn.in_proj_weight",
      "transformer.resblocks.10.attn.out_proj.weight",
      "transformer.resblocks.10.mlp.c_fc.weight",
      "transformer.resblocks.10.mlp.c_proj.weight"
    ],
    "lr_scale": 0.002403183829162165
  },
  "layer_11_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.10.attn.in_proj_bias",
      "transformer.resblocks.10.attn.out_proj.bias",
      "transformer.resblocks.10.ln_1.weight",
      "transformer.resblocks.10.ln_1.bias",
      "transformer.resblocks.10.mlp.c_fc.bias",
      "transformer.resblocks.10.mlp.c_proj.bias",
      "transformer.resblocks.10.ln_2.weight",
      "transformer.resblocks.10.ln_2.bias"
    ],
    "lr_scale": 0.002403183829162165
  },
  "layer_12_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.11.attn.in_proj_weight",
      "transformer.resblocks.11.attn.out_proj.weight",
      "transformer.resblocks.11.mlp.c_fc.weight",
      "transformer.resblocks.11.mlp.c_proj.weight"
    ],
    "lr_scale": 0.003697205891018715
  },
  "layer_12_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.11.attn.in_proj_bias",
      "transformer.resblocks.11.attn.out_proj.bias",
      "transformer.resblocks.11.ln_1.weight",
      "transformer.resblocks.11.ln_1.bias",
      "transformer.resblocks.11.mlp.c_fc.bias",
      "transformer.resblocks.11.mlp.c_proj.bias",
      "transformer.resblocks.11.ln_2.weight",
      "transformer.resblocks.11.ln_2.bias"
    ],
    "lr_scale": 0.003697205891018715
  },
  "layer_13_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.12.attn.in_proj_weight",
      "transformer.resblocks.12.attn.out_proj.weight",
      "transformer.resblocks.12.mlp.c_fc.weight",
      "transformer.resblocks.12.mlp.c_proj.weight"
    ],
    "lr_scale": 0.005688009063105715
  },
  "layer_13_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.12.attn.in_proj_bias",
      "transformer.resblocks.12.attn.out_proj.bias",
      "transformer.resblocks.12.ln_1.weight",
      "transformer.resblocks.12.ln_1.bias",
      "transformer.resblocks.12.mlp.c_fc.bias",
      "transformer.resblocks.12.mlp.c_proj.bias",
      "transformer.resblocks.12.ln_2.weight",
      "transformer.resblocks.12.ln_2.bias"
    ],
    "lr_scale": 0.005688009063105715
  },
  "layer_14_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.13.attn.in_proj_weight",
      "transformer.resblocks.13.attn.out_proj.weight",
      "transformer.resblocks.13.mlp.c_fc.weight",
      "transformer.resblocks.13.mlp.c_proj.weight"
    ],
    "lr_scale": 0.008750783174008792
  },
  "layer_14_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.13.attn.in_proj_bias",
      "transformer.resblocks.13.attn.out_proj.bias",
      "transformer.resblocks.13.ln_1.weight",
      "transformer.resblocks.13.ln_1.bias",
      "transformer.resblocks.13.mlp.c_fc.bias",
      "transformer.resblocks.13.mlp.c_proj.bias",
      "transformer.resblocks.13.ln_2.weight",
      "transformer.resblocks.13.ln_2.bias"
    ],
    "lr_scale": 0.008750783174008792
  },
  "layer_15_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.14.attn.in_proj_weight",
      "transformer.resblocks.14.attn.out_proj.weight",
      "transformer.resblocks.14.mlp.c_fc.weight",
      "transformer.resblocks.14.mlp.c_proj.weight"
    ],
    "lr_scale": 0.013462743344628911
  },
  "layer_15_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.14.attn.in_proj_bias",
      "transformer.resblocks.14.attn.out_proj.bias",
      "transformer.resblocks.14.ln_1.weight",
      "transformer.resblocks.14.ln_1.bias",
      "transformer.resblocks.14.mlp.c_fc.bias",
      "transformer.resblocks.14.mlp.c_proj.bias",
      "transformer.resblocks.14.ln_2.weight",
      "transformer.resblocks.14.ln_2.bias"
    ],
    "lr_scale": 0.013462743344628911
  },
  "layer_16_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.15.attn.in_proj_weight",
      "transformer.resblocks.15.attn.out_proj.weight",
      "transformer.resblocks.15.mlp.c_fc.weight",
      "transformer.resblocks.15.mlp.c_proj.weight"
    ],
    "lr_scale": 0.02071191283789063
  },
  "layer_16_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.15.attn.in_proj_bias",
      "transformer.resblocks.15.attn.out_proj.bias",
      "transformer.resblocks.15.ln_1.weight",
      "transformer.resblocks.15.ln_1.bias",
      "transformer.resblocks.15.mlp.c_fc.bias",
      "transformer.resblocks.15.mlp.c_proj.bias",
      "transformer.resblocks.15.ln_2.weight",
      "transformer.resblocks.15.ln_2.bias"
    ],
    "lr_scale": 0.02071191283789063
  },
  "layer_17_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.16.attn.in_proj_weight",
      "transformer.resblocks.16.attn.out_proj.weight",
      "transformer.resblocks.16.mlp.c_fc.weight",
      "transformer.resblocks.16.mlp.c_proj.weight"
    ],
    "lr_scale": 0.03186448128906251
  },
  "layer_17_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.16.attn.in_proj_bias",
      "transformer.resblocks.16.attn.out_proj.bias",
      "transformer.resblocks.16.ln_1.weight",
      "transformer.resblocks.16.ln_1.bias",
      "transformer.resblocks.16.mlp.c_fc.bias",
      "transformer.resblocks.16.mlp.c_proj.bias",
      "transformer.resblocks.16.ln_2.weight",
      "transformer.resblocks.16.ln_2.bias"
    ],
    "lr_scale": 0.03186448128906251
  },
  "layer_18_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.17.attn.in_proj_weight",
      "transformer.resblocks.17.attn.out_proj.weight",
      "transformer.resblocks.17.mlp.c_fc.weight",
      "transformer.resblocks.17.mlp.c_proj.weight"
    ],
    "lr_scale": 0.049022278906250015
  },
  "layer_18_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.17.attn.in_proj_bias",
      "transformer.resblocks.17.attn.out_proj.bias",
      "transformer.resblocks.17.ln_1.weight",
      "transformer.resblocks.17.ln_1.bias",
      "transformer.resblocks.17.mlp.c_fc.bias",
      "transformer.resblocks.17.mlp.c_proj.bias",
      "transformer.resblocks.17.ln_2.weight",
      "transformer.resblocks.17.ln_2.bias"
    ],
    "lr_scale": 0.049022278906250015
  },
  "layer_19_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.18.attn.in_proj_weight",
      "transformer.resblocks.18.attn.out_proj.weight",
      "transformer.resblocks.18.mlp.c_fc.weight",
      "transformer.resblocks.18.mlp.c_proj.weight"
    ],
    "lr_scale": 0.07541889062500001
  },
  "layer_19_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.18.attn.in_proj_bias",
      "transformer.resblocks.18.attn.out_proj.bias",
      "transformer.resblocks.18.ln_1.weight",
      "transformer.resblocks.18.ln_1.bias",
      "transformer.resblocks.18.mlp.c_fc.bias",
      "transformer.resblocks.18.mlp.c_proj.bias",
      "transformer.resblocks.18.ln_2.weight",
      "transformer.resblocks.18.ln_2.bias"
    ],
    "lr_scale": 0.07541889062500001
  },
  "layer_20_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.19.attn.in_proj_weight",
      "transformer.resblocks.19.attn.out_proj.weight",
      "transformer.resblocks.19.mlp.c_fc.weight",
      "transformer.resblocks.19.mlp.c_proj.weight"
    ],
    "lr_scale": 0.11602906250000002
  },
  "layer_20_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.19.attn.in_proj_bias",
      "transformer.resblocks.19.attn.out_proj.bias",
      "transformer.resblocks.19.ln_1.weight",
      "transformer.resblocks.19.ln_1.bias",
      "transformer.resblocks.19.mlp.c_fc.bias",
      "transformer.resblocks.19.mlp.c_proj.bias",
      "transformer.resblocks.19.ln_2.weight",
      "transformer.resblocks.19.ln_2.bias"
    ],
    "lr_scale": 0.11602906250000002
  },
  "layer_21_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.20.attn.in_proj_weight",
      "transformer.resblocks.20.attn.out_proj.weight",
      "transformer.resblocks.20.mlp.c_fc.weight",
      "transformer.resblocks.20.mlp.c_proj.weight"
    ],
    "lr_scale": 0.17850625000000003
  },
  "layer_21_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.20.attn.in_proj_bias",
      "transformer.resblocks.20.attn.out_proj.bias",
      "transformer.resblocks.20.ln_1.weight",
      "transformer.resblocks.20.ln_1.bias",
      "transformer.resblocks.20.mlp.c_fc.bias",
      "transformer.resblocks.20.mlp.c_proj.bias",
      "transformer.resblocks.20.ln_2.weight",
      "transformer.resblocks.20.ln_2.bias"
    ],
    "lr_scale": 0.17850625000000003
  },
  "layer_22_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.21.attn.in_proj_weight",
      "transformer.resblocks.21.attn.out_proj.weight",
      "transformer.resblocks.21.mlp.c_fc.weight",
      "transformer.resblocks.21.mlp.c_proj.weight"
    ],
    "lr_scale": 0.274625
  },
  "layer_22_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.21.attn.in_proj_bias",
      "transformer.resblocks.21.attn.out_proj.bias",
      "transformer.resblocks.21.ln_1.weight",
      "transformer.resblocks.21.ln_1.bias",
      "transformer.resblocks.21.mlp.c_fc.bias",
      "transformer.resblocks.21.mlp.c_proj.bias",
      "transformer.resblocks.21.ln_2.weight",
      "transformer.resblocks.21.ln_2.bias"
    ],
    "lr_scale": 0.274625
  },
  "layer_23_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.22.attn.in_proj_weight",
      "transformer.resblocks.22.attn.out_proj.weight",
      "transformer.resblocks.22.mlp.c_fc.weight",
      "transformer.resblocks.22.mlp.c_proj.weight"
    ],
    "lr_scale": 0.42250000000000004
  },
  "layer_23_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.22.attn.in_proj_bias",
      "transformer.resblocks.22.attn.out_proj.bias",
      "transformer.resblocks.22.ln_1.weight",
      "transformer.resblocks.22.ln_1.bias",
      "transformer.resblocks.22.mlp.c_fc.bias",
      "transformer.resblocks.22.mlp.c_proj.bias",
      "transformer.resblocks.22.ln_2.weight",
      "transformer.resblocks.22.ln_2.bias"
    ],
    "lr_scale": 0.42250000000000004
  },
  "layer_24_decay": {
    "weight_decay": 0.05,
    "params": [
      "transformer.resblocks.23.attn.in_proj_weight",
      "transformer.resblocks.23.attn.out_proj.weight",
      "transformer.resblocks.23.mlp.c_fc.weight",
      "transformer.resblocks.23.mlp.c_proj.weight"
    ],
    "lr_scale": 0.65
  },
  "layer_24_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "transformer.resblocks.23.attn.in_proj_bias",
      "transformer.resblocks.23.attn.out_proj.bias",
      "transformer.resblocks.23.ln_1.weight",
      "transformer.resblocks.23.ln_1.bias",
      "transformer.resblocks.23.mlp.c_fc.bias",
      "transformer.resblocks.23.mlp.c_proj.bias",
      "transformer.resblocks.23.ln_2.weight",
      "transformer.resblocks.23.ln_2.bias"
    ],
    "lr_scale": 0.65
  },
  "layer_25_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "fc_norm.weight",
      "fc_norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  },
  "layer_25_decay": {
    "weight_decay": 0.05,
    "params": [
      "head.weight"
    ],
    "lr_scale": 1.0
  }
}
speec363a0001JL:64164:64609 [3] NCCL INFO Trees [0] 2/-1/-1->3->4 [1] 2/-1/-1->3->4 [2] 2/-1/-1->3->4 [3] 2/-1/-1->3->4 [4] 2/-1/-1->3->4 [5] 2/-1/-1->3->4 [6] 2/-1/-1->3->4 [7] 2/-1/-1->3->4 [8] 2/-1/-1->3->4 [9] 2/-1/-1->3->4 [10] 2/-1/-1->3->4 [11] 2/-1/-1->3->4 [12] 2/-1/-1->3->4 [13] 2/-1/-1->3->4 [14] 2/-1/-1->3->4 [15] 2/-1/-1->3->4 [16] 2/-1/-1->3->4 [17] 2/-1/-1->3->4 [18] 2/-1/-1->3->4 [19] 2/-1/-1->3->4 [20] 2/-1/-1->3->4 [21] 2/-1/-1->3->4 [22] 2/-1/-1->3->4 [23] 2/-1/-1->3->4
speec363a0001JL:64165:64610 [4] NCCL INFO Trees [0] 3/-1/-1->4->5 [1] 3/-1/-1->4->5 [2] 3/-1/-1->4->5 [3] 3/-1/-1->4->5 [4] 3/-1/-1->4->5 [5] 3/-1/-1->4->5 [6] 3/-1/-1->4->5 [7] 3/-1/-1->4->5 [8] 3/-1/-1->4->5 [9] 3/-1/-1->4->5 [10] 3/-1/-1->4->5 [11] 3/-1/-1->4->5 [12] 3/-1/-1->4->5 [13] 3/-1/-1->4->5 [14] 3/-1/-1->4->5 [15] 3/-1/-1->4->5 [16] 3/-1/-1->4->5 [17] 3/-1/-1->4->5 [18] 3/-1/-1->4->5 [19] 3/-1/-1->4->5 [20] 3/-1/-1->4->5 [21] 3/-1/-1->4->5 [22] 3/-1/-1->4->5 [23] 3/-1/-1->4->5
speec363a0001JL:64165:64610 [4] NCCL INFO Setting affinity for GPU 3 to ffff,0000ffff
speec363a0001JL:64164:64609 [3] NCCL INFO Setting affinity for GPU 4 to ffff,0000ffff
speec363a0001JL:64167:64613 [6] NCCL INFO Trees [0] 5/-1/-1->6->7 [1] 5/-1/-1->6->7 [2] 5/-1/-1->6->7 [3] 5/-1/-1->6->7 [4] 5/-1/-1->6->7 [5] 5/-1/-1->6->7 [6] 5/-1/-1->6->7 [7] 5/-1/-1->6->7 [8] 5/-1/-1->6->7 [9] 5/-1/-1->6->7 [10] 5/-1/-1->6->7 [11] 5/-1/-1->6->7 [12] 5/-1/-1->6->7 [13] 5/-1/-1->6->7 [14] 5/-1/-1->6->7 [15] 5/-1/-1->6->7 [16] 5/-1/-1->6->7 [17] 5/-1/-1->6->7 [18] 5/-1/-1->6->7 [19] 5/-1/-1->6->7 [20] 5/-1/-1->6->7 [21] 5/-1/-1->6->7 [22] 5/-1/-1->6->7 [23] 5/-1/-1->6->7
speec363a0001JL:64167:64613 [6] NCCL INFO Setting affinity for GPU 1 to ffff,0000ffff
speec363a0001JL:64166:64611 [5] NCCL INFO Trees [0] 4/-1/-1->5->6 [1] 4/-1/-1->5->6 [2] 4/-1/-1->5->6 [3] 4/-1/-1->5->6 [4] 4/-1/-1->5->6 [5] 4/-1/-1->5->6 [6] 4/-1/-1->5->6 [7] 4/-1/-1->5->6 [8] 4/-1/-1->5->6 [9] 4/-1/-1->5->6 [10] 4/-1/-1->5->6 [11] 4/-1/-1->5->6 [12] 4/-1/-1->5->6 [13] 4/-1/-1->5->6 [14] 4/-1/-1->5->6 [15] 4/-1/-1->5->6 [16] 4/-1/-1->5->6 [17] 4/-1/-1->5->6 [18] 4/-1/-1->5->6 [19] 4/-1/-1->5->6 [20] 4/-1/-1->5->6 [21] 4/-1/-1->5->6 [22] 4/-1/-1->5->6 [23] 4/-1/-1->5->6
speec363a0001JL:64166:64611 [5] NCCL INFO Setting affinity for GPU 2 to ffff,0000ffff
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 00/24 :    0   7   6   5   4   3   2   1
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 01/24 :    0   7   6   5   4   3   2   1
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 02/24 :    0   7   6   5   4   3   2   1
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 03/24 :    0   7   6   5   4   3   2   1
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 04/24 :    0   7   6   5   4   3   2   1
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 05/24 :    0   7   6   5   4   3   2   1
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 06/24 :    0   7   6   5   4   3   2   1
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 07/24 :    0   7   6   5   4   3   2   1
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 08/24 :    0   7   6   5   4   3   2   1
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 09/24 :    0   7   6   5   4   3   2   1
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 10/24 :    0   7   6   5   4   3   2   1
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 11/24 :    0   7   6   5   4   3   2   1
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 12/24 :    0   7   6   5   4   3   2   1
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 13/24 :    0   7   6   5   4   3   2   1
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 14/24 :    0   7   6   5   4   3   2   1
speec363a0001JL:64168:64614 [7] NCCL INFO Trees [0] 6/-1/-1->7->-1 [1] 6/-1/-1->7->-1 [2] 6/-1/-1->7->-1 [3] 6/-1/-1->7->-1 [4] 6/-1/-1->7->-1 [5] 6/-1/-1->7->-1 [6] 6/-1/-1->7->-1 [7] 6/-1/-1->7->-1 [8] 6/-1/-1->7->-1 [9] 6/-1/-1->7->-1 [10] 6/-1/-1->7->-1 [11] 6/-1/-1->7->-1 [12] 6/-1/-1->7->-1 [13] 6/-1/-1->7->-1 [14] 6/-1/-1->7->-1 [15] 6/-1/-1->7->-1 [16] 6/-1/-1->7->-1 [17] 6/-1/-1->7->-1 [18] 6/-1/-1->7->-1 [19] 6/-1/-1->7->-1 [20] 6/-1/-1->7->-1 [21] 6/-1/-1->7->-1 [22] 6/-1/-1->7->-1 [23] 6/-1/-1->7->-1
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 15/24 :    0   7   6   5   4   3   2   1
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 16/24 :    0   7   6   5   4   3   2   1
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 17/24 :    0   7   6   5   4   3   2   1
speec363a0001JL:64168:64614 [7] NCCL INFO Setting affinity for GPU 0 to ffff,0000ffff
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 18/24 :    0   7   6   5   4   3   2   1
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 19/24 :    0   7   6   5   4   3   2   1
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 20/24 :    0   7   6   5   4   3   2   1
speec363a0001JL:64163:64612 [2] NCCL INFO Trees [0] 1/-1/-1->2->3 [1] 1/-1/-1->2->3 [2] 1/-1/-1->2->3 [3] 1/-1/-1->2->3 [4] 1/-1/-1->2->3 [5] 1/-1/-1->2->3 [6] 1/-1/-1->2->3 [7] 1/-1/-1->2->3 [8] 1/-1/-1->2->3 [9] 1/-1/-1->2->3 [10] 1/-1/-1->2->3 [11] 1/-1/-1->2->3 [12] 1/-1/-1->2->3 [13] 1/-1/-1->2->3 [14] 1/-1/-1->2->3 [15] 1/-1/-1->2->3 [16] 1/-1/-1->2->3 [17] 1/-1/-1->2->3 [18] 1/-1/-1->2->3 [19] 1/-1/-1->2->3 [20] 1/-1/-1->2->3 [21] 1/-1/-1->2->3 [22] 1/-1/-1->2->3 [23] 1/-1/-1->2->3
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 21/24 :    0   7   6   5   4   3   2   1
speec363a0001JL:64162:64608 [1] NCCL INFO Trees [0] 0/-1/-1->1->2 [1] 0/-1/-1->1->2 [2] 0/-1/-1->1->2 [3] 0/-1/-1->1->2 [4] 0/-1/-1->1->2 [5] 0/-1/-1->1->2 [6] 0/-1/-1->1->2 [7] 0/-1/-1->1->2 [8] 0/-1/-1->1->2 [9] 0/-1/-1->1->2 [10] 0/-1/-1->1->2 [11] 0/-1/-1->1->2 [12] 0/-1/-1->1->2 [13] 0/-1/-1->1->2 [14] 0/-1/-1->1->2 [15] 0/-1/-1->1->2 [16] 0/-1/-1->1->2 [17] 0/-1/-1->1->2 [18] 0/-1/-1->1->2 [19] 0/-1/-1->1->2 [20] 0/-1/-1->1->2 [21] 0/-1/-1->1->2 [22] 0/-1/-1->1->2 [23] 0/-1/-1->1->2
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 22/24 :    0   7   6   5   4   3   2   1
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 23/24 :    0   7   6   5   4   3   2   1
speec363a0001JL:64162:64608 [1] NCCL INFO Setting affinity for GPU 6 to ffff,0000ffff
speec363a0001JL:64163:64612 [2] NCCL INFO Setting affinity for GPU 5 to ffff,0000ffff
speec363a0001JL:64161:64607 [0] NCCL INFO Trees [0] -1/-1/-1->0->1 [1] -1/-1/-1->0->1 [2] -1/-1/-1->0->1 [3] -1/-1/-1->0->1 [4] -1/-1/-1->0->1 [5] -1/-1/-1->0->1 [6] -1/-1/-1->0->1 [7] -1/-1/-1->0->1 [8] -1/-1/-1->0->1 [9] -1/-1/-1->0->1 [10] -1/-1/-1->0->1 [11] -1/-1/-1->0->1 [12] -1/-1/-1->0->1 [13] -1/-1/-1->0->1 [14] -1/-1/-1->0->1 [15] -1/-1/-1->0->1 [16] -1/-1/-1->0->1 [17] -1/-1/-1->0->1 [18] -1/-1/-1->0->1 [19] -1/-1/-1->0->1 [20] -1/-1/-1->0->1 [21] -1/-1/-1->0->1 [22] -1/-1/-1->0->1 [23] -1/-1/-1->0->1
speec363a0001JL:64161:64607 [0] NCCL INFO Setting affinity for GPU 7 to ffff,0000ffff
speec363a0001JL:64165:64610 [4] NCCL INFO Channel 00 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64167:64613 [6] NCCL INFO Channel 00 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64168:64614 [7] NCCL INFO Channel 00 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 00 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64162:64608 [1] NCCL INFO Channel 00 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JL:64163:64612 [2] NCCL INFO Channel 00 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64166:64611 [5] NCCL INFO Channel 00 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64164:64609 [3] NCCL INFO Channel 00 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64165:64610 [4] NCCL INFO Channel 01 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64167:64613 [6] NCCL INFO Channel 01 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64168:64614 [7] NCCL INFO Channel 01 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 01 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64162:64608 [1] NCCL INFO Channel 01 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JL:64163:64612 [2] NCCL INFO Channel 01 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64166:64611 [5] NCCL INFO Channel 01 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64164:64609 [3] NCCL INFO Channel 01 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64165:64610 [4] NCCL INFO Channel 02 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64167:64613 [6] NCCL INFO Channel 02 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64168:64614 [7] NCCL INFO Channel 02 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 02 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64162:64608 [1] NCCL INFO Channel 02 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JL:64163:64612 [2] NCCL INFO Channel 02 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64166:64611 [5] NCCL INFO Channel 02 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64164:64609 [3] NCCL INFO Channel 02 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64165:64610 [4] NCCL INFO Channel 03 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64167:64613 [6] NCCL INFO Channel 03 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64168:64614 [7] NCCL INFO Channel 03 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 03 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64162:64608 [1] NCCL INFO Channel 03 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JL:64163:64612 [2] NCCL INFO Channel 03 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64166:64611 [5] NCCL INFO Channel 03 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64164:64609 [3] NCCL INFO Channel 03 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64165:64610 [4] NCCL INFO Channel 04 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64167:64613 [6] NCCL INFO Channel 04 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64168:64614 [7] NCCL INFO Channel 04 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 04 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64162:64608 [1] NCCL INFO Channel 04 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JL:64163:64612 [2] NCCL INFO Channel 04 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64166:64611 [5] NCCL INFO Channel 04 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64164:64609 [3] NCCL INFO Channel 04 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64165:64610 [4] NCCL INFO Channel 05 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64167:64613 [6] NCCL INFO Channel 05 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64168:64614 [7] NCCL INFO Channel 05 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 05 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64162:64608 [1] NCCL INFO Channel 05 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JL:64163:64612 [2] NCCL INFO Channel 05 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64166:64611 [5] NCCL INFO Channel 05 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64164:64609 [3] NCCL INFO Channel 05 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64165:64610 [4] NCCL INFO Channel 06 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64167:64613 [6] NCCL INFO Channel 06 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64168:64614 [7] NCCL INFO Channel 06 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 06 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64162:64608 [1] NCCL INFO Channel 06 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JL:64163:64612 [2] NCCL INFO Channel 06 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64166:64611 [5] NCCL INFO Channel 06 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64164:64609 [3] NCCL INFO Channel 06 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64165:64610 [4] NCCL INFO Channel 07 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64167:64613 [6] NCCL INFO Channel 07 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64168:64614 [7] NCCL INFO Channel 07 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 07 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64162:64608 [1] NCCL INFO Channel 07 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JL:64163:64612 [2] NCCL INFO Channel 07 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64166:64611 [5] NCCL INFO Channel 07 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64164:64609 [3] NCCL INFO Channel 07 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64165:64610 [4] NCCL INFO Channel 08 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64167:64613 [6] NCCL INFO Channel 08 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64168:64614 [7] NCCL INFO Channel 08 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 08 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64162:64608 [1] NCCL INFO Channel 08 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JL:64163:64612 [2] NCCL INFO Channel 08 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64166:64611 [5] NCCL INFO Channel 08 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64164:64609 [3] NCCL INFO Channel 08 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64165:64610 [4] NCCL INFO Channel 09 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64167:64613 [6] NCCL INFO Channel 09 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64168:64614 [7] NCCL INFO Channel 09 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 09 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64162:64608 [1] NCCL INFO Channel 09 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JL:64163:64612 [2] NCCL INFO Channel 09 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64166:64611 [5] NCCL INFO Channel 09 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64164:64609 [3] NCCL INFO Channel 09 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64165:64610 [4] NCCL INFO Channel 10 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64167:64613 [6] NCCL INFO Channel 10 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64168:64614 [7] NCCL INFO Channel 10 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 10 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64162:64608 [1] NCCL INFO Channel 10 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JL:64163:64612 [2] NCCL INFO Channel 10 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64166:64611 [5] NCCL INFO Channel 10 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64164:64609 [3] NCCL INFO Channel 10 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64165:64610 [4] NCCL INFO Channel 11 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64167:64613 [6] NCCL INFO Channel 11 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64168:64614 [7] NCCL INFO Channel 11 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 11 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64162:64608 [1] NCCL INFO Channel 11 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JL:64163:64612 [2] NCCL INFO Channel 11 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64166:64611 [5] NCCL INFO Channel 11 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64164:64609 [3] NCCL INFO Channel 11 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64165:64610 [4] NCCL INFO Channel 12 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64168:64614 [7] NCCL INFO Channel 12 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64167:64613 [6] NCCL INFO Channel 12 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 12 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64162:64608 [1] NCCL INFO Channel 12 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JL:64163:64612 [2] NCCL INFO Channel 12 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64166:64611 [5] NCCL INFO Channel 12 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64164:64609 [3] NCCL INFO Channel 12 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64165:64610 [4] NCCL INFO Channel 13 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64168:64614 [7] NCCL INFO Channel 13 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64167:64613 [6] NCCL INFO Channel 13 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 13 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64162:64608 [1] NCCL INFO Channel 13 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JL:64163:64612 [2] NCCL INFO Channel 13 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64166:64611 [5] NCCL INFO Channel 13 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64164:64609 [3] NCCL INFO Channel 13 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64165:64610 [4] NCCL INFO Channel 14 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64168:64614 [7] NCCL INFO Channel 14 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64167:64613 [6] NCCL INFO Channel 14 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 14 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64162:64608 [1] NCCL INFO Channel 14 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JL:64163:64612 [2] NCCL INFO Channel 14 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64166:64611 [5] NCCL INFO Channel 14 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64164:64609 [3] NCCL INFO Channel 14 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64165:64610 [4] NCCL INFO Channel 15 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64168:64614 [7] NCCL INFO Channel 15 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64167:64613 [6] NCCL INFO Channel 15 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 15 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64162:64608 [1] NCCL INFO Channel 15 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JL:64163:64612 [2] NCCL INFO Channel 15 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64166:64611 [5] NCCL INFO Channel 15 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64164:64609 [3] NCCL INFO Channel 15 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64165:64610 [4] NCCL INFO Channel 16 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64168:64614 [7] NCCL INFO Channel 16 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64167:64613 [6] NCCL INFO Channel 16 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 16 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64162:64608 [1] NCCL INFO Channel 16 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JL:64163:64612 [2] NCCL INFO Channel 16 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64166:64611 [5] NCCL INFO Channel 16 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64164:64609 [3] NCCL INFO Channel 16 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64165:64610 [4] NCCL INFO Channel 17 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64168:64614 [7] NCCL INFO Channel 17 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64167:64613 [6] NCCL INFO Channel 17 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 17 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64162:64608 [1] NCCL INFO Channel 17 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JL:64163:64612 [2] NCCL INFO Channel 17 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64166:64611 [5] NCCL INFO Channel 17 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64164:64609 [3] NCCL INFO Channel 17 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64165:64610 [4] NCCL INFO Channel 18 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64168:64614 [7] NCCL INFO Channel 18 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64167:64613 [6] NCCL INFO Channel 18 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 18 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64162:64608 [1] NCCL INFO Channel 18 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JL:64163:64612 [2] NCCL INFO Channel 18 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64166:64611 [5] NCCL INFO Channel 18 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64164:64609 [3] NCCL INFO Channel 18 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64165:64610 [4] NCCL INFO Channel 19 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64168:64614 [7] NCCL INFO Channel 19 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64167:64613 [6] NCCL INFO Channel 19 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 19 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64162:64608 [1] NCCL INFO Channel 19 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JL:64163:64612 [2] NCCL INFO Channel 19 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64166:64611 [5] NCCL INFO Channel 19 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64164:64609 [3] NCCL INFO Channel 19 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64165:64610 [4] NCCL INFO Channel 20 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64168:64614 [7] NCCL INFO Channel 20 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64167:64613 [6] NCCL INFO Channel 20 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 20 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64162:64608 [1] NCCL INFO Channel 20 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JL:64163:64612 [2] NCCL INFO Channel 20 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64166:64611 [5] NCCL INFO Channel 20 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64164:64609 [3] NCCL INFO Channel 20 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64165:64610 [4] NCCL INFO Channel 21 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64168:64614 [7] NCCL INFO Channel 21 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64167:64613 [6] NCCL INFO Channel 21 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 21 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64162:64608 [1] NCCL INFO Channel 21 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JL:64163:64612 [2] NCCL INFO Channel 21 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64166:64611 [5] NCCL INFO Channel 21 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64164:64609 [3] NCCL INFO Channel 21 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64165:64610 [4] NCCL INFO Channel 22 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64168:64614 [7] NCCL INFO Channel 22 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64167:64613 [6] NCCL INFO Channel 22 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 22 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64162:64608 [1] NCCL INFO Channel 22 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JL:64163:64612 [2] NCCL INFO Channel 22 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64166:64611 [5] NCCL INFO Channel 22 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64164:64609 [3] NCCL INFO Channel 22 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64165:64610 [4] NCCL INFO Channel 23 : 4[400000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64168:64614 [7] NCCL INFO Channel 23 : 7[100000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64167:64613 [6] NCCL INFO Channel 23 : 6[200000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 23 : 0[e00000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64162:64608 [1] NCCL INFO Channel 23 : 1[d00000] -> 0[e00000] via P2P/IPC/read
speec363a0001JL:64163:64612 [2] NCCL INFO Channel 23 : 2[c00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64166:64611 [5] NCCL INFO Channel 23 : 5[300000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64164:64609 [3] NCCL INFO Channel 23 : 3[b00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64168:64614 [7] NCCL INFO Connected all rings
speec363a0001JL:64161:64607 [0] NCCL INFO Connected all rings
speec363a0001JL:64162:64608 [1] NCCL INFO Connected all rings
speec363a0001JL:64167:64613 [6] NCCL INFO Connected all rings
speec363a0001JL:64165:64610 [4] NCCL INFO Connected all rings
speec363a0001JL:64166:64611 [5] NCCL INFO Connected all rings
speec363a0001JL:64163:64612 [2] NCCL INFO Connected all rings
speec363a0001JL:64164:64609 [3] NCCL INFO Connected all rings
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 00 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 01 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 02 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 03 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 04 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 05 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 06 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 07 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 08 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 09 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 10 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 11 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 12 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 13 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 14 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 15 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 16 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 17 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 18 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 19 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 20 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 21 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 22 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64161:64607 [0] NCCL INFO Channel 23 : 0[e00000] -> 1[d00000] via P2P/IPC/read
speec363a0001JL:64162:64608 [1] NCCL INFO Channel 00 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64166:64611 [5] NCCL INFO Channel 00 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64163:64612 [2] NCCL INFO Channel 00 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64165:64610 [4] NCCL INFO Channel 00 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64167:64613 [6] NCCL INFO Channel 00 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64164:64609 [3] NCCL INFO Channel 00 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64162:64608 [1] NCCL INFO Channel 01 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64166:64611 [5] NCCL INFO Channel 01 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64163:64612 [2] NCCL INFO Channel 01 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64165:64610 [4] NCCL INFO Channel 01 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64167:64613 [6] NCCL INFO Channel 01 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64164:64609 [3] NCCL INFO Channel 01 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64162:64608 [1] NCCL INFO Channel 02 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64166:64611 [5] NCCL INFO Channel 02 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64167:64613 [6] NCCL INFO Channel 02 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64163:64612 [2] NCCL INFO Channel 02 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64165:64610 [4] NCCL INFO Channel 02 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64164:64609 [3] NCCL INFO Channel 02 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64162:64608 [1] NCCL INFO Channel 03 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64166:64611 [5] NCCL INFO Channel 03 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64167:64613 [6] NCCL INFO Channel 03 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64163:64612 [2] NCCL INFO Channel 03 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64165:64610 [4] NCCL INFO Channel 03 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64164:64609 [3] NCCL INFO Channel 03 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64162:64608 [1] NCCL INFO Channel 04 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64166:64611 [5] NCCL INFO Channel 04 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64163:64612 [2] NCCL INFO Channel 04 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64167:64613 [6] NCCL INFO Channel 04 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64165:64610 [4] NCCL INFO Channel 04 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64164:64609 [3] NCCL INFO Channel 04 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64162:64608 [1] NCCL INFO Channel 05 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64166:64611 [5] NCCL INFO Channel 05 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64163:64612 [2] NCCL INFO Channel 05 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64165:64610 [4] NCCL INFO Channel 05 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64167:64613 [6] NCCL INFO Channel 05 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64164:64609 [3] NCCL INFO Channel 05 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64162:64608 [1] NCCL INFO Channel 06 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64166:64611 [5] NCCL INFO Channel 06 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64163:64612 [2] NCCL INFO Channel 06 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64165:64610 [4] NCCL INFO Channel 06 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64167:64613 [6] NCCL INFO Channel 06 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64164:64609 [3] NCCL INFO Channel 06 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64162:64608 [1] NCCL INFO Channel 07 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64166:64611 [5] NCCL INFO Channel 07 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64163:64612 [2] NCCL INFO Channel 07 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64167:64613 [6] NCCL INFO Channel 07 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64165:64610 [4] NCCL INFO Channel 07 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64164:64609 [3] NCCL INFO Channel 07 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64162:64608 [1] NCCL INFO Channel 08 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64166:64611 [5] NCCL INFO Channel 08 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64163:64612 [2] NCCL INFO Channel 08 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64167:64613 [6] NCCL INFO Channel 08 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64165:64610 [4] NCCL INFO Channel 08 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64164:64609 [3] NCCL INFO Channel 08 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64162:64608 [1] NCCL INFO Channel 09 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64166:64611 [5] NCCL INFO Channel 09 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64163:64612 [2] NCCL INFO Channel 09 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64167:64613 [6] NCCL INFO Channel 09 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64165:64610 [4] NCCL INFO Channel 09 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64164:64609 [3] NCCL INFO Channel 09 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64162:64608 [1] NCCL INFO Channel 10 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64166:64611 [5] NCCL INFO Channel 10 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64163:64612 [2] NCCL INFO Channel 10 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64165:64610 [4] NCCL INFO Channel 10 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64167:64613 [6] NCCL INFO Channel 10 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64164:64609 [3] NCCL INFO Channel 10 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64162:64608 [1] NCCL INFO Channel 11 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64166:64611 [5] NCCL INFO Channel 11 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64163:64612 [2] NCCL INFO Channel 11 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64165:64610 [4] NCCL INFO Channel 11 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64167:64613 [6] NCCL INFO Channel 11 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64164:64609 [3] NCCL INFO Channel 11 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64162:64608 [1] NCCL INFO Channel 12 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64166:64611 [5] NCCL INFO Channel 12 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64163:64612 [2] NCCL INFO Channel 12 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64167:64613 [6] NCCL INFO Channel 12 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64164:64609 [3] NCCL INFO Channel 12 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64165:64610 [4] NCCL INFO Channel 12 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64162:64608 [1] NCCL INFO Channel 13 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64166:64611 [5] NCCL INFO Channel 13 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64163:64612 [2] NCCL INFO Channel 13 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64167:64613 [6] NCCL INFO Channel 13 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64164:64609 [3] NCCL INFO Channel 13 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64165:64610 [4] NCCL INFO Channel 13 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64162:64608 [1] NCCL INFO Channel 14 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64166:64611 [5] NCCL INFO Channel 14 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64163:64612 [2] NCCL INFO Channel 14 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64167:64613 [6] NCCL INFO Channel 14 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64164:64609 [3] NCCL INFO Channel 14 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64165:64610 [4] NCCL INFO Channel 14 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64162:64608 [1] NCCL INFO Channel 15 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64166:64611 [5] NCCL INFO Channel 15 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64163:64612 [2] NCCL INFO Channel 15 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64165:64610 [4] NCCL INFO Channel 15 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64167:64613 [6] NCCL INFO Channel 15 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64164:64609 [3] NCCL INFO Channel 15 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64162:64608 [1] NCCL INFO Channel 16 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64166:64611 [5] NCCL INFO Channel 16 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64163:64612 [2] NCCL INFO Channel 16 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64165:64610 [4] NCCL INFO Channel 16 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64167:64613 [6] NCCL INFO Channel 16 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64164:64609 [3] NCCL INFO Channel 16 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64162:64608 [1] NCCL INFO Channel 17 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64166:64611 [5] NCCL INFO Channel 17 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64163:64612 [2] NCCL INFO Channel 17 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64167:64613 [6] NCCL INFO Channel 17 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64165:64610 [4] NCCL INFO Channel 17 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64164:64609 [3] NCCL INFO Channel 17 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64162:64608 [1] NCCL INFO Channel 18 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64166:64611 [5] NCCL INFO Channel 18 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64163:64612 [2] NCCL INFO Channel 18 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64167:64613 [6] NCCL INFO Channel 18 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64165:64610 [4] NCCL INFO Channel 18 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64164:64609 [3] NCCL INFO Channel 18 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64162:64608 [1] NCCL INFO Channel 19 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64166:64611 [5] NCCL INFO Channel 19 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64163:64612 [2] NCCL INFO Channel 19 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64167:64613 [6] NCCL INFO Channel 19 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64165:64610 [4] NCCL INFO Channel 19 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64164:64609 [3] NCCL INFO Channel 19 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64162:64608 [1] NCCL INFO Channel 20 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64166:64611 [5] NCCL INFO Channel 20 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64163:64612 [2] NCCL INFO Channel 20 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64165:64610 [4] NCCL INFO Channel 20 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64167:64613 [6] NCCL INFO Channel 20 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64164:64609 [3] NCCL INFO Channel 20 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64162:64608 [1] NCCL INFO Channel 21 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64166:64611 [5] NCCL INFO Channel 21 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64165:64610 [4] NCCL INFO Channel 21 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64163:64612 [2] NCCL INFO Channel 21 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64167:64613 [6] NCCL INFO Channel 21 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64164:64609 [3] NCCL INFO Channel 21 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64162:64608 [1] NCCL INFO Channel 22 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64166:64611 [5] NCCL INFO Channel 22 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64165:64610 [4] NCCL INFO Channel 22 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64167:64613 [6] NCCL INFO Channel 22 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64163:64612 [2] NCCL INFO Channel 22 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64164:64609 [3] NCCL INFO Channel 22 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64162:64608 [1] NCCL INFO Channel 23 : 1[d00000] -> 2[c00000] via P2P/IPC/read
speec363a0001JL:64166:64611 [5] NCCL INFO Channel 23 : 5[300000] -> 6[200000] via P2P/IPC/read
speec363a0001JL:64165:64610 [4] NCCL INFO Channel 23 : 4[400000] -> 5[300000] via P2P/IPC/read
speec363a0001JL:64167:64613 [6] NCCL INFO Channel 23 : 6[200000] -> 7[100000] via P2P/IPC/read
speec363a0001JL:64163:64612 [2] NCCL INFO Channel 23 : 2[c00000] -> 3[b00000] via P2P/IPC/read
speec363a0001JL:64164:64609 [3] NCCL INFO Channel 23 : 3[b00000] -> 4[400000] via P2P/IPC/read
speec363a0001JL:64161:64607 [0] NCCL INFO Connected all trees
speec363a0001JL:64161:64607 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
speec363a0001JL:64168:64614 [7] NCCL INFO Connected all trees
speec363a0001JL:64168:64614 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
speec363a0001JL:64161:64607 [0] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
speec363a0001JL:64168:64614 [7] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
speec363a0001JL:64164:64609 [3] NCCL INFO Connected all trees
speec363a0001JL:64164:64609 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
speec363a0001JL:64163:64612 [2] NCCL INFO Connected all trees
speec363a0001JL:64163:64612 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
speec363a0001JL:64165:64610 [4] NCCL INFO Connected all trees
speec363a0001JL:64165:64610 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
speec363a0001JL:64167:64613 [6] NCCL INFO Connected all trees
speec363a0001JL:64167:64613 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
speec363a0001JL:64162:64608 [1] NCCL INFO Connected all trees
speec363a0001JL:64162:64608 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
speec363a0001JL:64166:64611 [5] NCCL INFO Connected all trees
speec363a0001JL:64166:64611 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
speec363a0001JL:64163:64612 [2] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
speec363a0001JL:64164:64609 [3] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
speec363a0001JL:64167:64613 [6] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
speec363a0001JL:64165:64610 [4] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
speec363a0001JL:64162:64608 [1] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
speec363a0001JL:64166:64611 [5] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
speec363a0001JL:64163:64612 [2] NCCL INFO comm 0x7fbd10002fb0 rank 2 nranks 8 cudaDev 2 busId c00000 - Init COMPLETE
speec363a0001JL:64161:64607 [0] NCCL INFO comm 0x7fe80c002fb0 rank 0 nranks 8 cudaDev 0 busId e00000 - Init COMPLETE
speec363a0001JL:64167:64613 [6] NCCL INFO comm 0x7f3520002fb0 rank 6 nranks 8 cudaDev 6 busId 200000 - Init COMPLETE
speec363a0001JL:64162:64608 [1] NCCL INFO comm 0x7f0e3c002fb0 rank 1 nranks 8 cudaDev 1 busId d00000 - Init COMPLETE
speec363a0001JL:64166:64611 [5] NCCL INFO comm 0x7f0e24002fb0 rank 5 nranks 8 cudaDev 5 busId 300000 - Init COMPLETE
speec363a0001JL:64165:64610 [4] NCCL INFO comm 0x7ff2c4002fb0 rank 4 nranks 8 cudaDev 4 busId 400000 - Init COMPLETE
speec363a0001JL:64161:64161 [0] NCCL INFO Launch mode Parallel
speec363a0001JL:64164:64609 [3] NCCL INFO comm 0x7fa01c002fb0 rank 3 nranks 8 cudaDev 3 busId b00000 - Init COMPLETE
speec363a0001JL:64168:64614 [7] NCCL INFO comm 0x7fda50002fb0 rank 7 nranks 8 cudaDev 7 busId 100000 - Init COMPLETE
Using /home/dochen/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/dochen/.cache/torch_extensions/py38_cu113/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.8096728324890137 seconds
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
    "stage": 0, 
    "contiguous_gradients": false, 
    "reduce_scatter": true, 
    "reduce_bucket_size": 5.000000e+08, 
    "allgather_partitions": true, 
    "allgather_bucket_size": 5.000000e+08, 
    "overlap_comm": false, 
    "load_from_fp32_weights": true, 
    "elastic_checkpoint": true, 
    "offload_param": null, 
    "offload_optimizer": null, 
    "sub_group_size": 1.000000e+12, 
    "prefetch_bucket_size": 5.000000e+07, 
    "param_persistence_threshold": 1.000000e+05, 
    "max_live_parameters": 1.000000e+09, 
    "max_reuse_distance": 1.000000e+09, 
    "gather_fp16_weights_on_model_save": false, 
    "ignore_unused_parameters": true, 
    "legacy_stage1": false
}
    "train_batch_size": 2.048000e+03, 
    "train_micro_batch_size_per_gpu": 64, 
    "steps_per_print": 1000, 
    "optimizer": {
        "type": "Adam", 
        "adam_w_mode": true, 
        "params": {
            "lr": 0.0004, 
            "weight_decay": 0.05, 
            "bias_correction": true, 
            "betas": [0.9, 0.999], 
            "eps": 1e-08
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 7, 
        "loss_scale_window": 128
    }
}
Using /home/dochen/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
Emitting ninja build file /home/dochen/.cache/torch_extensions/py38_cu113/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.7865333557128906 seconds
model.gradient_accumulation_steps() = 4
Use step level LR scheduler!
Set warmup steps = 3125
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
criterion = LabelSmoothingCrossEntropy()
latest_ckpt: -1
Start training for 30 epochs
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Epoch: [0]  [   0/2502]  eta: 10:43:58  lr: 0.000000  min_lr: 0.000000  loss: 6.9062 (6.9062)  class_acc: 0.0000 (0.0000)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 15.4430  data: 13.9599  max mem: 27232
Epoch: [0]  [ 100/2502]  eta: 0:28:18  lr: 0.000003  min_lr: 0.000000  loss: 6.9062 (6.9062)  class_acc: 0.0312 (0.0147)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5577  data: 0.0002  max mem: 27806
Epoch: [0]  [ 200/2502]  eta: 0:24:17  lr: 0.000006  min_lr: 0.000000  loss: 6.9023 (6.9050)  class_acc: 0.0781 (0.0379)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5596  data: 0.0002  max mem: 27806
Epoch: [0]  [ 300/2502]  eta: 0:22:20  lr: 0.000010  min_lr: 0.000000  loss: 6.8867 (6.9013)  class_acc: 0.1406 (0.0701)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5593  data: 0.0002  max mem: 27806
Epoch: [0]  [ 400/2502]  eta: 0:20:54  lr: 0.000013  min_lr: 0.000000  loss: 6.8555 (6.8923)  class_acc: 0.0469 (0.0807)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5602  data: 0.0003  max mem: 27806
Epoch: [0]  [ 500/2502]  eta: 0:19:39  lr: 0.000016  min_lr: 0.000000  loss: 6.8281 (6.8827)  class_acc: 0.0000 (0.0674)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5592  data: 0.0003  max mem: 27806
Epoch: [0]  [ 600/2502]  eta: 0:18:31  lr: 0.000019  min_lr: 0.000000  loss: 6.5938 (6.8515)  class_acc: 0.0000 (0.0574)  loss_scale: 256.0000 (146.3161)  weight_decay: 0.0500 (0.0500)  time: 0.5602  data: 0.0002  max mem: 27806
Epoch: [0]  [ 700/2502]  eta: 0:17:26  lr: 0.000022  min_lr: 0.000000  loss: 6.4414 (6.8019)  class_acc: 0.0000 (0.0502)  loss_scale: 256.0000 (161.9629)  weight_decay: 0.0500 (0.0500)  time: 0.5604  data: 0.0002  max mem: 27806
Epoch: [0]  [ 800/2502]  eta: 0:16:24  lr: 0.000026  min_lr: 0.000000  loss: 6.2578 (6.7430)  class_acc: 0.0156 (0.0458)  loss_scale: 256.0000 (173.7029)  weight_decay: 0.0500 (0.0500)  time: 0.5603  data: 0.0002  max mem: 27806
Epoch: [0]  [ 900/2502]  eta: 0:15:23  lr: 0.000029  min_lr: 0.000000  loss: 6.1094 (6.6792)  class_acc: 0.0312 (0.0427)  loss_scale: 256.0000 (182.8368)  weight_decay: 0.0500 (0.0500)  time: 0.5600  data: 0.0002  max mem: 27806
Epoch: [0]  [1000/2502]  eta: 0:14:23  lr: 0.000032  min_lr: 0.000000  loss: 5.9570 (6.6134)  class_acc: 0.0156 (0.0404)  loss_scale: 256.0000 (190.1459)  weight_decay: 0.0500 (0.0500)  time: 0.5604  data: 0.0003  max mem: 27806
Epoch: [0]  [1100/2502]  eta: 0:13:23  lr: 0.000035  min_lr: 0.000000  loss: 5.8750 (6.5492)  class_acc: 0.0312 (0.0395)  loss_scale: 512.0000 (213.3333)  weight_decay: 0.0500 (0.0500)  time: 0.5605  data: 0.0002  max mem: 27806
Epoch: [0]  [1200/2502]  eta: 0:12:25  lr: 0.000038  min_lr: 0.000000  loss: 5.6914 (6.4852)  class_acc: 0.0312 (0.0396)  loss_scale: 512.0000 (238.2015)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0002  max mem: 27806
Epoch: [0]  [1300/2502]  eta: 0:11:26  lr: 0.000042  min_lr: 0.000000  loss: 5.6172 (6.4214)  class_acc: 0.0469 (0.0404)  loss_scale: 512.0000 (259.2467)  weight_decay: 0.0500 (0.0500)  time: 0.5604  data: 0.0002  max mem: 27806
Epoch: [0]  [1400/2502]  eta: 0:10:28  lr: 0.000045  min_lr: 0.000000  loss: 5.4414 (6.3576)  class_acc: 0.0625 (0.0424)  loss_scale: 512.0000 (277.2877)  weight_decay: 0.0500 (0.0500)  time: 0.5605  data: 0.0002  max mem: 27806
Epoch: [0]  [1500/2502]  eta: 0:09:31  lr: 0.000048  min_lr: 0.000000  loss: 5.2695 (6.2903)  class_acc: 0.0938 (0.0454)  loss_scale: 512.0000 (292.9247)  weight_decay: 0.0500 (0.0500)  time: 0.5600  data: 0.0002  max mem: 27806
Epoch: [0]  [1600/2502]  eta: 0:08:33  lr: 0.000051  min_lr: 0.000000  loss: 5.0469 (6.2182)  class_acc: 0.1094 (0.0504)  loss_scale: 1024.0000 (326.4360)  weight_decay: 0.0500 (0.0500)  time: 0.5607  data: 0.0002  max mem: 27806
Epoch: [0]  [1700/2502]  eta: 0:07:36  lr: 0.000054  min_lr: 0.000000  loss: 4.8047 (6.1420)  class_acc: 0.1875 (0.0576)  loss_scale: 1024.0000 (367.4450)  weight_decay: 0.0500 (0.0500)  time: 0.5604  data: 0.0002  max mem: 27806
Epoch: [0]  [1800/2502]  eta: 0:06:39  lr: 0.000058  min_lr: 0.000000  loss: 4.6523 (6.0625)  class_acc: 0.2188 (0.0658)  loss_scale: 1024.0000 (403.9001)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.0002  max mem: 27806
Epoch: [0]  [1900/2502]  eta: 0:05:41  lr: 0.000061  min_lr: 0.000000  loss: 4.3906 (5.9799)  class_acc: 0.2812 (0.0767)  loss_scale: 1024.0000 (436.5197)  weight_decay: 0.0500 (0.0500)  time: 0.5605  data: 0.0002  max mem: 27806
Epoch: [0]  [2000/2502]  eta: 0:04:44  lr: 0.000064  min_lr: 0.000000  loss: 4.1289 (5.8938)  class_acc: 0.3281 (0.0889)  loss_scale: 1024.0000 (465.8791)  weight_decay: 0.0500 (0.0500)  time: 0.5607  data: 0.0002  max mem: 27806
Epoch: [0]  [2100/2502]  eta: 0:03:48  lr: 0.000067  min_lr: 0.000000  loss: 3.9590 (5.8070)  class_acc: 0.3281 (0.1010)  loss_scale: 2048.0000 (516.8129)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0002  max mem: 27806
Epoch: [0]  [2200/2502]  eta: 0:02:51  lr: 0.000070  min_lr: 0.000000  loss: 3.7168 (5.7168)  class_acc: 0.3906 (0.1146)  loss_scale: 2048.0000 (586.3807)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0002  max mem: 27806
Epoch: [0]  [2300/2502]  eta: 0:01:54  lr: 0.000074  min_lr: 0.000000  loss: 3.6035 (5.6266)  class_acc: 0.4375 (0.1287)  loss_scale: 2048.0000 (649.9018)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0002  max mem: 27806
Epoch: [0]  [2400/2502]  eta: 0:00:57  lr: 0.000077  min_lr: 0.000000  loss: 3.3086 (5.5353)  class_acc: 0.4844 (0.1431)  loss_scale: 2048.0000 (708.1316)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.0002  max mem: 27806
Epoch: [0]  [2500/2502]  eta: 0:00:01  lr: 0.000080  min_lr: 0.000000  loss: 3.2305 (5.4449)  class_acc: 0.5156 (0.1580)  loss_scale: 2048.0000 (761.1904)  weight_decay: 0.0500 (0.0500)  time: 0.5335  data: 0.0005  max mem: 27806
Epoch: [0]  [2501/2502]  eta: 0:00:00  lr: 0.000080  min_lr: 0.000000  loss: 3.2305 (5.4449)  class_acc: 0.5156 (0.1580)  loss_scale: 2048.0000 (761.1904)  weight_decay: 0.0500 (0.0500)  time: 0.5062  data: 0.0005  max mem: 27806
Epoch: [0] Total time: 0:23:36 (0.5661 s / it)
Averaged stats: lr: 0.000080  min_lr: 0.000000  loss: 3.2305 (5.4420)  class_acc: 0.5156 (0.1581)  loss_scale: 2048.0000 (761.1904)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Test:  [ 0/98]  eta: 0:02:36  loss: 1.5047 (1.5047)  acc1: 68.7500 (68.7500)  acc5: 96.8750 (96.8750)  time: 1.5957  data: 1.3896  max mem: 27806
Test:  [97/98]  eta: 0:00:00  loss: 1.8813 (1.9245)  acc1: 65.6250 (66.7840)  acc5: 90.6250 (91.4880)  time: 0.2593  data: 0.0001  max mem: 27806
Test: Total time: 0:00:22 (0.2315 s / it)
* Acc@1 66.984 Acc@5 91.754 loss 1.924
Accuracy of the network on the 50000 test images: 67.0%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Test:  [ 0/98]  eta: 0:03:10  loss: 6.8555 (6.8555)  acc1: 34.3750 (34.3750)  acc5: 68.7500 (68.7500)  time: 1.9444  data: 1.7371  max mem: 27806
Test:  [97/98]  eta: 0:00:00  loss: 6.8601 (6.8629)  acc1: 21.4286 (17.3760)  acc5: 48.4375 (37.2960)  time: 0.2036  data: 0.0002  max mem: 27806
Test: Total time: 0:00:22 (0.2262 s / it)
* Acc@1 17.452 Acc@5 37.386 loss 6.863
EMA Accuracy of the network on the 50000 test images: 17.5%
Max accuracy: 17.45%
{"train_lr": 3.994878361075545e-05, "train_min_lr": 8.401125572701762e-10, "train_loss": 5.44198271484375, "train_class_acc": 0.15810859375, "train_loss_scale": 761.1904, "train_weight_decay": 0.04999999999999801, "test_loss": 6.862752207687923, "test_acc1": 17.452000000801085, "test_acc5": 37.386000005493166, "epoch": 0, "n_parameters": 303602664}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Epoch: [1]  [   0/2502]  eta: 8:04:58  lr: 0.000080  min_lr: 0.000000  loss: 3.0391 (3.0391)  class_acc: 0.6250 (0.6250)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  time: 11.6302  data: 11.0230  max mem: 27806
Epoch: [1]  [ 100/2502]  eta: 0:26:49  lr: 0.000083  min_lr: 0.000000  loss: 2.9453 (3.1029)  class_acc: 0.5938 (0.5370)  loss_scale: 4096.0000 (2818.5347)  weight_decay: 0.0500 (0.0500)  time: 0.5605  data: 0.0003  max mem: 27806
Epoch: [1]  [ 200/2502]  eta: 0:23:37  lr: 0.000086  min_lr: 0.000000  loss: 2.9355 (3.0549)  class_acc: 0.5469 (0.5426)  loss_scale: 4096.0000 (3454.0896)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0003  max mem: 27806
Epoch: [1]  [ 300/2502]  eta: 0:21:55  lr: 0.000090  min_lr: 0.000000  loss: 2.7656 (2.9871)  class_acc: 0.5781 (0.5548)  loss_scale: 4096.0000 (3667.3488)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0003  max mem: 27806
Epoch: [1]  [ 400/2502]  eta: 0:20:36  lr: 0.000093  min_lr: 0.000000  loss: 2.7441 (2.9376)  class_acc: 0.5938 (0.5629)  loss_scale: 4096.0000 (3774.2444)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0002  max mem: 27806
Epoch: [1]  [ 500/2502]  eta: 0:19:26  lr: 0.000096  min_lr: 0.000000  loss: 2.7168 (2.8975)  class_acc: 0.6094 (0.5685)  loss_scale: 4096.0000 (3838.4671)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.0004  max mem: 27806
Epoch: [1]  [ 600/2502]  eta: 0:18:21  lr: 0.000099  min_lr: 0.000000  loss: 2.6504 (2.8576)  class_acc: 0.5781 (0.5736)  loss_scale: 8192.0000 (4058.5158)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0003  max mem: 27806
Epoch: [1]  [ 700/2502]  eta: 0:17:19  lr: 0.000102  min_lr: 0.000000  loss: 2.5938 (2.8185)  class_acc: 0.6094 (0.5806)  loss_scale: 8192.0000 (4648.1712)  weight_decay: 0.0500 (0.0500)  time: 0.5607  data: 0.0002  max mem: 27806
Epoch: [1]  [ 800/2502]  eta: 0:16:18  lr: 0.000106  min_lr: 0.000000  loss: 2.5137 (2.7894)  class_acc: 0.6250 (0.5853)  loss_scale: 8192.0000 (5090.5968)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0002  max mem: 27806
Epoch: [1]  [ 900/2502]  eta: 0:15:18  lr: 0.000109  min_lr: 0.000000  loss: 2.3750 (2.7563)  class_acc: 0.6250 (0.5913)  loss_scale: 8192.0000 (5434.8147)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.0002  max mem: 27806
Epoch: [1]  [1000/2502]  eta: 0:14:19  lr: 0.000112  min_lr: 0.000000  loss: 2.4434 (2.7291)  class_acc: 0.6406 (0.5963)  loss_scale: 8192.0000 (5710.2577)  weight_decay: 0.0500 (0.0500)  time: 0.5618  data: 0.0003  max mem: 27806
Epoch: [1]  [1100/2502]  eta: 0:13:20  lr: 0.000115  min_lr: 0.000000  loss: 2.4512 (2.7050)  class_acc: 0.6562 (0.6008)  loss_scale: 16384.0000 (6039.8329)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0003  max mem: 27806
Epoch: [1]  [1200/2502]  eta: 0:12:22  lr: 0.000118  min_lr: 0.000000  loss: 2.3984 (2.6827)  class_acc: 0.6562 (0.6043)  loss_scale: 16384.0000 (6901.1291)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0002  max mem: 27806
Epoch: [1]  [1300/2502]  eta: 0:11:24  lr: 0.000122  min_lr: 0.000000  loss: 2.4473 (2.6643)  class_acc: 0.6406 (0.6068)  loss_scale: 16384.0000 (7630.0200)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0002  max mem: 27806
Epoch: [1]  [1400/2502]  eta: 0:10:26  lr: 0.000125  min_lr: 0.000000  loss: 2.3438 (2.6467)  class_acc: 0.6719 (0.6101)  loss_scale: 16384.0000 (8254.8580)  weight_decay: 0.0500 (0.0500)  time: 0.5618  data: 0.0002  max mem: 27806
Epoch: [1]  [1500/2502]  eta: 0:09:29  lr: 0.000128  min_lr: 0.000000  loss: 2.3438 (2.6296)  class_acc: 0.6875 (0.6134)  loss_scale: 16384.0000 (8796.4397)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.0002  max mem: 27806
Epoch: [1]  [1600/2502]  eta: 0:08:32  lr: 0.000131  min_lr: 0.000000  loss: 2.4141 (2.6122)  class_acc: 0.6250 (0.6162)  loss_scale: 16384.0000 (9290.8332)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.0003  max mem: 27806
Epoch: [1]  [1700/2502]  eta: 0:07:35  lr: 0.000134  min_lr: 0.000000  loss: 2.2520 (2.5961)  class_acc: 0.6875 (0.6192)  loss_scale: 32768.0000 (10671.0312)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.0002  max mem: 27806
Epoch: [1]  [1800/2502]  eta: 0:06:38  lr: 0.000138  min_lr: 0.000000  loss: 2.4492 (2.5843)  class_acc: 0.6406 (0.6211)  loss_scale: 32768.0000 (11897.9589)  weight_decay: 0.0500 (0.0500)  time: 0.5616  data: 0.0003  max mem: 27806
Epoch: [1]  [1900/2502]  eta: 0:05:41  lr: 0.000141  min_lr: 0.000000  loss: 2.3750 (2.5702)  class_acc: 0.6562 (0.6231)  loss_scale: 32768.0000 (12995.8043)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.0003  max mem: 27806
Epoch: [1]  [2000/2502]  eta: 0:04:44  lr: 0.000144  min_lr: 0.000000  loss: 2.2812 (2.5566)  class_acc: 0.6875 (0.6254)  loss_scale: 32768.0000 (13983.9200)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0003  max mem: 27806
Epoch: [1]  [2100/2502]  eta: 0:03:47  lr: 0.000147  min_lr: 0.000000  loss: 2.4199 (2.5455)  class_acc: 0.6562 (0.6278)  loss_scale: 32768.0000 (14877.9743)  weight_decay: 0.0500 (0.0500)  time: 0.5607  data: 0.0003  max mem: 27806
Epoch: [1]  [2200/2502]  eta: 0:02:50  lr: 0.000150  min_lr: 0.000000  loss: 2.3320 (2.5356)  class_acc: 0.6562 (0.6295)  loss_scale: 65536.0000 (17030.6879)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0003  max mem: 27806
Epoch: [1]  [2300/2502]  eta: 0:01:54  lr: 0.000154  min_lr: 0.000000  loss: 2.2285 (2.5265)  class_acc: 0.6719 (0.6308)  loss_scale: 32768.0000 (17913.9922)  weight_decay: 0.0500 (0.0500)  time: 0.5607  data: 0.0003  max mem: 27806
Epoch: [1]  [2400/2502]  eta: 0:00:57  lr: 0.000157  min_lr: 0.000000  loss: 2.2852 (2.5146)  class_acc: 0.6719 (0.6331)  loss_scale: 32768.0000 (18532.6514)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.0002  max mem: 27806
Epoch: [1]  [2500/2502]  eta: 0:00:01  lr: 0.000160  min_lr: 0.000000  loss: 2.2871 (2.5056)  class_acc: 0.6719 (0.6347)  loss_scale: 32768.0000 (19096.3712)  weight_decay: 0.0500 (0.0500)  time: 0.5337  data: 0.0006  max mem: 27806
Epoch: [1]  [2501/2502]  eta: 0:00:00  lr: 0.000160  min_lr: 0.000000  loss: 2.2871 (2.5056)  class_acc: 0.6719 (0.6347)  loss_scale: 32768.0000 (19096.3712)  weight_decay: 0.0500 (0.0500)  time: 0.5063  data: 0.0006  max mem: 27806
Epoch: [1] Total time: 0:23:34 (0.5653 s / it)
Averaged stats: lr: 0.000160  min_lr: 0.000000  loss: 2.2871 (2.4989)  class_acc: 0.6719 (0.6363)  loss_scale: 32768.0000 (19096.3712)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Test:  [ 0/98]  eta: 0:02:39  loss: 0.4338 (0.4338)  acc1: 90.6250 (90.6250)  acc5: 98.4375 (98.4375)  time: 1.6258  data: 1.3950  max mem: 27806
Test:  [97/98]  eta: 0:00:00  loss: 0.7225 (0.7352)  acc1: 81.2500 (83.0560)  acc5: 96.8750 (97.1360)  time: 0.2013  data: 0.0001  max mem: 27806
Test: Total time: 0:00:21 (0.2200 s / it)
* Acc@1 83.188 Acc@5 97.106 loss 0.730
Accuracy of the network on the 50000 test images: 83.2%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Test:  [ 0/98]  eta: 0:02:47  loss: 6.5779 (6.5779)  acc1: 71.8750 (71.8750)  acc5: 87.5000 (87.5000)  time: 1.7061  data: 1.4992  max mem: 27806
Test:  [97/98]  eta: 0:00:00  loss: 6.6362 (6.6441)  acc1: 40.6250 (36.2080)  acc5: 64.0625 (63.1200)  time: 0.2041  data: 0.0001  max mem: 27806
Test: Total time: 0:00:21 (0.2232 s / it)
* Acc@1 36.334 Acc@5 63.552 loss 6.643
EMA Accuracy of the network on the 50000 test images: 36.3%
Max accuracy: 36.33%
{"train_lr": 0.00011997439180537776, "train_min_lr": 2.5230303402633172e-09, "train_loss": 2.4989158203125, "train_class_acc": 0.636346875, "train_loss_scale": 19096.3712, "train_weight_decay": 0.04999999999999801, "test_loss": 6.643442205020359, "test_acc1": 36.33400000350952, "test_acc5": 63.55200001312256, "epoch": 1, "n_parameters": 303602664}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Epoch: [2]  [   0/2502]  eta: 8:07:59  lr: 0.000160  min_lr: 0.000000  loss: 2.2090 (2.2090)  class_acc: 0.7188 (0.7188)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 11.7024  data: 11.1089  max mem: 27806
Epoch: [2]  [ 100/2502]  eta: 0:26:51  lr: 0.000163  min_lr: 0.000000  loss: 2.1426 (2.2269)  class_acc: 0.7031 (0.6856)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.0003  max mem: 27806
Epoch: [2]  [ 200/2502]  eta: 0:23:38  lr: 0.000166  min_lr: 0.000000  loss: 2.1934 (2.2255)  class_acc: 0.6875 (0.6891)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.0004  max mem: 27806
Epoch: [2]  [ 300/2502]  eta: 0:21:56  lr: 0.000170  min_lr: 0.000000  loss: 2.1758 (2.2186)  class_acc: 0.7031 (0.6910)  loss_scale: 65536.0000 (40388.4651)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.0002  max mem: 27806
Epoch: [2]  [ 400/2502]  eta: 0:20:37  lr: 0.000173  min_lr: 0.000000  loss: 2.2324 (2.2178)  class_acc: 0.6875 (0.6902)  loss_scale: 65536.0000 (46659.6708)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0003  max mem: 27806
Epoch: [2]  [ 500/2502]  eta: 0:19:28  lr: 0.000176  min_lr: 0.000000  loss: 2.1367 (2.2140)  class_acc: 0.6719 (0.6907)  loss_scale: 65536.0000 (50427.4012)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.0002  max mem: 27806
Epoch: [2]  [ 600/2502]  eta: 0:18:22  lr: 0.000179  min_lr: 0.000000  loss: 2.3086 (2.2133)  class_acc: 0.6719 (0.6901)  loss_scale: 32768.0000 (49779.0083)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0002  max mem: 27806
Epoch: [2]  [ 700/2502]  eta: 0:17:19  lr: 0.000182  min_lr: 0.000000  loss: 2.2480 (2.2156)  class_acc: 0.7031 (0.6896)  loss_scale: 32768.0000 (47352.3310)  weight_decay: 0.0500 (0.0500)  time: 0.5607  data: 0.0002  max mem: 27806
Epoch: [2]  [ 800/2502]  eta: 0:16:18  lr: 0.000186  min_lr: 0.000000  loss: 2.1875 (2.2127)  class_acc: 0.6562 (0.6895)  loss_scale: 32768.0000 (45531.5655)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0002  max mem: 27806
Epoch: [2]  [ 900/2502]  eta: 0:15:18  lr: 0.000189  min_lr: 0.000000  loss: 2.2246 (2.2126)  class_acc: 0.6875 (0.6897)  loss_scale: 32768.0000 (44114.9656)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0002  max mem: 27806
Epoch: [2]  [1000/2502]  eta: 0:14:19  lr: 0.000192  min_lr: 0.000000  loss: 2.1465 (2.2075)  class_acc: 0.7031 (0.6911)  loss_scale: 32768.0000 (42981.4026)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0002  max mem: 27806
Epoch: [2]  [1100/2502]  eta: 0:13:20  lr: 0.000195  min_lr: 0.000000  loss: 2.0742 (2.2015)  class_acc: 0.7031 (0.6931)  loss_scale: 65536.0000 (43303.7602)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.0002  max mem: 27806
Epoch: [2]  [1200/2502]  eta: 0:12:22  lr: 0.000198  min_lr: 0.000000  loss: 2.2168 (2.1976)  class_acc: 0.6719 (0.6936)  loss_scale: 65536.0000 (45154.9042)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0002  max mem: 27806
Epoch: [2]  [1300/2502]  eta: 0:11:24  lr: 0.000202  min_lr: 0.000000  loss: 2.1211 (2.1976)  class_acc: 0.6719 (0.6933)  loss_scale: 65536.0000 (46721.4758)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.0002  max mem: 27806
Epoch: [2]  [1400/2502]  eta: 0:10:27  lr: 0.000205  min_lr: 0.000000  loss: 2.1250 (2.1931)  class_acc: 0.6875 (0.6942)  loss_scale: 65536.0000 (48064.4111)  weight_decay: 0.0500 (0.0500)  time: 0.5619  data: 0.0003  max mem: 27806
Epoch: [2]  [1500/2502]  eta: 0:09:29  lr: 0.000208  min_lr: 0.000000  loss: 2.1328 (2.1907)  class_acc: 0.7188 (0.6948)  loss_scale: 32768.0000 (47088.9913)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0003  max mem: 27806
Epoch: [2]  [1600/2502]  eta: 0:08:32  lr: 0.000211  min_lr: 0.000000  loss: 2.2715 (2.1888)  class_acc: 0.6875 (0.6952)  loss_scale: 32768.0000 (46194.4884)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0003  max mem: 27806
Epoch: [2]  [1700/2502]  eta: 0:07:35  lr: 0.000214  min_lr: 0.000000  loss: 2.0391 (2.1870)  class_acc: 0.6875 (0.6956)  loss_scale: 32768.0000 (45405.1593)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0002  max mem: 27806
Epoch: [2]  [1800/2502]  eta: 0:06:38  lr: 0.000218  min_lr: 0.000000  loss: 2.1094 (2.1843)  class_acc: 0.6875 (0.6961)  loss_scale: 32768.0000 (44703.4847)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0003  max mem: 27806
Epoch: [2]  [1900/2502]  eta: 0:05:41  lr: 0.000221  min_lr: 0.000000  loss: 2.2480 (2.1831)  class_acc: 0.7031 (0.6965)  loss_scale: 32768.0000 (44075.6318)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.0003  max mem: 27806
Epoch: [2]  [2000/2502]  eta: 0:04:44  lr: 0.000224  min_lr: 0.000000  loss: 2.0430 (2.1818)  class_acc: 0.6875 (0.6966)  loss_scale: 65536.0000 (44853.3493)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.0002  max mem: 27806
Epoch: [2]  [2100/2502]  eta: 0:03:47  lr: 0.000227  min_lr: 0.000000  loss: 2.1445 (2.1794)  class_acc: 0.7031 (0.6969)  loss_scale: 65536.0000 (45837.7687)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.0003  max mem: 27806
Epoch: [2]  [2200/2502]  eta: 0:02:50  lr: 0.000230  min_lr: 0.000000  loss: 2.2227 (2.1768)  class_acc: 0.7031 (0.6975)  loss_scale: 65536.0000 (46732.7360)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0002  max mem: 27806
Epoch: [2]  [2300/2502]  eta: 0:01:54  lr: 0.000234  min_lr: 0.000000  loss: 2.1641 (2.1752)  class_acc: 0.6875 (0.6978)  loss_scale: 65536.0000 (47549.9140)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0002  max mem: 27806
Epoch: [2]  [2400/2502]  eta: 0:00:57  lr: 0.000237  min_lr: 0.000000  loss: 2.0195 (2.1723)  class_acc: 0.7188 (0.6984)  loss_scale: 65536.0000 (48299.0221)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.0003  max mem: 27806
Epoch: [2]  [2500/2502]  eta: 0:00:01  lr: 0.000240  min_lr: 0.000000  loss: 2.0703 (2.1703)  class_acc: 0.7031 (0.6991)  loss_scale: 131072.0000 (50790.4000)  weight_decay: 0.0500 (0.0500)  time: 0.5339  data: 0.0006  max mem: 27806
Epoch: [2]  [2501/2502]  eta: 0:00:00  lr: 0.000240  min_lr: 0.000000  loss: 2.0703 (2.1703)  class_acc: 0.7031 (0.6991)  loss_scale: 131072.0000 (50790.4000)  weight_decay: 0.0500 (0.0500)  time: 0.5066  data: 0.0006  max mem: 27806
Epoch: [2] Total time: 0:23:34 (0.5654 s / it)
Averaged stats: lr: 0.000240  min_lr: 0.000000  loss: 2.0703 (2.1736)  class_acc: 0.7031 (0.6984)  loss_scale: 131072.0000 (50790.4000)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Test:  [ 0/98]  eta: 0:02:44  loss: 0.3839 (0.3839)  acc1: 92.1875 (92.1875)  acc5: 98.4375 (98.4375)  time: 1.6740  data: 1.4366  max mem: 27806
Test:  [97/98]  eta: 0:00:00  loss: 0.6183 (0.6551)  acc1: 82.8125 (84.6880)  acc5: 96.8750 (97.4080)  time: 0.2013  data: 0.0001  max mem: 27806
Test: Total time: 0:00:21 (0.2210 s / it)
* Acc@1 84.550 Acc@5 97.520 loss 0.654
Accuracy of the network on the 50000 test images: 84.6%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Test:  [ 0/98]  eta: 0:02:43  loss: 6.1000 (6.1000)  acc1: 81.2500 (81.2500)  acc5: 93.7500 (93.7500)  time: 1.6705  data: 1.4635  max mem: 27806
Test:  [97/98]  eta: 0:00:00  loss: 6.2360 (6.2443)  acc1: 59.5238 (55.7920)  acc5: 82.8125 (82.0480)  time: 0.2035  data: 0.0001  max mem: 27806
Test: Total time: 0:00:21 (0.2228 s / it)
* Acc@1 55.840 Acc@5 82.448 loss 6.243
EMA Accuracy of the network on the 50000 test images: 55.8%
Max accuracy: 55.84%
{"train_lr": 0.00019999999999999998, "train_min_lr": 4.205948123256455e-09, "train_loss": 2.173592431640625, "train_class_acc": 0.69836328125, "train_loss_scale": 50790.4, "train_weight_decay": 0.04999999999999801, "test_loss": 6.243162014654705, "test_acc1": 55.84000000366211, "test_acc5": 82.44800001251221, "epoch": 2, "n_parameters": 303602664}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Epoch: [3]  [   0/2502]  eta: 8:28:19  lr: 0.000240  min_lr: 0.000000  loss: 1.8340 (1.8340)  class_acc: 0.8125 (0.8125)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 12.1899  data: 11.6247  max mem: 27806
Epoch: [3]  [ 100/2502]  eta: 0:27:02  lr: 0.000243  min_lr: 0.000000  loss: 2.0605 (2.0914)  class_acc: 0.7031 (0.7166)  loss_scale: 65536.0000 (85651.0099)  weight_decay: 0.0500 (0.0500)  time: 0.5617  data: 0.0002  max mem: 27806
Epoch: [3]  [ 200/2502]  eta: 0:23:44  lr: 0.000246  min_lr: 0.000000  loss: 2.0332 (2.0901)  class_acc: 0.7188 (0.7152)  loss_scale: 65536.0000 (75643.5423)  weight_decay: 0.0500 (0.0500)  time: 0.5623  data: 0.0002  max mem: 27806
Epoch: [3]  [ 300/2502]  eta: 0:22:00  lr: 0.000250  min_lr: 0.000000  loss: 2.1094 (2.0876)  class_acc: 0.7188 (0.7156)  loss_scale: 65536.0000 (72285.5548)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0003  max mem: 27806
Epoch: [3]  [ 400/2502]  eta: 0:20:40  lr: 0.000253  min_lr: 0.000000  loss: 2.1250 (2.0949)  class_acc: 0.7031 (0.7126)  loss_scale: 65536.0000 (70602.3741)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0003  max mem: 27806
Epoch: [3]  [ 500/2502]  eta: 0:19:29  lr: 0.000256  min_lr: 0.000000  loss: 2.2227 (2.0904)  class_acc: 0.6875 (0.7142)  loss_scale: 65536.0000 (69198.6906)  weight_decay: 0.0500 (0.0500)  time: 0.5600  data: 0.0003  max mem: 27806
Epoch: [3]  [ 600/2502]  eta: 0:18:24  lr: 0.000259  min_lr: 0.000000  loss: 2.1270 (2.0929)  class_acc: 0.7188 (0.7146)  loss_scale: 32768.0000 (63137.0116)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.0003  max mem: 27806
Epoch: [3]  [ 700/2502]  eta: 0:17:21  lr: 0.000262  min_lr: 0.000000  loss: 1.9922 (2.0918)  class_acc: 0.7344 (0.7148)  loss_scale: 32768.0000 (58804.7703)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.0002  max mem: 27806
Epoch: [3]  [ 800/2502]  eta: 0:16:20  lr: 0.000266  min_lr: 0.000000  loss: 2.1074 (2.0894)  class_acc: 0.7031 (0.7151)  loss_scale: 32768.0000 (55554.2372)  weight_decay: 0.0500 (0.0500)  time: 0.5605  data: 0.0002  max mem: 27806
Epoch: [3]  [ 900/2502]  eta: 0:15:20  lr: 0.000269  min_lr: 0.000000  loss: 2.0742 (2.0929)  class_acc: 0.7188 (0.7147)  loss_scale: 32768.0000 (53025.2431)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.0002  max mem: 27806
Epoch: [3]  [1000/2502]  eta: 0:14:20  lr: 0.000272  min_lr: 0.000000  loss: 2.0039 (2.0924)  class_acc: 0.7500 (0.7148)  loss_scale: 32768.0000 (51001.5425)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0003  max mem: 27806
Epoch: [3]  [1100/2502]  eta: 0:13:21  lr: 0.000275  min_lr: 0.000000  loss: 1.9980 (2.0904)  class_acc: 0.7500 (0.7157)  loss_scale: 65536.0000 (52024.0363)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0003  max mem: 27806
Epoch: [3]  [1200/2502]  eta: 0:12:23  lr: 0.000278  min_lr: 0.000000  loss: 1.9854 (2.0863)  class_acc: 0.7031 (0.7168)  loss_scale: 32768.0000 (52330.5779)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0002  max mem: 27806
Epoch: [3]  [1300/2502]  eta: 0:11:25  lr: 0.000282  min_lr: 0.000000  loss: 2.0586 (2.0865)  class_acc: 0.7031 (0.7164)  loss_scale: 32768.0000 (50826.9208)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0002  max mem: 27806
Epoch: [3]  [1400/2502]  eta: 0:10:27  lr: 0.000285  min_lr: 0.000000  loss: 2.0566 (2.0846)  class_acc: 0.7188 (0.7168)  loss_scale: 32768.0000 (49537.9186)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.0002  max mem: 27806
Epoch: [3]  [1500/2502]  eta: 0:09:30  lr: 0.000288  min_lr: 0.000000  loss: 2.0762 (2.0854)  class_acc: 0.6875 (0.7164)  loss_scale: 32768.0000 (48420.6689)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.0003  max mem: 27806
Epoch: [3]  [1600/2502]  eta: 0:08:32  lr: 0.000291  min_lr: 0.000000  loss: 2.0684 (2.0845)  class_acc: 0.7344 (0.7169)  loss_scale: 32768.0000 (47442.9881)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0002  max mem: 27806
Epoch: [3]  [1700/2502]  eta: 0:07:35  lr: 0.000294  min_lr: 0.000000  loss: 2.0371 (2.0820)  class_acc: 0.7188 (0.7179)  loss_scale: 65536.0000 (46849.9565)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0002  max mem: 27806
Epoch: [3]  [1800/2502]  eta: 0:06:38  lr: 0.000298  min_lr: 0.000000  loss: 2.0605 (2.0816)  class_acc: 0.7188 (0.7180)  loss_scale: 65536.0000 (47887.4936)  weight_decay: 0.0500 (0.0500)  time: 0.5686  data: 0.0002  max mem: 27806
Epoch: [3]  [1900/2502]  eta: 0:05:41  lr: 0.000301  min_lr: 0.000000  loss: 2.0195 (2.0795)  class_acc: 0.7188 (0.7185)  loss_scale: 65536.0000 (48815.8738)  weight_decay: 0.0500 (0.0500)  time: 0.5605  data: 0.0002  max mem: 27806
Epoch: [3]  [2000/2502]  eta: 0:04:44  lr: 0.000304  min_lr: 0.000000  loss: 2.0215 (2.0786)  class_acc: 0.7188 (0.7185)  loss_scale: 65536.0000 (49651.4623)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.0003  max mem: 27806
Epoch: [3]  [2100/2502]  eta: 0:03:47  lr: 0.000307  min_lr: 0.000000  loss: 2.0332 (2.0782)  class_acc: 0.7031 (0.7185)  loss_scale: 32768.0000 (49440.5331)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0002  max mem: 27806
Epoch: [3]  [2200/2502]  eta: 0:02:51  lr: 0.000310  min_lr: 0.000000  loss: 2.0020 (2.0768)  class_acc: 0.7500 (0.7190)  loss_scale: 32768.0000 (48683.0350)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0002  max mem: 27806
Epoch: [3]  [2300/2502]  eta: 0:01:54  lr: 0.000314  min_lr: 0.000000  loss: 1.9775 (2.0753)  class_acc: 0.7344 (0.7192)  loss_scale: 32768.0000 (47991.3777)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0003  max mem: 27806
Epoch: [3]  [2400/2502]  eta: 0:00:57  lr: 0.000317  min_lr: 0.000000  loss: 2.0684 (2.0741)  class_acc: 0.7344 (0.7197)  loss_scale: 32768.0000 (47357.3344)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.0003  max mem: 27806
Epoch: [3]  [2500/2502]  eta: 0:00:01  lr: 0.000320  min_lr: 0.000000  loss: 2.1367 (2.0742)  class_acc: 0.7031 (0.7197)  loss_scale: 32768.0000 (46779.5968)  weight_decay: 0.0500 (0.0500)  time: 0.5332  data: 0.0006  max mem: 27806
Epoch: [3]  [2501/2502]  eta: 0:00:00  lr: 0.000320  min_lr: 0.000000  loss: 2.1367 (2.0742)  class_acc: 0.7031 (0.7197)  loss_scale: 32768.0000 (46779.5968)  weight_decay: 0.0500 (0.0500)  time: 0.5059  data: 0.0006  max mem: 27806
Epoch: [3] Total time: 0:23:35 (0.5658 s / it)
Averaged stats: lr: 0.000320  min_lr: 0.000000  loss: 2.1367 (2.0739)  class_acc: 0.7031 (0.7211)  loss_scale: 32768.0000 (46779.5968)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Test:  [ 0/98]  eta: 0:02:49  loss: 0.3586 (0.3586)  acc1: 95.3125 (95.3125)  acc5: 98.4375 (98.4375)  time: 1.7286  data: 1.4838  max mem: 27806
Test:  [97/98]  eta: 0:00:00  loss: 0.5531 (0.6318)  acc1: 84.3750 (85.2000)  acc5: 98.4375 (97.7440)  time: 0.2023  data: 0.0001  max mem: 27806
Test: Total time: 0:00:21 (0.2216 s / it)
* Acc@1 85.224 Acc@5 97.804 loss 0.631
Accuracy of the network on the 50000 test images: 85.2%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Test:  [ 0/98]  eta: 0:02:43  loss: 5.4225 (5.4225)  acc1: 85.9375 (85.9375)  acc5: 96.8750 (96.8750)  time: 1.6732  data: 1.4662  max mem: 27806
Test:  [97/98]  eta: 0:00:00  loss: 5.6683 (5.6524)  acc1: 70.3125 (69.4880)  acc5: 92.1875 (91.8880)  time: 0.2032  data: 0.0001  max mem: 27806
Test: Total time: 0:00:21 (0.2223 s / it)
* Acc@1 69.254 Acc@5 91.686 loss 5.651
EMA Accuracy of the network on the 50000 test images: 69.3%
Max accuracy: 69.25%
{"train_lr": 0.0002800256081946225, "train_min_lr": 5.8888659062495994e-09, "train_loss": 2.073862548828125, "train_class_acc": 0.7211484375, "train_loss_scale": 46779.5968, "train_weight_decay": 0.04999999999999801, "test_loss": 5.650724131233838, "test_acc1": 69.25400000854492, "test_acc5": 91.68600001068116, "epoch": 3, "n_parameters": 303602664}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Epoch: [4]  [   0/2502]  eta: 8:24:32  lr: 0.000320  min_lr: 0.000000  loss: 2.2461 (2.2461)  class_acc: 0.6562 (0.6562)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 12.0995  data: 11.5205  max mem: 27806
Epoch: [4]  [ 100/2502]  eta: 0:27:01  lr: 0.000323  min_lr: 0.000000  loss: 2.0039 (2.0216)  class_acc: 0.7188 (0.7304)  loss_scale: 65536.0000 (47692.0396)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0003  max mem: 27806
Epoch: [4]  [ 200/2502]  eta: 0:23:43  lr: 0.000327  min_lr: 0.000000  loss: 2.0020 (2.0319)  class_acc: 0.7344 (0.7307)  loss_scale: 65536.0000 (56569.6318)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.0002  max mem: 27806
Epoch: [4]  [ 300/2502]  eta: 0:22:02  lr: 0.000330  min_lr: 0.000000  loss: 1.9600 (2.0240)  class_acc: 0.7500 (0.7321)  loss_scale: 65536.0000 (59548.4917)  weight_decay: 0.0500 (0.0500)  time: 0.5701  data: 0.0002  max mem: 27806
Epoch: [4]  [ 400/2502]  eta: 0:20:41  lr: 0.000333  min_lr: 0.000000  loss: 2.0215 (2.0290)  class_acc: 0.7344 (0.7316)  loss_scale: 65536.0000 (61041.6359)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.0004  max mem: 27806
Epoch: [4]  [ 500/2502]  eta: 0:19:30  lr: 0.000336  min_lr: 0.000000  loss: 2.0391 (2.0297)  class_acc: 0.7344 (0.7320)  loss_scale: 65536.0000 (61938.7146)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0002  max mem: 27806
Epoch: [4]  [ 600/2502]  eta: 0:18:24  lr: 0.000339  min_lr: 0.000000  loss: 2.0371 (2.0291)  class_acc: 0.7344 (0.7325)  loss_scale: 65536.0000 (65154.3428)  weight_decay: 0.0500 (0.0500)  time: 0.5601  data: 0.0003  max mem: 27806
Epoch: [4]  [ 700/2502]  eta: 0:17:21  lr: 0.000343  min_lr: 0.000000  loss: 1.9512 (2.0233)  class_acc: 0.7344 (0.7334)  loss_scale: 65536.0000 (65208.7874)  weight_decay: 0.0500 (0.0500)  time: 0.5607  data: 0.0002  max mem: 27806
Epoch: [4]  [ 800/2502]  eta: 0:16:20  lr: 0.000346  min_lr: 0.000000  loss: 2.0195 (2.0210)  class_acc: 0.7188 (0.7331)  loss_scale: 65536.0000 (65249.6380)  weight_decay: 0.0500 (0.0500)  time: 0.5618  data: 0.0002  max mem: 27806
Epoch: [4]  [ 900/2502]  eta: 0:15:19  lr: 0.000349  min_lr: 0.000000  loss: 2.0605 (2.0250)  class_acc: 0.7188 (0.7314)  loss_scale: 65536.0000 (65281.4206)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0003  max mem: 27806
Epoch: [4]  [1000/2502]  eta: 0:14:20  lr: 0.000352  min_lr: 0.000000  loss: 2.0312 (2.0247)  class_acc: 0.7344 (0.7318)  loss_scale: 65536.0000 (65306.8531)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0002  max mem: 27806
Epoch: [4]  [1100/2502]  eta: 0:13:22  lr: 0.000355  min_lr: 0.000000  loss: 2.0352 (2.0240)  class_acc: 0.7344 (0.7320)  loss_scale: 32768.0000 (63363.3715)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.0002  max mem: 27806
Epoch: [4]  [1200/2502]  eta: 0:12:23  lr: 0.000359  min_lr: 0.000000  loss: 2.1562 (2.0227)  class_acc: 0.7188 (0.7324)  loss_scale: 32768.0000 (60815.8801)  weight_decay: 0.0500 (0.0500)  time: 0.5603  data: 0.0002  max mem: 27806
Epoch: [4]  [1300/2502]  eta: 0:11:25  lr: 0.000362  min_lr: 0.000000  loss: 1.9512 (2.0203)  class_acc: 0.7500 (0.7334)  loss_scale: 32768.0000 (58660.0092)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.0002  max mem: 27806
Epoch: [4]  [1400/2502]  eta: 0:10:27  lr: 0.000365  min_lr: 0.000000  loss: 1.9209 (2.0198)  class_acc: 0.7500 (0.7338)  loss_scale: 32768.0000 (56811.9001)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0002  max mem: 27806
Epoch: [4]  [1500/2502]  eta: 0:09:30  lr: 0.000368  min_lr: 0.000000  loss: 1.9844 (2.0203)  class_acc: 0.7500 (0.7338)  loss_scale: 32768.0000 (55210.0413)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.0002  max mem: 27806
Epoch: [4]  [1600/2502]  eta: 0:08:32  lr: 0.000371  min_lr: 0.000000  loss: 1.9307 (2.0189)  class_acc: 0.7656 (0.7342)  loss_scale: 65536.0000 (54831.6502)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0002  max mem: 27806
Epoch: [4]  [1700/2502]  eta: 0:07:35  lr: 0.000375  min_lr: 0.000000  loss: 1.9834 (2.0187)  class_acc: 0.7188 (0.7339)  loss_scale: 65536.0000 (55460.9477)  weight_decay: 0.0500 (0.0500)  time: 0.5616  data: 0.0002  max mem: 27806
Epoch: [4]  [1800/2502]  eta: 0:06:38  lr: 0.000378  min_lr: 0.000000  loss: 1.9688 (2.0183)  class_acc: 0.7500 (0.7343)  loss_scale: 65536.0000 (56020.3620)  weight_decay: 0.0500 (0.0500)  time: 0.5617  data: 0.0003  max mem: 27806
Epoch: [4]  [1900/2502]  eta: 0:05:41  lr: 0.000381  min_lr: 0.000000  loss: 1.9434 (2.0192)  class_acc: 0.7500 (0.7341)  loss_scale: 65536.0000 (56520.9216)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.0003  max mem: 27806
Epoch: [4]  [2000/2502]  eta: 0:04:44  lr: 0.000384  min_lr: 0.000000  loss: 2.0898 (2.0182)  class_acc: 0.7188 (0.7341)  loss_scale: 65536.0000 (56971.4503)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.0002  max mem: 27806
Epoch: [4]  [2100/2502]  eta: 0:03:47  lr: 0.000387  min_lr: 0.000000  loss: 1.9863 (2.0182)  class_acc: 0.7500 (0.7342)  loss_scale: 131072.0000 (58564.4169)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0002  max mem: 27806
Epoch: [4]  [2200/2502]  eta: 0:02:51  lr: 0.000391  min_lr: 0.000000  loss: 1.9600 (2.0156)  class_acc: 0.7500 (0.7349)  loss_scale: 65536.0000 (60965.4521)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.0002  max mem: 27806
Epoch: [4]  [2300/2502]  eta: 0:01:54  lr: 0.000394  min_lr: 0.000000  loss: 1.9521 (2.0161)  class_acc: 0.7188 (0.7349)  loss_scale: 65536.0000 (61164.0852)  weight_decay: 0.0500 (0.0500)  time: 0.5620  data: 0.0002  max mem: 27806
Epoch: [4]  [2400/2502]  eta: 0:00:57  lr: 0.000397  min_lr: 0.000000  loss: 1.8828 (2.0164)  class_acc: 0.7188 (0.7344)  loss_scale: 65536.0000 (61346.1724)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0002  max mem: 27806
Epoch: [4]  [2500/2502]  eta: 0:00:01  lr: 0.000400  min_lr: 0.000000  loss: 1.9062 (2.0155)  class_acc: 0.7500 (0.7346)  loss_scale: 65536.0000 (61512.0896)  weight_decay: 0.0500 (0.0500)  time: 0.5337  data: 0.0006  max mem: 27806
Epoch: [4]  [2501/2502]  eta: 0:00:00  lr: 0.000400  min_lr: 0.000000  loss: 1.9062 (2.0155)  class_acc: 0.7500 (0.7346)  loss_scale: 65536.0000 (61512.0896)  weight_decay: 0.0500 (0.0500)  time: 0.5063  data: 0.0006  max mem: 27806
Epoch: [4] Total time: 0:23:36 (0.5660 s / it)
Averaged stats: lr: 0.000400  min_lr: 0.000000  loss: 1.9062 (2.0154)  class_acc: 0.7500 (0.7347)  loss_scale: 65536.0000 (61512.0896)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Test:  [ 0/98]  eta: 0:02:51  loss: 0.3099 (0.3099)  acc1: 93.7500 (93.7500)  acc5: 100.0000 (100.0000)  time: 1.7486  data: 1.5101  max mem: 27806
Test:  [97/98]  eta: 0:00:00  loss: 0.5975 (0.6165)  acc1: 84.3750 (85.6320)  acc5: 98.4375 (98.0480)  time: 0.2014  data: 0.0001  max mem: 27806
Test: Total time: 0:00:21 (0.2215 s / it)
* Acc@1 85.626 Acc@5 97.854 loss 0.623
Accuracy of the network on the 50000 test images: 85.6%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Test:  [ 0/98]  eta: 0:02:53  loss: 4.5897 (4.5897)  acc1: 89.0625 (89.0625)  acc5: 98.4375 (98.4375)  time: 1.7703  data: 1.5633  max mem: 27806
Test:  [97/98]  eta: 0:00:00  loss: 4.9178 (4.8847)  acc1: 78.1250 (77.4720)  acc5: 93.7500 (94.8480)  time: 0.2049  data: 0.0002  max mem: 27806
Test: Total time: 0:00:22 (0.2249 s / it)
* Acc@1 77.136 Acc@5 95.120 loss 4.882
EMA Accuracy of the network on the 50000 test images: 77.1%
Max accuracy: 77.14%
{"train_lr": 0.0003600512163892446, "train_min_lr": 7.571783689242742e-09, "train_loss": 2.015421923828125, "train_class_acc": 0.7346890625, "train_loss_scale": 61512.0896, "train_weight_decay": 0.04999999999999801, "test_loss": 4.882394063229463, "test_acc1": 77.13599999328613, "test_acc5": 95.12000000854492, "epoch": 4, "n_parameters": 303602664}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Epoch: [5]  [   0/2502]  eta: 8:52:42  lr: 0.000400  min_lr: 0.000000  loss: 2.3301 (2.3301)  class_acc: 0.6250 (0.6250)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 12.7748  data: 12.1800  max mem: 27806
Epoch: [5]  [ 100/2502]  eta: 0:27:25  lr: 0.000400  min_lr: 0.000000  loss: 1.8672 (1.9443)  class_acc: 0.7656 (0.7542)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5701  data: 0.0003  max mem: 27806
Epoch: [5]  [ 200/2502]  eta: 0:23:54  lr: 0.000400  min_lr: 0.000000  loss: 1.8906 (1.9528)  class_acc: 0.7500 (0.7510)  loss_scale: 131072.0000 (70100.6965)  weight_decay: 0.0500 (0.0500)  time: 0.5605  data: 0.0002  max mem: 27806
Epoch: [5]  [ 300/2502]  eta: 0:22:06  lr: 0.000400  min_lr: 0.000000  loss: 1.9990 (1.9621)  class_acc: 0.7344 (0.7484)  loss_scale: 65536.0000 (75116.0133)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0002  max mem: 27806
Epoch: [5]  [ 400/2502]  eta: 0:20:44  lr: 0.000400  min_lr: 0.000000  loss: 1.9443 (1.9584)  class_acc: 0.7344 (0.7496)  loss_scale: 65536.0000 (72726.9825)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0003  max mem: 27806
Epoch: [5]  [ 500/2502]  eta: 0:19:33  lr: 0.000400  min_lr: 0.000000  loss: 1.9355 (1.9602)  class_acc: 0.7500 (0.7487)  loss_scale: 65536.0000 (71291.6567)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0003  max mem: 27806
Epoch: [5]  [ 600/2502]  eta: 0:18:27  lr: 0.000400  min_lr: 0.000000  loss: 1.9648 (1.9629)  class_acc: 0.7500 (0.7472)  loss_scale: 32768.0000 (65426.9551)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0003  max mem: 27806
Epoch: [5]  [ 700/2502]  eta: 0:17:23  lr: 0.000400  min_lr: 0.000000  loss: 1.9170 (1.9610)  class_acc: 0.7500 (0.7478)  loss_scale: 32768.0000 (60768.0456)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0002  max mem: 27806
Epoch: [5]  [ 800/2502]  eta: 0:16:21  lr: 0.000400  min_lr: 0.000000  loss: 2.0156 (1.9609)  class_acc: 0.7500 (0.7484)  loss_scale: 32768.0000 (57272.4095)  weight_decay: 0.0500 (0.0500)  time: 0.5604  data: 0.0002  max mem: 27806
Epoch: [5]  [ 900/2502]  eta: 0:15:21  lr: 0.000400  min_lr: 0.000000  loss: 1.9189 (1.9607)  class_acc: 0.7188 (0.7484)  loss_scale: 32768.0000 (54552.7192)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.0003  max mem: 27806
Epoch: [5]  [1000/2502]  eta: 0:14:21  lr: 0.000400  min_lr: 0.000000  loss: 1.9131 (1.9616)  class_acc: 0.7344 (0.7482)  loss_scale: 32768.0000 (52376.4236)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0002  max mem: 27806
Epoch: [5]  [1100/2502]  eta: 0:13:23  lr: 0.000400  min_lr: 0.000000  loss: 1.9346 (1.9611)  class_acc: 0.7344 (0.7482)  loss_scale: 65536.0000 (52797.8492)  weight_decay: 0.0500 (0.0500)  time: 0.5690  data: 0.0002  max mem: 27806
Epoch: [5]  [1200/2502]  eta: 0:12:24  lr: 0.000400  min_lr: 0.000000  loss: 1.9053 (1.9620)  class_acc: 0.7500 (0.7482)  loss_scale: 65536.0000 (53858.4779)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.0002  max mem: 27806
Epoch: [5]  [1300/2502]  eta: 0:11:26  lr: 0.000400  min_lr: 0.000000  loss: 1.8799 (1.9600)  class_acc: 0.7344 (0.7486)  loss_scale: 65536.0000 (54756.0584)  weight_decay: 0.0500 (0.0500)  time: 0.5696  data: 0.0003  max mem: 27806
Epoch: [5]  [1400/2502]  eta: 0:10:28  lr: 0.000400  min_lr: 0.000000  loss: 1.9434 (1.9600)  class_acc: 0.7500 (0.7488)  loss_scale: 32768.0000 (53981.8301)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.0002  max mem: 27806
Epoch: [5]  [1500/2502]  eta: 0:09:31  lr: 0.000399  min_lr: 0.000000  loss: 1.9678 (1.9607)  class_acc: 0.7500 (0.7487)  loss_scale: 32768.0000 (52568.5170)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.0003  max mem: 27806
Epoch: [5]  [1600/2502]  eta: 0:08:33  lr: 0.000399  min_lr: 0.000000  loss: 1.8730 (1.9599)  class_acc: 0.7500 (0.7486)  loss_scale: 32768.0000 (51331.7577)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0002  max mem: 27806
Epoch: [5]  [1700/2502]  eta: 0:07:36  lr: 0.000399  min_lr: 0.000000  loss: 1.9824 (1.9614)  class_acc: 0.7500 (0.7485)  loss_scale: 32768.0000 (50240.4139)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0002  max mem: 27806
Epoch: [5]  [1800/2502]  eta: 0:06:39  lr: 0.000399  min_lr: 0.000000  loss: 1.9355 (1.9603)  class_acc: 0.7656 (0.7487)  loss_scale: 32768.0000 (49270.2632)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.0002  max mem: 27806
Epoch: [5]  [1900/2502]  eta: 0:05:42  lr: 0.000399  min_lr: 0.000000  loss: 1.9004 (1.9595)  class_acc: 0.7656 (0.7489)  loss_scale: 65536.0000 (49264.0421)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.0003  max mem: 27806
Epoch: [5]  [2000/2502]  eta: 0:04:45  lr: 0.000399  min_lr: 0.000000  loss: 1.8857 (1.9596)  class_acc: 0.7500 (0.7488)  loss_scale: 65536.0000 (50077.2334)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0002  max mem: 27806
Epoch: [5]  [2100/2502]  eta: 0:03:48  lr: 0.000399  min_lr: 0.000000  loss: 1.9844 (1.9586)  class_acc: 0.7188 (0.7491)  loss_scale: 65536.0000 (50813.0148)  weight_decay: 0.0500 (0.0500)  time: 0.5616  data: 0.0003  max mem: 27806
Epoch: [5]  [2200/2502]  eta: 0:02:51  lr: 0.000399  min_lr: 0.000000  loss: 1.8994 (1.9594)  class_acc: 0.7656 (0.7490)  loss_scale: 65536.0000 (51481.9373)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.0002  max mem: 27806
Epoch: [5]  [2300/2502]  eta: 0:01:54  lr: 0.000399  min_lr: 0.000000  loss: 1.8809 (1.9580)  class_acc: 0.7500 (0.7493)  loss_scale: 65536.0000 (52092.7179)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0002  max mem: 27806
Epoch: [5]  [2400/2502]  eta: 0:00:57  lr: 0.000399  min_lr: 0.000000  loss: 1.8877 (1.9566)  class_acc: 0.7500 (0.7495)  loss_scale: 65536.0000 (53307.7085)  weight_decay: 0.0500 (0.0500)  time: 0.5594  data: 0.0003  max mem: 27806
Epoch: [5]  [2500/2502]  eta: 0:00:01  lr: 0.000398  min_lr: 0.000000  loss: 1.8916 (1.9562)  class_acc: 0.7656 (0.7496)  loss_scale: 65536.0000 (53791.9488)  weight_decay: 0.0500 (0.0500)  time: 0.5334  data: 0.0006  max mem: 27806
Epoch: [5]  [2501/2502]  eta: 0:00:00  lr: 0.000398  min_lr: 0.000000  loss: 1.8916 (1.9562)  class_acc: 0.7656 (0.7496)  loss_scale: 65536.0000 (53791.9488)  weight_decay: 0.0500 (0.0500)  time: 0.5061  data: 0.0006  max mem: 27806
Epoch: [5] Total time: 0:23:36 (0.5662 s / it)
Averaged stats: lr: 0.000398  min_lr: 0.000000  loss: 1.8916 (1.9616)  class_acc: 0.7656 (0.7479)  loss_scale: 65536.0000 (53791.9488)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Test:  [ 0/98]  eta: 0:02:46  loss: 0.2975 (0.2975)  acc1: 93.7500 (93.7500)  acc5: 98.4375 (98.4375)  time: 1.7034  data: 1.4976  max mem: 27806
Test:  [97/98]  eta: 0:00:00  loss: 0.5590 (0.6021)  acc1: 85.9375 (86.1120)  acc5: 98.4375 (98.1440)  time: 0.2012  data: 0.0001  max mem: 27806
Test: Total time: 0:00:21 (0.2214 s / it)
* Acc@1 86.074 Acc@5 98.010 loss 0.601
Accuracy of the network on the 50000 test images: 86.1%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Test:  [ 0/98]  eta: 0:02:38  loss: 3.6618 (3.6618)  acc1: 89.0625 (89.0625)  acc5: 98.4375 (98.4375)  time: 1.6151  data: 1.4083  max mem: 27806
Test:  [97/98]  eta: 0:00:00  loss: 4.0605 (3.9951)  acc1: 81.2500 (81.5680)  acc5: 96.8750 (96.3200)  time: 0.2032  data: 0.0001  max mem: 27806
Test: Total time: 0:00:21 (0.2218 s / it)
* Acc@1 81.296 Acc@5 96.518 loss 3.993
EMA Accuracy of the network on the 50000 test images: 81.3%
Max accuracy: 81.30%
{"train_lr": 0.0003994766092864718, "train_min_lr": 8.400889475566443e-09, "train_loss": 1.96158681640625, "train_class_acc": 0.74793671875, "train_loss_scale": 53791.9488, "train_weight_decay": 0.04999999999999801, "test_loss": 3.992535061678108, "test_acc1": 81.29600000854492, "test_acc5": 96.51800000610352, "epoch": 5, "n_parameters": 303602664}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Epoch: [6]  [   0/2502]  eta: 8:33:07  lr: 0.000398  min_lr: 0.000000  loss: 1.7549 (1.7549)  class_acc: 0.8125 (0.8125)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 12.3052  data: 11.7313  max mem: 27806
Epoch: [6]  [ 100/2502]  eta: 0:27:05  lr: 0.000398  min_lr: 0.000000  loss: 1.8379 (1.9192)  class_acc: 0.7656 (0.7585)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5607  data: 0.0003  max mem: 27806
Epoch: [6]  [ 200/2502]  eta: 0:23:45  lr: 0.000398  min_lr: 0.000000  loss: 1.9072 (1.9178)  class_acc: 0.7812 (0.7585)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0002  max mem: 27806
Epoch: [6]  [ 300/2502]  eta: 0:22:00  lr: 0.000398  min_lr: 0.000000  loss: 1.9150 (1.9179)  class_acc: 0.7500 (0.7555)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0002  max mem: 27806
Epoch: [6]  [ 400/2502]  eta: 0:20:40  lr: 0.000398  min_lr: 0.000000  loss: 1.8994 (1.9168)  class_acc: 0.7656 (0.7574)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5618  data: 0.0002  max mem: 27806
Epoch: [6]  [ 500/2502]  eta: 0:19:30  lr: 0.000398  min_lr: 0.000000  loss: 1.8418 (1.9167)  class_acc: 0.7656 (0.7580)  loss_scale: 131072.0000 (78355.4172)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0002  max mem: 27806
Epoch: [6]  [ 600/2502]  eta: 0:18:24  lr: 0.000398  min_lr: 0.000000  loss: 1.8652 (1.9156)  class_acc: 0.7656 (0.7582)  loss_scale: 65536.0000 (79493.7504)  weight_decay: 0.0500 (0.0500)  time: 0.5679  data: 0.0002  max mem: 27806
Epoch: [6]  [ 700/2502]  eta: 0:17:21  lr: 0.000397  min_lr: 0.000000  loss: 1.8330 (1.9192)  class_acc: 0.7500 (0.7577)  loss_scale: 65536.0000 (77502.6305)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.0002  max mem: 27806
Epoch: [6]  [ 800/2502]  eta: 0:16:20  lr: 0.000397  min_lr: 0.000000  loss: 1.8203 (1.9175)  class_acc: 0.7656 (0.7576)  loss_scale: 65536.0000 (76008.6692)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0003  max mem: 27806
Epoch: [6]  [ 900/2502]  eta: 0:15:20  lr: 0.000397  min_lr: 0.000000  loss: 1.8760 (1.9160)  class_acc: 0.7656 (0.7579)  loss_scale: 65536.0000 (74846.3307)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0002  max mem: 27806
Epoch: [6]  [1000/2502]  eta: 0:14:20  lr: 0.000397  min_lr: 0.000000  loss: 1.9023 (1.9166)  class_acc: 0.7812 (0.7582)  loss_scale: 65536.0000 (73916.2278)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0003  max mem: 27806
Epoch: [6]  [1100/2502]  eta: 0:13:21  lr: 0.000397  min_lr: 0.000000  loss: 1.7988 (1.9123)  class_acc: 0.7812 (0.7592)  loss_scale: 32768.0000 (71428.8828)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0003  max mem: 27806
Epoch: [6]  [1200/2502]  eta: 0:12:23  lr: 0.000397  min_lr: 0.000000  loss: 1.8584 (1.9146)  class_acc: 0.7500 (0.7587)  loss_scale: 32768.0000 (68209.8251)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0002  max mem: 27806
Epoch: [6]  [1300/2502]  eta: 0:11:25  lr: 0.000396  min_lr: 0.000000  loss: 1.9111 (1.9139)  class_acc: 0.7656 (0.7592)  loss_scale: 32768.0000 (65485.6264)  weight_decay: 0.0500 (0.0500)  time: 0.5605  data: 0.0002  max mem: 27806
Epoch: [6]  [1400/2502]  eta: 0:10:27  lr: 0.000396  min_lr: 0.000000  loss: 1.9180 (1.9140)  class_acc: 0.7500 (0.7593)  loss_scale: 32768.0000 (63150.3212)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.0003  max mem: 27806
Epoch: [6]  [1500/2502]  eta: 0:09:30  lr: 0.000396  min_lr: 0.000000  loss: 1.8896 (1.9138)  class_acc: 0.7656 (0.7593)  loss_scale: 32768.0000 (61126.1825)  weight_decay: 0.0500 (0.0500)  time: 0.5619  data: 0.0003  max mem: 27806
Epoch: [6]  [1600/2502]  eta: 0:08:33  lr: 0.000396  min_lr: 0.000000  loss: 1.8135 (1.9140)  class_acc: 0.7656 (0.7593)  loss_scale: 65536.0000 (60214.5259)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.0002  max mem: 27806
Epoch: [6]  [1700/2502]  eta: 0:07:35  lr: 0.000396  min_lr: 0.000000  loss: 1.9141 (1.9140)  class_acc: 0.7500 (0.7592)  loss_scale: 65536.0000 (60527.3698)  weight_decay: 0.0500 (0.0500)  time: 0.5620  data: 0.0002  max mem: 27806
Epoch: [6]  [1800/2502]  eta: 0:06:38  lr: 0.000395  min_lr: 0.000000  loss: 1.8750 (1.9145)  class_acc: 0.7656 (0.7593)  loss_scale: 32768.0000 (60041.3104)  weight_decay: 0.0500 (0.0500)  time: 0.5617  data: 0.0003  max mem: 27806
Epoch: [6]  [1900/2502]  eta: 0:05:41  lr: 0.000395  min_lr: 0.000000  loss: 1.8818 (1.9142)  class_acc: 0.7656 (0.7595)  loss_scale: 32768.0000 (58606.6281)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.0003  max mem: 27806
Epoch: [6]  [2000/2502]  eta: 0:04:44  lr: 0.000395  min_lr: 0.000000  loss: 1.8887 (1.9142)  class_acc: 0.7500 (0.7594)  loss_scale: 32768.0000 (57315.3423)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.0003  max mem: 27806
Epoch: [6]  [2100/2502]  eta: 0:03:47  lr: 0.000395  min_lr: 0.000000  loss: 1.8984 (1.9131)  class_acc: 0.7500 (0.7593)  loss_scale: 32768.0000 (56146.9776)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0002  max mem: 27806
Epoch: [6]  [2200/2502]  eta: 0:02:51  lr: 0.000394  min_lr: 0.000000  loss: 1.8789 (1.9127)  class_acc: 0.7188 (0.7592)  loss_scale: 32768.0000 (55084.7796)  weight_decay: 0.0500 (0.0500)  time: 0.5618  data: 0.0003  max mem: 27806
Epoch: [6]  [2300/2502]  eta: 0:01:54  lr: 0.000394  min_lr: 0.000000  loss: 1.9346 (1.9113)  class_acc: 0.7656 (0.7597)  loss_scale: 65536.0000 (54485.1664)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.0002  max mem: 27806
Epoch: [6]  [2400/2502]  eta: 0:00:57  lr: 0.000394  min_lr: 0.000000  loss: 1.9053 (1.9110)  class_acc: 0.7500 (0.7597)  loss_scale: 65536.0000 (54945.4261)  weight_decay: 0.0500 (0.0500)  time: 0.5708  data: 0.0003  max mem: 27806
Epoch: [6]  [2500/2502]  eta: 0:00:01  lr: 0.000394  min_lr: 0.000000  loss: 1.8555 (1.9112)  class_acc: 0.7500 (0.7596)  loss_scale: 65536.0000 (55364.8128)  weight_decay: 0.0500 (0.0500)  time: 0.5342  data: 0.0006  max mem: 27806
Epoch: [6]  [2501/2502]  eta: 0:00:00  lr: 0.000394  min_lr: 0.000000  loss: 1.8555 (1.9112)  class_acc: 0.7500 (0.7596)  loss_scale: 65536.0000 (55364.8128)  weight_decay: 0.0500 (0.0500)  time: 0.5069  data: 0.0006  max mem: 27806
Epoch: [6] Total time: 0:23:36 (0.5661 s / it)
Averaged stats: lr: 0.000394  min_lr: 0.000000  loss: 1.8555 (1.9103)  class_acc: 0.7500 (0.7605)  loss_scale: 65536.0000 (55364.8128)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Test:  [ 0/98]  eta: 0:02:42  loss: 0.2208 (0.2208)  acc1: 93.7500 (93.7500)  acc5: 100.0000 (100.0000)  time: 1.6547  data: 1.4493  max mem: 27806
Test:  [97/98]  eta: 0:00:00  loss: 0.5941 (0.5787)  acc1: 85.7143 (86.2720)  acc5: 98.4375 (98.0320)  time: 0.2015  data: 0.0001  max mem: 27806
Test: Total time: 0:00:21 (0.2207 s / it)
* Acc@1 86.310 Acc@5 98.060 loss 0.582
Accuracy of the network on the 50000 test images: 86.3%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Test:  [ 0/98]  eta: 0:02:40  loss: 2.7444 (2.7444)  acc1: 92.1875 (92.1875)  acc5: 98.4375 (98.4375)  time: 1.6358  data: 1.4285  max mem: 27806
Test:  [97/98]  eta: 0:00:00  loss: 3.1794 (3.0918)  acc1: 82.8125 (83.7920)  acc5: 96.8750 (97.1520)  time: 0.2034  data: 0.0001  max mem: 27806
Test: Total time: 0:00:21 (0.2221 s / it)
* Acc@1 83.546 Acc@5 97.318 loss 3.089
EMA Accuracy of the network on the 50000 test images: 83.5%
Max accuracy: 83.55%
{"train_lr": 0.00039634114628249646, "train_min_lr": 8.334951501880952e-09, "train_loss": 1.910315625, "train_class_acc": 0.76045078125, "train_loss_scale": 55364.8128, "train_weight_decay": 0.04999999999999801, "test_loss": 3.089000583303218, "test_acc1": 83.54600001495362, "test_acc5": 97.31800000488282, "epoch": 6, "n_parameters": 303602664}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Epoch: [7]  [   0/2502]  eta: 8:00:49  lr: 0.000394  min_lr: 0.000000  loss: 2.0234 (2.0234)  class_acc: 0.7500 (0.7500)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 11.5305  data: 10.9291  max mem: 27806
Epoch: [7]  [ 100/2502]  eta: 0:26:47  lr: 0.000393  min_lr: 0.000000  loss: 1.8770 (1.8787)  class_acc: 0.7656 (0.7743)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.0004  max mem: 27806
Epoch: [7]  [ 200/2502]  eta: 0:23:38  lr: 0.000393  min_lr: 0.000000  loss: 1.8428 (1.8784)  class_acc: 0.7656 (0.7720)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5616  data: 0.0004  max mem: 27806
Epoch: [7]  [ 300/2502]  eta: 0:21:56  lr: 0.000393  min_lr: 0.000000  loss: 1.8438 (1.8715)  class_acc: 0.7656 (0.7739)  loss_scale: 131072.0000 (68584.1860)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0002  max mem: 27806
Epoch: [7]  [ 400/2502]  eta: 0:20:37  lr: 0.000393  min_lr: 0.000000  loss: 1.8682 (1.8745)  class_acc: 0.7656 (0.7725)  loss_scale: 131072.0000 (84167.1820)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.0003  max mem: 27806
Epoch: [7]  [ 500/2502]  eta: 0:19:27  lr: 0.000392  min_lr: 0.000000  loss: 1.8340 (1.8764)  class_acc: 0.7812 (0.7722)  loss_scale: 65536.0000 (81756.4870)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0003  max mem: 27806
Epoch: [7]  [ 600/2502]  eta: 0:18:22  lr: 0.000392  min_lr: 0.000000  loss: 1.8545 (1.8786)  class_acc: 0.7656 (0.7713)  loss_scale: 65536.0000 (79057.5707)  weight_decay: 0.0500 (0.0500)  time: 0.5617  data: 0.0003  max mem: 27806
Epoch: [7]  [ 700/2502]  eta: 0:17:19  lr: 0.000392  min_lr: 0.000000  loss: 1.8369 (1.8739)  class_acc: 0.7812 (0.7716)  loss_scale: 65536.0000 (77128.6733)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0003  max mem: 27806
Epoch: [7]  [ 800/2502]  eta: 0:16:19  lr: 0.000392  min_lr: 0.000000  loss: 1.7959 (1.8754)  class_acc: 0.7656 (0.7709)  loss_scale: 65536.0000 (75681.3983)  weight_decay: 0.0500 (0.0500)  time: 0.5695  data: 0.0003  max mem: 27806
Epoch: [7]  [ 900/2502]  eta: 0:15:19  lr: 0.000391  min_lr: 0.000000  loss: 1.8447 (1.8764)  class_acc: 0.7969 (0.7702)  loss_scale: 65536.0000 (74555.3829)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0003  max mem: 27806
Epoch: [7]  [1000/2502]  eta: 0:14:20  lr: 0.000391  min_lr: 0.000000  loss: 1.8525 (1.8755)  class_acc: 0.7656 (0.7697)  loss_scale: 65536.0000 (77320.6953)  weight_decay: 0.0500 (0.0500)  time: 0.5597  data: 0.0003  max mem: 27806
Epoch: [7]  [1100/2502]  eta: 0:13:21  lr: 0.000391  min_lr: 0.000000  loss: 1.8506 (1.8754)  class_acc: 0.7500 (0.7695)  loss_scale: 65536.0000 (76250.3324)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.0004  max mem: 27806
Epoch: [7]  [1200/2502]  eta: 0:12:23  lr: 0.000390  min_lr: 0.000000  loss: 1.8623 (1.8769)  class_acc: 0.7656 (0.7691)  loss_scale: 65536.0000 (75358.2148)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.0003  max mem: 27806
Epoch: [7]  [1300/2502]  eta: 0:11:25  lr: 0.000390  min_lr: 0.000000  loss: 1.8809 (1.8776)  class_acc: 0.7656 (0.7688)  loss_scale: 65536.0000 (74603.2406)  weight_decay: 0.0500 (0.0500)  time: 0.5616  data: 0.0003  max mem: 27806
Epoch: [7]  [1400/2502]  eta: 0:10:27  lr: 0.000390  min_lr: 0.000000  loss: 1.8232 (1.8785)  class_acc: 0.7656 (0.7683)  loss_scale: 65536.0000 (73956.0428)  weight_decay: 0.0500 (0.0500)  time: 0.5604  data: 0.0002  max mem: 27806
Epoch: [7]  [1500/2502]  eta: 0:09:30  lr: 0.000389  min_lr: 0.000000  loss: 1.7471 (1.8774)  class_acc: 0.7500 (0.7684)  loss_scale: 65536.0000 (73482.4037)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0004  max mem: 27806
Epoch: [7]  [1600/2502]  eta: 0:08:33  lr: 0.000389  min_lr: 0.000000  loss: 1.8047 (1.8743)  class_acc: 0.7969 (0.7689)  loss_scale: 65536.0000 (75687.7352)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0003  max mem: 27806
Epoch: [7]  [1700/2502]  eta: 0:07:35  lr: 0.000389  min_lr: 0.000000  loss: 1.8145 (1.8735)  class_acc: 0.7500 (0.7693)  loss_scale: 65536.0000 (75090.9253)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.0003  max mem: 27806
Epoch: [7]  [1800/2502]  eta: 0:06:38  lr: 0.000388  min_lr: 0.000000  loss: 1.8311 (1.8724)  class_acc: 0.7656 (0.7696)  loss_scale: 65536.0000 (74560.3909)  weight_decay: 0.0500 (0.0500)  time: 0.5619  data: 0.0003  max mem: 27806
Epoch: [7]  [1900/2502]  eta: 0:05:41  lr: 0.000388  min_lr: 0.000000  loss: 1.8896 (1.8736)  class_acc: 0.7500 (0.7692)  loss_scale: 65536.0000 (74085.6728)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.0003  max mem: 27806
Epoch: [7]  [2000/2502]  eta: 0:04:44  lr: 0.000388  min_lr: 0.000000  loss: 1.8643 (1.8738)  class_acc: 0.7656 (0.7691)  loss_scale: 65536.0000 (73658.4028)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0002  max mem: 27806
Epoch: [7]  [2100/2502]  eta: 0:03:47  lr: 0.000387  min_lr: 0.000000  loss: 1.8164 (1.8745)  class_acc: 0.7812 (0.7690)  loss_scale: 131072.0000 (73833.2756)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.0003  max mem: 27806
Epoch: [7]  [2200/2502]  eta: 0:02:51  lr: 0.000387  min_lr: 0.000000  loss: 1.7705 (1.8730)  class_acc: 0.7812 (0.7693)  loss_scale: 65536.0000 (74825.9736)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0003  max mem: 27806
Epoch: [7]  [2300/2502]  eta: 0:01:54  lr: 0.000387  min_lr: 0.000000  loss: 1.8994 (1.8742)  class_acc: 0.7656 (0.7690)  loss_scale: 65536.0000 (74422.2373)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0002  max mem: 27806
Epoch: [7]  [2400/2502]  eta: 0:00:57  lr: 0.000386  min_lr: 0.000000  loss: 1.9102 (1.8746)  class_acc: 0.7500 (0.7690)  loss_scale: 65536.0000 (74052.1316)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0003  max mem: 27806
Epoch: [7]  [2500/2502]  eta: 0:00:01  lr: 0.000386  min_lr: 0.000000  loss: 1.7979 (1.8742)  class_acc: 0.7812 (0.7691)  loss_scale: 65536.0000 (73714.8928)  weight_decay: 0.0500 (0.0500)  time: 0.5346  data: 0.0006  max mem: 27806
Epoch: [7]  [2501/2502]  eta: 0:00:00  lr: 0.000386  min_lr: 0.000000  loss: 1.7979 (1.8742)  class_acc: 0.7812 (0.7691)  loss_scale: 65536.0000 (73714.8928)  weight_decay: 0.0500 (0.0500)  time: 0.5073  data: 0.0006  max mem: 27806
Epoch: [7] Total time: 0:23:36 (0.5660 s / it)
Averaged stats: lr: 0.000386  min_lr: 0.000000  loss: 1.7979 (1.8734)  class_acc: 0.7812 (0.7695)  loss_scale: 65536.0000 (73714.8928)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Test:  [ 0/98]  eta: 0:02:46  loss: 0.2398 (0.2398)  acc1: 95.3125 (95.3125)  acc5: 100.0000 (100.0000)  time: 1.7012  data: 1.4648  max mem: 27806
Test:  [97/98]  eta: 0:00:00  loss: 0.5962 (0.5810)  acc1: 85.7143 (86.6400)  acc5: 98.4375 (98.0160)  time: 0.2015  data: 0.0001  max mem: 27806
Test: Total time: 0:00:21 (0.2209 s / it)
* Acc@1 86.328 Acc@5 98.088 loss 0.582
Accuracy of the network on the 50000 test images: 86.3%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Test:  [ 0/98]  eta: 0:02:48  loss: 1.9322 (1.9322)  acc1: 92.1875 (92.1875)  acc5: 98.4375 (98.4375)  time: 1.7171  data: 1.5101  max mem: 27806
Test:  [97/98]  eta: 0:00:00  loss: 2.3706 (2.2816)  acc1: 84.3750 (84.8320)  acc5: 97.6190 (97.6800)  time: 0.2031  data: 0.0001  max mem: 27806
Test: Total time: 0:00:21 (0.2231 s / it)
* Acc@1 84.930 Acc@5 97.706 loss 2.279
EMA Accuracy of the network on the 50000 test images: 84.9%
Max accuracy: 84.93%
{"train_lr": 0.0003901171514118208, "train_min_lr": 8.204062504153569e-09, "train_loss": 1.87338955078125, "train_class_acc": 0.7694609375, "train_loss_scale": 73714.8928, "train_weight_decay": 0.04999999999999801, "test_loss": 2.2793724743687376, "test_acc1": 84.93000001495362, "test_acc5": 97.70600000610352, "epoch": 7, "n_parameters": 303602664}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Epoch: [8]  [   0/2502]  eta: 8:28:55  lr: 0.000386  min_lr: 0.000000  loss: 1.7920 (1.7920)  class_acc: 0.7812 (0.7812)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 12.2045  data: 11.6241  max mem: 27806
Epoch: [8]  [ 100/2502]  eta: 0:27:08  lr: 0.000386  min_lr: 0.000000  loss: 1.7910 (1.8427)  class_acc: 0.7656 (0.7760)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0002  max mem: 27806
Epoch: [8]  [ 200/2502]  eta: 0:23:46  lr: 0.000385  min_lr: 0.000000  loss: 1.8604 (1.8382)  class_acc: 0.7812 (0.7784)  loss_scale: 65536.0000 (70752.7960)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0002  max mem: 27806
Epoch: [8]  [ 300/2502]  eta: 0:22:01  lr: 0.000385  min_lr: 0.000000  loss: 1.8867 (1.8380)  class_acc: 0.7500 (0.7782)  loss_scale: 65536.0000 (69019.6412)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0003  max mem: 27806
Epoch: [8]  [ 400/2502]  eta: 0:20:40  lr: 0.000384  min_lr: 0.000000  loss: 1.8965 (1.8423)  class_acc: 0.7500 (0.7765)  loss_scale: 32768.0000 (66026.2943)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.0002  max mem: 27806
Epoch: [8]  [ 500/2502]  eta: 0:19:29  lr: 0.000384  min_lr: 0.000000  loss: 1.7490 (1.8379)  class_acc: 0.7969 (0.7771)  loss_scale: 32768.0000 (59387.9122)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0002  max mem: 27806
Epoch: [8]  [ 600/2502]  eta: 0:18:24  lr: 0.000384  min_lr: 0.000000  loss: 1.8877 (1.8367)  class_acc: 0.7344 (0.7765)  loss_scale: 32768.0000 (54958.6423)  weight_decay: 0.0500 (0.0500)  time: 0.5616  data: 0.0002  max mem: 27806
Epoch: [8]  [ 700/2502]  eta: 0:17:21  lr: 0.000383  min_lr: 0.000000  loss: 1.7695 (1.8380)  class_acc: 0.7812 (0.7771)  loss_scale: 32768.0000 (51793.0728)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0002  max mem: 27806
Epoch: [8]  [ 800/2502]  eta: 0:16:19  lr: 0.000383  min_lr: 0.000000  loss: 1.8418 (1.8380)  class_acc: 0.7500 (0.7771)  loss_scale: 32768.0000 (49417.9076)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0003  max mem: 27806
Epoch: [8]  [ 900/2502]  eta: 0:15:19  lr: 0.000382  min_lr: 0.000000  loss: 1.8076 (1.8365)  class_acc: 0.7812 (0.7778)  loss_scale: 32768.0000 (47933.6559)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0004  max mem: 27806
Epoch: [8]  [1000/2502]  eta: 0:14:20  lr: 0.000382  min_lr: 0.000000  loss: 1.9160 (1.8385)  class_acc: 0.7812 (0.7773)  loss_scale: 65536.0000 (49692.1319)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.0003  max mem: 27806
Epoch: [8]  [1100/2502]  eta: 0:13:21  lr: 0.000382  min_lr: 0.000000  loss: 1.8721 (1.8368)  class_acc: 0.7656 (0.7775)  loss_scale: 65536.0000 (51131.1753)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0003  max mem: 27806
Epoch: [8]  [1200/2502]  eta: 0:12:23  lr: 0.000381  min_lr: 0.000000  loss: 1.7666 (1.8361)  class_acc: 0.7969 (0.7779)  loss_scale: 65536.0000 (52330.5779)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.0003  max mem: 27806
Epoch: [8]  [1300/2502]  eta: 0:11:25  lr: 0.000381  min_lr: 0.000000  loss: 1.8848 (1.8368)  class_acc: 0.7656 (0.7780)  loss_scale: 65536.0000 (53345.5988)  weight_decay: 0.0500 (0.0500)  time: 0.5616  data: 0.0003  max mem: 27806
Epoch: [8]  [1400/2502]  eta: 0:10:27  lr: 0.000380  min_lr: 0.000000  loss: 1.8408 (1.8362)  class_acc: 0.7969 (0.7782)  loss_scale: 65536.0000 (54215.7202)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0003  max mem: 27806
Epoch: [8]  [1500/2502]  eta: 0:09:30  lr: 0.000380  min_lr: 0.000000  loss: 1.8350 (1.8363)  class_acc: 0.7656 (0.7784)  loss_scale: 131072.0000 (58986.7662)  weight_decay: 0.0500 (0.0500)  time: 0.5593  data: 0.0002  max mem: 27806
Epoch: [8]  [1600/2502]  eta: 0:08:32  lr: 0.000379  min_lr: 0.000000  loss: 1.7686 (1.8350)  class_acc: 0.7969 (0.7789)  loss_scale: 65536.0000 (59395.8376)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0003  max mem: 27806
Epoch: [8]  [1700/2502]  eta: 0:07:35  lr: 0.000379  min_lr: 0.000000  loss: 1.8438 (1.8342)  class_acc: 0.7656 (0.7791)  loss_scale: 65536.0000 (59756.8113)  weight_decay: 0.0500 (0.0500)  time: 0.5617  data: 0.0003  max mem: 27806
Epoch: [8]  [1800/2502]  eta: 0:06:38  lr: 0.000379  min_lr: 0.000000  loss: 1.8662 (1.8357)  class_acc: 0.7812 (0.7790)  loss_scale: 65536.0000 (60077.6991)  weight_decay: 0.0500 (0.0500)  time: 0.5616  data: 0.0003  max mem: 27806
Epoch: [8]  [1900/2502]  eta: 0:05:41  lr: 0.000378  min_lr: 0.000000  loss: 1.8506 (1.8364)  class_acc: 0.7656 (0.7789)  loss_scale: 65536.0000 (60364.8269)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0004  max mem: 27806
Epoch: [8]  [2000/2502]  eta: 0:04:44  lr: 0.000378  min_lr: 0.000000  loss: 1.8613 (1.8368)  class_acc: 0.7812 (0.7789)  loss_scale: 65536.0000 (60623.2564)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.0003  max mem: 27806
Epoch: [8]  [2100/2502]  eta: 0:03:47  lr: 0.000377  min_lr: 0.000000  loss: 1.7939 (1.8371)  class_acc: 0.7812 (0.7788)  loss_scale: 131072.0000 (63664.4341)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.0003  max mem: 27806
Epoch: [8]  [2200/2502]  eta: 0:02:51  lr: 0.000377  min_lr: 0.000000  loss: 1.9404 (1.8375)  class_acc: 0.7656 (0.7786)  loss_scale: 131072.0000 (66727.0223)  weight_decay: 0.0500 (0.0500)  time: 0.5711  data: 0.0003  max mem: 27806
Epoch: [8]  [2300/2502]  eta: 0:01:54  lr: 0.000376  min_lr: 0.000000  loss: 1.8164 (1.8370)  class_acc: 0.7500 (0.7785)  loss_scale: 65536.0000 (69010.7466)  weight_decay: 0.0500 (0.0500)  time: 0.5596  data: 0.0003  max mem: 27806
Epoch: [8]  [2400/2502]  eta: 0:00:57  lr: 0.000376  min_lr: 0.000000  loss: 1.9463 (1.8369)  class_acc: 0.7656 (0.7787)  loss_scale: 32768.0000 (67910.6905)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0002  max mem: 27806
Epoch: [8]  [2500/2502]  eta: 0:00:01  lr: 0.000375  min_lr: 0.000000  loss: 1.8574 (1.8367)  class_acc: 0.7812 (0.7788)  loss_scale: 32768.0000 (66519.0400)  weight_decay: 0.0500 (0.0500)  time: 0.5334  data: 0.0004  max mem: 27806
Epoch: [8]  [2501/2502]  eta: 0:00:00  lr: 0.000375  min_lr: 0.000000  loss: 1.8574 (1.8367)  class_acc: 0.7812 (0.7788)  loss_scale: 32768.0000 (66519.0400)  weight_decay: 0.0500 (0.0500)  time: 0.5061  data: 0.0004  max mem: 27806
Epoch: [8] Total time: 0:23:36 (0.5660 s / it)
Averaged stats: lr: 0.000375  min_lr: 0.000000  loss: 1.8574 (1.8382)  class_acc: 0.7812 (0.7785)  loss_scale: 32768.0000 (66519.0400)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Test:  [ 0/98]  eta: 0:02:20  loss: 0.2358 (0.2358)  acc1: 96.8750 (96.8750)  acc5: 100.0000 (100.0000)  time: 1.4317  data: 1.1940  max mem: 27806
Test:  [97/98]  eta: 0:00:00  loss: 0.5468 (0.5718)  acc1: 85.9375 (87.0880)  acc5: 98.4375 (98.0000)  time: 0.2013  data: 0.0001  max mem: 27806
Test: Total time: 0:00:21 (0.2183 s / it)
* Acc@1 86.672 Acc@5 98.142 loss 0.577
Accuracy of the network on the 50000 test images: 86.7%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Test:  [ 0/98]  eta: 0:02:25  loss: 1.3150 (1.3150)  acc1: 92.1875 (92.1875)  acc5: 98.4375 (98.4375)  time: 1.4862  data: 1.2792  max mem: 27806
Test:  [97/98]  eta: 0:00:00  loss: 1.7188 (1.6471)  acc1: 85.9375 (85.8720)  acc5: 98.4375 (97.9520)  time: 0.2033  data: 0.0001  max mem: 27806
Test: Total time: 0:00:21 (0.2203 s / it)
* Acc@1 85.790 Acc@5 97.948 loss 1.646
EMA Accuracy of the network on the 50000 test images: 85.8%
Max accuracy: 85.79%
{"train_lr": 0.0003809027807915837, "train_min_lr": 8.010286680067654e-09, "train_loss": 1.838232275390625, "train_class_acc": 0.7785078125, "train_loss_scale": 66519.04, "train_weight_decay": 0.04999999999999801, "test_loss": 1.646060728144889, "test_acc1": 85.79000001495362, "test_acc5": 97.94800000732423, "epoch": 8, "n_parameters": 303602664}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Epoch: [9]  [   0/2502]  eta: 8:29:08  lr: 0.000375  min_lr: 0.000000  loss: 1.8701 (1.8701)  class_acc: 0.7344 (0.7344)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 12.2096  data: 11.6171  max mem: 27806
Epoch: [9]  [ 100/2502]  eta: 0:27:04  lr: 0.000375  min_lr: 0.000000  loss: 1.8760 (1.8062)  class_acc: 0.7969 (0.7899)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0002  max mem: 27806
Epoch: [9]  [ 200/2502]  eta: 0:23:44  lr: 0.000374  min_lr: 0.000000  loss: 1.7871 (1.8068)  class_acc: 0.7812 (0.7889)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0002  max mem: 27806
Epoch: [9]  [ 300/2502]  eta: 0:22:00  lr: 0.000374  min_lr: 0.000000  loss: 1.8086 (1.8113)  class_acc: 0.7656 (0.7881)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5607  data: 0.0002  max mem: 27806
Epoch: [9]  [ 400/2502]  eta: 0:20:41  lr: 0.000373  min_lr: 0.000000  loss: 1.7451 (1.8062)  class_acc: 0.7969 (0.7882)  loss_scale: 65536.0000 (37180.6484)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0002  max mem: 27806
Epoch: [9]  [ 500/2502]  eta: 0:19:31  lr: 0.000373  min_lr: 0.000000  loss: 1.7451 (1.8043)  class_acc: 0.7812 (0.7891)  loss_scale: 65536.0000 (42840.3992)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.0002  max mem: 27806
Epoch: [9]  [ 600/2502]  eta: 0:18:25  lr: 0.000372  min_lr: 0.000000  loss: 1.8047 (1.8075)  class_acc: 0.7812 (0.7886)  loss_scale: 65536.0000 (46616.7055)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.0002  max mem: 27806
Epoch: [9]  [ 700/2502]  eta: 0:17:21  lr: 0.000372  min_lr: 0.000000  loss: 1.8223 (1.8102)  class_acc: 0.7812 (0.7883)  loss_scale: 65536.0000 (49315.6063)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0002  max mem: 27806
Epoch: [9]  [ 800/2502]  eta: 0:16:20  lr: 0.000371  min_lr: 0.000000  loss: 1.8350 (1.8095)  class_acc: 0.7969 (0.7881)  loss_scale: 65536.0000 (51340.6242)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0003  max mem: 27806
Epoch: [9]  [ 900/2502]  eta: 0:15:20  lr: 0.000371  min_lr: 0.000000  loss: 1.7305 (1.8090)  class_acc: 0.8125 (0.7885)  loss_scale: 65536.0000 (54370.8768)  weight_decay: 0.0500 (0.0500)  time: 0.5692  data: 0.0002  max mem: 27806
Epoch: [9]  [1000/2502]  eta: 0:14:21  lr: 0.000370  min_lr: 0.000000  loss: 1.7988 (1.8094)  class_acc: 0.8125 (0.7888)  loss_scale: 65536.0000 (55486.2737)  weight_decay: 0.0500 (0.0500)  time: 0.5616  data: 0.0002  max mem: 27806
Epoch: [9]  [1100/2502]  eta: 0:13:22  lr: 0.000370  min_lr: 0.000000  loss: 1.8057 (1.8085)  class_acc: 0.7812 (0.7890)  loss_scale: 65536.0000 (56399.0554)  weight_decay: 0.0500 (0.0500)  time: 0.5616  data: 0.0002  max mem: 27806
Epoch: [9]  [1200/2502]  eta: 0:12:23  lr: 0.000369  min_lr: 0.000000  loss: 1.8291 (1.8085)  class_acc: 0.7812 (0.7884)  loss_scale: 65536.0000 (57159.8335)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.0002  max mem: 27806
Epoch: [9]  [1300/2502]  eta: 0:11:25  lr: 0.000369  min_lr: 0.000000  loss: 1.7598 (1.8084)  class_acc: 0.7969 (0.7879)  loss_scale: 32768.0000 (55738.3428)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.0002  max mem: 27806
Epoch: [9]  [1400/2502]  eta: 0:10:28  lr: 0.000368  min_lr: 0.000000  loss: 1.8877 (1.8100)  class_acc: 0.7812 (0.7875)  loss_scale: 32768.0000 (54098.7752)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0003  max mem: 27806
Epoch: [9]  [1500/2502]  eta: 0:09:30  lr: 0.000368  min_lr: 0.000000  loss: 1.9229 (1.8114)  class_acc: 0.7500 (0.7870)  loss_scale: 32768.0000 (52677.6709)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0002  max mem: 27806
Epoch: [9]  [1600/2502]  eta: 0:08:33  lr: 0.000367  min_lr: 0.000000  loss: 1.8242 (1.8102)  class_acc: 0.7812 (0.7871)  loss_scale: 32768.0000 (51434.0937)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0002  max mem: 27806
Epoch: [9]  [1700/2502]  eta: 0:07:35  lr: 0.000366  min_lr: 0.000000  loss: 1.8115 (1.8124)  class_acc: 0.7656 (0.7863)  loss_scale: 32768.0000 (50336.7337)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.0002  max mem: 27806
Epoch: [9]  [1800/2502]  eta: 0:06:38  lr: 0.000366  min_lr: 0.000000  loss: 1.8027 (1.8114)  class_acc: 0.7812 (0.7863)  loss_scale: 65536.0000 (50562.0611)  weight_decay: 0.0500 (0.0500)  time: 0.5618  data: 0.0002  max mem: 27806
Epoch: [9]  [1900/2502]  eta: 0:05:41  lr: 0.000365  min_lr: 0.000000  loss: 1.7100 (1.8107)  class_acc: 0.7969 (0.7864)  loss_scale: 65536.0000 (51349.7486)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.0002  max mem: 27806
Epoch: [9]  [2000/2502]  eta: 0:04:44  lr: 0.000365  min_lr: 0.000000  loss: 1.7383 (1.8109)  class_acc: 0.7969 (0.7862)  loss_scale: 65536.0000 (52058.7066)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0002  max mem: 27806
Epoch: [9]  [2100/2502]  eta: 0:03:47  lr: 0.000364  min_lr: 0.000000  loss: 1.7646 (1.8102)  class_acc: 0.7812 (0.7860)  loss_scale: 65536.0000 (52700.1771)  weight_decay: 0.0500 (0.0500)  time: 0.5619  data: 0.0002  max mem: 27806
Epoch: [9]  [2200/2502]  eta: 0:02:51  lr: 0.000364  min_lr: 0.000000  loss: 1.7334 (1.8083)  class_acc: 0.8125 (0.7866)  loss_scale: 65536.0000 (53283.3585)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0003  max mem: 27806
Epoch: [9]  [2300/2502]  eta: 0:01:54  lr: 0.000363  min_lr: 0.000000  loss: 1.7373 (1.8070)  class_acc: 0.8125 (0.7871)  loss_scale: 65536.0000 (54613.3333)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0003  max mem: 27806
Epoch: [9]  [2400/2502]  eta: 0:00:57  lr: 0.000362  min_lr: 0.000000  loss: 1.7520 (1.8076)  class_acc: 0.7969 (0.7869)  loss_scale: 65536.0000 (55068.2549)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0003  max mem: 27806
Epoch: [9]  [2500/2502]  eta: 0:00:01  lr: 0.000362  min_lr: 0.000000  loss: 1.7207 (1.8085)  class_acc: 0.7969 (0.7867)  loss_scale: 65536.0000 (55482.7776)  weight_decay: 0.0500 (0.0500)  time: 0.5341  data: 0.0006  max mem: 27806
Epoch: [9]  [2501/2502]  eta: 0:00:00  lr: 0.000362  min_lr: 0.000000  loss: 1.7207 (1.8085)  class_acc: 0.7969 (0.7867)  loss_scale: 65536.0000 (55482.7776)  weight_decay: 0.0500 (0.0500)  time: 0.5068  data: 0.0006  max mem: 27806
Epoch: [9] Total time: 0:23:35 (0.5659 s / it)
Averaged stats: lr: 0.000362  min_lr: 0.000000  loss: 1.7207 (1.8085)  class_acc: 0.7969 (0.7865)  loss_scale: 65536.0000 (55482.7776)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Test:  [ 0/98]  eta: 0:02:36  loss: 0.2218 (0.2218)  acc1: 95.3125 (95.3125)  acc5: 100.0000 (100.0000)  time: 1.5958  data: 1.3469  max mem: 27806
Test:  [97/98]  eta: 0:00:00  loss: 0.5809 (0.5717)  acc1: 85.7143 (86.6560)  acc5: 98.4375 (98.1600)  time: 0.2013  data: 0.0001  max mem: 27806
Test: Total time: 0:00:21 (0.2196 s / it)
* Acc@1 86.650 Acc@5 98.150 loss 0.570
Accuracy of the network on the 50000 test images: 86.7%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Test:  [ 0/98]  eta: 0:02:53  loss: 0.8974 (0.8974)  acc1: 93.7500 (93.7500)  acc5: 98.4375 (98.4375)  time: 1.7663  data: 1.5592  max mem: 27806
Test:  [97/98]  eta: 0:00:00  loss: 1.2436 (1.2087)  acc1: 85.9375 (86.4000)  acc5: 98.4375 (98.0800)  time: 0.2044  data: 0.0001  max mem: 27806
Test: Total time: 0:00:21 (0.2239 s / it)
* Acc@1 86.300 Acc@5 98.122 loss 1.209
EMA Accuracy of the network on the 50000 test images: 86.3%
Max accuracy: 86.30%
{"train_lr": 0.00036884335055086665, "train_min_lr": 7.756679990125196e-09, "train_loss": 1.808483447265625, "train_class_acc": 0.78646484375, "train_loss_scale": 55482.7776, "train_weight_decay": 0.04999999999999801, "test_loss": 1.208968785116259, "test_acc1": 86.3000000112915, "test_acc5": 98.12200000610352, "epoch": 9, "n_parameters": 303602664}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Epoch: [10]  [   0/2502]  eta: 8:35:50  lr: 0.000362  min_lr: 0.000000  loss: 1.9658 (1.9658)  class_acc: 0.7500 (0.7500)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 12.3701  data: 11.7773  max mem: 27806
Epoch: [10]  [ 100/2502]  eta: 0:27:07  lr: 0.000361  min_lr: 0.000000  loss: 1.7920 (1.7753)  class_acc: 0.7812 (0.7978)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5617  data: 0.0003  max mem: 27806
Epoch: [10]  [ 200/2502]  eta: 0:23:50  lr: 0.000361  min_lr: 0.000000  loss: 1.7803 (1.7837)  class_acc: 0.8125 (0.7950)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5621  data: 0.0003  max mem: 27806
Epoch: [10]  [ 300/2502]  eta: 0:22:04  lr: 0.000360  min_lr: 0.000000  loss: 1.6533 (1.7839)  class_acc: 0.8125 (0.7960)  loss_scale: 65536.0000 (67713.2757)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0002  max mem: 27806
Epoch: [10]  [ 400/2502]  eta: 0:20:43  lr: 0.000360  min_lr: 0.000000  loss: 1.7061 (1.7834)  class_acc: 0.7969 (0.7957)  loss_scale: 65536.0000 (76649.3367)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0004  max mem: 27806
Epoch: [10]  [ 500/2502]  eta: 0:19:32  lr: 0.000359  min_lr: 0.000000  loss: 1.8164 (1.7882)  class_acc: 0.7656 (0.7936)  loss_scale: 32768.0000 (71422.4671)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0003  max mem: 27806
Epoch: [10]  [ 600/2502]  eta: 0:18:26  lr: 0.000358  min_lr: 0.000000  loss: 1.6846 (1.7844)  class_acc: 0.8281 (0.7944)  loss_scale: 32768.0000 (64990.7754)  weight_decay: 0.0500 (0.0500)  time: 0.5622  data: 0.0003  max mem: 27806
Epoch: [10]  [ 700/2502]  eta: 0:17:22  lr: 0.000358  min_lr: 0.000000  loss: 1.6934 (1.7802)  class_acc: 0.8281 (0.7956)  loss_scale: 32768.0000 (60394.0884)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0002  max mem: 27806
Epoch: [10]  [ 800/2502]  eta: 0:16:21  lr: 0.000357  min_lr: 0.000000  loss: 1.6934 (1.7784)  class_acc: 0.8281 (0.7960)  loss_scale: 32768.0000 (56945.1386)  weight_decay: 0.0500 (0.0500)  time: 0.5616  data: 0.0003  max mem: 27806
Epoch: [10]  [ 900/2502]  eta: 0:15:21  lr: 0.000356  min_lr: 0.000000  loss: 1.8047 (1.7807)  class_acc: 0.7969 (0.7951)  loss_scale: 32768.0000 (54261.7714)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0003  max mem: 27806
Epoch: [10]  [1000/2502]  eta: 0:14:21  lr: 0.000356  min_lr: 0.000000  loss: 1.7031 (1.7805)  class_acc: 0.8125 (0.7950)  loss_scale: 65536.0000 (53096.5994)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0003  max mem: 27806
Epoch: [10]  [1100/2502]  eta: 0:13:22  lr: 0.000355  min_lr: 0.000000  loss: 1.7686 (1.7792)  class_acc: 0.7812 (0.7951)  loss_scale: 65536.0000 (54226.4269)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.0003  max mem: 27806
Epoch: [10]  [1200/2502]  eta: 0:12:24  lr: 0.000355  min_lr: 0.000000  loss: 1.7686 (1.7810)  class_acc: 0.8125 (0.7949)  loss_scale: 65536.0000 (55168.1066)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0003  max mem: 27806
Epoch: [10]  [1300/2502]  eta: 0:11:26  lr: 0.000354  min_lr: 0.000000  loss: 1.7764 (1.7809)  class_acc: 0.7969 (0.7949)  loss_scale: 65536.0000 (55965.0238)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.0003  max mem: 27806
Epoch: [10]  [1400/2502]  eta: 0:10:28  lr: 0.000353  min_lr: 0.000000  loss: 1.7734 (1.7780)  class_acc: 0.7969 (0.7956)  loss_scale: 65536.0000 (56648.1770)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0004  max mem: 27806
Epoch: [10]  [1500/2502]  eta: 0:09:30  lr: 0.000353  min_lr: 0.000000  loss: 1.7607 (1.7775)  class_acc: 0.7812 (0.7954)  loss_scale: 131072.0000 (58026.2119)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.0004  max mem: 27806
Epoch: [10]  [1600/2502]  eta: 0:08:33  lr: 0.000352  min_lr: 0.000000  loss: 1.7354 (1.7755)  class_acc: 0.8125 (0.7961)  loss_scale: 65536.0000 (60214.5259)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.0003  max mem: 27806
Epoch: [10]  [1700/2502]  eta: 0:07:36  lr: 0.000351  min_lr: 0.000000  loss: 1.7168 (1.7750)  class_acc: 0.8125 (0.7963)  loss_scale: 32768.0000 (58716.5573)  weight_decay: 0.0500 (0.0500)  time: 0.5618  data: 0.0003  max mem: 27806
Epoch: [10]  [1800/2502]  eta: 0:06:38  lr: 0.000351  min_lr: 0.000000  loss: 1.7793 (1.7770)  class_acc: 0.7812 (0.7957)  loss_scale: 32768.0000 (57275.7712)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.0003  max mem: 27806
Epoch: [10]  [1900/2502]  eta: 0:05:41  lr: 0.000350  min_lr: 0.000000  loss: 1.7568 (1.7761)  class_acc: 0.7969 (0.7956)  loss_scale: 32768.0000 (55986.5671)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0004  max mem: 27806
Epoch: [10]  [2000/2502]  eta: 0:04:44  lr: 0.000349  min_lr: 0.000000  loss: 1.7637 (1.7773)  class_acc: 0.8125 (0.7953)  loss_scale: 32768.0000 (54826.2189)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0003  max mem: 27806
Epoch: [10]  [2100/2502]  eta: 0:03:48  lr: 0.000349  min_lr: 0.000000  loss: 1.7891 (1.7778)  class_acc: 0.7969 (0.7951)  loss_scale: 32768.0000 (53776.3275)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0003  max mem: 27806
Epoch: [10]  [2200/2502]  eta: 0:02:51  lr: 0.000348  min_lr: 0.000000  loss: 1.8203 (1.7787)  class_acc: 0.7812 (0.7948)  loss_scale: 32768.0000 (53715.1040)  weight_decay: 0.0500 (0.0500)  time: 0.5591  data: 0.0003  max mem: 27806
Epoch: [10]  [2300/2502]  eta: 0:01:54  lr: 0.000347  min_lr: 0.000000  loss: 1.7051 (1.7788)  class_acc: 0.7969 (0.7946)  loss_scale: 32768.0000 (52804.7562)  weight_decay: 0.0500 (0.0500)  time: 0.5618  data: 0.0003  max mem: 27806
Epoch: [10]  [2400/2502]  eta: 0:00:57  lr: 0.000347  min_lr: 0.000000  loss: 1.6885 (1.7784)  class_acc: 0.8281 (0.7946)  loss_scale: 32768.0000 (51970.2391)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0003  max mem: 27806
Epoch: [10]  [2500/2502]  eta: 0:00:01  lr: 0.000346  min_lr: 0.000000  loss: 1.6572 (1.7767)  class_acc: 0.8281 (0.7950)  loss_scale: 32768.0000 (51209.8304)  weight_decay: 0.0500 (0.0500)  time: 0.5338  data: 0.0007  max mem: 27806
Epoch: [10]  [2501/2502]  eta: 0:00:00  lr: 0.000346  min_lr: 0.000000  loss: 1.6572 (1.7767)  class_acc: 0.8281 (0.7950)  loss_scale: 32768.0000 (51209.8304)  weight_decay: 0.0500 (0.0500)  time: 0.5064  data: 0.0007  max mem: 27806
Epoch: [10] Total time: 0:23:36 (0.5662 s / it)
Averaged stats: lr: 0.000346  min_lr: 0.000000  loss: 1.6572 (1.7790)  class_acc: 0.8281 (0.7942)  loss_scale: 32768.0000 (51209.8304)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Test:  [ 0/98]  eta: 0:02:45  loss: 0.2280 (0.2280)  acc1: 95.3125 (95.3125)  acc5: 100.0000 (100.0000)  time: 1.6864  data: 1.4491  max mem: 27806
Test:  [97/98]  eta: 0:00:00  loss: 0.5670 (0.5757)  acc1: 85.9375 (87.1360)  acc5: 98.4375 (98.1440)  time: 0.2019  data: 0.0001  max mem: 27806
Test: Total time: 0:00:21 (0.2211 s / it)
* Acc@1 86.778 Acc@5 98.174 loss 0.578
Accuracy of the network on the 50000 test images: 86.8%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Test:  [ 0/98]  eta: 0:02:42  loss: 0.6343 (0.6343)  acc1: 93.7500 (93.7500)  acc5: 98.4375 (98.4375)  time: 1.6549  data: 1.4481  max mem: 27806
Test:  [97/98]  eta: 0:00:00  loss: 0.9701 (0.9316)  acc1: 85.9375 (86.7360)  acc5: 98.4375 (98.2400)  time: 0.2035  data: 0.0001  max mem: 27806
Test: Total time: 0:00:21 (0.2226 s / it)
* Acc@1 86.714 Acc@5 98.234 loss 0.932
EMA Accuracy of the network on the 50000 test images: 86.7%
Max accuracy: 86.71%
{"train_lr": 0.00035412904510851714, "train_min_lr": 7.447241963323829e-09, "train_loss": 1.77899345703125, "train_class_acc": 0.79415703125, "train_loss_scale": 51209.8304, "train_weight_decay": 0.04999999999999801, "test_loss": 0.9322150160311437, "test_acc1": 86.7140000125122, "test_acc5": 98.23400000488282, "epoch": 10, "n_parameters": 303602664}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Epoch: [11]  [   0/2502]  eta: 8:08:28  lr: 0.000346  min_lr: 0.000000  loss: 1.8652 (1.8652)  class_acc: 0.7344 (0.7344)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 11.7139  data: 11.1318  max mem: 27806
Epoch: [11]  [ 100/2502]  eta: 0:26:55  lr: 0.000345  min_lr: 0.000000  loss: 1.7754 (1.7548)  class_acc: 0.7656 (0.7983)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0002  max mem: 27806
Epoch: [11]  [ 200/2502]  eta: 0:23:39  lr: 0.000345  min_lr: 0.000000  loss: 1.6855 (1.7547)  class_acc: 0.7969 (0.8019)  loss_scale: 32768.0000 (33094.0498)  weight_decay: 0.0500 (0.0500)  time: 0.5605  data: 0.0003  max mem: 27806
Epoch: [11]  [ 300/2502]  eta: 0:21:57  lr: 0.000344  min_lr: 0.000000  loss: 1.6973 (1.7587)  class_acc: 0.8125 (0.7994)  loss_scale: 65536.0000 (43872.1063)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0003  max mem: 27806
Epoch: [11]  [ 400/2502]  eta: 0:20:38  lr: 0.000343  min_lr: 0.000000  loss: 1.6592 (1.7563)  class_acc: 0.8125 (0.7997)  loss_scale: 65536.0000 (49274.5736)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.0002  max mem: 27806
Epoch: [11]  [ 500/2502]  eta: 0:19:28  lr: 0.000342  min_lr: 0.000000  loss: 1.7529 (1.7475)  class_acc: 0.8125 (0.8021)  loss_scale: 65536.0000 (52520.3673)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0002  max mem: 27806
Epoch: [11]  [ 600/2502]  eta: 0:18:22  lr: 0.000342  min_lr: 0.000000  loss: 1.7451 (1.7522)  class_acc: 0.7969 (0.8014)  loss_scale: 65536.0000 (54686.0300)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0002  max mem: 27806
Epoch: [11]  [ 700/2502]  eta: 0:17:20  lr: 0.000341  min_lr: 0.000000  loss: 1.7930 (1.7559)  class_acc: 0.7812 (0.8007)  loss_scale: 65536.0000 (56233.8146)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0003  max mem: 27806
Epoch: [11]  [ 800/2502]  eta: 0:16:18  lr: 0.000340  min_lr: 0.000000  loss: 1.7588 (1.7566)  class_acc: 0.7969 (0.8004)  loss_scale: 131072.0000 (64758.7316)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.0002  max mem: 27806
Epoch: [11]  [ 900/2502]  eta: 0:15:19  lr: 0.000340  min_lr: 0.000000  loss: 1.7432 (1.7559)  class_acc: 0.7969 (0.8002)  loss_scale: 131072.0000 (72118.6948)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.0003  max mem: 27806
Epoch: [11]  [1000/2502]  eta: 0:14:19  lr: 0.000339  min_lr: 0.000000  loss: 1.7412 (1.7542)  class_acc: 0.7969 (0.8007)  loss_scale: 131072.0000 (78008.1359)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0003  max mem: 27806
Epoch: [11]  [1100/2502]  eta: 0:13:21  lr: 0.000338  min_lr: 0.000000  loss: 1.7998 (1.7549)  class_acc: 0.7969 (0.8006)  loss_scale: 131072.0000 (82827.7421)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0002  max mem: 27806
Epoch: [11]  [1200/2502]  eta: 0:12:22  lr: 0.000337  min_lr: 0.000000  loss: 1.6992 (1.7546)  class_acc: 0.7969 (0.8007)  loss_scale: 32768.0000 (81879.0741)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0003  max mem: 27806
Epoch: [11]  [1300/2502]  eta: 0:11:24  lr: 0.000337  min_lr: 0.000000  loss: 1.6787 (1.7543)  class_acc: 0.7969 (0.8004)  loss_scale: 32768.0000 (78104.2029)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0002  max mem: 27806
Epoch: [11]  [1400/2502]  eta: 0:10:27  lr: 0.000336  min_lr: 0.000000  loss: 1.7012 (1.7535)  class_acc: 0.8125 (0.8007)  loss_scale: 32768.0000 (74868.2141)  weight_decay: 0.0500 (0.0500)  time: 0.5603  data: 0.0003  max mem: 27806
Epoch: [11]  [1500/2502]  eta: 0:09:29  lr: 0.000335  min_lr: 0.000000  loss: 1.7275 (1.7534)  class_acc: 0.8125 (0.8006)  loss_scale: 32768.0000 (72063.4031)  weight_decay: 0.0500 (0.0500)  time: 0.5602  data: 0.0002  max mem: 27806
Epoch: [11]  [1600/2502]  eta: 0:08:32  lr: 0.000334  min_lr: 0.000000  loss: 1.7666 (1.7540)  class_acc: 0.7812 (0.8005)  loss_scale: 32768.0000 (69608.9744)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.0003  max mem: 27806
Epoch: [11]  [1700/2502]  eta: 0:07:35  lr: 0.000334  min_lr: 0.000000  loss: 1.7031 (1.7524)  class_acc: 0.7969 (0.8011)  loss_scale: 65536.0000 (67943.9953)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.0003  max mem: 27806
Epoch: [11]  [1800/2502]  eta: 0:06:38  lr: 0.000333  min_lr: 0.000000  loss: 1.7549 (1.7532)  class_acc: 0.7969 (0.8012)  loss_scale: 65536.0000 (67810.2921)  weight_decay: 0.0500 (0.0500)  time: 0.5605  data: 0.0003  max mem: 27806
Epoch: [11]  [1900/2502]  eta: 0:05:41  lr: 0.000332  min_lr: 0.000000  loss: 1.7549 (1.7523)  class_acc: 0.8125 (0.8012)  loss_scale: 65536.0000 (67690.6554)  weight_decay: 0.0500 (0.0500)  time: 0.5616  data: 0.0003  max mem: 27806
Epoch: [11]  [2000/2502]  eta: 0:04:44  lr: 0.000331  min_lr: 0.000000  loss: 1.6689 (1.7522)  class_acc: 0.8281 (0.8015)  loss_scale: 65536.0000 (67582.9765)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0002  max mem: 27806
Epoch: [11]  [2100/2502]  eta: 0:03:47  lr: 0.000331  min_lr: 0.000000  loss: 1.7207 (1.7521)  class_acc: 0.7969 (0.8017)  loss_scale: 65536.0000 (67485.5478)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0002  max mem: 27806
Epoch: [11]  [2200/2502]  eta: 0:02:51  lr: 0.000330  min_lr: 0.000000  loss: 1.7197 (1.7530)  class_acc: 0.7969 (0.8016)  loss_scale: 131072.0000 (67813.8301)  weight_decay: 0.0500 (0.0500)  time: 0.5617  data: 0.0002  max mem: 27806
Epoch: [11]  [2300/2502]  eta: 0:01:54  lr: 0.000329  min_lr: 0.000000  loss: 1.7168 (1.7524)  class_acc: 0.7969 (0.8016)  loss_scale: 65536.0000 (67885.7262)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.0002  max mem: 27806
Epoch: [11]  [2400/2502]  eta: 0:00:57  lr: 0.000328  min_lr: 0.000000  loss: 1.7266 (1.7526)  class_acc: 0.7812 (0.8015)  loss_scale: 65536.0000 (67787.8617)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0002  max mem: 27806
Epoch: [11]  [2500/2502]  eta: 0:00:01  lr: 0.000328  min_lr: 0.000000  loss: 1.7129 (1.7522)  class_acc: 0.7969 (0.8017)  loss_scale: 65536.0000 (67580.7232)  weight_decay: 0.0500 (0.0500)  time: 0.5315  data: 0.0008  max mem: 27806
Epoch: [11]  [2501/2502]  eta: 0:00:00  lr: 0.000328  min_lr: 0.000000  loss: 1.7129 (1.7522)  class_acc: 0.7969 (0.8017)  loss_scale: 65536.0000 (67580.7232)  weight_decay: 0.0500 (0.0500)  time: 0.5042  data: 0.0008  max mem: 27806
Epoch: [11] Total time: 0:23:35 (0.5657 s / it)
Averaged stats: lr: 0.000328  min_lr: 0.000000  loss: 1.7129 (1.7526)  class_acc: 0.7969 (0.8013)  loss_scale: 65536.0000 (67580.7232)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Test:  [ 0/98]  eta: 0:02:46  loss: 0.2108 (0.2108)  acc1: 96.8750 (96.8750)  acc5: 100.0000 (100.0000)  time: 1.7019  data: 1.4702  max mem: 27806
Test:  [97/98]  eta: 0:00:00  loss: 0.5743 (0.5729)  acc1: 85.9375 (87.2960)  acc5: 98.4375 (98.1600)  time: 0.2015  data: 0.0001  max mem: 27806
Test: Total time: 0:00:21 (0.2210 s / it)
* Acc@1 86.872 Acc@5 98.154 loss 0.575
Accuracy of the network on the 50000 test images: 86.9%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Test:  [ 0/98]  eta: 0:02:40  loss: 0.4723 (0.4723)  acc1: 93.7500 (93.7500)  acc5: 100.0000 (100.0000)  time: 1.6404  data: 1.4334  max mem: 27806
Test:  [97/98]  eta: 0:00:00  loss: 0.7931 (0.7640)  acc1: 87.5000 (86.9920)  acc5: 98.4375 (98.3200)  time: 0.2035  data: 0.0001  max mem: 27806
Test: Total time: 0:00:21 (0.2223 s / it)
* Acc@1 87.018 Acc@5 98.330 loss 0.764
EMA Accuracy of the network on the 50000 test images: 87.0%
Max accuracy: 87.02%
{"train_lr": 0.00033699191785126377, "train_min_lr": 7.0868526221955845e-09, "train_loss": 1.752630029296875, "train_class_acc": 0.8013359375, "train_loss_scale": 67580.7232, "train_weight_decay": 0.04999999999999801, "test_loss": 0.7643677239135211, "test_acc1": 87.0180000125122, "test_acc5": 98.33000000488282, "epoch": 11, "n_parameters": 303602664}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Epoch: [12]  [   0/2502]  eta: 8:07:04  lr: 0.000328  min_lr: 0.000000  loss: 1.6826 (1.6826)  class_acc: 0.8594 (0.8594)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 11.6803  data: 11.1077  max mem: 27806
Epoch: [12]  [ 100/2502]  eta: 0:26:50  lr: 0.000327  min_lr: 0.000000  loss: 1.6367 (1.6796)  class_acc: 0.8125 (0.8192)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5603  data: 0.0002  max mem: 27806
Epoch: [12]  [ 200/2502]  eta: 0:23:37  lr: 0.000326  min_lr: 0.000000  loss: 1.7588 (1.7063)  class_acc: 0.7969 (0.8109)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0002  max mem: 27806
Epoch: [12]  [ 300/2502]  eta: 0:21:56  lr: 0.000325  min_lr: 0.000000  loss: 1.7471 (1.7172)  class_acc: 0.7812 (0.8093)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0003  max mem: 27806
Epoch: [12]  [ 400/2502]  eta: 0:20:37  lr: 0.000325  min_lr: 0.000000  loss: 1.7285 (1.7227)  class_acc: 0.7969 (0.8081)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0002  max mem: 27806
Epoch: [12]  [ 500/2502]  eta: 0:19:27  lr: 0.000324  min_lr: 0.000000  loss: 1.6543 (1.7191)  class_acc: 0.8281 (0.8093)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.0003  max mem: 27806
Epoch: [12]  [ 600/2502]  eta: 0:18:22  lr: 0.000323  min_lr: 0.000000  loss: 1.7666 (1.7235)  class_acc: 0.7969 (0.8083)  loss_scale: 65536.0000 (37893.1115)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.0003  max mem: 27806
Epoch: [12]  [ 700/2502]  eta: 0:17:19  lr: 0.000322  min_lr: 0.000000  loss: 1.7178 (1.7225)  class_acc: 0.7969 (0.8079)  loss_scale: 65536.0000 (41836.4622)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.0002  max mem: 27806
Epoch: [12]  [ 800/2502]  eta: 0:16:18  lr: 0.000321  min_lr: 0.000000  loss: 1.7510 (1.7198)  class_acc: 0.7812 (0.8087)  loss_scale: 65536.0000 (44795.2060)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0002  max mem: 27806
Epoch: [12]  [ 900/2502]  eta: 0:15:18  lr: 0.000321  min_lr: 0.000000  loss: 1.7705 (1.7231)  class_acc: 0.7969 (0.8082)  loss_scale: 65536.0000 (47097.1809)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0002  max mem: 27806
Epoch: [12]  [1000/2502]  eta: 0:14:19  lr: 0.000320  min_lr: 0.000000  loss: 1.6924 (1.7255)  class_acc: 0.8281 (0.8079)  loss_scale: 65536.0000 (48939.2208)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0002  max mem: 27806
Epoch: [12]  [1100/2502]  eta: 0:13:20  lr: 0.000319  min_lr: 0.000000  loss: 1.7227 (1.7252)  class_acc: 0.7969 (0.8080)  loss_scale: 131072.0000 (55327.6222)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0002  max mem: 27806
Epoch: [12]  [1200/2502]  eta: 0:12:22  lr: 0.000318  min_lr: 0.000000  loss: 1.7676 (1.7235)  class_acc: 0.7812 (0.8079)  loss_scale: 65536.0000 (60870.4480)  weight_decay: 0.0500 (0.0500)  time: 0.5597  data: 0.0002  max mem: 27806
Epoch: [12]  [1300/2502]  eta: 0:11:24  lr: 0.000317  min_lr: 0.000000  loss: 1.6865 (1.7223)  class_acc: 0.8125 (0.8084)  loss_scale: 65536.0000 (61229.0607)  weight_decay: 0.0500 (0.0500)  time: 0.5621  data: 0.0002  max mem: 27806
Epoch: [12]  [1400/2502]  eta: 0:10:27  lr: 0.000317  min_lr: 0.000000  loss: 1.6855 (1.7223)  class_acc: 0.8125 (0.8088)  loss_scale: 65536.0000 (61536.4797)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.0003  max mem: 27806
Epoch: [12]  [1500/2502]  eta: 0:09:29  lr: 0.000316  min_lr: 0.000000  loss: 1.7451 (1.7208)  class_acc: 0.7812 (0.8091)  loss_scale: 32768.0000 (61148.0133)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.0003  max mem: 27806
Epoch: [12]  [1600/2502]  eta: 0:08:32  lr: 0.000315  min_lr: 0.000000  loss: 1.7461 (1.7215)  class_acc: 0.7812 (0.8090)  loss_scale: 32768.0000 (59375.3704)  weight_decay: 0.0500 (0.0500)  time: 0.5605  data: 0.0002  max mem: 27806
Epoch: [12]  [1700/2502]  eta: 0:07:35  lr: 0.000314  min_lr: 0.000000  loss: 1.6992 (1.7233)  class_acc: 0.7969 (0.8086)  loss_scale: 32768.0000 (57811.1511)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0002  max mem: 27806
Epoch: [12]  [1800/2502]  eta: 0:06:38  lr: 0.000313  min_lr: 0.000000  loss: 1.6895 (1.7241)  class_acc: 0.7969 (0.8082)  loss_scale: 32768.0000 (56420.6374)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.0002  max mem: 27806
Epoch: [12]  [1900/2502]  eta: 0:05:41  lr: 0.000312  min_lr: 0.000000  loss: 1.7021 (1.7256)  class_acc: 0.8125 (0.8080)  loss_scale: 32768.0000 (55176.4166)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0002  max mem: 27806
Epoch: [12]  [2000/2502]  eta: 0:04:44  lr: 0.000312  min_lr: 0.000000  loss: 1.7549 (1.7264)  class_acc: 0.7969 (0.8076)  loss_scale: 65536.0000 (54285.8171)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0002  max mem: 27806
Epoch: [12]  [2100/2502]  eta: 0:03:47  lr: 0.000311  min_lr: 0.000000  loss: 1.7402 (1.7279)  class_acc: 0.7969 (0.8072)  loss_scale: 65536.0000 (54821.2851)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0002  max mem: 27806
Epoch: [12]  [2200/2502]  eta: 0:02:51  lr: 0.000310  min_lr: 0.000000  loss: 1.7432 (1.7298)  class_acc: 0.7812 (0.8068)  loss_scale: 65536.0000 (55308.0963)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0002  max mem: 27806
Epoch: [12]  [2300/2502]  eta: 0:01:54  lr: 0.000309  min_lr: 0.000000  loss: 1.6533 (1.7290)  class_acc: 0.8281 (0.8072)  loss_scale: 65536.0000 (55752.5945)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0002  max mem: 27806
Epoch: [12]  [2400/2502]  eta: 0:00:57  lr: 0.000308  min_lr: 0.000000  loss: 1.6455 (1.7267)  class_acc: 0.7969 (0.8075)  loss_scale: 65536.0000 (56160.0666)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0002  max mem: 27806
Epoch: [12]  [2500/2502]  eta: 0:00:01  lr: 0.000307  min_lr: 0.000000  loss: 1.6494 (1.7265)  class_acc: 0.7969 (0.8075)  loss_scale: 65536.0000 (56557.5680)  weight_decay: 0.0500 (0.0500)  time: 0.5336  data: 0.0006  max mem: 27806
Epoch: [12]  [2501/2502]  eta: 0:00:00  lr: 0.000307  min_lr: 0.000000  loss: 1.6494 (1.7265)  class_acc: 0.7969 (0.8075)  loss_scale: 65536.0000 (56557.5680)  weight_decay: 0.0500 (0.0500)  time: 0.5063  data: 0.0006  max mem: 27806
Epoch: [12] Total time: 0:23:35 (0.5656 s / it)
Averaged stats: lr: 0.000307  min_lr: 0.000000  loss: 1.6494 (1.7284)  class_acc: 0.7969 (0.8074)  loss_scale: 65536.0000 (56557.5680)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Test:  [ 0/98]  eta: 0:02:48  loss: 0.2189 (0.2189)  acc1: 96.8750 (96.8750)  acc5: 100.0000 (100.0000)  time: 1.7182  data: 1.4713  max mem: 27806
Test:  [97/98]  eta: 0:00:00  loss: 0.6169 (0.5738)  acc1: 84.3750 (87.4080)  acc5: 98.4375 (98.1920)  time: 0.2022  data: 0.0001  max mem: 27806
Test: Total time: 0:00:21 (0.2211 s / it)
* Acc@1 86.850 Acc@5 98.190 loss 0.574
Accuracy of the network on the 50000 test images: 86.8%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Test:  [ 0/98]  eta: 0:02:44  loss: 0.3748 (0.3748)  acc1: 93.7500 (93.7500)  acc5: 100.0000 (100.0000)  time: 1.6835  data: 1.4766  max mem: 27806
Test:  [97/98]  eta: 0:00:00  loss: 0.6717 (0.6630)  acc1: 87.5000 (87.2160)  acc5: 98.4375 (98.3200)  time: 0.2032  data: 0.0001  max mem: 27806
Test: Total time: 0:00:21 (0.2224 s / it)
* Acc@1 87.292 Acc@5 98.364 loss 0.663
EMA Accuracy of the network on the 50000 test images: 87.3%
Max accuracy: 87.29%
{"train_lr": 0.00031770223151317707, "train_min_lr": 6.681195521936177e-09, "train_loss": 1.728393017578125, "train_class_acc": 0.80739765625, "train_loss_scale": 56557.568, "train_weight_decay": 0.04999999999999801, "test_loss": 0.6632096089650782, "test_acc1": 87.2920000125122, "test_acc5": 98.36400000488281, "epoch": 12, "n_parameters": 303602664}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Epoch: [13]  [   0/2502]  eta: 8:01:09  lr: 0.000307  min_lr: 0.000000  loss: 1.5693 (1.5693)  class_acc: 0.7969 (0.7969)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 11.5387  data: 10.9510  max mem: 27806
Epoch: [13]  [ 100/2502]  eta: 0:26:53  lr: 0.000307  min_lr: 0.000000  loss: 1.6689 (1.7326)  class_acc: 0.8281 (0.8096)  loss_scale: 65536.0000 (116796.8317)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0002  max mem: 27806
Epoch: [13]  [ 200/2502]  eta: 0:23:39  lr: 0.000306  min_lr: 0.000000  loss: 1.6348 (1.7090)  class_acc: 0.8125 (0.8150)  loss_scale: 65536.0000 (91293.9303)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0004  max mem: 27806
Epoch: [13]  [ 300/2502]  eta: 0:21:57  lr: 0.000305  min_lr: 0.000000  loss: 1.6914 (1.6957)  class_acc: 0.8125 (0.8185)  loss_scale: 65536.0000 (82736.4784)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0003  max mem: 27806
Epoch: [13]  [ 400/2502]  eta: 0:20:37  lr: 0.000304  min_lr: 0.000000  loss: 1.7695 (1.7065)  class_acc: 0.8125 (0.8148)  loss_scale: 65536.0000 (78283.6509)  weight_decay: 0.0500 (0.0500)  time: 0.5598  data: 0.0003  max mem: 27806
Epoch: [13]  [ 500/2502]  eta: 0:19:27  lr: 0.000303  min_lr: 0.000000  loss: 1.6465 (1.7008)  class_acc: 0.8125 (0.8161)  loss_scale: 32768.0000 (69198.6906)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.0003  max mem: 27806
Epoch: [13]  [ 600/2502]  eta: 0:18:22  lr: 0.000302  min_lr: 0.000000  loss: 1.8145 (1.7024)  class_acc: 0.7812 (0.8151)  loss_scale: 32768.0000 (63137.0116)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.0003  max mem: 27806
Epoch: [13]  [ 700/2502]  eta: 0:17:20  lr: 0.000301  min_lr: 0.000000  loss: 1.7148 (1.7036)  class_acc: 0.8125 (0.8149)  loss_scale: 32768.0000 (58804.7703)  weight_decay: 0.0500 (0.0500)  time: 0.5691  data: 0.0003  max mem: 27806
Epoch: [13]  [ 800/2502]  eta: 0:16:19  lr: 0.000301  min_lr: 0.000000  loss: 1.6807 (1.7010)  class_acc: 0.7969 (0.8152)  loss_scale: 32768.0000 (55554.2372)  weight_decay: 0.0500 (0.0500)  time: 0.5679  data: 0.0002  max mem: 27806
Epoch: [13]  [ 900/2502]  eta: 0:15:19  lr: 0.000300  min_lr: 0.000000  loss: 1.5889 (1.7003)  class_acc: 0.8125 (0.8157)  loss_scale: 32768.0000 (53025.2431)  weight_decay: 0.0500 (0.0500)  time: 0.5619  data: 0.0002  max mem: 27806
Epoch: [13]  [1000/2502]  eta: 0:14:20  lr: 0.000299  min_lr: 0.000000  loss: 1.6816 (1.7022)  class_acc: 0.8281 (0.8150)  loss_scale: 65536.0000 (53816.7752)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.0003  max mem: 27806
Epoch: [13]  [1100/2502]  eta: 0:13:21  lr: 0.000298  min_lr: 0.000000  loss: 1.7588 (1.7028)  class_acc: 0.7969 (0.8151)  loss_scale: 65536.0000 (54881.1916)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0003  max mem: 27806
Epoch: [13]  [1200/2502]  eta: 0:12:23  lr: 0.000297  min_lr: 0.000000  loss: 1.7256 (1.7050)  class_acc: 0.7969 (0.8145)  loss_scale: 65536.0000 (55768.3530)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0002  max mem: 27806
Epoch: [13]  [1300/2502]  eta: 0:11:25  lr: 0.000296  min_lr: 0.000000  loss: 1.6699 (1.7044)  class_acc: 0.8125 (0.8148)  loss_scale: 65536.0000 (56519.1330)  weight_decay: 0.0500 (0.0500)  time: 0.5813  data: 0.0003  max mem: 27806
Epoch: [13]  [1400/2502]  eta: 0:10:28  lr: 0.000295  min_lr: 0.000000  loss: 1.7109 (1.7042)  class_acc: 0.8125 (0.8148)  loss_scale: 65536.0000 (57162.7352)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0002  max mem: 27806
Epoch: [13]  [1500/2502]  eta: 0:09:30  lr: 0.000294  min_lr: 0.000000  loss: 1.6895 (1.7050)  class_acc: 0.8125 (0.8142)  loss_scale: 65536.0000 (59292.3971)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0003  max mem: 27806
Epoch: [13]  [1600/2502]  eta: 0:08:33  lr: 0.000294  min_lr: 0.000000  loss: 1.6895 (1.7043)  class_acc: 0.8125 (0.8143)  loss_scale: 65536.0000 (59682.3785)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0002  max mem: 27806
Epoch: [13]  [1700/2502]  eta: 0:07:35  lr: 0.000293  min_lr: 0.000000  loss: 1.7031 (1.7052)  class_acc: 0.8125 (0.8141)  loss_scale: 65536.0000 (60026.5068)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.0002  max mem: 27806
Epoch: [13]  [1800/2502]  eta: 0:06:38  lr: 0.000292  min_lr: 0.000000  loss: 1.6631 (1.7058)  class_acc: 0.8281 (0.8142)  loss_scale: 32768.0000 (58549.3748)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.0003  max mem: 27806
Epoch: [13]  [1900/2502]  eta: 0:05:41  lr: 0.000291  min_lr: 0.000000  loss: 1.7734 (1.7047)  class_acc: 0.7969 (0.8144)  loss_scale: 32768.0000 (57193.1741)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0003  max mem: 27806
Epoch: [13]  [2000/2502]  eta: 0:04:44  lr: 0.000290  min_lr: 0.000000  loss: 1.6973 (1.7061)  class_acc: 0.8125 (0.8143)  loss_scale: 32768.0000 (55972.5257)  weight_decay: 0.0500 (0.0500)  time: 0.5605  data: 0.0003  max mem: 27806
Epoch: [13]  [2100/2502]  eta: 0:03:47  lr: 0.000289  min_lr: 0.000000  loss: 1.6982 (1.7082)  class_acc: 0.8125 (0.8136)  loss_scale: 32768.0000 (54868.0743)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0004  max mem: 27806
Epoch: [13]  [2200/2502]  eta: 0:02:51  lr: 0.000288  min_lr: 0.000000  loss: 1.6846 (1.7087)  class_acc: 0.8125 (0.8135)  loss_scale: 32768.0000 (53863.9818)  weight_decay: 0.0500 (0.0500)  time: 0.5604  data: 0.0003  max mem: 27806
Epoch: [13]  [2300/2502]  eta: 0:01:54  lr: 0.000287  min_lr: 0.000000  loss: 1.7686 (1.7091)  class_acc: 0.7969 (0.8135)  loss_scale: 65536.0000 (54114.9066)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0002  max mem: 27806
Epoch: [13]  [2400/2502]  eta: 0:00:57  lr: 0.000286  min_lr: 0.000000  loss: 1.6797 (1.7084)  class_acc: 0.8125 (0.8137)  loss_scale: 65536.0000 (54590.5873)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0003  max mem: 27806
Epoch: [13]  [2500/2502]  eta: 0:00:01  lr: 0.000285  min_lr: 0.000000  loss: 1.6533 (1.7086)  class_acc: 0.8281 (0.8138)  loss_scale: 32768.0000 (53752.6272)  weight_decay: 0.0500 (0.0500)  time: 0.5333  data: 0.0006  max mem: 27806
Epoch: [13]  [2501/2502]  eta: 0:00:00  lr: 0.000285  min_lr: 0.000000  loss: 1.6533 (1.7086)  class_acc: 0.8281 (0.8138)  loss_scale: 32768.0000 (53752.6272)  weight_decay: 0.0500 (0.0500)  time: 0.5059  data: 0.0006  max mem: 27806
Epoch: [13] Total time: 0:23:35 (0.5658 s / it)
Averaged stats: lr: 0.000285  min_lr: 0.000000  loss: 1.6533 (1.7052)  class_acc: 0.8281 (0.8142)  loss_scale: 32768.0000 (53752.6272)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Test:  [ 0/98]  eta: 0:02:49  loss: 0.1886 (0.1886)  acc1: 95.3125 (95.3125)  acc5: 100.0000 (100.0000)  time: 1.7303  data: 1.5248  max mem: 27806
Test:  [97/98]  eta: 0:00:00  loss: 0.6208 (0.5687)  acc1: 85.9375 (87.3920)  acc5: 98.4375 (98.0800)  time: 0.2016  data: 0.0001  max mem: 27806
Test: Total time: 0:00:21 (0.2211 s / it)
* Acc@1 87.040 Acc@5 98.132 loss 0.566
Accuracy of the network on the 50000 test images: 87.0%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Test:  [ 0/98]  eta: 0:02:49  loss: 0.3131 (0.3131)  acc1: 93.7500 (93.7500)  acc5: 100.0000 (100.0000)  time: 1.7341  data: 1.5271  max mem: 27806
Test:  [97/98]  eta: 0:00:00  loss: 0.5845 (0.6019)  acc1: 87.5000 (87.5680)  acc5: 98.4375 (98.4000)  time: 0.2034  data: 0.0001  max mem: 27806
Test: Total time: 0:00:21 (0.2230 s / it)
* Acc@1 87.462 Acc@5 98.408 loss 0.602
EMA Accuracy of the network on the 50000 test images: 87.5%
Max accuracy: 87.46%
{"train_lr": 0.0002965641959709077, "train_min_lr": 6.236668117344555e-09, "train_loss": 1.705159814453125, "train_class_acc": 0.81416015625, "train_loss_scale": 53752.6272, "train_weight_decay": 0.04999999999999801, "test_loss": 0.6017582678825272, "test_acc1": 87.46200001129151, "test_acc5": 98.40800000488281, "epoch": 13, "n_parameters": 303602664}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Epoch: [14]  [   0/2502]  eta: 8:52:09  lr: 0.000285  min_lr: 0.000000  loss: 2.0566 (2.0566)  class_acc: 0.7812 (0.7812)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 12.7615  data: 12.1797  max mem: 27806
Epoch: [14]  [ 100/2502]  eta: 0:27:15  lr: 0.000285  min_lr: 0.000000  loss: 1.6318 (1.6828)  class_acc: 0.8281 (0.8247)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5605  data: 0.0003  max mem: 27806
Epoch: [14]  [ 200/2502]  eta: 0:23:49  lr: 0.000284  min_lr: 0.000000  loss: 1.5967 (1.6754)  class_acc: 0.8281 (0.8253)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0002  max mem: 27806
Epoch: [14]  [ 300/2502]  eta: 0:22:03  lr: 0.000283  min_lr: 0.000000  loss: 1.6982 (1.6751)  class_acc: 0.8125 (0.8265)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0002  max mem: 27806
Epoch: [14]  [ 400/2502]  eta: 0:20:43  lr: 0.000282  min_lr: 0.000000  loss: 1.6064 (1.6740)  class_acc: 0.8281 (0.8265)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.0002  max mem: 27806
Epoch: [14]  [ 500/2502]  eta: 0:19:31  lr: 0.000281  min_lr: 0.000000  loss: 1.7432 (1.6802)  class_acc: 0.8125 (0.8243)  loss_scale: 65536.0000 (38131.2255)  weight_decay: 0.0500 (0.0500)  time: 0.5619  data: 0.0003  max mem: 27806
Epoch: [14]  [ 600/2502]  eta: 0:18:25  lr: 0.000280  min_lr: 0.000000  loss: 1.6562 (1.6788)  class_acc: 0.8281 (0.8239)  loss_scale: 65536.0000 (42691.0882)  weight_decay: 0.0500 (0.0500)  time: 0.5618  data: 0.0002  max mem: 27806
Epoch: [14]  [ 700/2502]  eta: 0:17:23  lr: 0.000279  min_lr: 0.000000  loss: 1.6367 (1.6800)  class_acc: 0.8281 (0.8228)  loss_scale: 65536.0000 (45949.9914)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0002  max mem: 27806
Epoch: [14]  [ 800/2502]  eta: 0:16:21  lr: 0.000278  min_lr: 0.000000  loss: 1.6523 (1.6820)  class_acc: 0.7969 (0.8223)  loss_scale: 65536.0000 (48395.1860)  weight_decay: 0.0500 (0.0500)  time: 0.5607  data: 0.0002  max mem: 27806
Epoch: [14]  [ 900/2502]  eta: 0:15:21  lr: 0.000277  min_lr: 0.000000  loss: 1.5879 (1.6829)  class_acc: 0.8438 (0.8221)  loss_scale: 65536.0000 (50297.6071)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.0003  max mem: 27806
Epoch: [14]  [1000/2502]  eta: 0:14:21  lr: 0.000276  min_lr: 0.000000  loss: 1.5449 (1.6833)  class_acc: 0.8281 (0.8222)  loss_scale: 131072.0000 (56402.8611)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0002  max mem: 27806
Epoch: [14]  [1100/2502]  eta: 0:13:22  lr: 0.000275  min_lr: 0.000000  loss: 1.6729 (1.6826)  class_acc: 0.8281 (0.8219)  loss_scale: 32768.0000 (55268.0981)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.0003  max mem: 27806
Epoch: [14]  [1200/2502]  eta: 0:12:24  lr: 0.000274  min_lr: 0.000000  loss: 1.7100 (1.6841)  class_acc: 0.8125 (0.8213)  loss_scale: 32768.0000 (53394.6511)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.0003  max mem: 27806
Epoch: [14]  [1300/2502]  eta: 0:11:26  lr: 0.000273  min_lr: 0.000000  loss: 1.6924 (1.6851)  class_acc: 0.8125 (0.8212)  loss_scale: 32768.0000 (51809.2052)  weight_decay: 0.0500 (0.0500)  time: 0.5607  data: 0.0002  max mem: 27806
Epoch: [14]  [1400/2502]  eta: 0:10:28  lr: 0.000273  min_lr: 0.000000  loss: 1.6875 (1.6832)  class_acc: 0.8125 (0.8215)  loss_scale: 32768.0000 (50450.0899)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0002  max mem: 27806
Epoch: [14]  [1500/2502]  eta: 0:09:30  lr: 0.000272  min_lr: 0.000000  loss: 1.6426 (1.6841)  class_acc: 0.8125 (0.8212)  loss_scale: 32768.0000 (49272.0693)  weight_decay: 0.0500 (0.0500)  time: 0.5616  data: 0.0002  max mem: 27806
Epoch: [14]  [1600/2502]  eta: 0:08:33  lr: 0.000271  min_lr: 0.000000  loss: 1.6299 (1.6839)  class_acc: 0.8125 (0.8209)  loss_scale: 65536.0000 (49346.4385)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0002  max mem: 27806
Epoch: [14]  [1700/2502]  eta: 0:07:35  lr: 0.000270  min_lr: 0.000000  loss: 1.7002 (1.6835)  class_acc: 0.8125 (0.8206)  loss_scale: 65536.0000 (50298.2058)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0002  max mem: 27806
Epoch: [14]  [1800/2502]  eta: 0:06:38  lr: 0.000269  min_lr: 0.000000  loss: 1.7275 (1.6834)  class_acc: 0.7969 (0.8204)  loss_scale: 65536.0000 (51144.2798)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.0002  max mem: 27806
Epoch: [14]  [1900/2502]  eta: 0:05:41  lr: 0.000268  min_lr: 0.000000  loss: 1.5908 (1.6840)  class_acc: 0.8281 (0.8200)  loss_scale: 65536.0000 (51901.3403)  weight_decay: 0.0500 (0.0500)  time: 0.5607  data: 0.0002  max mem: 27806
Epoch: [14]  [2000/2502]  eta: 0:04:44  lr: 0.000267  min_lr: 0.000000  loss: 1.6396 (1.6832)  class_acc: 0.8438 (0.8201)  loss_scale: 65536.0000 (52582.7326)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0002  max mem: 27806
Epoch: [14]  [2100/2502]  eta: 0:03:47  lr: 0.000266  min_lr: 0.000000  loss: 1.6924 (1.6838)  class_acc: 0.8125 (0.8199)  loss_scale: 32768.0000 (52169.9000)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0002  max mem: 27806
Epoch: [14]  [2200/2502]  eta: 0:02:51  lr: 0.000265  min_lr: 0.000000  loss: 1.6367 (1.6842)  class_acc: 0.8125 (0.8198)  loss_scale: 32768.0000 (51288.3962)  weight_decay: 0.0500 (0.0500)  time: 0.5682  data: 0.0003  max mem: 27806
Epoch: [14]  [2300/2502]  eta: 0:01:54  lr: 0.000264  min_lr: 0.000000  loss: 1.6729 (1.6838)  class_acc: 0.8125 (0.8199)  loss_scale: 32768.0000 (50483.5115)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.0002  max mem: 27806
Epoch: [14]  [2400/2502]  eta: 0:00:57  lr: 0.000263  min_lr: 0.000000  loss: 1.6123 (1.6837)  class_acc: 0.8281 (0.8200)  loss_scale: 32768.0000 (49745.6726)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.0002  max mem: 27806
Epoch: [14]  [2500/2502]  eta: 0:00:01  lr: 0.000262  min_lr: 0.000000  loss: 1.6504 (1.6842)  class_acc: 0.8281 (0.8199)  loss_scale: 32768.0000 (49073.3568)  weight_decay: 0.0500 (0.0500)  time: 0.5333  data: 0.0006  max mem: 27806
Epoch: [14]  [2501/2502]  eta: 0:00:00  lr: 0.000262  min_lr: 0.000000  loss: 1.6504 (1.6842)  class_acc: 0.8281 (0.8199)  loss_scale: 32768.0000 (49073.3568)  weight_decay: 0.0500 (0.0500)  time: 0.5060  data: 0.0006  max mem: 27806
Epoch: [14] Total time: 0:23:36 (0.5661 s / it)
Averaged stats: lr: 0.000262  min_lr: 0.000000  loss: 1.6504 (1.6820)  class_acc: 0.8281 (0.8204)  loss_scale: 32768.0000 (49073.3568)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Test:  [ 0/98]  eta: 0:04:04  loss: 0.2057 (0.2057)  acc1: 96.8750 (96.8750)  acc5: 100.0000 (100.0000)  time: 2.4940  data: 2.2474  max mem: 27806
Test:  [97/98]  eta: 0:00:00  loss: 0.6031 (0.5903)  acc1: 85.9375 (87.1040)  acc5: 96.8750 (97.7280)  time: 0.2016  data: 0.0002  max mem: 27806
Test: Total time: 0:00:22 (0.2293 s / it)
* Acc@1 86.950 Acc@5 97.984 loss 0.588
Accuracy of the network on the 50000 test images: 87.0%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Test:  [ 0/98]  eta: 0:03:10  loss: 0.2723 (0.2723)  acc1: 93.7500 (93.7500)  acc5: 100.0000 (100.0000)  time: 1.9488  data: 1.7052  max mem: 27806
Test:  [97/98]  eta: 0:00:00  loss: 0.5724 (0.5649)  acc1: 87.5000 (87.7120)  acc5: 98.4375 (98.3520)  time: 0.2050  data: 0.0002  max mem: 27806
Test: Total time: 0:00:22 (0.2265 s / it)
* Acc@1 87.628 Acc@5 98.418 loss 0.564
EMA Accuracy of the network on the 50000 test images: 87.6%
Max accuracy: 87.63%
{"train_lr": 0.0002739111706722099, "train_min_lr": 5.760280871138786e-09, "train_loss": 1.68196875, "train_class_acc": 0.8204375, "train_loss_scale": 49073.3568, "train_weight_decay": 0.04999999999999801, "test_loss": 0.5640400053211012, "test_acc1": 87.6280000125122, "test_acc5": 98.41800000488281, "epoch": 14, "n_parameters": 303602664}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Epoch: [15]  [   0/2502]  eta: 8:41:51  lr: 0.000262  min_lr: 0.000000  loss: 1.5645 (1.5645)  class_acc: 0.8438 (0.8438)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 12.5148  data: 11.9164  max mem: 27806
Epoch: [15]  [ 100/2502]  eta: 0:27:10  lr: 0.000261  min_lr: 0.000000  loss: 1.6475 (1.6698)  class_acc: 0.8281 (0.8255)  loss_scale: 65536.0000 (48989.7822)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.0003  max mem: 27806
Epoch: [15]  [ 200/2502]  eta: 0:23:49  lr: 0.000260  min_lr: 0.000000  loss: 1.6406 (1.6676)  class_acc: 0.8281 (0.8263)  loss_scale: 32768.0000 (53635.1841)  weight_decay: 0.0500 (0.0500)  time: 0.5603  data: 0.0002  max mem: 27806
Epoch: [15]  [ 300/2502]  eta: 0:22:05  lr: 0.000259  min_lr: 0.000000  loss: 1.5938 (1.6636)  class_acc: 0.8281 (0.8271)  loss_scale: 32768.0000 (46702.5648)  weight_decay: 0.0500 (0.0500)  time: 0.5719  data: 0.0002  max mem: 27806
Epoch: [15]  [ 400/2502]  eta: 0:20:45  lr: 0.000258  min_lr: 0.000000  loss: 1.6826 (1.6623)  class_acc: 0.8438 (0.8282)  loss_scale: 32768.0000 (43227.6110)  weight_decay: 0.0500 (0.0500)  time: 0.5622  data: 0.0002  max mem: 27806
Epoch: [15]  [ 500/2502]  eta: 0:19:33  lr: 0.000257  min_lr: 0.000000  loss: 1.5840 (1.6668)  class_acc: 0.8281 (0.8264)  loss_scale: 32768.0000 (41139.8643)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.0003  max mem: 27806
Epoch: [15]  [ 600/2502]  eta: 0:18:27  lr: 0.000256  min_lr: 0.000000  loss: 1.5732 (1.6630)  class_acc: 0.8281 (0.8267)  loss_scale: 32768.0000 (39746.8752)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.0003  max mem: 27806
Epoch: [15]  [ 700/2502]  eta: 0:17:23  lr: 0.000255  min_lr: 0.000000  loss: 1.6572 (1.6623)  class_acc: 0.8125 (0.8264)  loss_scale: 32768.0000 (39031.7832)  weight_decay: 0.0500 (0.0500)  time: 0.5607  data: 0.0002  max mem: 27806
Epoch: [15]  [ 800/2502]  eta: 0:16:21  lr: 0.000254  min_lr: 0.000000  loss: 1.5518 (1.6594)  class_acc: 0.8281 (0.8268)  loss_scale: 32768.0000 (40131.5955)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0002  max mem: 27806
Epoch: [15]  [ 900/2502]  eta: 0:15:21  lr: 0.000254  min_lr: 0.000000  loss: 1.7236 (1.6611)  class_acc: 0.8125 (0.8259)  loss_scale: 32768.0000 (39314.3263)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.0002  max mem: 27806
Epoch: [15]  [1000/2502]  eta: 0:14:21  lr: 0.000253  min_lr: 0.000000  loss: 1.6035 (1.6635)  class_acc: 0.8594 (0.8258)  loss_scale: 32768.0000 (38660.3477)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0002  max mem: 27806
Epoch: [15]  [1100/2502]  eta: 0:13:22  lr: 0.000252  min_lr: 0.000000  loss: 1.6611 (1.6597)  class_acc: 0.8281 (0.8274)  loss_scale: 32768.0000 (38125.1662)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0002  max mem: 27806
Epoch: [15]  [1200/2502]  eta: 0:12:24  lr: 0.000251  min_lr: 0.000000  loss: 1.6338 (1.6609)  class_acc: 0.8281 (0.8269)  loss_scale: 32768.0000 (37679.1074)  weight_decay: 0.0500 (0.0500)  time: 0.5605  data: 0.0002  max mem: 27806
Epoch: [15]  [1300/2502]  eta: 0:11:26  lr: 0.000250  min_lr: 0.000000  loss: 1.6787 (1.6637)  class_acc: 0.8125 (0.8262)  loss_scale: 65536.0000 (38258.7179)  weight_decay: 0.0500 (0.0500)  time: 0.5692  data: 0.0002  max mem: 27806
Epoch: [15]  [1400/2502]  eta: 0:10:28  lr: 0.000249  min_lr: 0.000000  loss: 1.5811 (1.6658)  class_acc: 0.8438 (0.8259)  loss_scale: 65536.0000 (40205.7045)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0003  max mem: 27806
Epoch: [15]  [1500/2502]  eta: 0:09:30  lr: 0.000248  min_lr: 0.000000  loss: 1.6992 (1.6647)  class_acc: 0.8281 (0.8262)  loss_scale: 65536.0000 (41893.2658)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.0002  max mem: 27806
Epoch: [15]  [1600/2502]  eta: 0:08:33  lr: 0.000247  min_lr: 0.000000  loss: 1.6592 (1.6662)  class_acc: 0.8281 (0.8258)  loss_scale: 32768.0000 (42674.1287)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0003  max mem: 27806
Epoch: [15]  [1700/2502]  eta: 0:07:36  lr: 0.000246  min_lr: 0.000000  loss: 1.5508 (1.6659)  class_acc: 0.8594 (0.8257)  loss_scale: 32768.0000 (42091.7578)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0002  max mem: 27806
Epoch: [15]  [1800/2502]  eta: 0:06:38  lr: 0.000245  min_lr: 0.000000  loss: 1.5693 (1.6660)  class_acc: 0.8438 (0.8256)  loss_scale: 32768.0000 (41574.0589)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0002  max mem: 27806
Epoch: [15]  [1900/2502]  eta: 0:05:41  lr: 0.000244  min_lr: 0.000000  loss: 1.6143 (1.6657)  class_acc: 0.8438 (0.8258)  loss_scale: 32768.0000 (41110.8259)  weight_decay: 0.0500 (0.0500)  time: 0.5604  data: 0.0002  max mem: 27806
Epoch: [15]  [2000/2502]  eta: 0:04:44  lr: 0.000243  min_lr: 0.000000  loss: 1.6797 (1.6657)  class_acc: 0.8125 (0.8257)  loss_scale: 32768.0000 (40693.8931)  weight_decay: 0.0500 (0.0500)  time: 0.5616  data: 0.0002  max mem: 27806
Epoch: [15]  [2100/2502]  eta: 0:03:48  lr: 0.000242  min_lr: 0.000000  loss: 1.5850 (1.6649)  class_acc: 0.8281 (0.8261)  loss_scale: 65536.0000 (40597.3841)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.0002  max mem: 27806
Epoch: [15]  [2200/2502]  eta: 0:02:51  lr: 0.000241  min_lr: 0.000000  loss: 1.6338 (1.6641)  class_acc: 0.8281 (0.8261)  loss_scale: 32768.0000 (40688.2980)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0002  max mem: 27806
Epoch: [15]  [2300/2502]  eta: 0:01:54  lr: 0.000240  min_lr: 0.000000  loss: 1.6992 (1.6648)  class_acc: 0.8125 (0.8258)  loss_scale: 32768.0000 (40344.0869)  weight_decay: 0.0500 (0.0500)  time: 0.5607  data: 0.0002  max mem: 27806
Epoch: [15]  [2400/2502]  eta: 0:00:57  lr: 0.000239  min_lr: 0.000000  loss: 1.6006 (1.6643)  class_acc: 0.8438 (0.8259)  loss_scale: 32768.0000 (40028.5481)  weight_decay: 0.0500 (0.0500)  time: 0.5604  data: 0.0003  max mem: 27806
Epoch: [15]  [2500/2502]  eta: 0:00:01  lr: 0.000238  min_lr: 0.000000  loss: 1.6826 (1.6638)  class_acc: 0.8125 (0.8260)  loss_scale: 32768.0000 (39741.0304)  weight_decay: 0.0500 (0.0500)  time: 0.5337  data: 0.0006  max mem: 27806
Epoch: [15]  [2501/2502]  eta: 0:00:00  lr: 0.000238  min_lr: 0.000000  loss: 1.6826 (1.6638)  class_acc: 0.8125 (0.8260)  loss_scale: 32768.0000 (39741.0304)  weight_decay: 0.0500 (0.0500)  time: 0.5063  data: 0.0006  max mem: 27806
Epoch: [15] Total time: 0:23:36 (0.5662 s / it)
Averaged stats: lr: 0.000238  min_lr: 0.000000  loss: 1.6826 (1.6634)  class_acc: 0.8125 (0.8255)  loss_scale: 32768.0000 (39741.0304)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Test:  [ 0/98]  eta: 0:02:46  loss: 0.2066 (0.2066)  acc1: 95.3125 (95.3125)  acc5: 100.0000 (100.0000)  time: 1.6966  data: 1.4714  max mem: 27806
Test:  [97/98]  eta: 0:00:00  loss: 0.5831 (0.5875)  acc1: 85.9375 (87.0560)  acc5: 96.8750 (98.0640)  time: 0.2017  data: 0.0001  max mem: 27806
Test: Total time: 0:00:21 (0.2215 s / it)
* Acc@1 86.940 Acc@5 98.026 loss 0.587
Accuracy of the network on the 50000 test images: 86.9%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Test:  [ 0/98]  eta: 0:02:36  loss: 0.2425 (0.2425)  acc1: 95.3125 (95.3125)  acc5: 100.0000 (100.0000)  time: 1.5983  data: 1.3915  max mem: 27806
Test:  [97/98]  eta: 0:00:00  loss: 0.5700 (0.5422)  acc1: 87.5000 (87.7440)  acc5: 98.4375 (98.4000)  time: 0.2033  data: 0.0001  max mem: 27806
Test: Total time: 0:00:21 (0.2217 s / it)
* Acc@1 87.722 Acc@5 98.462 loss 0.541
EMA Accuracy of the network on the 50000 test images: 87.7%
Max accuracy: 87.72%
{"train_lr": 0.00025010040735830156, "train_min_lr": 5.259546694771628e-09, "train_loss": 1.663375927734375, "train_class_acc": 0.825509375, "train_loss_scale": 39741.0304, "train_weight_decay": 0.04999999999999801, "test_loss": 0.5411074014798719, "test_acc1": 87.7220000112915, "test_acc5": 98.46200000488281, "epoch": 15, "n_parameters": 303602664}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Epoch: [16]  [   0/2502]  eta: 8:32:52  lr: 0.000238  min_lr: 0.000000  loss: 1.6143 (1.6143)  class_acc: 0.8594 (0.8594)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 12.2991  data: 11.7085  max mem: 27806
Epoch: [16]  [ 100/2502]  eta: 0:27:08  lr: 0.000237  min_lr: 0.000000  loss: 1.5371 (1.6130)  class_acc: 0.8438 (0.8352)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.0003  max mem: 27806
Epoch: [16]  [ 200/2502]  eta: 0:23:48  lr: 0.000236  min_lr: 0.000000  loss: 1.5732 (1.6188)  class_acc: 0.8594 (0.8392)  loss_scale: 65536.0000 (41571.3433)  weight_decay: 0.0500 (0.0500)  time: 0.5618  data: 0.0003  max mem: 27806
Epoch: [16]  [ 300/2502]  eta: 0:22:02  lr: 0.000235  min_lr: 0.000000  loss: 1.6387 (1.6375)  class_acc: 0.8281 (0.8338)  loss_scale: 65536.0000 (49533.0233)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.0003  max mem: 27806
Epoch: [16]  [ 400/2502]  eta: 0:20:41  lr: 0.000234  min_lr: 0.000000  loss: 1.6357 (1.6374)  class_acc: 0.8281 (0.8327)  loss_scale: 65536.0000 (53523.7905)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.0003  max mem: 27806
Epoch: [16]  [ 500/2502]  eta: 0:19:30  lr: 0.000233  min_lr: 0.000000  loss: 1.6172 (1.6390)  class_acc: 0.8281 (0.8323)  loss_scale: 65536.0000 (55921.4371)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.0003  max mem: 27806
Epoch: [16]  [ 600/2502]  eta: 0:18:25  lr: 0.000232  min_lr: 0.000000  loss: 1.6260 (1.6393)  class_acc: 0.8281 (0.8324)  loss_scale: 65536.0000 (57521.1980)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0003  max mem: 27806
Epoch: [16]  [ 700/2502]  eta: 0:17:21  lr: 0.000231  min_lr: 0.000000  loss: 1.6406 (1.6391)  class_acc: 0.8125 (0.8322)  loss_scale: 131072.0000 (62591.0870)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.0003  max mem: 27806
Epoch: [16]  [ 800/2502]  eta: 0:16:20  lr: 0.000230  min_lr: 0.000000  loss: 1.6797 (1.6390)  class_acc: 0.8281 (0.8325)  loss_scale: 131072.0000 (71140.5144)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0002  max mem: 27806
Epoch: [16]  [ 900/2502]  eta: 0:15:20  lr: 0.000229  min_lr: 0.000000  loss: 1.6846 (1.6400)  class_acc: 0.8125 (0.8325)  loss_scale: 131072.0000 (77792.1776)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0003  max mem: 27806
Epoch: [16]  [1000/2502]  eta: 0:14:20  lr: 0.000228  min_lr: 0.000000  loss: 1.6426 (1.6405)  class_acc: 0.8281 (0.8324)  loss_scale: 65536.0000 (81936.3676)  weight_decay: 0.0500 (0.0500)  time: 0.5597  data: 0.0002  max mem: 27806
Epoch: [16]  [1100/2502]  eta: 0:13:22  lr: 0.000227  min_lr: 0.000000  loss: 1.5879 (1.6412)  class_acc: 0.8594 (0.8320)  loss_scale: 65536.0000 (80446.7793)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0002  max mem: 27806
Epoch: [16]  [1200/2502]  eta: 0:12:23  lr: 0.000226  min_lr: 0.000000  loss: 1.6533 (1.6417)  class_acc: 0.8125 (0.8317)  loss_scale: 65536.0000 (79205.2490)  weight_decay: 0.0500 (0.0500)  time: 0.5617  data: 0.0003  max mem: 27806
Epoch: [16]  [1300/2502]  eta: 0:11:25  lr: 0.000225  min_lr: 0.000000  loss: 1.5928 (1.6431)  class_acc: 0.8594 (0.8314)  loss_scale: 65536.0000 (78154.5765)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0003  max mem: 27806
Epoch: [16]  [1400/2502]  eta: 0:10:27  lr: 0.000224  min_lr: 0.000000  loss: 1.6094 (1.6440)  class_acc: 0.8281 (0.8310)  loss_scale: 65536.0000 (77253.8929)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.0003  max mem: 27806
Epoch: [16]  [1500/2502]  eta: 0:09:30  lr: 0.000223  min_lr: 0.000000  loss: 1.6328 (1.6447)  class_acc: 0.8438 (0.8310)  loss_scale: 65536.0000 (76560.5436)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.0003  max mem: 27806
Epoch: [16]  [1600/2502]  eta: 0:08:32  lr: 0.000222  min_lr: 0.000000  loss: 1.5957 (1.6447)  class_acc: 0.8438 (0.8310)  loss_scale: 65536.0000 (77918.6608)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0002  max mem: 27806
Epoch: [16]  [1700/2502]  eta: 0:07:35  lr: 0.000221  min_lr: 0.000000  loss: 1.6406 (1.6450)  class_acc: 0.8125 (0.8308)  loss_scale: 65536.0000 (77190.6972)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.0002  max mem: 27806
Epoch: [16]  [1800/2502]  eta: 0:06:38  lr: 0.000220  min_lr: 0.000000  loss: 1.6523 (1.6445)  class_acc: 0.8281 (0.8310)  loss_scale: 32768.0000 (75561.0794)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.0003  max mem: 27806
Epoch: [16]  [1900/2502]  eta: 0:05:41  lr: 0.000219  min_lr: 0.000000  loss: 1.6426 (1.6449)  class_acc: 0.8125 (0.8308)  loss_scale: 32768.0000 (73309.9968)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0003  max mem: 27806
Epoch: [16]  [2000/2502]  eta: 0:04:44  lr: 0.000218  min_lr: 0.000000  loss: 1.5527 (1.6449)  class_acc: 0.8281 (0.8306)  loss_scale: 32768.0000 (71283.9100)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.0003  max mem: 27806
Epoch: [16]  [2100/2502]  eta: 0:03:47  lr: 0.000217  min_lr: 0.000000  loss: 1.6328 (1.6440)  class_acc: 0.8438 (0.8307)  loss_scale: 32768.0000 (69450.6921)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0003  max mem: 27806
Epoch: [16]  [2200/2502]  eta: 0:02:51  lr: 0.000216  min_lr: 0.000000  loss: 1.6387 (1.6448)  class_acc: 0.8281 (0.8304)  loss_scale: 32768.0000 (67784.0545)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0002  max mem: 27806
Epoch: [16]  [2300/2502]  eta: 0:01:54  lr: 0.000215  min_lr: 0.000000  loss: 1.6357 (1.6446)  class_acc: 0.8438 (0.8304)  loss_scale: 65536.0000 (66803.4281)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0003  max mem: 27806
Epoch: [16]  [2400/2502]  eta: 0:00:57  lr: 0.000214  min_lr: 0.000000  loss: 1.8066 (1.6455)  class_acc: 0.7969 (0.8304)  loss_scale: 65536.0000 (66750.6406)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0002  max mem: 27806
Epoch: [16]  [2500/2502]  eta: 0:00:01  lr: 0.000213  min_lr: 0.000000  loss: 1.6221 (1.6443)  class_acc: 0.8281 (0.8307)  loss_scale: 65536.0000 (66702.5408)  weight_decay: 0.0500 (0.0500)  time: 0.5335  data: 0.0006  max mem: 27806
Epoch: [16]  [2501/2502]  eta: 0:00:00  lr: 0.000213  min_lr: 0.000000  loss: 1.6221 (1.6443)  class_acc: 0.8281 (0.8307)  loss_scale: 65536.0000 (66702.5408)  weight_decay: 0.0500 (0.0500)  time: 0.5062  data: 0.0006  max mem: 27806
Epoch: [16] Total time: 0:23:35 (0.5659 s / it)
Averaged stats: lr: 0.000213  min_lr: 0.000000  loss: 1.6221 (1.6450)  class_acc: 0.8281 (0.8309)  loss_scale: 65536.0000 (66702.5408)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Test:  [ 0/98]  eta: 0:02:23  loss: 0.1747 (0.1747)  acc1: 98.4375 (98.4375)  acc5: 100.0000 (100.0000)  time: 1.4681  data: 1.2388  max mem: 27806
Test:  [97/98]  eta: 0:00:00  loss: 0.6011 (0.5880)  acc1: 85.9375 (87.0240)  acc5: 98.4375 (97.8240)  time: 0.2012  data: 0.0001  max mem: 27806
Test: Total time: 0:00:21 (0.2187 s / it)
* Acc@1 87.076 Acc@5 98.006 loss 0.583
Accuracy of the network on the 50000 test images: 87.1%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Test:  [ 0/98]  eta: 0:02:25  loss: 0.2229 (0.2229)  acc1: 95.3125 (95.3125)  acc5: 100.0000 (100.0000)  time: 1.4874  data: 1.2806  max mem: 27806
Test:  [97/98]  eta: 0:00:00  loss: 0.5501 (0.5287)  acc1: 87.5000 (87.9680)  acc5: 98.4375 (98.4160)  time: 0.2034  data: 0.0001  max mem: 27806
Test: Total time: 0:00:21 (0.2204 s / it)
* Acc@1 87.820 Acc@5 98.468 loss 0.527
EMA Accuracy of the network on the 50000 test images: 87.8%
Max accuracy: 87.82%
{"train_lr": 0.0002255074159905053, "train_min_lr": 4.742362465328438e-09, "train_loss": 1.644961083984375, "train_class_acc": 0.8309375, "train_loss_scale": 66702.5408, "train_weight_decay": 0.04999999999999801, "test_loss": 0.5272826961792854, "test_acc1": 87.8200000112915, "test_acc5": 98.46800000366211, "epoch": 16, "n_parameters": 303602664}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Epoch: [17]  [   0/2502]  eta: 9:00:31  lr: 0.000213  min_lr: 0.000000  loss: 1.7666 (1.7666)  class_acc: 0.8594 (0.8594)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 12.9622  data: 12.3920  max mem: 27806
Epoch: [17]  [ 100/2502]  eta: 0:27:20  lr: 0.000212  min_lr: 0.000000  loss: 1.6553 (1.6233)  class_acc: 0.8281 (0.8411)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.0002  max mem: 27806
Epoch: [17]  [ 200/2502]  eta: 0:23:52  lr: 0.000211  min_lr: 0.000000  loss: 1.6836 (1.6158)  class_acc: 0.8281 (0.8416)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0002  max mem: 27806
Epoch: [17]  [ 300/2502]  eta: 0:22:05  lr: 0.000210  min_lr: 0.000000  loss: 1.5605 (1.6134)  class_acc: 0.8438 (0.8417)  loss_scale: 131072.0000 (71196.9169)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.0002  max mem: 27806
Epoch: [17]  [ 400/2502]  eta: 0:20:43  lr: 0.000209  min_lr: 0.000000  loss: 1.5430 (1.6070)  class_acc: 0.8281 (0.8428)  loss_scale: 65536.0000 (70765.8055)  weight_decay: 0.0500 (0.0500)  time: 0.5607  data: 0.0002  max mem: 27806
Epoch: [17]  [ 500/2502]  eta: 0:19:32  lr: 0.000208  min_lr: 0.000000  loss: 1.5830 (1.6108)  class_acc: 0.8281 (0.8412)  loss_scale: 65536.0000 (69721.9321)  weight_decay: 0.0500 (0.0500)  time: 0.5688  data: 0.0002  max mem: 27806
Epoch: [17]  [ 600/2502]  eta: 0:18:26  lr: 0.000207  min_lr: 0.000000  loss: 1.6133 (1.6139)  class_acc: 0.8281 (0.8402)  loss_scale: 65536.0000 (69025.4376)  weight_decay: 0.0500 (0.0500)  time: 0.5607  data: 0.0002  max mem: 27806
Epoch: [17]  [ 700/2502]  eta: 0:17:23  lr: 0.000206  min_lr: 0.000000  loss: 1.6055 (1.6140)  class_acc: 0.8438 (0.8397)  loss_scale: 65536.0000 (68527.6576)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0003  max mem: 27806
Epoch: [17]  [ 800/2502]  eta: 0:16:21  lr: 0.000205  min_lr: 0.000000  loss: 1.6260 (1.6136)  class_acc: 0.8281 (0.8398)  loss_scale: 65536.0000 (68154.1673)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.0002  max mem: 27806
Epoch: [17]  [ 900/2502]  eta: 0:15:21  lr: 0.000204  min_lr: 0.000000  loss: 1.5771 (1.6116)  class_acc: 0.8438 (0.8402)  loss_scale: 65536.0000 (71645.9046)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.0002  max mem: 27806
Epoch: [17]  [1000/2502]  eta: 0:14:21  lr: 0.000203  min_lr: 0.000000  loss: 1.6260 (1.6122)  class_acc: 0.8438 (0.8402)  loss_scale: 65536.0000 (71035.5245)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.0002  max mem: 27806
Epoch: [17]  [1100/2502]  eta: 0:13:22  lr: 0.000202  min_lr: 0.000000  loss: 1.6191 (1.6155)  class_acc: 0.8281 (0.8391)  loss_scale: 65536.0000 (70536.0218)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0003  max mem: 27806
Epoch: [17]  [1200/2502]  eta: 0:12:24  lr: 0.000201  min_lr: 0.000000  loss: 1.5742 (1.6148)  class_acc: 0.8438 (0.8391)  loss_scale: 65536.0000 (70119.7002)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0002  max mem: 27806
Epoch: [17]  [1300/2502]  eta: 0:11:26  lr: 0.000200  min_lr: 0.000000  loss: 1.5508 (1.6148)  class_acc: 0.8438 (0.8393)  loss_scale: 65536.0000 (69767.3789)  weight_decay: 0.0500 (0.0500)  time: 0.5617  data: 0.0002  max mem: 27806
Epoch: [17]  [1400/2502]  eta: 0:10:28  lr: 0.000199  min_lr: 0.000000  loss: 1.5771 (1.6154)  class_acc: 0.8438 (0.8393)  loss_scale: 65536.0000 (69933.1335)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0002  max mem: 27806
Epoch: [17]  [1500/2502]  eta: 0:09:30  lr: 0.000198  min_lr: 0.000000  loss: 1.7197 (1.6170)  class_acc: 0.8125 (0.8388)  loss_scale: 65536.0000 (69902.1559)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0002  max mem: 27806
Epoch: [17]  [1600/2502]  eta: 0:08:33  lr: 0.000197  min_lr: 0.000000  loss: 1.6055 (1.6163)  class_acc: 0.8438 (0.8392)  loss_scale: 65536.0000 (69629.4416)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.0002  max mem: 27806
Epoch: [17]  [1700/2502]  eta: 0:07:36  lr: 0.000196  min_lr: 0.000000  loss: 1.6465 (1.6178)  class_acc: 0.8281 (0.8389)  loss_scale: 65536.0000 (69388.7925)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.0002  max mem: 27806
Epoch: [17]  [1800/2502]  eta: 0:06:38  lr: 0.000195  min_lr: 0.000000  loss: 1.6250 (1.6198)  class_acc: 0.8438 (0.8382)  loss_scale: 65536.0000 (69174.8673)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0002  max mem: 27806
Epoch: [17]  [1900/2502]  eta: 0:05:41  lr: 0.000194  min_lr: 0.000000  loss: 1.5713 (1.6191)  class_acc: 0.8281 (0.8382)  loss_scale: 65536.0000 (68983.4487)  weight_decay: 0.0500 (0.0500)  time: 0.5621  data: 0.0002  max mem: 27806
Epoch: [17]  [2000/2502]  eta: 0:04:44  lr: 0.000193  min_lr: 0.000000  loss: 1.6543 (1.6195)  class_acc: 0.8281 (0.8379)  loss_scale: 131072.0000 (71365.7891)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0002  max mem: 27806
Epoch: [17]  [2100/2502]  eta: 0:03:48  lr: 0.000192  min_lr: 0.000000  loss: 1.6670 (1.6196)  class_acc: 0.8281 (0.8378)  loss_scale: 32768.0000 (72679.1433)  weight_decay: 0.0500 (0.0500)  time: 0.5678  data: 0.0002  max mem: 27806
Epoch: [17]  [2200/2502]  eta: 0:02:51  lr: 0.000191  min_lr: 0.000000  loss: 1.5762 (1.6204)  class_acc: 0.8281 (0.8377)  loss_scale: 32768.0000 (70865.8246)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.0002  max mem: 27806
Epoch: [17]  [2300/2502]  eta: 0:01:54  lr: 0.000190  min_lr: 0.000000  loss: 1.6631 (1.6219)  class_acc: 0.8281 (0.8371)  loss_scale: 32768.0000 (69210.1173)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0003  max mem: 27806
Epoch: [17]  [2400/2502]  eta: 0:00:57  lr: 0.000189  min_lr: 0.000000  loss: 1.6260 (1.6220)  class_acc: 0.8281 (0.8371)  loss_scale: 32768.0000 (67692.3282)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.0003  max mem: 27806
Epoch: [17]  [2500/2502]  eta: 0:00:01  lr: 0.000188  min_lr: 0.000000  loss: 1.6387 (1.6215)  class_acc: 0.8438 (0.8372)  loss_scale: 32768.0000 (66309.3248)  weight_decay: 0.0500 (0.0500)  time: 0.5336  data: 0.0006  max mem: 27806
Epoch: [17]  [2501/2502]  eta: 0:00:00  lr: 0.000188  min_lr: 0.000000  loss: 1.6387 (1.6215)  class_acc: 0.8438 (0.8372)  loss_scale: 32768.0000 (66309.3248)  weight_decay: 0.0500 (0.0500)  time: 0.5062  data: 0.0006  max mem: 27806
Epoch: [17] Total time: 0:23:36 (0.5662 s / it)
Averaged stats: lr: 0.000188  min_lr: 0.000000  loss: 1.6387 (1.6229)  class_acc: 0.8438 (0.8371)  loss_scale: 32768.0000 (66309.3248)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Test:  [ 0/98]  eta: 0:02:38  loss: 0.2177 (0.2177)  acc1: 95.3125 (95.3125)  acc5: 100.0000 (100.0000)  time: 1.6181  data: 1.3887  max mem: 27806
Test:  [97/98]  eta: 0:00:00  loss: 0.6291 (0.5954)  acc1: 87.5000 (87.1680)  acc5: 96.8750 (97.8240)  time: 0.2024  data: 0.0001  max mem: 27806
Test: Total time: 0:00:21 (0.2203 s / it)
* Acc@1 86.972 Acc@5 97.930 loss 0.599
Accuracy of the network on the 50000 test images: 87.0%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Test:  [ 0/98]  eta: 0:02:41  loss: 0.2094 (0.2094)  acc1: 95.3125 (95.3125)  acc5: 100.0000 (100.0000)  time: 1.6429  data: 1.4359  max mem: 27806
Test:  [97/98]  eta: 0:00:00  loss: 0.5370 (0.5210)  acc1: 88.0952 (88.0960)  acc5: 98.4375 (98.4000)  time: 0.2033  data: 0.0001  max mem: 27806
Test: Total time: 0:00:21 (0.2219 s / it)
* Acc@1 87.896 Acc@5 98.456 loss 0.519
EMA Accuracy of the network on the 50000 test images: 87.9%
Max accuracy: 87.90%
{"train_lr": 0.00020052004273383357, "train_min_lr": 4.2168844870583674e-09, "train_loss": 1.622941748046875, "train_class_acc": 0.83705078125, "train_loss_scale": 66309.3248, "train_weight_decay": 0.04999999999999801, "test_loss": 0.5192900996047015, "test_acc1": 87.8960000125122, "test_acc5": 98.45600000366211, "epoch": 17, "n_parameters": 303602664}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Epoch: [18]  [   0/2502]  eta: 8:18:33  lr: 0.000188  min_lr: 0.000000  loss: 1.6924 (1.6924)  class_acc: 0.8125 (0.8125)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 11.9560  data: 11.3716  max mem: 27806
Epoch: [18]  [ 100/2502]  eta: 0:26:57  lr: 0.000187  min_lr: 0.000000  loss: 1.5908 (1.6121)  class_acc: 0.8281 (0.8405)  loss_scale: 65536.0000 (37310.0990)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0003  max mem: 27806
Epoch: [18]  [ 200/2502]  eta: 0:23:42  lr: 0.000186  min_lr: 0.000000  loss: 1.5430 (1.6025)  class_acc: 0.8438 (0.8427)  loss_scale: 65536.0000 (51352.8358)  weight_decay: 0.0500 (0.0500)  time: 0.5617  data: 0.0002  max mem: 27806
Epoch: [18]  [ 300/2502]  eta: 0:21:59  lr: 0.000185  min_lr: 0.000000  loss: 1.5430 (1.6027)  class_acc: 0.8438 (0.8416)  loss_scale: 65536.0000 (56064.8505)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.0003  max mem: 27806
Epoch: [18]  [ 400/2502]  eta: 0:20:39  lr: 0.000184  min_lr: 0.000000  loss: 1.6738 (1.6019)  class_acc: 0.8281 (0.8419)  loss_scale: 65536.0000 (58426.7332)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0003  max mem: 27806
Epoch: [18]  [ 500/2502]  eta: 0:19:29  lr: 0.000183  min_lr: 0.000000  loss: 1.5293 (1.5962)  class_acc: 0.8438 (0.8439)  loss_scale: 65536.0000 (59845.7485)  weight_decay: 0.0500 (0.0500)  time: 0.5607  data: 0.0002  max mem: 27806
Epoch: [18]  [ 600/2502]  eta: 0:18:23  lr: 0.000182  min_lr: 0.000000  loss: 1.4541 (1.5961)  class_acc: 0.8594 (0.8447)  loss_scale: 65536.0000 (61010.6356)  weight_decay: 0.0500 (0.0500)  time: 0.5605  data: 0.0003  max mem: 27806
Epoch: [18]  [ 700/2502]  eta: 0:17:20  lr: 0.000181  min_lr: 0.000000  loss: 1.5986 (1.5967)  class_acc: 0.8438 (0.8444)  loss_scale: 65536.0000 (61843.1726)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0002  max mem: 27806
Epoch: [18]  [ 800/2502]  eta: 0:16:19  lr: 0.000180  min_lr: 0.000000  loss: 1.5791 (1.5978)  class_acc: 0.8438 (0.8447)  loss_scale: 65536.0000 (62304.1998)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0003  max mem: 27806
Epoch: [18]  [ 900/2502]  eta: 0:15:19  lr: 0.000179  min_lr: 0.000000  loss: 1.5879 (1.5986)  class_acc: 0.8438 (0.8447)  loss_scale: 65536.0000 (62662.8901)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0003  max mem: 27806
Epoch: [18]  [1000/2502]  eta: 0:14:20  lr: 0.000178  min_lr: 0.000000  loss: 1.5488 (1.5993)  class_acc: 0.8594 (0.8444)  loss_scale: 65536.0000 (62949.9141)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0003  max mem: 27806
Epoch: [18]  [1100/2502]  eta: 0:13:21  lr: 0.000177  min_lr: 0.000000  loss: 1.5830 (1.5999)  class_acc: 0.8438 (0.8442)  loss_scale: 65536.0000 (63184.7993)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0002  max mem: 27806
Epoch: [18]  [1200/2502]  eta: 0:12:23  lr: 0.000176  min_lr: 0.000000  loss: 1.6211 (1.5996)  class_acc: 0.8281 (0.8440)  loss_scale: 131072.0000 (67855.1341)  weight_decay: 0.0500 (0.0500)  time: 0.5619  data: 0.0002  max mem: 27806
Epoch: [18]  [1300/2502]  eta: 0:11:25  lr: 0.000175  min_lr: 0.000000  loss: 1.5166 (1.5978)  class_acc: 0.8750 (0.8444)  loss_scale: 65536.0000 (68180.6118)  weight_decay: 0.0500 (0.0500)  time: 0.5684  data: 0.0002  max mem: 27806
Epoch: [18]  [1400/2502]  eta: 0:10:27  lr: 0.000174  min_lr: 0.000000  loss: 1.5879 (1.5959)  class_acc: 0.8594 (0.8450)  loss_scale: 65536.0000 (67991.8458)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0003  max mem: 27806
Epoch: [18]  [1500/2502]  eta: 0:09:30  lr: 0.000173  min_lr: 0.000000  loss: 1.5654 (1.5966)  class_acc: 0.8438 (0.8447)  loss_scale: 32768.0000 (66824.0160)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0003  max mem: 27806
Epoch: [18]  [1600/2502]  eta: 0:08:32  lr: 0.000172  min_lr: 0.000000  loss: 1.5674 (1.5980)  class_acc: 0.8438 (0.8442)  loss_scale: 32768.0000 (64696.8445)  weight_decay: 0.0500 (0.0500)  time: 0.5605  data: 0.0002  max mem: 27806
Epoch: [18]  [1700/2502]  eta: 0:07:35  lr: 0.000171  min_lr: 0.000000  loss: 1.5625 (1.5986)  class_acc: 0.8438 (0.8442)  loss_scale: 32768.0000 (62819.7813)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0002  max mem: 27806
Epoch: [18]  [1800/2502]  eta: 0:06:38  lr: 0.000170  min_lr: 0.000000  loss: 1.5898 (1.6010)  class_acc: 0.8281 (0.8434)  loss_scale: 32768.0000 (61151.1649)  weight_decay: 0.0500 (0.0500)  time: 0.5605  data: 0.0002  max mem: 27806
Epoch: [18]  [1900/2502]  eta: 0:05:41  lr: 0.000169  min_lr: 0.000000  loss: 1.5293 (1.6001)  class_acc: 0.8438 (0.8438)  loss_scale: 32768.0000 (59658.0999)  weight_decay: 0.0500 (0.0500)  time: 0.5605  data: 0.0002  max mem: 27806
Epoch: [18]  [2000/2502]  eta: 0:04:44  lr: 0.000168  min_lr: 0.000000  loss: 1.6143 (1.6001)  class_acc: 0.8281 (0.8436)  loss_scale: 65536.0000 (58805.5412)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0003  max mem: 27806
Epoch: [18]  [2100/2502]  eta: 0:03:47  lr: 0.000167  min_lr: 0.000000  loss: 1.6055 (1.6009)  class_acc: 0.8438 (0.8435)  loss_scale: 65536.0000 (59125.8867)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.0002  max mem: 27806
Epoch: [18]  [2200/2502]  eta: 0:02:51  lr: 0.000166  min_lr: 0.000000  loss: 1.6191 (1.6023)  class_acc: 0.8438 (0.8430)  loss_scale: 65536.0000 (59417.1231)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.0002  max mem: 27806
Epoch: [18]  [2300/2502]  eta: 0:01:54  lr: 0.000165  min_lr: 0.000000  loss: 1.6494 (1.6029)  class_acc: 0.8125 (0.8427)  loss_scale: 65536.0000 (59683.0456)  weight_decay: 0.0500 (0.0500)  time: 0.5607  data: 0.0003  max mem: 27806
Epoch: [18]  [2400/2502]  eta: 0:00:57  lr: 0.000164  min_lr: 0.000000  loss: 1.6094 (1.6037)  class_acc: 0.8281 (0.8424)  loss_scale: 65536.0000 (59926.8172)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0003  max mem: 27806
Epoch: [18]  [2500/2502]  eta: 0:00:01  lr: 0.000163  min_lr: 0.000000  loss: 1.6035 (1.6046)  class_acc: 0.8438 (0.8424)  loss_scale: 131072.0000 (60594.5856)  weight_decay: 0.0500 (0.0500)  time: 0.5338  data: 0.0006  max mem: 27806
Epoch: [18]  [2501/2502]  eta: 0:00:00  lr: 0.000163  min_lr: 0.000000  loss: 1.6035 (1.6046)  class_acc: 0.8438 (0.8424)  loss_scale: 131072.0000 (60594.5856)  weight_decay: 0.0500 (0.0500)  time: 0.5065  data: 0.0006  max mem: 27806
Epoch: [18] Total time: 0:23:35 (0.5659 s / it)
Averaged stats: lr: 0.000163  min_lr: 0.000000  loss: 1.6035 (1.6063)  class_acc: 0.8438 (0.8421)  loss_scale: 131072.0000 (60594.5856)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Test:  [ 0/98]  eta: 0:02:41  loss: 0.1680 (0.1680)  acc1: 95.3125 (95.3125)  acc5: 100.0000 (100.0000)  time: 1.6472  data: 1.4198  max mem: 27806
Test:  [97/98]  eta: 0:00:00  loss: 0.5916 (0.5936)  acc1: 87.5000 (87.2320)  acc5: 96.8750 (97.8400)  time: 0.2013  data: 0.0001  max mem: 27806
Test: Total time: 0:00:21 (0.2203 s / it)
* Acc@1 86.842 Acc@5 97.936 loss 0.604
Accuracy of the network on the 50000 test images: 86.8%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Test:  [ 0/98]  eta: 0:02:49  loss: 0.1977 (0.1977)  acc1: 95.3125 (95.3125)  acc5: 100.0000 (100.0000)  time: 1.7259  data: 1.5190  max mem: 27806
Test:  [97/98]  eta: 0:00:00  loss: 0.5325 (0.5175)  acc1: 88.0952 (88.2560)  acc5: 98.4375 (98.4160)  time: 0.2034  data: 0.0001  max mem: 27806
Test: Total time: 0:00:22 (0.2248 s / it)
* Acc@1 87.912 Acc@5 98.464 loss 0.516
EMA Accuracy of the network on the 50000 test images: 87.9%
Max accuracy: 87.91%
{"train_lr": 0.00017553235339127634, "train_min_lr": 3.6913998615841395e-09, "train_loss": 1.606259228515625, "train_class_acc": 0.84206171875, "train_loss_scale": 60594.5856, "train_weight_decay": 0.04999999999999801, "test_loss": 0.5156066953679737, "test_acc1": 87.91200001251221, "test_acc5": 98.46400000244141, "epoch": 18, "n_parameters": 303602664}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Epoch: [19]  [   0/2502]  eta: 9:14:26  lr: 0.000163  min_lr: 0.000000  loss: 1.7158 (1.7158)  class_acc: 0.8594 (0.8594)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 13.2959  data: 12.7212  max mem: 27806
Epoch: [19]  [ 100/2502]  eta: 0:27:34  lr: 0.000162  min_lr: 0.000000  loss: 1.5703 (1.5969)  class_acc: 0.8594 (0.8442)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.0004  max mem: 27806
Epoch: [19]  [ 200/2502]  eta: 0:23:59  lr: 0.000161  min_lr: 0.000000  loss: 1.5117 (1.5887)  class_acc: 0.8594 (0.8462)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0003  max mem: 27806
Epoch: [19]  [ 300/2502]  eta: 0:22:10  lr: 0.000160  min_lr: 0.000000  loss: 1.5625 (1.5839)  class_acc: 0.8438 (0.8493)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.0004  max mem: 27806
Epoch: [19]  [ 400/2502]  eta: 0:20:47  lr: 0.000159  min_lr: 0.000000  loss: 1.5645 (1.5844)  class_acc: 0.8438 (0.8487)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.0003  max mem: 27806
Epoch: [19]  [ 500/2502]  eta: 0:19:34  lr: 0.000158  min_lr: 0.000000  loss: 1.5010 (1.5866)  class_acc: 0.8438 (0.8481)  loss_scale: 65536.0000 (123484.9980)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.0004  max mem: 27806
Epoch: [19]  [ 600/2502]  eta: 0:18:27  lr: 0.000157  min_lr: 0.000000  loss: 1.5479 (1.5889)  class_acc: 0.8281 (0.8475)  loss_scale: 65536.0000 (113842.9018)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0003  max mem: 27806
Epoch: [19]  [ 700/2502]  eta: 0:17:24  lr: 0.000156  min_lr: 0.000000  loss: 1.4795 (1.5883)  class_acc: 0.8594 (0.8477)  loss_scale: 65536.0000 (106951.7603)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0003  max mem: 27806
Epoch: [19]  [ 800/2502]  eta: 0:16:22  lr: 0.000155  min_lr: 0.000000  loss: 1.6064 (1.5888)  class_acc: 0.8438 (0.8476)  loss_scale: 65536.0000 (101781.2534)  weight_decay: 0.0500 (0.0500)  time: 0.5616  data: 0.0003  max mem: 27806
Epoch: [19]  [ 900/2502]  eta: 0:15:22  lr: 0.000154  min_lr: 0.000000  loss: 1.4912 (1.5896)  class_acc: 0.8750 (0.8476)  loss_scale: 65536.0000 (97758.4728)  weight_decay: 0.0500 (0.0500)  time: 0.5617  data: 0.0004  max mem: 27806
Epoch: [19]  [1000/2502]  eta: 0:14:22  lr: 0.000153  min_lr: 0.000000  loss: 1.6514 (1.5909)  class_acc: 0.7969 (0.8469)  loss_scale: 131072.0000 (97289.2068)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0003  max mem: 27806
Epoch: [19]  [1100/2502]  eta: 0:13:23  lr: 0.000152  min_lr: 0.000000  loss: 1.5029 (1.5890)  class_acc: 0.8750 (0.8473)  loss_scale: 65536.0000 (96428.9918)  weight_decay: 0.0500 (0.0500)  time: 0.5688  data: 0.0003  max mem: 27806
Epoch: [19]  [1200/2502]  eta: 0:12:24  lr: 0.000151  min_lr: 0.000000  loss: 1.5137 (1.5894)  class_acc: 0.8750 (0.8470)  loss_scale: 32768.0000 (92492.5229)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0002  max mem: 27806
Epoch: [19]  [1300/2502]  eta: 0:11:26  lr: 0.000150  min_lr: 0.000000  loss: 1.6172 (1.5908)  class_acc: 0.8438 (0.8468)  loss_scale: 32768.0000 (87901.8601)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0002  max mem: 27806
Epoch: [19]  [1400/2502]  eta: 0:10:28  lr: 0.000149  min_lr: 0.000000  loss: 1.6045 (1.5910)  class_acc: 0.8281 (0.8465)  loss_scale: 32768.0000 (83966.5382)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.0003  max mem: 27806
Epoch: [19]  [1500/2502]  eta: 0:09:31  lr: 0.000148  min_lr: 0.000000  loss: 1.6006 (1.5918)  class_acc: 0.8594 (0.8463)  loss_scale: 32768.0000 (80555.5763)  weight_decay: 0.0500 (0.0500)  time: 0.5616  data: 0.0003  max mem: 27806
Epoch: [19]  [1600/2502]  eta: 0:08:33  lr: 0.000147  min_lr: 0.000000  loss: 1.5791 (1.5902)  class_acc: 0.8438 (0.8467)  loss_scale: 32768.0000 (77570.7183)  weight_decay: 0.0500 (0.0500)  time: 0.5605  data: 0.0003  max mem: 27806
Epoch: [19]  [1700/2502]  eta: 0:07:36  lr: 0.000147  min_lr: 0.000000  loss: 1.5576 (1.5897)  class_acc: 0.8438 (0.8469)  loss_scale: 65536.0000 (75591.7884)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.0004  max mem: 27806
Epoch: [19]  [1800/2502]  eta: 0:06:39  lr: 0.000146  min_lr: 0.000000  loss: 1.6123 (1.5902)  class_acc: 0.8594 (0.8466)  loss_scale: 65536.0000 (75033.4436)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0004  max mem: 27806
Epoch: [19]  [1900/2502]  eta: 0:05:41  lr: 0.000145  min_lr: 0.000000  loss: 1.5352 (1.5894)  class_acc: 0.8594 (0.8469)  loss_scale: 32768.0000 (73258.2851)  weight_decay: 0.0500 (0.0500)  time: 0.5607  data: 0.0004  max mem: 27806
Epoch: [19]  [2000/2502]  eta: 0:04:45  lr: 0.000144  min_lr: 0.000000  loss: 1.5596 (1.5897)  class_acc: 0.8594 (0.8467)  loss_scale: 32768.0000 (71234.7826)  weight_decay: 0.0500 (0.0500)  time: 0.5676  data: 0.0005  max mem: 27806
Epoch: [19]  [2100/2502]  eta: 0:03:48  lr: 0.000143  min_lr: 0.000000  loss: 1.5742 (1.5892)  class_acc: 0.8438 (0.8470)  loss_scale: 32768.0000 (69403.9029)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.0003  max mem: 27806
Epoch: [19]  [2200/2502]  eta: 0:02:51  lr: 0.000142  min_lr: 0.000000  loss: 1.5420 (1.5885)  class_acc: 0.8594 (0.8472)  loss_scale: 32768.0000 (67739.3912)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0003  max mem: 27806
Epoch: [19]  [2300/2502]  eta: 0:01:54  lr: 0.000141  min_lr: 0.000000  loss: 1.5488 (1.5884)  class_acc: 0.8594 (0.8474)  loss_scale: 32768.0000 (66219.5567)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.0002  max mem: 27806
Epoch: [19]  [2400/2502]  eta: 0:00:57  lr: 0.000140  min_lr: 0.000000  loss: 1.5645 (1.5877)  class_acc: 0.8438 (0.8476)  loss_scale: 65536.0000 (65617.8859)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0003  max mem: 27806
Epoch: [19]  [2500/2502]  eta: 0:00:01  lr: 0.000139  min_lr: 0.000000  loss: 1.6318 (1.5884)  class_acc: 0.8281 (0.8475)  loss_scale: 65536.0000 (65614.6432)  weight_decay: 0.0500 (0.0500)  time: 0.5337  data: 0.0006  max mem: 27806
Epoch: [19]  [2501/2502]  eta: 0:00:00  lr: 0.000139  min_lr: 0.000000  loss: 1.6318 (1.5884)  class_acc: 0.8281 (0.8475)  loss_scale: 65536.0000 (65614.6432)  weight_decay: 0.0500 (0.0500)  time: 0.5064  data: 0.0006  max mem: 27806
Epoch: [19] Total time: 0:23:37 (0.5664 s / it)
Averaged stats: lr: 0.000139  min_lr: 0.000000  loss: 1.6318 (1.5890)  class_acc: 0.8281 (0.8470)  loss_scale: 65536.0000 (65614.6432)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Test:  [ 0/98]  eta: 0:02:48  loss: 0.1877 (0.1877)  acc1: 96.8750 (96.8750)  acc5: 100.0000 (100.0000)  time: 1.7157  data: 1.4645  max mem: 27806
Test:  [97/98]  eta: 0:00:00  loss: 0.5923 (0.6063)  acc1: 85.9375 (87.3280)  acc5: 96.8750 (97.7280)  time: 0.2011  data: 0.0001  max mem: 27806
Test: Total time: 0:00:21 (0.2207 s / it)
* Acc@1 86.960 Acc@5 97.856 loss 0.609
Accuracy of the network on the 50000 test images: 87.0%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Test:  [ 0/98]  eta: 0:02:55  loss: 0.1881 (0.1881)  acc1: 93.7500 (93.7500)  acc5: 100.0000 (100.0000)  time: 1.7909  data: 1.5741  max mem: 27806
Test:  [97/98]  eta: 0:00:00  loss: 0.5298 (0.5165)  acc1: 88.0952 (88.3360)  acc5: 98.4375 (98.3520)  time: 0.2047  data: 0.0001  max mem: 27806
Test: Total time: 0:00:22 (0.2336 s / it)
* Acc@1 87.974 Acc@5 98.420 loss 0.515
EMA Accuracy of the network on the 50000 test images: 88.0%
Max accuracy: 87.97%
{"train_lr": 0.00015093841875068691, "train_min_lr": 3.174195795358756e-09, "train_loss": 1.589023974609375, "train_class_acc": 0.84695703125, "train_loss_scale": 65614.6432, "train_weight_decay": 0.04999999999999801, "test_loss": 0.5147663853226268, "test_acc1": 87.9740000137329, "test_acc5": 98.42000000244141, "epoch": 19, "n_parameters": 303602664}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Epoch: [20]  [   0/2502]  eta: 8:13:32  lr: 0.000139  min_lr: 0.000000  loss: 1.5527 (1.5527)  class_acc: 0.8125 (0.8125)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 11.8355  data: 11.2409  max mem: 27806
Epoch: [20]  [ 100/2502]  eta: 0:26:55  lr: 0.000138  min_lr: 0.000000  loss: 1.4795 (1.5385)  class_acc: 0.8750 (0.8600)  loss_scale: 32768.0000 (51909.7030)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.0004  max mem: 27806
Epoch: [20]  [ 200/2502]  eta: 0:23:40  lr: 0.000137  min_lr: 0.000000  loss: 1.5713 (1.5450)  class_acc: 0.8281 (0.8574)  loss_scale: 32768.0000 (42386.4677)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.0002  max mem: 27806
Epoch: [20]  [ 300/2502]  eta: 0:21:57  lr: 0.000136  min_lr: 0.000000  loss: 1.6064 (1.5498)  class_acc: 0.8438 (0.8559)  loss_scale: 32768.0000 (39190.9635)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0002  max mem: 27806
Epoch: [20]  [ 400/2502]  eta: 0:20:39  lr: 0.000135  min_lr: 0.000000  loss: 1.4893 (1.5502)  class_acc: 0.8750 (0.8570)  loss_scale: 32768.0000 (37589.2269)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0003  max mem: 27806
Epoch: [20]  [ 500/2502]  eta: 0:19:28  lr: 0.000134  min_lr: 0.000000  loss: 1.6719 (1.5619)  class_acc: 0.8281 (0.8546)  loss_scale: 32768.0000 (36626.9062)  weight_decay: 0.0500 (0.0500)  time: 0.5601  data: 0.0003  max mem: 27806
Epoch: [20]  [ 600/2502]  eta: 0:18:23  lr: 0.000133  min_lr: 0.000000  loss: 1.5459 (1.5597)  class_acc: 0.8281 (0.8548)  loss_scale: 65536.0000 (37402.4093)  weight_decay: 0.0500 (0.0500)  time: 0.5713  data: 0.0002  max mem: 27806
Epoch: [20]  [ 700/2502]  eta: 0:17:20  lr: 0.000132  min_lr: 0.000000  loss: 1.5996 (1.5597)  class_acc: 0.8281 (0.8543)  loss_scale: 65536.0000 (41415.7603)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.0002  max mem: 27806
Epoch: [20]  [ 800/2502]  eta: 0:16:19  lr: 0.000131  min_lr: 0.000000  loss: 1.5049 (1.5631)  class_acc: 0.8750 (0.8540)  loss_scale: 65536.0000 (44427.0262)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.0002  max mem: 27806
Epoch: [20]  [ 900/2502]  eta: 0:15:19  lr: 0.000130  min_lr: 0.000000  loss: 1.5439 (1.5620)  class_acc: 0.8750 (0.8547)  loss_scale: 65536.0000 (46769.8646)  weight_decay: 0.0500 (0.0500)  time: 0.5604  data: 0.0002  max mem: 27806
Epoch: [20]  [1000/2502]  eta: 0:14:20  lr: 0.000129  min_lr: 0.000000  loss: 1.5781 (1.5637)  class_acc: 0.8438 (0.8541)  loss_scale: 65536.0000 (48448.1918)  weight_decay: 0.0500 (0.0500)  time: 0.5597  data: 0.0002  max mem: 27806
Epoch: [20]  [1100/2502]  eta: 0:13:21  lr: 0.000128  min_lr: 0.000000  loss: 1.5635 (1.5645)  class_acc: 0.8594 (0.8542)  loss_scale: 32768.0000 (47024.0145)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0003  max mem: 27806
Epoch: [20]  [1200/2502]  eta: 0:12:23  lr: 0.000128  min_lr: 0.000000  loss: 1.6006 (1.5659)  class_acc: 0.8438 (0.8536)  loss_scale: 32768.0000 (45837.0025)  weight_decay: 0.0500 (0.0500)  time: 0.5621  data: 0.0002  max mem: 27806
Epoch: [20]  [1300/2502]  eta: 0:11:25  lr: 0.000127  min_lr: 0.000000  loss: 1.5928 (1.5681)  class_acc: 0.8281 (0.8535)  loss_scale: 32768.0000 (44832.4673)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0002  max mem: 27806
Epoch: [20]  [1400/2502]  eta: 0:10:27  lr: 0.000126  min_lr: 0.000000  loss: 1.5146 (1.5698)  class_acc: 0.8594 (0.8530)  loss_scale: 32768.0000 (43971.3348)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0002  max mem: 27806
Epoch: [20]  [1500/2502]  eta: 0:09:30  lr: 0.000125  min_lr: 0.000000  loss: 1.6045 (1.5704)  class_acc: 0.8438 (0.8529)  loss_scale: 32768.0000 (43224.9434)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0003  max mem: 27806
Epoch: [20]  [1600/2502]  eta: 0:08:32  lr: 0.000124  min_lr: 0.000000  loss: 1.6357 (1.5692)  class_acc: 0.8438 (0.8531)  loss_scale: 65536.0000 (44413.8413)  weight_decay: 0.0500 (0.0500)  time: 0.5618  data: 0.0002  max mem: 27806
Epoch: [20]  [1700/2502]  eta: 0:07:35  lr: 0.000123  min_lr: 0.000000  loss: 1.5273 (1.5704)  class_acc: 0.8594 (0.8525)  loss_scale: 65536.0000 (45655.5908)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.0002  max mem: 27806
Epoch: [20]  [1800/2502]  eta: 0:06:38  lr: 0.000122  min_lr: 0.000000  loss: 1.5684 (1.5719)  class_acc: 0.8438 (0.8523)  loss_scale: 32768.0000 (45995.2826)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.0002  max mem: 27806
Epoch: [20]  [1900/2502]  eta: 0:05:41  lr: 0.000121  min_lr: 0.000000  loss: 1.5596 (1.5734)  class_acc: 0.8438 (0.8515)  loss_scale: 32768.0000 (45299.4761)  weight_decay: 0.0500 (0.0500)  time: 0.5603  data: 0.0002  max mem: 27806
Epoch: [20]  [2000/2502]  eta: 0:04:44  lr: 0.000120  min_lr: 0.000000  loss: 1.5566 (1.5718)  class_acc: 0.8438 (0.8518)  loss_scale: 32768.0000 (44673.2154)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.0002  max mem: 27806
Epoch: [20]  [2100/2502]  eta: 0:03:47  lr: 0.000119  min_lr: 0.000000  loss: 1.5029 (1.5715)  class_acc: 0.8750 (0.8518)  loss_scale: 32768.0000 (44106.5702)  weight_decay: 0.0500 (0.0500)  time: 0.5607  data: 0.0002  max mem: 27806
Epoch: [20]  [2200/2502]  eta: 0:02:51  lr: 0.000118  min_lr: 0.000000  loss: 1.5752 (1.5705)  class_acc: 0.8594 (0.8520)  loss_scale: 32768.0000 (43591.4148)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.0002  max mem: 27806
Epoch: [20]  [2300/2502]  eta: 0:01:54  lr: 0.000117  min_lr: 0.000000  loss: 1.5625 (1.5699)  class_acc: 0.8438 (0.8521)  loss_scale: 65536.0000 (43491.2960)  weight_decay: 0.0500 (0.0500)  time: 0.5681  data: 0.0002  max mem: 27806
Epoch: [20]  [2400/2502]  eta: 0:00:57  lr: 0.000116  min_lr: 0.000000  loss: 1.6357 (1.5712)  class_acc: 0.8281 (0.8518)  loss_scale: 65536.0000 (44409.4427)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0003  max mem: 27806
Epoch: [20]  [2500/2502]  eta: 0:00:01  lr: 0.000116  min_lr: 0.000000  loss: 1.4932 (1.5709)  class_acc: 0.8594 (0.8518)  loss_scale: 65536.0000 (45246.0544)  weight_decay: 0.0500 (0.0500)  time: 0.5335  data: 0.0006  max mem: 27806
Epoch: [20]  [2501/2502]  eta: 0:00:00  lr: 0.000116  min_lr: 0.000000  loss: 1.4932 (1.5709)  class_acc: 0.8594 (0.8518)  loss_scale: 65536.0000 (45246.0544)  weight_decay: 0.0500 (0.0500)  time: 0.5062  data: 0.0006  max mem: 27806
Epoch: [20] Total time: 0:23:35 (0.5658 s / it)
Averaged stats: lr: 0.000116  min_lr: 0.000000  loss: 1.4932 (1.5733)  class_acc: 0.8594 (0.8516)  loss_scale: 65536.0000 (45246.0544)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Test:  [ 0/98]  eta: 0:02:43  loss: 0.1780 (0.1780)  acc1: 95.3125 (95.3125)  acc5: 100.0000 (100.0000)  time: 1.6706  data: 1.4647  max mem: 27806
Test:  [97/98]  eta: 0:00:00  loss: 0.6058 (0.6111)  acc1: 87.5000 (87.3280)  acc5: 96.8750 (97.6960)  time: 0.2012  data: 0.0001  max mem: 27806
Test: Total time: 0:00:21 (0.2205 s / it)
* Acc@1 86.920 Acc@5 97.852 loss 0.612
Accuracy of the network on the 50000 test images: 86.9%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Test:  [ 0/98]  eta: 0:02:46  loss: 0.1811 (0.1811)  acc1: 93.7500 (93.7500)  acc5: 100.0000 (100.0000)  time: 1.7013  data: 1.4943  max mem: 27806
Test:  [97/98]  eta: 0:00:00  loss: 0.5202 (0.5174)  acc1: 87.5000 (88.2720)  acc5: 98.4375 (98.3520)  time: 0.2044  data: 0.0001  max mem: 27806
Test: Total time: 0:00:21 (0.2236 s / it)
* Acc@1 87.948 Acc@5 98.420 loss 0.516
EMA Accuracy of the network on the 50000 test images: 87.9%
Max accuracy: 87.97%
{"train_lr": 0.00012712609985305307, "train_min_lr": 2.6734289054693203e-09, "train_loss": 1.573287890625, "train_class_acc": 0.8515546875, "train_loss_scale": 45246.0544, "train_weight_decay": 0.04999999999999801, "test_loss": 0.5159243353243385, "test_acc1": 87.9480000149536, "test_acc5": 98.42000000244141, "epoch": 20, "n_parameters": 303602664}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Epoch: [21]  [   0/2502]  eta: 9:54:46  lr: 0.000116  min_lr: 0.000000  loss: 1.6240 (1.6240)  class_acc: 0.8281 (0.8281)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 14.2630  data: 12.9531  max mem: 27806
Epoch: [21]  [ 100/2502]  eta: 0:27:57  lr: 0.000115  min_lr: 0.000000  loss: 1.5010 (1.5753)  class_acc: 0.8594 (0.8521)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5623  data: 0.0004  max mem: 27806
Epoch: [21]  [ 200/2502]  eta: 0:24:12  lr: 0.000114  min_lr: 0.000000  loss: 1.5439 (1.5580)  class_acc: 0.8594 (0.8562)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5616  data: 0.0004  max mem: 27806
Epoch: [21]  [ 300/2502]  eta: 0:22:18  lr: 0.000113  min_lr: 0.000000  loss: 1.5596 (1.5564)  class_acc: 0.8438 (0.8569)  loss_scale: 131072.0000 (68584.1860)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.0004  max mem: 27806
Epoch: [21]  [ 400/2502]  eta: 0:20:52  lr: 0.000112  min_lr: 0.000000  loss: 1.5664 (1.5552)  class_acc: 0.8594 (0.8564)  loss_scale: 131072.0000 (84167.1820)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0002  max mem: 27806
Epoch: [21]  [ 500/2502]  eta: 0:19:39  lr: 0.000111  min_lr: 0.000000  loss: 1.5762 (1.5575)  class_acc: 0.8594 (0.8570)  loss_scale: 65536.0000 (84372.6946)  weight_decay: 0.0500 (0.0500)  time: 0.5616  data: 0.0002  max mem: 27806
Epoch: [21]  [ 600/2502]  eta: 0:18:31  lr: 0.000110  min_lr: 0.000000  loss: 1.6318 (1.5567)  class_acc: 0.8438 (0.8570)  loss_scale: 65536.0000 (81238.4692)  weight_decay: 0.0500 (0.0500)  time: 0.5607  data: 0.0003  max mem: 27806
Epoch: [21]  [ 700/2502]  eta: 0:17:27  lr: 0.000109  min_lr: 0.000000  loss: 1.5850 (1.5591)  class_acc: 0.8438 (0.8564)  loss_scale: 65536.0000 (78998.4593)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.0003  max mem: 27806
Epoch: [21]  [ 800/2502]  eta: 0:16:25  lr: 0.000108  min_lr: 0.000000  loss: 1.5537 (1.5562)  class_acc: 0.8438 (0.8573)  loss_scale: 65536.0000 (77317.7528)  weight_decay: 0.0500 (0.0500)  time: 0.5619  data: 0.0003  max mem: 27806
Epoch: [21]  [ 900/2502]  eta: 0:15:24  lr: 0.000107  min_lr: 0.000000  loss: 1.5088 (1.5580)  class_acc: 0.8438 (0.8568)  loss_scale: 65536.0000 (76010.1221)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.0003  max mem: 27806
Epoch: [21]  [1000/2502]  eta: 0:14:23  lr: 0.000107  min_lr: 0.000000  loss: 1.5361 (1.5573)  class_acc: 0.8594 (0.8567)  loss_scale: 65536.0000 (75749.4026)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0003  max mem: 27806
Epoch: [21]  [1100/2502]  eta: 0:13:24  lr: 0.000106  min_lr: 0.000000  loss: 1.4502 (1.5568)  class_acc: 0.8750 (0.8570)  loss_scale: 65536.0000 (74821.7548)  weight_decay: 0.0500 (0.0500)  time: 0.5607  data: 0.0003  max mem: 27806
Epoch: [21]  [1200/2502]  eta: 0:12:25  lr: 0.000105  min_lr: 0.000000  loss: 1.5439 (1.5568)  class_acc: 0.8594 (0.8571)  loss_scale: 65536.0000 (74048.5862)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.0002  max mem: 27806
Epoch: [21]  [1300/2502]  eta: 0:11:27  lr: 0.000104  min_lr: 0.000000  loss: 1.5127 (1.5560)  class_acc: 0.8594 (0.8569)  loss_scale: 65536.0000 (73394.2752)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.0003  max mem: 27806
Epoch: [21]  [1400/2502]  eta: 0:10:29  lr: 0.000103  min_lr: 0.000000  loss: 1.5156 (1.5570)  class_acc: 0.8594 (0.8564)  loss_scale: 65536.0000 (72833.3704)  weight_decay: 0.0500 (0.0500)  time: 0.5618  data: 0.0003  max mem: 27806
Epoch: [21]  [1500/2502]  eta: 0:09:31  lr: 0.000102  min_lr: 0.000000  loss: 1.5586 (1.5575)  class_acc: 0.8594 (0.8561)  loss_scale: 65536.0000 (72521.8494)  weight_decay: 0.0500 (0.0500)  time: 0.5603  data: 0.0004  max mem: 27806
Epoch: [21]  [1600/2502]  eta: 0:08:34  lr: 0.000101  min_lr: 0.000000  loss: 1.5244 (1.5569)  class_acc: 0.8594 (0.8563)  loss_scale: 65536.0000 (72085.5066)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.0004  max mem: 27806
Epoch: [21]  [1700/2502]  eta: 0:07:36  lr: 0.000100  min_lr: 0.000000  loss: 1.5361 (1.5557)  class_acc: 0.8594 (0.8563)  loss_scale: 65536.0000 (71700.4680)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0003  max mem: 27806
Epoch: [21]  [1800/2502]  eta: 0:06:39  lr: 0.000100  min_lr: 0.000000  loss: 1.4756 (1.5556)  class_acc: 0.8750 (0.8564)  loss_scale: 65536.0000 (71358.1877)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0002  max mem: 27806
Epoch: [21]  [1900/2502]  eta: 0:05:42  lr: 0.000099  min_lr: 0.000000  loss: 1.4814 (1.5549)  class_acc: 0.8594 (0.8565)  loss_scale: 65536.0000 (71051.9179)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0003  max mem: 27806
Epoch: [21]  [2000/2502]  eta: 0:04:45  lr: 0.000098  min_lr: 0.000000  loss: 1.5439 (1.5546)  class_acc: 0.8594 (0.8567)  loss_scale: 65536.0000 (70972.7696)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.0002  max mem: 27806
Epoch: [21]  [2100/2502]  eta: 0:03:48  lr: 0.000097  min_lr: 0.000000  loss: 1.4980 (1.5535)  class_acc: 0.8750 (0.8570)  loss_scale: 65536.0000 (72772.7216)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0003  max mem: 27806
Epoch: [21]  [2200/2502]  eta: 0:02:51  lr: 0.000096  min_lr: 0.000000  loss: 1.5059 (1.5525)  class_acc: 0.8750 (0.8570)  loss_scale: 65536.0000 (72443.9291)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0003  max mem: 27806
Epoch: [21]  [2300/2502]  eta: 0:01:54  lr: 0.000095  min_lr: 0.000000  loss: 1.6084 (1.5534)  class_acc: 0.8438 (0.8567)  loss_scale: 65536.0000 (72143.7149)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.0003  max mem: 27806
Epoch: [21]  [2400/2502]  eta: 0:00:57  lr: 0.000094  min_lr: 0.000000  loss: 1.5176 (1.5540)  class_acc: 0.8438 (0.8566)  loss_scale: 65536.0000 (71868.5081)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0002  max mem: 27806
Epoch: [21]  [2500/2502]  eta: 0:00:01  lr: 0.000094  min_lr: 0.000000  loss: 1.5713 (1.5551)  class_acc: 0.8438 (0.8563)  loss_scale: 65536.0000 (71617.7408)  weight_decay: 0.0500 (0.0500)  time: 0.5338  data: 0.0007  max mem: 27806
Epoch: [21]  [2501/2502]  eta: 0:00:00  lr: 0.000094  min_lr: 0.000000  loss: 1.5713 (1.5551)  class_acc: 0.8438 (0.8563)  loss_scale: 65536.0000 (71617.7408)  weight_decay: 0.0500 (0.0500)  time: 0.5064  data: 0.0007  max mem: 27806
Epoch: [21] Total time: 0:23:38 (0.5668 s / it)
Averaged stats: lr: 0.000094  min_lr: 0.000000  loss: 1.5713 (1.5576)  class_acc: 0.8438 (0.8559)  loss_scale: 65536.0000 (71617.7408)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Test:  [ 0/98]  eta: 0:02:21  loss: 0.1783 (0.1783)  acc1: 95.3125 (95.3125)  acc5: 100.0000 (100.0000)  time: 1.4416  data: 1.2355  max mem: 27806
Test:  [97/98]  eta: 0:00:00  loss: 0.6524 (0.6162)  acc1: 85.9375 (87.4080)  acc5: 96.8750 (97.6000)  time: 0.2012  data: 0.0001  max mem: 27806
Test: Total time: 0:00:21 (0.2182 s / it)
* Acc@1 86.946 Acc@5 97.832 loss 0.616
Accuracy of the network on the 50000 test images: 86.9%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Test:  [ 0/98]  eta: 0:02:16  loss: 0.1741 (0.1741)  acc1: 95.3125 (95.3125)  acc5: 100.0000 (100.0000)  time: 1.3939  data: 1.1869  max mem: 27806
Test:  [97/98]  eta: 0:00:00  loss: 0.5196 (0.5197)  acc1: 87.5000 (88.3200)  acc5: 98.4375 (98.3520)  time: 0.2034  data: 0.0001  max mem: 27806
Test: Total time: 0:00:21 (0.2196 s / it)
* Acc@1 87.886 Acc@5 98.416 loss 0.519
EMA Accuracy of the network on the 50000 test images: 87.9%
Max accuracy: 87.97%
{"train_lr": 0.00010447093119218069, "train_min_lr": 2.196996584913038e-09, "train_loss": 1.557635302734375, "train_class_acc": 0.8559015625, "train_loss_scale": 71617.7408, "train_weight_decay": 0.04999999999999801, "test_loss": 0.5186806692912871, "test_acc1": 87.88600001373291, "test_acc5": 98.41600000244141, "epoch": 21, "n_parameters": 303602664}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Epoch: [22]  [   0/2502]  eta: 10:01:19  lr: 0.000094  min_lr: 0.000000  loss: 1.6143 (1.6143)  class_acc: 0.8281 (0.8281)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 14.4201  data: 12.3131  max mem: 27806
Epoch: [22]  [ 100/2502]  eta: 0:27:58  lr: 0.000093  min_lr: 0.000000  loss: 1.4697 (1.5299)  class_acc: 0.8594 (0.8580)  loss_scale: 131072.0000 (77215.6832)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.0002  max mem: 27806
Epoch: [22]  [ 200/2502]  eta: 0:24:11  lr: 0.000092  min_lr: 0.000000  loss: 1.4580 (1.5278)  class_acc: 0.8594 (0.8590)  loss_scale: 131072.0000 (104009.8706)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0002  max mem: 27806
Epoch: [22]  [ 300/2502]  eta: 0:22:19  lr: 0.000091  min_lr: 0.000000  loss: 1.6377 (1.5339)  class_acc: 0.8438 (0.8586)  loss_scale: 131072.0000 (113000.6113)  weight_decay: 0.0500 (0.0500)  time: 0.5688  data: 0.0002  max mem: 27806
Epoch: [22]  [ 400/2502]  eta: 0:20:53  lr: 0.000090  min_lr: 0.000000  loss: 1.5977 (1.5469)  class_acc: 0.8438 (0.8562)  loss_scale: 65536.0000 (112604.2494)  weight_decay: 0.0500 (0.0500)  time: 0.5607  data: 0.0002  max mem: 27806
Epoch: [22]  [ 500/2502]  eta: 0:19:39  lr: 0.000089  min_lr: 0.000000  loss: 1.4814 (1.5467)  class_acc: 0.8750 (0.8576)  loss_scale: 65536.0000 (103209.3892)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.0003  max mem: 27806
Epoch: [22]  [ 600/2502]  eta: 0:18:32  lr: 0.000089  min_lr: 0.000000  loss: 1.4893 (1.5462)  class_acc: 0.8750 (0.8587)  loss_scale: 65536.0000 (96940.9384)  weight_decay: 0.0500 (0.0500)  time: 0.5684  data: 0.0002  max mem: 27806
Epoch: [22]  [ 700/2502]  eta: 0:17:28  lr: 0.000088  min_lr: 0.000000  loss: 1.5635 (1.5493)  class_acc: 0.8438 (0.8580)  loss_scale: 65536.0000 (92460.9187)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.0002  max mem: 27806
Epoch: [22]  [ 800/2502]  eta: 0:16:25  lr: 0.000087  min_lr: 0.000000  loss: 1.5342 (1.5503)  class_acc: 0.8750 (0.8579)  loss_scale: 65536.0000 (89099.5056)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.0004  max mem: 27806
Epoch: [22]  [ 900/2502]  eta: 0:15:24  lr: 0.000086  min_lr: 0.000000  loss: 1.5684 (1.5481)  class_acc: 0.8438 (0.8582)  loss_scale: 131072.0000 (87502.5616)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0004  max mem: 27806
Epoch: [22]  [1000/2502]  eta: 0:14:24  lr: 0.000085  min_lr: 0.000000  loss: 1.5557 (1.5494)  class_acc: 0.8438 (0.8579)  loss_scale: 65536.0000 (86224.6873)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.0004  max mem: 27806
Epoch: [22]  [1100/2502]  eta: 0:13:25  lr: 0.000084  min_lr: 0.000000  loss: 1.4902 (1.5503)  class_acc: 0.8750 (0.8581)  loss_scale: 65536.0000 (84345.6058)  weight_decay: 0.0500 (0.0500)  time: 0.5620  data: 0.0003  max mem: 27806
Epoch: [22]  [1200/2502]  eta: 0:12:26  lr: 0.000084  min_lr: 0.000000  loss: 1.5498 (1.5493)  class_acc: 0.8438 (0.8580)  loss_scale: 65536.0000 (82779.4438)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.0003  max mem: 27806
Epoch: [22]  [1300/2502]  eta: 0:11:28  lr: 0.000083  min_lr: 0.000000  loss: 1.5098 (1.5464)  class_acc: 0.8438 (0.8586)  loss_scale: 65536.0000 (81454.0446)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0002  max mem: 27806
Epoch: [22]  [1400/2502]  eta: 0:10:30  lr: 0.000082  min_lr: 0.000000  loss: 1.4932 (1.5461)  class_acc: 0.8750 (0.8587)  loss_scale: 65536.0000 (80317.8530)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.0003  max mem: 27806
Epoch: [22]  [1500/2502]  eta: 0:09:32  lr: 0.000081  min_lr: 0.000000  loss: 1.5684 (1.5459)  class_acc: 0.8594 (0.8588)  loss_scale: 131072.0000 (82389.3618)  weight_decay: 0.0500 (0.0500)  time: 0.5617  data: 0.0003  max mem: 27806
Epoch: [22]  [1600/2502]  eta: 0:08:34  lr: 0.000080  min_lr: 0.000000  loss: 1.5801 (1.5475)  class_acc: 0.8594 (0.8585)  loss_scale: 131072.0000 (85430.1262)  weight_decay: 0.0500 (0.0500)  time: 0.5621  data: 0.0004  max mem: 27806
Epoch: [22]  [1700/2502]  eta: 0:07:37  lr: 0.000080  min_lr: 0.000000  loss: 1.4600 (1.5467)  class_acc: 0.8750 (0.8585)  loss_scale: 65536.0000 (85108.1858)  weight_decay: 0.0500 (0.0500)  time: 0.5616  data: 0.0004  max mem: 27806
Epoch: [22]  [1800/2502]  eta: 0:06:39  lr: 0.000079  min_lr: 0.000000  loss: 1.5381 (1.5471)  class_acc: 0.8594 (0.8584)  loss_scale: 65536.0000 (84021.4459)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0003  max mem: 27806
Epoch: [22]  [1900/2502]  eta: 0:05:42  lr: 0.000078  min_lr: 0.000000  loss: 1.5107 (1.5470)  class_acc: 0.8594 (0.8586)  loss_scale: 65536.0000 (83049.0395)  weight_decay: 0.0500 (0.0500)  time: 0.5617  data: 0.0004  max mem: 27806
Epoch: [22]  [2000/2502]  eta: 0:04:45  lr: 0.000077  min_lr: 0.000000  loss: 1.5576 (1.5468)  class_acc: 0.8438 (0.8588)  loss_scale: 65536.0000 (82173.8251)  weight_decay: 0.0500 (0.0500)  time: 0.5604  data: 0.0003  max mem: 27806
Epoch: [22]  [2100/2502]  eta: 0:03:48  lr: 0.000076  min_lr: 0.000000  loss: 1.6211 (1.5472)  class_acc: 0.8438 (0.8583)  loss_scale: 65536.0000 (81381.9248)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0004  max mem: 27806
Epoch: [22]  [2200/2502]  eta: 0:02:51  lr: 0.000076  min_lr: 0.000000  loss: 1.4551 (1.5464)  class_acc: 0.8750 (0.8586)  loss_scale: 65536.0000 (80900.1872)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.0003  max mem: 27806
Epoch: [22]  [2300/2502]  eta: 0:01:54  lr: 0.000075  min_lr: 0.000000  loss: 1.6572 (1.5472)  class_acc: 0.8438 (0.8584)  loss_scale: 65536.0000 (80232.4694)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0004  max mem: 27806
Epoch: [22]  [2400/2502]  eta: 0:00:57  lr: 0.000074  min_lr: 0.000000  loss: 1.5059 (1.5468)  class_acc: 0.8594 (0.8584)  loss_scale: 65536.0000 (79620.3715)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.0003  max mem: 27806
Epoch: [22]  [2500/2502]  eta: 0:00:01  lr: 0.000073  min_lr: 0.000000  loss: 1.5068 (1.5458)  class_acc: 0.8594 (0.8587)  loss_scale: 65536.0000 (79062.6304)  weight_decay: 0.0500 (0.0500)  time: 0.5341  data: 0.0008  max mem: 27806
Epoch: [22]  [2501/2502]  eta: 0:00:00  lr: 0.000073  min_lr: 0.000000  loss: 1.5068 (1.5458)  class_acc: 0.8594 (0.8587)  loss_scale: 65536.0000 (79062.6304)  weight_decay: 0.0500 (0.0500)  time: 0.5067  data: 0.0008  max mem: 27806
Epoch: [22] Total time: 0:23:39 (0.5672 s / it)
Averaged stats: lr: 0.000073  min_lr: 0.000000  loss: 1.5068 (1.5465)  class_acc: 0.8594 (0.8594)  loss_scale: 65536.0000 (79062.6304)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Test:  [ 0/98]  eta: 0:03:28  loss: 0.1587 (0.1587)  acc1: 96.8750 (96.8750)  acc5: 100.0000 (100.0000)  time: 2.1249  data: 1.8942  max mem: 27806
Test:  [97/98]  eta: 0:00:00  loss: 0.6684 (0.6155)  acc1: 85.9375 (87.3280)  acc5: 96.8750 (97.6480)  time: 0.2016  data: 0.0001  max mem: 27806
Test: Total time: 0:00:22 (0.2251 s / it)
* Acc@1 86.892 Acc@5 97.806 loss 0.620
Accuracy of the network on the 50000 test images: 86.9%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Test:  [ 0/98]  eta: 0:02:47  loss: 0.1680 (0.1680)  acc1: 95.3125 (95.3125)  acc5: 100.0000 (100.0000)  time: 1.7089  data: 1.5021  max mem: 27806
Test:  [97/98]  eta: 0:00:00  loss: 0.5269 (0.5235)  acc1: 89.0625 (88.2720)  acc5: 98.4375 (98.2880)  time: 0.2033  data: 0.0001  max mem: 27806
Test: Total time: 0:00:21 (0.2230 s / it)
* Acc@1 87.844 Acc@5 98.376 loss 0.523
EMA Accuracy of the network on the 50000 test images: 87.8%
Max accuracy: 87.97%
{"train_lr": 8.333019831139274e-05, "train_min_lr": 1.752412455991942e-09, "train_loss": 1.546454931640625, "train_class_acc": 0.8593890625, "train_loss_scale": 79062.6304, "train_weight_decay": 0.04999999999999801, "test_loss": 0.5226983913429537, "test_acc1": 87.84400000854492, "test_acc5": 98.37600000366211, "epoch": 22, "n_parameters": 303602664}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Epoch: [23]  [   0/2502]  eta: 9:47:52  lr: 0.000073  min_lr: 0.000000  loss: 1.8779 (1.8779)  class_acc: 0.7500 (0.7500)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 14.0977  data: 11.7145  max mem: 27806
Epoch: [23]  [ 100/2502]  eta: 0:27:54  lr: 0.000073  min_lr: 0.000000  loss: 1.3955 (1.5091)  class_acc: 0.8906 (0.8676)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5692  data: 0.0002  max mem: 27806
Epoch: [23]  [ 200/2502]  eta: 0:24:08  lr: 0.000072  min_lr: 0.000000  loss: 1.4746 (1.5202)  class_acc: 0.8750 (0.8666)  loss_scale: 131072.0000 (77925.8905)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0002  max mem: 27806
Epoch: [23]  [ 300/2502]  eta: 0:22:16  lr: 0.000071  min_lr: 0.000000  loss: 1.5156 (1.5242)  class_acc: 0.8594 (0.8662)  loss_scale: 65536.0000 (75986.9236)  weight_decay: 0.0500 (0.0500)  time: 0.5620  data: 0.0003  max mem: 27806
Epoch: [23]  [ 400/2502]  eta: 0:20:52  lr: 0.000070  min_lr: 0.000000  loss: 1.4209 (1.5236)  class_acc: 0.8750 (0.8674)  loss_scale: 65536.0000 (73380.7082)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.0003  max mem: 27806
Epoch: [23]  [ 500/2502]  eta: 0:19:38  lr: 0.000070  min_lr: 0.000000  loss: 1.5771 (1.5269)  class_acc: 0.8594 (0.8657)  loss_scale: 65536.0000 (71814.8982)  weight_decay: 0.0500 (0.0500)  time: 0.5616  data: 0.0003  max mem: 27806
Epoch: [23]  [ 600/2502]  eta: 0:18:31  lr: 0.000069  min_lr: 0.000000  loss: 1.5674 (1.5291)  class_acc: 0.8594 (0.8644)  loss_scale: 32768.0000 (68698.3028)  weight_decay: 0.0500 (0.0500)  time: 0.5616  data: 0.0003  max mem: 27806
Epoch: [23]  [ 700/2502]  eta: 0:17:27  lr: 0.000068  min_lr: 0.000000  loss: 1.4551 (1.5267)  class_acc: 0.8594 (0.8648)  loss_scale: 32768.0000 (63572.7247)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.0003  max mem: 27806
Epoch: [23]  [ 800/2502]  eta: 0:16:25  lr: 0.000067  min_lr: 0.000000  loss: 1.4473 (1.5246)  class_acc: 0.8750 (0.8659)  loss_scale: 32768.0000 (59726.9413)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0003  max mem: 27806
Epoch: [23]  [ 900/2502]  eta: 0:15:24  lr: 0.000067  min_lr: 0.000000  loss: 1.5361 (1.5253)  class_acc: 0.8594 (0.8658)  loss_scale: 32768.0000 (56734.8280)  weight_decay: 0.0500 (0.0500)  time: 0.5607  data: 0.0002  max mem: 27806
Epoch: [23]  [1000/2502]  eta: 0:14:24  lr: 0.000066  min_lr: 0.000000  loss: 1.5625 (1.5276)  class_acc: 0.8594 (0.8649)  loss_scale: 32768.0000 (54340.5395)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.0002  max mem: 27806
Epoch: [23]  [1100/2502]  eta: 0:13:24  lr: 0.000065  min_lr: 0.000000  loss: 1.6143 (1.5287)  class_acc: 0.8281 (0.8647)  loss_scale: 65536.0000 (53035.9455)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.0002  max mem: 27806
Epoch: [23]  [1200/2502]  eta: 0:12:26  lr: 0.000064  min_lr: 0.000000  loss: 1.5098 (1.5296)  class_acc: 0.8594 (0.8645)  loss_scale: 65536.0000 (54076.7494)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0003  max mem: 27806
Epoch: [23]  [1300/2502]  eta: 0:11:27  lr: 0.000064  min_lr: 0.000000  loss: 1.5410 (1.5314)  class_acc: 0.8438 (0.8638)  loss_scale: 65536.0000 (54957.5527)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0003  max mem: 27806
Epoch: [23]  [1400/2502]  eta: 0:10:29  lr: 0.000063  min_lr: 0.000000  loss: 1.5332 (1.5327)  class_acc: 0.8438 (0.8632)  loss_scale: 65536.0000 (55712.6167)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0003  max mem: 27806
Epoch: [23]  [1500/2502]  eta: 0:09:31  lr: 0.000062  min_lr: 0.000000  loss: 1.4814 (1.5353)  class_acc: 0.8750 (0.8625)  loss_scale: 65536.0000 (56367.0726)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0002  max mem: 27806
Epoch: [23]  [1600/2502]  eta: 0:08:34  lr: 0.000061  min_lr: 0.000000  loss: 1.5498 (1.5356)  class_acc: 0.8594 (0.8623)  loss_scale: 65536.0000 (57349.1168)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0003  max mem: 27806
Epoch: [23]  [1700/2502]  eta: 0:07:36  lr: 0.000061  min_lr: 0.000000  loss: 1.4961 (1.5365)  class_acc: 0.8594 (0.8622)  loss_scale: 65536.0000 (59448.5879)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0003  max mem: 27806
Epoch: [23]  [1800/2502]  eta: 0:06:39  lr: 0.000060  min_lr: 0.000000  loss: 1.4131 (1.5362)  class_acc: 0.8906 (0.8621)  loss_scale: 65536.0000 (59786.5897)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.0003  max mem: 27806
Epoch: [23]  [1900/2502]  eta: 0:05:42  lr: 0.000059  min_lr: 0.000000  loss: 1.5264 (1.5362)  class_acc: 0.8594 (0.8621)  loss_scale: 65536.0000 (60089.0310)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.0003  max mem: 27806
Epoch: [23]  [2000/2502]  eta: 0:04:45  lr: 0.000059  min_lr: 0.000000  loss: 1.5986 (1.5361)  class_acc: 0.8438 (0.8621)  loss_scale: 65536.0000 (60361.2434)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0003  max mem: 27806
Epoch: [23]  [2100/2502]  eta: 0:03:48  lr: 0.000058  min_lr: 0.000000  loss: 1.4678 (1.5352)  class_acc: 0.8594 (0.8621)  loss_scale: 65536.0000 (60607.5431)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0002  max mem: 27806
Epoch: [23]  [2200/2502]  eta: 0:02:51  lr: 0.000057  min_lr: 0.000000  loss: 1.4463 (1.5349)  class_acc: 0.8594 (0.8622)  loss_scale: 131072.0000 (62082.0354)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0003  max mem: 27806
Epoch: [23]  [2300/2502]  eta: 0:01:54  lr: 0.000056  min_lr: 0.000000  loss: 1.5332 (1.5349)  class_acc: 0.8594 (0.8621)  loss_scale: 131072.0000 (65080.2955)  weight_decay: 0.0500 (0.0500)  time: 0.5619  data: 0.0003  max mem: 27806
Epoch: [23]  [2400/2502]  eta: 0:00:57  lr: 0.000056  min_lr: 0.000000  loss: 1.5342 (1.5354)  class_acc: 0.8438 (0.8620)  loss_scale: 32768.0000 (65017.3894)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0003  max mem: 27806
Epoch: [23]  [2500/2502]  eta: 0:00:01  lr: 0.000055  min_lr: 0.000000  loss: 1.5840 (1.5354)  class_acc: 0.8438 (0.8620)  loss_scale: 32768.0000 (63740.3136)  weight_decay: 0.0500 (0.0500)  time: 0.5339  data: 0.0007  max mem: 27806
Epoch: [23]  [2501/2502]  eta: 0:00:00  lr: 0.000055  min_lr: 0.000000  loss: 1.5840 (1.5354)  class_acc: 0.8438 (0.8620)  loss_scale: 32768.0000 (63740.3136)  weight_decay: 0.0500 (0.0500)  time: 0.5065  data: 0.0007  max mem: 27806
Epoch: [23] Total time: 0:23:37 (0.5667 s / it)
Averaged stats: lr: 0.000055  min_lr: 0.000000  loss: 1.5840 (1.5342)  class_acc: 0.8438 (0.8628)  loss_scale: 32768.0000 (63740.3136)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Test:  [ 0/98]  eta: 0:02:40  loss: 0.1709 (0.1709)  acc1: 96.8750 (96.8750)  acc5: 100.0000 (100.0000)  time: 1.6377  data: 1.4319  max mem: 27806
Test:  [97/98]  eta: 0:00:00  loss: 0.6899 (0.6246)  acc1: 85.9375 (87.3280)  acc5: 96.8750 (97.6800)  time: 0.2013  data: 0.0001  max mem: 27806
Test: Total time: 0:00:21 (0.2199 s / it)
* Acc@1 86.980 Acc@5 97.792 loss 0.624
Accuracy of the network on the 50000 test images: 87.0%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Test:  [ 0/98]  eta: 0:02:49  loss: 0.1635 (0.1635)  acc1: 96.8750 (96.8750)  acc5: 100.0000 (100.0000)  time: 1.7288  data: 1.5218  max mem: 27806
Test:  [97/98]  eta: 0:00:00  loss: 0.5338 (0.5281)  acc1: 87.5000 (88.2080)  acc5: 98.4375 (98.2720)  time: 0.2032  data: 0.0001  max mem: 27806
Test: Total time: 0:00:21 (0.2228 s / it)
* Acc@1 87.772 Acc@5 98.350 loss 0.528
EMA Accuracy of the network on the 50000 test images: 87.8%
Max accuracy: 87.97%
{"train_lr": 6.403730319707997e-05, "train_min_lr": 1.3466878760008228e-09, "train_loss": 1.534243310546875, "train_class_acc": 0.86277109375, "train_loss_scale": 63740.3136, "train_weight_decay": 0.04999999999999801, "test_loss": 0.5275402224565647, "test_acc1": 87.77200001220703, "test_acc5": 98.3500000036621, "epoch": 23, "n_parameters": 303602664}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Epoch: [24]  [   0/2502]  eta: 9:43:22  lr: 0.000055  min_lr: 0.000000  loss: 1.4102 (1.4102)  class_acc: 0.9219 (0.9219)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 13.9897  data: 11.9735  max mem: 27806
Epoch: [24]  [ 100/2502]  eta: 0:27:49  lr: 0.000054  min_lr: 0.000000  loss: 1.4756 (1.5453)  class_acc: 0.8750 (0.8626)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.0003  max mem: 27806
Epoch: [24]  [ 200/2502]  eta: 0:24:06  lr: 0.000054  min_lr: 0.000000  loss: 1.4600 (1.5361)  class_acc: 0.8750 (0.8655)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.0002  max mem: 27806
Epoch: [24]  [ 300/2502]  eta: 0:22:15  lr: 0.000053  min_lr: 0.000000  loss: 1.4814 (1.5291)  class_acc: 0.8750 (0.8661)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0002  max mem: 27806
Epoch: [24]  [ 400/2502]  eta: 0:20:51  lr: 0.000052  min_lr: 0.000000  loss: 1.4551 (1.5290)  class_acc: 0.8750 (0.8662)  loss_scale: 65536.0000 (35546.3342)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.0003  max mem: 27806
Epoch: [24]  [ 500/2502]  eta: 0:19:38  lr: 0.000052  min_lr: 0.000000  loss: 1.5117 (1.5292)  class_acc: 0.8750 (0.8653)  loss_scale: 65536.0000 (41532.2954)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0003  max mem: 27806
Epoch: [24]  [ 600/2502]  eta: 0:18:31  lr: 0.000051  min_lr: 0.000000  loss: 1.5205 (1.5249)  class_acc: 0.8750 (0.8663)  loss_scale: 65536.0000 (45526.2562)  weight_decay: 0.0500 (0.0500)  time: 0.5621  data: 0.0003  max mem: 27806
Epoch: [24]  [ 700/2502]  eta: 0:17:27  lr: 0.000050  min_lr: 0.000000  loss: 1.4785 (1.5257)  class_acc: 0.8750 (0.8658)  loss_scale: 65536.0000 (48380.7133)  weight_decay: 0.0500 (0.0500)  time: 0.5619  data: 0.0002  max mem: 27806
Epoch: [24]  [ 800/2502]  eta: 0:16:24  lr: 0.000050  min_lr: 0.000000  loss: 1.4814 (1.5245)  class_acc: 0.8750 (0.8660)  loss_scale: 65536.0000 (50522.4469)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0002  max mem: 27806
Epoch: [24]  [ 900/2502]  eta: 0:15:23  lr: 0.000049  min_lr: 0.000000  loss: 1.4414 (1.5271)  class_acc: 0.8906 (0.8655)  loss_scale: 131072.0000 (53788.9811)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0002  max mem: 27806
Epoch: [24]  [1000/2502]  eta: 0:14:24  lr: 0.000048  min_lr: 0.000000  loss: 1.5488 (1.5292)  class_acc: 0.8594 (0.8647)  loss_scale: 131072.0000 (61378.6214)  weight_decay: 0.0500 (0.0500)  time: 0.5664  data: 0.0002  max mem: 27806
Epoch: [24]  [1100/2502]  eta: 0:13:24  lr: 0.000048  min_lr: 0.000000  loss: 1.5059 (1.5295)  class_acc: 0.8594 (0.8642)  loss_scale: 65536.0000 (61756.2216)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.0002  max mem: 27806
Epoch: [24]  [1200/2502]  eta: 0:12:26  lr: 0.000047  min_lr: 0.000000  loss: 1.4531 (1.5284)  class_acc: 0.8750 (0.8646)  loss_scale: 65536.0000 (62070.9409)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.0002  max mem: 27806
Epoch: [24]  [1300/2502]  eta: 0:11:27  lr: 0.000046  min_lr: 0.000000  loss: 1.5664 (1.5270)  class_acc: 0.8750 (0.8650)  loss_scale: 65536.0000 (62337.2790)  weight_decay: 0.0500 (0.0500)  time: 0.5617  data: 0.0002  max mem: 27806
Epoch: [24]  [1400/2502]  eta: 0:10:29  lr: 0.000046  min_lr: 0.000000  loss: 1.4658 (1.5265)  class_acc: 0.8750 (0.8652)  loss_scale: 65536.0000 (62565.5960)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0003  max mem: 27806
Epoch: [24]  [1500/2502]  eta: 0:09:31  lr: 0.000045  min_lr: 0.000000  loss: 1.4844 (1.5261)  class_acc: 0.8594 (0.8653)  loss_scale: 65536.0000 (62763.4910)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0002  max mem: 27806
Epoch: [24]  [1600/2502]  eta: 0:08:34  lr: 0.000045  min_lr: 0.000000  loss: 1.5283 (1.5269)  class_acc: 0.8594 (0.8649)  loss_scale: 131072.0000 (66457.0244)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0003  max mem: 27806
Epoch: [24]  [1700/2502]  eta: 0:07:36  lr: 0.000044  min_lr: 0.000000  loss: 1.4717 (1.5289)  class_acc: 0.8594 (0.8645)  loss_scale: 65536.0000 (69099.8330)  weight_decay: 0.0500 (0.0500)  time: 0.5681  data: 0.0002  max mem: 27806
Epoch: [24]  [1800/2502]  eta: 0:06:39  lr: 0.000043  min_lr: 0.000000  loss: 1.5264 (1.5295)  class_acc: 0.8594 (0.8643)  loss_scale: 65536.0000 (68901.9522)  weight_decay: 0.0500 (0.0500)  time: 0.5607  data: 0.0002  max mem: 27806
Epoch: [24]  [1900/2502]  eta: 0:05:42  lr: 0.000043  min_lr: 0.000000  loss: 1.5244 (1.5298)  class_acc: 0.8438 (0.8640)  loss_scale: 65536.0000 (68724.8901)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.0003  max mem: 27806
Epoch: [24]  [2000/2502]  eta: 0:04:45  lr: 0.000042  min_lr: 0.000000  loss: 1.5205 (1.5297)  class_acc: 0.8594 (0.8641)  loss_scale: 65536.0000 (68565.5252)  weight_decay: 0.0500 (0.0500)  time: 0.5721  data: 0.0002  max mem: 27806
Epoch: [24]  [2100/2502]  eta: 0:03:48  lr: 0.000041  min_lr: 0.000000  loss: 1.4873 (1.5295)  class_acc: 0.8594 (0.8642)  loss_scale: 65536.0000 (68421.3308)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.0002  max mem: 27806
Epoch: [24]  [2200/2502]  eta: 0:02:51  lr: 0.000041  min_lr: 0.000000  loss: 1.4609 (1.5301)  class_acc: 0.8750 (0.8644)  loss_scale: 131072.0000 (68707.0968)  weight_decay: 0.0500 (0.0500)  time: 0.5624  data: 0.0002  max mem: 27806
Epoch: [24]  [2300/2502]  eta: 0:01:54  lr: 0.000040  min_lr: 0.000000  loss: 1.4482 (1.5303)  class_acc: 0.8750 (0.8643)  loss_scale: 131072.0000 (71417.4359)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.0002  max mem: 27806
Epoch: [24]  [2400/2502]  eta: 0:00:57  lr: 0.000040  min_lr: 0.000000  loss: 1.4746 (1.5298)  class_acc: 0.8750 (0.8644)  loss_scale: 65536.0000 (72537.2428)  weight_decay: 0.0500 (0.0500)  time: 0.5616  data: 0.0002  max mem: 27806
Epoch: [24]  [2500/2502]  eta: 0:00:01  lr: 0.000039  min_lr: 0.000000  loss: 1.4961 (1.5296)  class_acc: 0.8594 (0.8645)  loss_scale: 65536.0000 (72259.9936)  weight_decay: 0.0500 (0.0500)  time: 0.5345  data: 0.0007  max mem: 27806
Epoch: [24]  [2501/2502]  eta: 0:00:00  lr: 0.000039  min_lr: 0.000000  loss: 1.4961 (1.5296)  class_acc: 0.8594 (0.8645)  loss_scale: 65536.0000 (72259.9936)  weight_decay: 0.0500 (0.0500)  time: 0.5072  data: 0.0007  max mem: 27806
Epoch: [24] Total time: 0:23:38 (0.5669 s / it)
Averaged stats: lr: 0.000039  min_lr: 0.000000  loss: 1.4961 (1.5290)  class_acc: 0.8594 (0.8645)  loss_scale: 65536.0000 (72259.9936)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Test:  [ 0/98]  eta: 0:02:36  loss: 0.1615 (0.1615)  acc1: 98.4375 (98.4375)  acc5: 100.0000 (100.0000)  time: 1.5919  data: 1.3485  max mem: 27806
Test:  [97/98]  eta: 0:00:00  loss: 0.6581 (0.6271)  acc1: 85.9375 (87.4720)  acc5: 96.8750 (97.6800)  time: 0.2012  data: 0.0001  max mem: 27806
Test: Total time: 0:00:21 (0.2194 s / it)
* Acc@1 86.950 Acc@5 97.772 loss 0.629
Accuracy of the network on the 50000 test images: 87.0%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Test:  [ 0/98]  eta: 0:02:52  loss: 0.1599 (0.1599)  acc1: 96.8750 (96.8750)  acc5: 100.0000 (100.0000)  time: 1.7587  data: 1.5517  max mem: 27806
Test:  [97/98]  eta: 0:00:00  loss: 0.5416 (0.5336)  acc1: 89.0625 (88.2720)  acc5: 98.4375 (98.2400)  time: 0.2048  data: 0.0001  max mem: 27806
Test: Total time: 0:00:21 (0.2245 s / it)
* Acc@1 87.700 Acc@5 98.336 loss 0.533
EMA Accuracy of the network on the 50000 test images: 87.7%
Max accuracy: 87.97%
{"train_lr": 4.689650633021357e-05, "train_min_lr": 9.862213639342325e-10, "train_loss": 1.52896005859375, "train_class_acc": 0.86448671875, "train_loss_scale": 72259.9936, "train_weight_decay": 0.04999999999999801, "test_loss": 0.5329827316181392, "test_acc1": 87.70000001464844, "test_acc5": 98.33600000366211, "epoch": 24, "n_parameters": 303602664}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Epoch: [25]  [   0/2502]  eta: 9:43:19  lr: 0.000039  min_lr: 0.000000  loss: 1.4336 (1.4336)  class_acc: 0.9062 (0.9062)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 13.9885  data: 13.3200  max mem: 27806
Epoch: [25]  [ 100/2502]  eta: 0:27:51  lr: 0.000039  min_lr: 0.000000  loss: 1.4346 (1.5288)  class_acc: 0.8750 (0.8680)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.0003  max mem: 27806
Epoch: [25]  [ 200/2502]  eta: 0:24:09  lr: 0.000038  min_lr: 0.000000  loss: 1.5234 (1.5235)  class_acc: 0.8438 (0.8668)  loss_scale: 16384.0000 (50374.6866)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.0002  max mem: 27806
Epoch: [25]  [ 300/2502]  eta: 0:22:16  lr: 0.000037  min_lr: 0.000000  loss: 1.5303 (1.5288)  class_acc: 0.8594 (0.8643)  loss_scale: 16384.0000 (39082.0997)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0002  max mem: 27806
Epoch: [25]  [ 400/2502]  eta: 0:20:51  lr: 0.000037  min_lr: 0.000000  loss: 1.5215 (1.5257)  class_acc: 0.8438 (0.8649)  loss_scale: 16384.0000 (33421.7257)  weight_decay: 0.0500 (0.0500)  time: 0.5607  data: 0.0002  max mem: 27806
Epoch: [25]  [ 500/2502]  eta: 0:19:38  lr: 0.000036  min_lr: 0.000000  loss: 1.5303 (1.5287)  class_acc: 0.8594 (0.8643)  loss_scale: 16384.0000 (30020.9820)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.0002  max mem: 27806
Epoch: [25]  [ 600/2502]  eta: 0:18:31  lr: 0.000036  min_lr: 0.000000  loss: 1.4551 (1.5308)  class_acc: 0.8906 (0.8642)  loss_scale: 16384.0000 (27751.9334)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0003  max mem: 27806
Epoch: [25]  [ 700/2502]  eta: 0:17:27  lr: 0.000035  min_lr: 0.000000  loss: 1.4287 (1.5259)  class_acc: 0.8906 (0.8655)  loss_scale: 16384.0000 (26270.4936)  weight_decay: 0.0500 (0.0500)  time: 0.5604  data: 0.0002  max mem: 27806
Epoch: [25]  [ 800/2502]  eta: 0:16:24  lr: 0.000035  min_lr: 0.000000  loss: 1.5020 (1.5249)  class_acc: 0.8594 (0.8657)  loss_scale: 32768.0000 (27081.6679)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0002  max mem: 27806
Epoch: [25]  [ 900/2502]  eta: 0:15:23  lr: 0.000034  min_lr: 0.000000  loss: 1.4932 (1.5253)  class_acc: 0.8750 (0.8655)  loss_scale: 32768.0000 (27712.7814)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0003  max mem: 27806
Epoch: [25]  [1000/2502]  eta: 0:14:24  lr: 0.000033  min_lr: 0.000000  loss: 1.4297 (1.5247)  class_acc: 0.8594 (0.8656)  loss_scale: 32768.0000 (28217.7982)  weight_decay: 0.0500 (0.0500)  time: 0.5697  data: 0.0002  max mem: 27806
Epoch: [25]  [1100/2502]  eta: 0:13:24  lr: 0.000033  min_lr: 0.000000  loss: 1.5205 (1.5238)  class_acc: 0.8438 (0.8661)  loss_scale: 32768.0000 (28631.0772)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.0003  max mem: 27806
Epoch: [25]  [1200/2502]  eta: 0:12:26  lr: 0.000032  min_lr: 0.000000  loss: 1.5322 (1.5233)  class_acc: 0.8594 (0.8662)  loss_scale: 32768.0000 (28975.5337)  weight_decay: 0.0500 (0.0500)  time: 0.5616  data: 0.0002  max mem: 27806
Epoch: [25]  [1300/2502]  eta: 0:11:27  lr: 0.000032  min_lr: 0.000000  loss: 1.5322 (1.5241)  class_acc: 0.8594 (0.8660)  loss_scale: 65536.0000 (31634.5949)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.0003  max mem: 27806
Epoch: [25]  [1400/2502]  eta: 0:10:29  lr: 0.000031  min_lr: 0.000000  loss: 1.4795 (1.5231)  class_acc: 0.8594 (0.8660)  loss_scale: 65536.0000 (34054.3954)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0002  max mem: 27806
Epoch: [25]  [1500/2502]  eta: 0:09:31  lr: 0.000031  min_lr: 0.000000  loss: 1.4805 (1.5230)  class_acc: 0.8594 (0.8659)  loss_scale: 65536.0000 (36151.7708)  weight_decay: 0.0500 (0.0500)  time: 0.5621  data: 0.0002  max mem: 27806
Epoch: [25]  [1600/2502]  eta: 0:08:34  lr: 0.000030  min_lr: 0.000000  loss: 1.5303 (1.5234)  class_acc: 0.8750 (0.8661)  loss_scale: 65536.0000 (37987.1380)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0002  max mem: 27806
Epoch: [25]  [1700/2502]  eta: 0:07:36  lr: 0.000030  min_lr: 0.000000  loss: 1.4844 (1.5241)  class_acc: 0.8750 (0.8658)  loss_scale: 65536.0000 (39606.7066)  weight_decay: 0.0500 (0.0500)  time: 0.5607  data: 0.0002  max mem: 27806
Epoch: [25]  [1800/2502]  eta: 0:06:39  lr: 0.000029  min_lr: 0.000000  loss: 1.4639 (1.5243)  class_acc: 0.8906 (0.8658)  loss_scale: 32768.0000 (39554.4875)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.0002  max mem: 27806
Epoch: [25]  [1900/2502]  eta: 0:05:42  lr: 0.000029  min_lr: 0.000000  loss: 1.5684 (1.5251)  class_acc: 0.8594 (0.8658)  loss_scale: 32768.0000 (39197.4918)  weight_decay: 0.0500 (0.0500)  time: 0.5618  data: 0.0002  max mem: 27806
Epoch: [25]  [2000/2502]  eta: 0:04:45  lr: 0.000028  min_lr: 0.000000  loss: 1.5127 (1.5246)  class_acc: 0.8750 (0.8659)  loss_scale: 32768.0000 (38876.1779)  weight_decay: 0.0500 (0.0500)  time: 0.5619  data: 0.0002  max mem: 27806
Epoch: [25]  [2100/2502]  eta: 0:03:48  lr: 0.000028  min_lr: 0.000000  loss: 1.4385 (1.5239)  class_acc: 0.8750 (0.8661)  loss_scale: 32768.0000 (38585.4507)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0002  max mem: 27806
Epoch: [25]  [2200/2502]  eta: 0:02:51  lr: 0.000027  min_lr: 0.000000  loss: 1.4727 (1.5239)  class_acc: 0.8750 (0.8661)  loss_scale: 32768.0000 (38321.1413)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0002  max mem: 27806
Epoch: [25]  [2300/2502]  eta: 0:01:54  lr: 0.000027  min_lr: 0.000000  loss: 1.4941 (1.5243)  class_acc: 0.8750 (0.8661)  loss_scale: 65536.0000 (39019.6958)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.0003  max mem: 27806
Epoch: [25]  [2400/2502]  eta: 0:00:57  lr: 0.000026  min_lr: 0.000000  loss: 1.5195 (1.5243)  class_acc: 0.8594 (0.8661)  loss_scale: 65536.0000 (40124.0816)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.0003  max mem: 27806
Epoch: [25]  [2500/2502]  eta: 0:00:01  lr: 0.000026  min_lr: 0.000000  loss: 1.5557 (1.5243)  class_acc: 0.8594 (0.8661)  loss_scale: 32768.0000 (40016.2816)  weight_decay: 0.0500 (0.0500)  time: 0.5336  data: 0.0007  max mem: 27806
Epoch: [25]  [2501/2502]  eta: 0:00:00  lr: 0.000026  min_lr: 0.000000  loss: 1.5557 (1.5243)  class_acc: 0.8594 (0.8661)  loss_scale: 32768.0000 (40016.2816)  weight_decay: 0.0500 (0.0500)  time: 0.5062  data: 0.0007  max mem: 27806
Epoch: [25] Total time: 0:23:37 (0.5667 s / it)
Averaged stats: lr: 0.000026  min_lr: 0.000000  loss: 1.5557 (1.5186)  class_acc: 0.8594 (0.8677)  loss_scale: 32768.0000 (40016.2816)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Test:  [ 0/98]  eta: 0:03:20  loss: 0.1613 (0.1613)  acc1: 98.4375 (98.4375)  acc5: 100.0000 (100.0000)  time: 2.0447  data: 1.8027  max mem: 27806
Test:  [97/98]  eta: 0:00:00  loss: 0.7004 (0.6256)  acc1: 87.5000 (87.5040)  acc5: 96.8750 (97.6800)  time: 0.2010  data: 0.0001  max mem: 27806
Test: Total time: 0:00:22 (0.2250 s / it)
* Acc@1 86.932 Acc@5 97.762 loss 0.631
Accuracy of the network on the 50000 test images: 86.9%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Test:  [ 0/98]  eta: 0:02:54  loss: 0.1573 (0.1573)  acc1: 98.4375 (98.4375)  acc5: 100.0000 (100.0000)  time: 1.7810  data: 1.5739  max mem: 27806
Test:  [97/98]  eta: 0:00:00  loss: 0.5470 (0.5392)  acc1: 89.0625 (88.2240)  acc5: 98.4375 (98.2560)  time: 0.2034  data: 0.0001  max mem: 27806
Test: Total time: 0:00:21 (0.2239 s / it)
* Acc@1 87.660 Acc@5 98.314 loss 0.539
EMA Accuracy of the network on the 50000 test images: 87.7%
Max accuracy: 87.97%
{"train_lr": 3.217812831679914e-05, "train_min_lr": 6.766976920197322e-10, "train_loss": 1.51857939453125, "train_class_acc": 0.86774375, "train_loss_scale": 40016.2816, "train_weight_decay": 0.04999999999999801, "test_loss": 0.538903061034424, "test_acc1": 87.66000001586914, "test_acc5": 98.3140000024414, "epoch": 25, "n_parameters": 303602664}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Epoch: [26]  [   0/2502]  eta: 10:04:10  lr: 0.000026  min_lr: 0.000000  loss: 1.5762 (1.5762)  class_acc: 0.8438 (0.8438)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 14.4887  data: 12.8654  max mem: 27806
Epoch: [26]  [ 100/2502]  eta: 0:28:02  lr: 0.000025  min_lr: 0.000000  loss: 1.4648 (1.5220)  class_acc: 0.8750 (0.8668)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5617  data: 0.0002  max mem: 27806
Epoch: [26]  [ 200/2502]  eta: 0:24:14  lr: 0.000025  min_lr: 0.000000  loss: 1.4951 (1.5158)  class_acc: 0.8594 (0.8696)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0003  max mem: 27806
Epoch: [26]  [ 300/2502]  eta: 0:22:19  lr: 0.000024  min_lr: 0.000000  loss: 1.5186 (1.5158)  class_acc: 0.8594 (0.8680)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5618  data: 0.0003  max mem: 27806
Epoch: [26]  [ 400/2502]  eta: 0:20:54  lr: 0.000024  min_lr: 0.000000  loss: 1.4365 (1.5115)  class_acc: 0.8906 (0.8699)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0004  max mem: 27806
Epoch: [26]  [ 500/2502]  eta: 0:19:40  lr: 0.000023  min_lr: 0.000000  loss: 1.4658 (1.5122)  class_acc: 0.8594 (0.8695)  loss_scale: 65536.0000 (37346.3633)  weight_decay: 0.0500 (0.0500)  time: 0.5620  data: 0.0003  max mem: 27806
Epoch: [26]  [ 600/2502]  eta: 0:18:32  lr: 0.000023  min_lr: 0.000000  loss: 1.4219 (1.5121)  class_acc: 0.8594 (0.8689)  loss_scale: 65536.0000 (42036.8186)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0003  max mem: 27806
Epoch: [26]  [ 700/2502]  eta: 0:17:28  lr: 0.000022  min_lr: 0.000000  loss: 1.5059 (1.5127)  class_acc: 0.8750 (0.8691)  loss_scale: 65536.0000 (45389.0556)  weight_decay: 0.0500 (0.0500)  time: 0.5605  data: 0.0003  max mem: 27806
Epoch: [26]  [ 800/2502]  eta: 0:16:25  lr: 0.000022  min_lr: 0.000000  loss: 1.6045 (1.5168)  class_acc: 0.8594 (0.8686)  loss_scale: 32768.0000 (46022.4719)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.0003  max mem: 27806
Epoch: [26]  [ 900/2502]  eta: 0:15:24  lr: 0.000022  min_lr: 0.000000  loss: 1.4971 (1.5141)  class_acc: 0.8750 (0.8689)  loss_scale: 32768.0000 (44551.3873)  weight_decay: 0.0500 (0.0500)  time: 0.5619  data: 0.0003  max mem: 27806
Epoch: [26]  [1000/2502]  eta: 0:14:24  lr: 0.000021  min_lr: 0.000000  loss: 1.4502 (1.5132)  class_acc: 0.8906 (0.8692)  loss_scale: 32768.0000 (43374.2258)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0003  max mem: 27806
Epoch: [26]  [1100/2502]  eta: 0:13:25  lr: 0.000021  min_lr: 0.000000  loss: 1.4951 (1.5154)  class_acc: 0.8750 (0.8687)  loss_scale: 32768.0000 (42410.8992)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.0003  max mem: 27806
Epoch: [26]  [1200/2502]  eta: 0:12:26  lr: 0.000020  min_lr: 0.000000  loss: 1.5225 (1.5145)  class_acc: 0.8594 (0.8690)  loss_scale: 32768.0000 (41607.9933)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0003  max mem: 27806
Epoch: [26]  [1300/2502]  eta: 0:11:28  lr: 0.000020  min_lr: 0.000000  loss: 1.5059 (1.5158)  class_acc: 0.8594 (0.8686)  loss_scale: 65536.0000 (41684.1199)  weight_decay: 0.0500 (0.0500)  time: 0.5625  data: 0.0003  max mem: 27806
Epoch: [26]  [1400/2502]  eta: 0:10:29  lr: 0.000019  min_lr: 0.000000  loss: 1.5547 (1.5175)  class_acc: 0.8281 (0.8683)  loss_scale: 65536.0000 (43386.6096)  weight_decay: 0.0500 (0.0500)  time: 0.5616  data: 0.0004  max mem: 27806
Epoch: [26]  [1500/2502]  eta: 0:09:32  lr: 0.000019  min_lr: 0.000000  loss: 1.5225 (1.5185)  class_acc: 0.8594 (0.8682)  loss_scale: 65536.0000 (44862.2518)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.0003  max mem: 27806
Epoch: [26]  [1600/2502]  eta: 0:08:34  lr: 0.000019  min_lr: 0.000000  loss: 1.4551 (1.5181)  class_acc: 0.8750 (0.8686)  loss_scale: 65536.0000 (46153.5540)  weight_decay: 0.0500 (0.0500)  time: 0.5621  data: 0.0003  max mem: 27806
Epoch: [26]  [1700/2502]  eta: 0:07:37  lr: 0.000018  min_lr: 0.000000  loss: 1.5068 (1.5172)  class_acc: 0.8594 (0.8685)  loss_scale: 65536.0000 (47293.0276)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.0003  max mem: 27806
Epoch: [26]  [1800/2502]  eta: 0:06:39  lr: 0.000018  min_lr: 0.000000  loss: 1.4736 (1.5163)  class_acc: 0.8594 (0.8688)  loss_scale: 131072.0000 (48960.9595)  weight_decay: 0.0500 (0.0500)  time: 0.5622  data: 0.0004  max mem: 27806
Epoch: [26]  [1900/2502]  eta: 0:05:42  lr: 0.000017  min_lr: 0.000000  loss: 1.4932 (1.5164)  class_acc: 0.8750 (0.8687)  loss_scale: 32768.0000 (50487.8864)  weight_decay: 0.0500 (0.0500)  time: 0.5605  data: 0.0004  max mem: 27806
Epoch: [26]  [2000/2502]  eta: 0:04:45  lr: 0.000017  min_lr: 0.000000  loss: 1.4355 (1.5158)  class_acc: 0.8750 (0.8687)  loss_scale: 32768.0000 (49602.3348)  weight_decay: 0.0500 (0.0500)  time: 0.5680  data: 0.0003  max mem: 27806
Epoch: [26]  [2100/2502]  eta: 0:03:48  lr: 0.000017  min_lr: 0.000000  loss: 1.4678 (1.5165)  class_acc: 0.8750 (0.8686)  loss_scale: 32768.0000 (48801.0814)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.0002  max mem: 27806
Epoch: [26]  [2200/2502]  eta: 0:02:51  lr: 0.000016  min_lr: 0.000000  loss: 1.5322 (1.5172)  class_acc: 0.8594 (0.8684)  loss_scale: 32768.0000 (48072.6361)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.0002  max mem: 27806
Epoch: [26]  [2300/2502]  eta: 0:01:54  lr: 0.000016  min_lr: 0.000000  loss: 1.5098 (1.5174)  class_acc: 0.8594 (0.8685)  loss_scale: 32768.0000 (47407.5063)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.0003  max mem: 27806
Epoch: [26]  [2400/2502]  eta: 0:00:57  lr: 0.000015  min_lr: 0.000000  loss: 1.5615 (1.5185)  class_acc: 0.8594 (0.8683)  loss_scale: 65536.0000 (47098.0292)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0003  max mem: 27806
Epoch: [26]  [2500/2502]  eta: 0:00:01  lr: 0.000015  min_lr: 0.000000  loss: 1.5010 (1.5178)  class_acc: 0.8750 (0.8684)  loss_scale: 65536.0000 (47828.1728)  weight_decay: 0.0500 (0.0500)  time: 0.5331  data: 0.0007  max mem: 27806
Epoch: [26]  [2501/2502]  eta: 0:00:00  lr: 0.000015  min_lr: 0.000000  loss: 1.5010 (1.5178)  class_acc: 0.8750 (0.8684)  loss_scale: 65536.0000 (47828.1728)  weight_decay: 0.0500 (0.0500)  time: 0.5057  data: 0.0007  max mem: 27806
Epoch: [26] Total time: 0:23:38 (0.5670 s / it)
Averaged stats: lr: 0.000015  min_lr: 0.000000  loss: 1.5010 (1.5141)  class_acc: 0.8750 (0.8692)  loss_scale: 65536.0000 (47828.1728)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Test:  [ 0/98]  eta: 0:02:39  loss: 0.1673 (0.1673)  acc1: 96.8750 (96.8750)  acc5: 100.0000 (100.0000)  time: 1.6273  data: 1.4217  max mem: 27806
Test:  [97/98]  eta: 0:00:00  loss: 0.6757 (0.6271)  acc1: 87.5000 (87.4240)  acc5: 96.8750 (97.6640)  time: 0.2014  data: 0.0001  max mem: 27806
Test: Total time: 0:00:21 (0.2200 s / it)
* Acc@1 86.998 Acc@5 97.768 loss 0.630
Accuracy of the network on the 50000 test images: 87.0%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Test:  [ 0/98]  eta: 0:02:41  loss: 0.1551 (0.1551)  acc1: 98.4375 (98.4375)  acc5: 100.0000 (100.0000)  time: 1.6486  data: 1.4415  max mem: 27806
Test:  [97/98]  eta: 0:00:00  loss: 0.5537 (0.5452)  acc1: 87.5000 (88.2080)  acc5: 98.4375 (98.0960)  time: 0.2034  data: 0.0001  max mem: 27806
Test: Total time: 0:00:21 (0.2220 s / it)
* Acc@1 87.610 Acc@5 98.272 loss 0.545
EMA Accuracy of the network on the 50000 test images: 87.6%
Max accuracy: 87.97%
{"train_lr": 2.011428677044083e-05, "train_min_lr": 4.229982334638897e-10, "train_loss": 1.51405029296875, "train_class_acc": 0.86915078125, "train_loss_scale": 47828.1728, "train_weight_decay": 0.04999999999999801, "test_loss": 0.5451352226414851, "test_acc1": 87.61000001586915, "test_acc5": 98.2720000024414, "epoch": 26, "n_parameters": 303602664}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Epoch: [27]  [   0/2502]  eta: 9:34:00  lr: 0.000015  min_lr: 0.000000  loss: 1.4785 (1.4785)  class_acc: 0.9219 (0.9219)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 13.7652  data: 12.1805  max mem: 27806
Epoch: [27]  [ 100/2502]  eta: 0:27:47  lr: 0.000015  min_lr: 0.000000  loss: 1.4668 (1.5138)  class_acc: 0.8594 (0.8637)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0004  max mem: 27806
Epoch: [27]  [ 200/2502]  eta: 0:24:05  lr: 0.000014  min_lr: 0.000000  loss: 1.4189 (1.5131)  class_acc: 0.8906 (0.8684)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0004  max mem: 27806
Epoch: [27]  [ 300/2502]  eta: 0:22:14  lr: 0.000014  min_lr: 0.000000  loss: 1.4707 (1.5197)  class_acc: 0.8594 (0.8676)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0003  max mem: 27806
Epoch: [27]  [ 400/2502]  eta: 0:20:50  lr: 0.000014  min_lr: 0.000000  loss: 1.4561 (1.5114)  class_acc: 0.8750 (0.8694)  loss_scale: 65536.0000 (67170.3142)  weight_decay: 0.0500 (0.0500)  time: 0.5623  data: 0.0003  max mem: 27806
Epoch: [27]  [ 500/2502]  eta: 0:19:37  lr: 0.000013  min_lr: 0.000000  loss: 1.5029 (1.5100)  class_acc: 0.8750 (0.8706)  loss_scale: 131072.0000 (79663.5210)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.0003  max mem: 27806
Epoch: [27]  [ 600/2502]  eta: 0:18:30  lr: 0.000013  min_lr: 0.000000  loss: 1.5586 (1.5144)  class_acc: 0.8438 (0.8687)  loss_scale: 65536.0000 (77312.8519)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0003  max mem: 27806
Epoch: [27]  [ 700/2502]  eta: 0:17:26  lr: 0.000013  min_lr: 0.000000  loss: 1.4688 (1.5134)  class_acc: 0.8906 (0.8690)  loss_scale: 65536.0000 (75352.3766)  weight_decay: 0.0500 (0.0500)  time: 0.5588  data: 0.0003  max mem: 27806
Epoch: [27]  [ 800/2502]  eta: 0:16:24  lr: 0.000012  min_lr: 0.000000  loss: 1.5439 (1.5146)  class_acc: 0.8594 (0.8690)  loss_scale: 32768.0000 (70035.9750)  weight_decay: 0.0500 (0.0500)  time: 0.5621  data: 0.0004  max mem: 27806
Epoch: [27]  [ 900/2502]  eta: 0:15:23  lr: 0.000012  min_lr: 0.000000  loss: 1.4180 (1.5116)  class_acc: 0.8906 (0.8698)  loss_scale: 32768.0000 (65899.6848)  weight_decay: 0.0500 (0.0500)  time: 0.5617  data: 0.0003  max mem: 27806
Epoch: [27]  [1000/2502]  eta: 0:14:23  lr: 0.000012  min_lr: 0.000000  loss: 1.4805 (1.5114)  class_acc: 0.8750 (0.8697)  loss_scale: 32768.0000 (62589.8262)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.0004  max mem: 27806
Epoch: [27]  [1100/2502]  eta: 0:13:24  lr: 0.000011  min_lr: 0.000000  loss: 1.4502 (1.5124)  class_acc: 0.8906 (0.8696)  loss_scale: 32768.0000 (59881.2134)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.0003  max mem: 27806
Epoch: [27]  [1200/2502]  eta: 0:12:26  lr: 0.000011  min_lr: 0.000000  loss: 1.4961 (1.5108)  class_acc: 0.8594 (0.8699)  loss_scale: 32768.0000 (57623.6603)  weight_decay: 0.0500 (0.0500)  time: 0.5607  data: 0.0002  max mem: 27806
Epoch: [27]  [1300/2502]  eta: 0:11:27  lr: 0.000011  min_lr: 0.000000  loss: 1.4678 (1.5080)  class_acc: 0.8906 (0.8709)  loss_scale: 32768.0000 (57627.3513)  weight_decay: 0.0500 (0.0500)  time: 0.5595  data: 0.0004  max mem: 27806
Epoch: [27]  [1400/2502]  eta: 0:10:29  lr: 0.000010  min_lr: 0.000000  loss: 1.4385 (1.5077)  class_acc: 0.8906 (0.8710)  loss_scale: 32768.0000 (55852.9507)  weight_decay: 0.0500 (0.0500)  time: 0.5680  data: 0.0004  max mem: 27806
Epoch: [27]  [1500/2502]  eta: 0:09:31  lr: 0.000010  min_lr: 0.000000  loss: 1.5234 (1.5080)  class_acc: 0.8750 (0.8711)  loss_scale: 32768.0000 (54314.9793)  weight_decay: 0.0500 (0.0500)  time: 0.5619  data: 0.0004  max mem: 27806
Epoch: [27]  [1600/2502]  eta: 0:08:34  lr: 0.000010  min_lr: 0.000000  loss: 1.4697 (1.5088)  class_acc: 0.8594 (0.8706)  loss_scale: 32768.0000 (52969.1343)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.0003  max mem: 27806
Epoch: [27]  [1700/2502]  eta: 0:07:36  lr: 0.000009  min_lr: 0.000000  loss: 1.4990 (1.5085)  class_acc: 0.8594 (0.8708)  loss_scale: 32768.0000 (51781.5309)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0003  max mem: 27806
Epoch: [27]  [1800/2502]  eta: 0:06:39  lr: 0.000009  min_lr: 0.000000  loss: 1.5146 (1.5087)  class_acc: 0.8750 (0.8707)  loss_scale: 32768.0000 (50725.8101)  weight_decay: 0.0500 (0.0500)  time: 0.5619  data: 0.0003  max mem: 27806
Epoch: [27]  [1900/2502]  eta: 0:05:42  lr: 0.000009  min_lr: 0.000000  loss: 1.4697 (1.5077)  class_acc: 0.8750 (0.8710)  loss_scale: 32768.0000 (50677.4961)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0002  max mem: 27806
Epoch: [27]  [2000/2502]  eta: 0:04:45  lr: 0.000009  min_lr: 0.000000  loss: 1.4775 (1.5070)  class_acc: 0.8750 (0.8712)  loss_scale: 32768.0000 (49782.4688)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.0003  max mem: 27806
Epoch: [27]  [2100/2502]  eta: 0:03:48  lr: 0.000008  min_lr: 0.000000  loss: 1.5400 (1.5083)  class_acc: 0.8594 (0.8709)  loss_scale: 32768.0000 (48972.6416)  weight_decay: 0.0500 (0.0500)  time: 0.5607  data: 0.0003  max mem: 27806
Epoch: [27]  [2200/2502]  eta: 0:02:51  lr: 0.000008  min_lr: 0.000000  loss: 1.5186 (1.5087)  class_acc: 0.8594 (0.8709)  loss_scale: 32768.0000 (48236.4016)  weight_decay: 0.0500 (0.0500)  time: 0.5613  data: 0.0003  max mem: 27806
Epoch: [27]  [2300/2502]  eta: 0:01:54  lr: 0.000008  min_lr: 0.000000  loss: 1.4785 (1.5086)  class_acc: 0.8594 (0.8708)  loss_scale: 32768.0000 (47564.1547)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0003  max mem: 27806
Epoch: [27]  [2400/2502]  eta: 0:00:57  lr: 0.000008  min_lr: 0.000000  loss: 1.6338 (1.5085)  class_acc: 0.8438 (0.8710)  loss_scale: 65536.0000 (47357.3344)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0003  max mem: 27806
Epoch: [27]  [2500/2502]  eta: 0:00:01  lr: 0.000007  min_lr: 0.000000  loss: 1.5430 (1.5090)  class_acc: 0.8750 (0.8707)  loss_scale: 65536.0000 (48077.2096)  weight_decay: 0.0500 (0.0500)  time: 0.5342  data: 0.0006  max mem: 27806
Epoch: [27]  [2501/2502]  eta: 0:00:00  lr: 0.000007  min_lr: 0.000000  loss: 1.5430 (1.5090)  class_acc: 0.8750 (0.8707)  loss_scale: 65536.0000 (48077.2096)  weight_decay: 0.0500 (0.0500)  time: 0.5069  data: 0.0006  max mem: 27806
Epoch: [27] Total time: 0:23:37 (0.5667 s / it)
Averaged stats: lr: 0.000007  min_lr: 0.000000  loss: 1.5430 (1.5096)  class_acc: 0.8750 (0.8701)  loss_scale: 65536.0000 (48077.2096)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Test:  [ 0/98]  eta: 0:02:41  loss: 0.1582 (0.1582)  acc1: 96.8750 (96.8750)  acc5: 100.0000 (100.0000)  time: 1.6439  data: 1.4380  max mem: 27806
Test:  [97/98]  eta: 0:00:00  loss: 0.6724 (0.6301)  acc1: 87.5000 (87.5360)  acc5: 96.8750 (97.6160)  time: 0.2015  data: 0.0001  max mem: 27806
Test: Total time: 0:00:21 (0.2203 s / it)
* Acc@1 87.044 Acc@5 97.744 loss 0.633
Accuracy of the network on the 50000 test images: 87.0%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Test:  [ 0/98]  eta: 0:02:44  loss: 0.1543 (0.1543)  acc1: 98.4375 (98.4375)  acc5: 100.0000 (100.0000)  time: 1.6771  data: 1.4701  max mem: 27806
Test:  [97/98]  eta: 0:00:00  loss: 0.5600 (0.5513)  acc1: 87.5000 (88.0640)  acc5: 98.4375 (98.0800)  time: 0.2039  data: 0.0001  max mem: 27806
Test: Total time: 0:00:21 (0.2227 s / it)
* Acc@1 87.550 Acc@5 98.228 loss 0.551
EMA Accuracy of the network on the 50000 test images: 87.6%
Max accuracy: 87.97%
{"train_lr": 1.0895235678914584e-05, "train_min_lr": 2.2912398028083777e-10, "train_loss": 1.50955234375, "train_class_acc": 0.8700875, "train_loss_scale": 48077.2096, "train_weight_decay": 0.04999999999999801, "test_loss": 0.5514982066364312, "test_acc1": 87.55000001708984, "test_acc5": 98.2280000024414, "epoch": 27, "n_parameters": 303602664}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Epoch: [28]  [   0/2502]  eta: 10:00:35  lr: 0.000007  min_lr: 0.000000  loss: 1.6426 (1.6426)  class_acc: 0.8594 (0.8594)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 14.4026  data: 12.1307  max mem: 27806
Epoch: [28]  [ 100/2502]  eta: 0:27:57  lr: 0.000007  min_lr: 0.000000  loss: 1.4648 (1.5002)  class_acc: 0.8750 (0.8700)  loss_scale: 32768.0000 (35039.0495)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0003  max mem: 27806
Epoch: [28]  [ 200/2502]  eta: 0:24:10  lr: 0.000007  min_lr: 0.000000  loss: 1.4756 (1.5131)  class_acc: 0.8750 (0.8693)  loss_scale: 32768.0000 (33909.1741)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0002  max mem: 27806
Epoch: [28]  [ 300/2502]  eta: 0:22:16  lr: 0.000007  min_lr: 0.000000  loss: 1.5029 (1.5144)  class_acc: 0.8906 (0.8703)  loss_scale: 32768.0000 (33530.0465)  weight_decay: 0.0500 (0.0500)  time: 0.5615  data: 0.0003  max mem: 27806
Epoch: [28]  [ 400/2502]  eta: 0:20:51  lr: 0.000006  min_lr: 0.000000  loss: 1.4775 (1.5068)  class_acc: 0.8750 (0.8726)  loss_scale: 32768.0000 (33340.0100)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0002  max mem: 27806
Epoch: [28]  [ 500/2502]  eta: 0:19:38  lr: 0.000006  min_lr: 0.000000  loss: 1.4502 (1.5084)  class_acc: 0.8750 (0.8723)  loss_scale: 32768.0000 (33225.8363)  weight_decay: 0.0500 (0.0500)  time: 0.5604  data: 0.0003  max mem: 27806
Epoch: [28]  [ 600/2502]  eta: 0:18:30  lr: 0.000006  min_lr: 0.000000  loss: 1.5283 (1.5086)  class_acc: 0.8438 (0.8719)  loss_scale: 65536.0000 (37402.4093)  weight_decay: 0.0500 (0.0500)  time: 0.5620  data: 0.0002  max mem: 27806
Epoch: [28]  [ 700/2502]  eta: 0:17:27  lr: 0.000006  min_lr: 0.000000  loss: 1.5244 (1.5067)  class_acc: 0.8594 (0.8721)  loss_scale: 65536.0000 (41415.7603)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0003  max mem: 27806
Epoch: [28]  [ 800/2502]  eta: 0:16:25  lr: 0.000005  min_lr: 0.000000  loss: 1.4463 (1.5083)  class_acc: 0.8906 (0.8721)  loss_scale: 65536.0000 (44427.0262)  weight_decay: 0.0500 (0.0500)  time: 0.5686  data: 0.0002  max mem: 27806
Epoch: [28]  [ 900/2502]  eta: 0:15:24  lr: 0.000005  min_lr: 0.000000  loss: 1.4873 (1.5087)  class_acc: 0.8594 (0.8717)  loss_scale: 65536.0000 (46769.8646)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0003  max mem: 27806
Epoch: [28]  [1000/2502]  eta: 0:14:24  lr: 0.000005  min_lr: 0.000000  loss: 1.5146 (1.5081)  class_acc: 0.9062 (0.8719)  loss_scale: 65536.0000 (48644.6034)  weight_decay: 0.0500 (0.0500)  time: 0.5616  data: 0.0004  max mem: 27806
Epoch: [28]  [1100/2502]  eta: 0:13:24  lr: 0.000005  min_lr: 0.000000  loss: 1.4473 (1.5072)  class_acc: 0.8594 (0.8717)  loss_scale: 131072.0000 (54107.3787)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0003  max mem: 27806
Epoch: [28]  [1200/2502]  eta: 0:12:25  lr: 0.000005  min_lr: 0.000000  loss: 1.5400 (1.5081)  class_acc: 0.8594 (0.8714)  loss_scale: 131072.0000 (60515.7569)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0003  max mem: 27806
Epoch: [28]  [1300/2502]  eta: 0:11:27  lr: 0.000004  min_lr: 0.000000  loss: 1.5518 (1.5105)  class_acc: 0.8750 (0.8707)  loss_scale: 131072.0000 (65938.9885)  weight_decay: 0.0500 (0.0500)  time: 0.5627  data: 0.0003  max mem: 27806
Epoch: [28]  [1400/2502]  eta: 0:10:29  lr: 0.000004  min_lr: 0.000000  loss: 1.4863 (1.5099)  class_acc: 0.8594 (0.8710)  loss_scale: 131072.0000 (70588.0257)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.0004  max mem: 27806
Epoch: [28]  [1500/2502]  eta: 0:09:31  lr: 0.000004  min_lr: 0.000000  loss: 1.4912 (1.5074)  class_acc: 0.8750 (0.8716)  loss_scale: 131072.0000 (74617.6043)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.0003  max mem: 27806
Epoch: [28]  [1600/2502]  eta: 0:08:34  lr: 0.000004  min_lr: 0.000000  loss: 1.4766 (1.5081)  class_acc: 0.8750 (0.8715)  loss_scale: 131072.0000 (78471.2755)  weight_decay: 0.0500 (0.0500)  time: 0.5616  data: 0.0003  max mem: 27806
Epoch: [28]  [1700/2502]  eta: 0:07:36  lr: 0.000004  min_lr: 0.000000  loss: 1.5635 (1.5084)  class_acc: 0.8750 (0.8714)  loss_scale: 65536.0000 (79945.4439)  weight_decay: 0.0500 (0.0500)  time: 0.5605  data: 0.0004  max mem: 27806
Epoch: [28]  [1800/2502]  eta: 0:06:39  lr: 0.000004  min_lr: 0.000000  loss: 1.4736 (1.5080)  class_acc: 0.8750 (0.8715)  loss_scale: 65536.0000 (79145.3637)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0002  max mem: 27806
Epoch: [28]  [1900/2502]  eta: 0:05:42  lr: 0.000003  min_lr: 0.000000  loss: 1.4482 (1.5074)  class_acc: 0.8750 (0.8719)  loss_scale: 65536.0000 (78429.4582)  weight_decay: 0.0500 (0.0500)  time: 0.5620  data: 0.0003  max mem: 27806
Epoch: [28]  [2000/2502]  eta: 0:04:45  lr: 0.000003  min_lr: 0.000000  loss: 1.4453 (1.5071)  class_acc: 0.8906 (0.8720)  loss_scale: 65536.0000 (77785.1074)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.0003  max mem: 27806
Epoch: [28]  [2100/2502]  eta: 0:03:48  lr: 0.000003  min_lr: 0.000000  loss: 1.4756 (1.5088)  class_acc: 0.8594 (0.8716)  loss_scale: 65536.0000 (77202.0942)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0002  max mem: 27806
Epoch: [28]  [2200/2502]  eta: 0:02:51  lr: 0.000003  min_lr: 0.000000  loss: 1.5049 (1.5093)  class_acc: 0.8750 (0.8714)  loss_scale: 131072.0000 (77446.2226)  weight_decay: 0.0500 (0.0500)  time: 0.5623  data: 0.0002  max mem: 27806
Epoch: [28]  [2300/2502]  eta: 0:01:54  lr: 0.000003  min_lr: 0.000000  loss: 1.5195 (1.5097)  class_acc: 0.8594 (0.8711)  loss_scale: 65536.0000 (77669.1317)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0002  max mem: 27806
Epoch: [28]  [2400/2502]  eta: 0:00:57  lr: 0.000003  min_lr: 0.000000  loss: 1.5039 (1.5094)  class_acc: 0.8594 (0.8712)  loss_scale: 65536.0000 (77163.7951)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.0003  max mem: 27806
Epoch: [28]  [2500/2502]  eta: 0:00:01  lr: 0.000003  min_lr: 0.000000  loss: 1.5293 (1.5098)  class_acc: 0.8438 (0.8711)  loss_scale: 65536.0000 (76703.3344)  weight_decay: 0.0500 (0.0500)  time: 0.5336  data: 0.0007  max mem: 27806
Epoch: [28]  [2501/2502]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000000  loss: 1.5293 (1.5098)  class_acc: 0.8438 (0.8711)  loss_scale: 65536.0000 (76703.3344)  weight_decay: 0.0500 (0.0500)  time: 0.5062  data: 0.0007  max mem: 27806
Epoch: [28] Total time: 0:23:38 (0.5668 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000000  loss: 1.5293 (1.5087)  class_acc: 0.8438 (0.8708)  loss_scale: 65536.0000 (76703.3344)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Test:  [ 0/98]  eta: 0:02:42  loss: 0.1692 (0.1692)  acc1: 96.8750 (96.8750)  acc5: 100.0000 (100.0000)  time: 1.6542  data: 1.4487  max mem: 27806
Test:  [97/98]  eta: 0:00:00  loss: 0.6826 (0.6315)  acc1: 87.5000 (87.4560)  acc5: 96.8750 (97.5680)  time: 0.2015  data: 0.0001  max mem: 27806
Test: Total time: 0:00:21 (0.2203 s / it)
* Acc@1 86.980 Acc@5 97.738 loss 0.634
Accuracy of the network on the 50000 test images: 87.0%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Test:  [ 0/98]  eta: 0:02:41  loss: 0.1540 (0.1540)  acc1: 98.4375 (98.4375)  acc5: 100.0000 (100.0000)  time: 1.6509  data: 1.4437  max mem: 27806
Test:  [97/98]  eta: 0:00:00  loss: 0.5662 (0.5574)  acc1: 87.5000 (88.0640)  acc5: 98.4375 (98.0160)  time: 0.2034  data: 0.0001  max mem: 27806
Test: Total time: 0:00:21 (0.2223 s / it)
* Acc@1 87.514 Acc@5 98.186 loss 0.558
EMA Accuracy of the network on the 50000 test images: 87.5%
Max accuracy: 87.97%
{"train_lr": 4.666364985127919e-06, "train_min_lr": 9.813244525814248e-11, "train_loss": 1.508709423828125, "train_class_acc": 0.87080234375, "train_loss_scale": 76703.3344, "train_weight_decay": 0.04999999999999801, "test_loss": 0.5578392585929559, "test_acc1": 87.51400001708984, "test_acc5": 98.1860000024414, "epoch": 28, "n_parameters": 303602664}
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Epoch: [29]  [   0/2502]  eta: 9:21:03  lr: 0.000003  min_lr: 0.000000  loss: 1.4082 (1.4082)  class_acc: 0.8906 (0.8906)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 13.4548  data: 12.8629  max mem: 27806
Epoch: [29]  [ 100/2502]  eta: 0:27:39  lr: 0.000002  min_lr: 0.000000  loss: 1.5234 (1.5016)  class_acc: 0.8750 (0.8718)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5616  data: 0.0003  max mem: 27806
Epoch: [29]  [ 200/2502]  eta: 0:24:01  lr: 0.000002  min_lr: 0.000000  loss: 1.5166 (1.4990)  class_acc: 0.8750 (0.8746)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0002  max mem: 27806
Epoch: [29]  [ 300/2502]  eta: 0:22:12  lr: 0.000002  min_lr: 0.000000  loss: 1.4980 (1.4972)  class_acc: 0.8594 (0.8751)  loss_scale: 131072.0000 (78164.1993)  weight_decay: 0.0500 (0.0500)  time: 0.5692  data: 0.0002  max mem: 27806
Epoch: [29]  [ 400/2502]  eta: 0:20:49  lr: 0.000002  min_lr: 0.000000  loss: 1.5332 (1.5045)  class_acc: 0.8594 (0.8735)  loss_scale: 65536.0000 (85147.7706)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0002  max mem: 27806
Epoch: [29]  [ 500/2502]  eta: 0:19:37  lr: 0.000002  min_lr: 0.000000  loss: 1.5020 (1.5081)  class_acc: 0.8594 (0.8726)  loss_scale: 65536.0000 (81233.2455)  weight_decay: 0.0500 (0.0500)  time: 0.5608  data: 0.0002  max mem: 27806
Epoch: [29]  [ 600/2502]  eta: 0:18:29  lr: 0.000002  min_lr: 0.000000  loss: 1.4805 (1.5096)  class_acc: 0.8594 (0.8725)  loss_scale: 65536.0000 (78621.3910)  weight_decay: 0.0500 (0.0500)  time: 0.5620  data: 0.0002  max mem: 27806
Epoch: [29]  [ 700/2502]  eta: 0:17:25  lr: 0.000002  min_lr: 0.000000  loss: 1.5430 (1.5063)  class_acc: 0.8594 (0.8734)  loss_scale: 65536.0000 (76754.7161)  weight_decay: 0.0500 (0.0500)  time: 0.5617  data: 0.0003  max mem: 27806
Epoch: [29]  [ 800/2502]  eta: 0:16:23  lr: 0.000002  min_lr: 0.000000  loss: 1.4697 (1.5082)  class_acc: 0.8750 (0.8728)  loss_scale: 65536.0000 (75354.1273)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0002  max mem: 27806
Epoch: [29]  [ 900/2502]  eta: 0:15:22  lr: 0.000002  min_lr: 0.000000  loss: 1.4580 (1.5090)  class_acc: 0.8750 (0.8724)  loss_scale: 131072.0000 (75864.6482)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0003  max mem: 27806
Epoch: [29]  [1000/2502]  eta: 0:14:23  lr: 0.000002  min_lr: 0.000000  loss: 1.4375 (1.5112)  class_acc: 0.8906 (0.8717)  loss_scale: 65536.0000 (77582.5774)  weight_decay: 0.0500 (0.0500)  time: 0.5616  data: 0.0002  max mem: 27806
Epoch: [29]  [1100/2502]  eta: 0:13:23  lr: 0.000001  min_lr: 0.000000  loss: 1.5059 (1.5109)  class_acc: 0.8750 (0.8716)  loss_scale: 32768.0000 (73928.8937)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.0002  max mem: 27806
Epoch: [29]  [1200/2502]  eta: 0:12:25  lr: 0.000001  min_lr: 0.000000  loss: 1.4531 (1.5099)  class_acc: 0.8750 (0.8714)  loss_scale: 32768.0000 (70501.6753)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0003  max mem: 27806
Epoch: [29]  [1300/2502]  eta: 0:11:26  lr: 0.000001  min_lr: 0.000000  loss: 1.5127 (1.5120)  class_acc: 0.8594 (0.8705)  loss_scale: 32768.0000 (67601.3159)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0002  max mem: 27806
Epoch: [29]  [1400/2502]  eta: 0:10:29  lr: 0.000001  min_lr: 0.000000  loss: 1.5215 (1.5114)  class_acc: 0.8750 (0.8707)  loss_scale: 32768.0000 (65114.9979)  weight_decay: 0.0500 (0.0500)  time: 0.5604  data: 0.0003  max mem: 27806
Epoch: [29]  [1500/2502]  eta: 0:09:31  lr: 0.000001  min_lr: 0.000000  loss: 1.5156 (1.5119)  class_acc: 0.8594 (0.8707)  loss_scale: 32768.0000 (62959.9680)  weight_decay: 0.0500 (0.0500)  time: 0.5612  data: 0.0003  max mem: 27806
Epoch: [29]  [1600/2502]  eta: 0:08:33  lr: 0.000001  min_lr: 0.000000  loss: 1.5049 (1.5123)  class_acc: 0.8594 (0.8707)  loss_scale: 65536.0000 (62506.8532)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.0002  max mem: 27806
Epoch: [29]  [1700/2502]  eta: 0:07:36  lr: 0.000001  min_lr: 0.000000  loss: 1.4668 (1.5113)  class_acc: 0.8750 (0.8708)  loss_scale: 32768.0000 (62261.1264)  weight_decay: 0.0500 (0.0500)  time: 0.5614  data: 0.0003  max mem: 27806
Epoch: [29]  [1800/2502]  eta: 0:06:39  lr: 0.000001  min_lr: 0.000000  loss: 1.5479 (1.5113)  class_acc: 0.8438 (0.8706)  loss_scale: 32768.0000 (60623.5292)  weight_decay: 0.0500 (0.0500)  time: 0.5616  data: 0.0003  max mem: 27806
Epoch: [29]  [1900/2502]  eta: 0:05:42  lr: 0.000001  min_lr: 0.000000  loss: 1.4688 (1.5108)  class_acc: 0.8594 (0.8706)  loss_scale: 32768.0000 (59158.2199)  weight_decay: 0.0500 (0.0500)  time: 0.5611  data: 0.0002  max mem: 27806
Epoch: [29]  [2000/2502]  eta: 0:04:45  lr: 0.000001  min_lr: 0.000000  loss: 1.4990 (1.5102)  class_acc: 0.8750 (0.8706)  loss_scale: 32768.0000 (57839.3683)  weight_decay: 0.0500 (0.0500)  time: 0.5610  data: 0.0002  max mem: 27806
Epoch: [29]  [2100/2502]  eta: 0:03:48  lr: 0.000001  min_lr: 0.000000  loss: 1.5410 (1.5095)  class_acc: 0.8594 (0.8708)  loss_scale: 32768.0000 (56646.0619)  weight_decay: 0.0500 (0.0500)  time: 0.5606  data: 0.0002  max mem: 27806
Epoch: [29]  [2200/2502]  eta: 0:02:51  lr: 0.000001  min_lr: 0.000000  loss: 1.5518 (1.5102)  class_acc: 0.8594 (0.8706)  loss_scale: 32768.0000 (55650.5152)  weight_decay: 0.0500 (0.0500)  time: 0.5616  data: 0.0003  max mem: 27806
Epoch: [29]  [2300/2502]  eta: 0:01:54  lr: 0.000001  min_lr: 0.000000  loss: 1.4424 (1.5095)  class_acc: 0.8906 (0.8707)  loss_scale: 65536.0000 (56080.1321)  weight_decay: 0.0500 (0.0500)  time: 0.5609  data: 0.0003  max mem: 27806
Epoch: [29]  [2400/2502]  eta: 0:00:57  lr: 0.000001  min_lr: 0.000000  loss: 1.4600 (1.5110)  class_acc: 0.8750 (0.8704)  loss_scale: 65536.0000 (56473.9625)  weight_decay: 0.0500 (0.0500)  time: 0.5619  data: 0.0002  max mem: 27806
Epoch: [29]  [2500/2502]  eta: 0:00:01  lr: 0.000001  min_lr: 0.000000  loss: 1.4717 (1.5106)  class_acc: 0.8750 (0.8703)  loss_scale: 65536.0000 (56832.8192)  weight_decay: 0.0500 (0.0500)  time: 0.5341  data: 0.0007  max mem: 27806
Epoch: [29]  [2501/2502]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.4717 (1.5106)  class_acc: 0.8750 (0.8703)  loss_scale: 65536.0000 (56832.8192)  weight_decay: 0.0500 (0.0500)  time: 0.5068  data: 0.0007  max mem: 27806
Epoch: [29] Total time: 0:23:37 (0.5664 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.4717 (1.5078)  class_acc: 0.8750 (0.8712)  loss_scale: 65536.0000 (56832.8192)  weight_decay: 0.0500 (0.0500)
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Test:  [ 0/98]  eta: 0:02:33  loss: 0.1664 (0.1664)  acc1: 96.8750 (96.8750)  acc5: 100.0000 (100.0000)  time: 1.5689  data: 1.3382  max mem: 27806
Test:  [97/98]  eta: 0:00:00  loss: 0.6824 (0.6316)  acc1: 87.5000 (87.5200)  acc5: 96.8750 (97.5840)  time: 0.2014  data: 0.0001  max mem: 27806
Test: Total time: 0:00:21 (0.2199 s / it)
* Acc@1 87.018 Acc@5 97.732 loss 0.634
Accuracy of the network on the 50000 test images: 87.0%
-----------------------------------------------------------
set PatchEmbed(
  (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
).requires_grad to False
set Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False).requires_grad to False
Test:  [ 0/98]  eta: 0:02:43  loss: 0.1541 (0.1541)  acc1: 98.4375 (98.4375)  acc5: 100.0000 (100.0000)  time: 1.6641  data: 1.4572  max mem: 27806
Test:  [97/98]  eta: 0:00:00  loss: 0.5727 (0.5635)  acc1: 87.5000 (88.0480)  acc5: 97.6190 (97.9360)  time: 0.2048  data: 0.0001  max mem: 27806
Test: Total time: 0:00:21 (0.2234 s / it)
* Acc@1 87.486 Acc@5 98.132 loss 0.564
EMA Accuracy of the network on the 50000 test images: 87.5%
Max accuracy: 87.97%
{"train_lr": 1.525907700868909e-06, "train_min_lr": 3.2089443153660855e-11, "train_loss": 1.507789501953125, "train_class_acc": 0.8712390625, "train_loss_scale": 56832.8192, "train_weight_decay": 0.04999999999999801, "test_loss": 0.5641131756104985, "test_acc1": 87.48600001831055, "test_acc5": 98.1320000024414, "epoch": 29, "n_parameters": 303602664}
Training time 13:12:11
